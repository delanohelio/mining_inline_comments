{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkyMTAyNTQx", "number": 12924, "title": "[BEAM-10124] Add ContextualTextIO ", "bodyText": "This PR adds ContextualTextIO. It is based on the work started in #12645.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-09-23T23:52:59Z", "url": "https://github.com/apache/beam/pull/12924", "merged": true, "mergeCommit": {"oid": "29787b38b594e29428adaf230b45f9b33e24fa66"}, "closed": true, "closedAt": "2020-10-05T17:49:09Z", "author": {"login": "rezarokni"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdL2my3gH2gAyNDkyMTAyNTQxOmJlMGJkOThlMGM0NWY4YjUyZjUwMmQwZjgyMDRmZGNiZDc5ZmI3YzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdPnhJSAFqTUwMjI4NDgzNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "be0bd98e0c45f8b52f502d0f8204fdcbd79fb7c4", "author": {"user": {"login": "abhiy13", "name": "Abhishek Yadav"}}, "url": "https://github.com/apache/beam/commit/be0bd98e0c45f8b52f502d0f8204fdcbd79fb7c4", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Add ContextualTextIO as a copy of TextIO"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07b4d173a24d683f82bafa259024ef7677193d87", "author": {"user": {"login": "abhiy13", "name": "Abhishek Yadav"}}, "url": "https://github.com/apache/beam/commit/07b4d173a24d683f82bafa259024ef7677193d87", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Change access modifiers.\n* Change access modifiers of the following function for visibility outside the package\n* modify access modifier for getEmptyMatchTreatment() and getWatchInterval() from FileIO\n* modify access modifier for getSingleFileMetadata() from FileBasedSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f61aad061c4fa0ce6c3d546e73b34801ce4f75d9", "author": {"user": {"login": "abhiy13", "name": "Abhishek Yadav"}}, "url": "https://github.com/apache/beam/commit/f61aad061c4fa0ce6c3d546e73b34801ce4f75d9", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Add ContextualTextIO Implementation\n* Modify and add additional tests for ContextualTextIO\n* Add implementation for ContextualTextIO"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "90617ce3fe22a218a633c4203bc6f0a9586d1bcd", "author": {"user": {"login": "abhiy13", "name": "Abhishek Yadav"}}, "url": "https://github.com/apache/beam/commit/90617ce3fe22a218a633c4203bc6f0a9586d1bcd", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Refactor Code and Add option for recordOffset\n* Refactored requested changes\n* Add recordOffset feild that gives the offset of a record in the file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "85d576e9907a263ca0f063b9fc93de7db918db19", "author": {"user": {"login": "abhiy13", "name": "Abhishek Yadav"}}, "url": "https://github.com/apache/beam/commit/85d576e9907a263ca0f063b9fc93de7db918db19", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Refactor requested changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e60c4fa986a87e88f98d46500bb7ae097ca2059c", "author": {"user": {"login": "abhiy13", "name": "Abhishek Yadav"}}, "url": "https://github.com/apache/beam/commit/e60c4fa986a87e88f98d46500bb7ae097ca2059c", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Refactor requested changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bbd2ca912d1e3f691e3df42c72b1831c8dc3d9f", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/4bbd2ca912d1e3f691e3df42c72b1831c8dc3d9f", "committedDate": "2020-09-24T01:07:39Z", "message": "[BEAM-10124] Change from RecordWithMetadata To Row"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "committedDate": "2020-09-24T01:48:49Z", "message": "Fixup\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "189ea70977a360ad29996146ed3152a1ddd2f9c7", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/189ea70977a360ad29996146ed3152a1ddd2f9c7", "committedDate": "2020-09-23T23:49:36Z", "message": "Update sdks/java/io/contextual-text-io/src/test/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOTest.java\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}, "afterCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2", "committedDate": "2020-09-24T01:48:49Z", "message": "Fixup\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2NDU5NzQ4", "url": "https://github.com/apache/beam/pull/12924#pullrequestreview-496459748", "createdAt": "2020-09-25T14:11:23Z", "commit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNDoxMToyM1rOHYFWew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxNTowODozNFrOHYHoYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNTU0Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    automaticModuleName: 'org.apache.beam.sdk.io.contextual-text-io',\n          \n          \n            \n                    automaticModuleName: 'org.apache.beam.sdk.io.contextualtextio',\n          \n      \n    \n    \n  \n\nBy convention we use the highest level package defined within the module", "url": "https://github.com/apache/beam/pull/12924#discussion_r495015547", "createdAt": "2020-09-25T14:11:23Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/build.gradle", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins { id 'org.apache.beam.module' }\n+applyJavaNature(\n+        automaticModuleName: 'org.apache.beam.sdk.io.contextual-text-io',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNjU0Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            }\n          \n          \n            \n            }", "url": "https://github.com/apache/beam/pull/12924#discussion_r495016543", "createdAt": "2020-09-25T14:12:51Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/build.gradle", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins { id 'org.apache.beam.module' }\n+applyJavaNature(\n+        automaticModuleName: 'org.apache.beam.sdk.io.contextual-text-io',\n+        enableChecker: false,\n+        ignoreRawtypeErrors: true)\n+\n+description = \"Apache Beam :: SDKs :: Java :: Contextual-Text-IO\"\n+ext.summary = \"Context-aware Text IO.\"\n+\n+dependencies {\n+    compile library.java.vendored_guava_26_0_jre\n+    compile library.java.protobuf_java\n+    compile project(path: \":sdks:java:core\", configuration: \"shadow\")\n+    testCompile project(path: \":sdks:java:core\", configuration: \"shadowTest\")\n+\n+    testCompile library.java.guava_testlib\n+    testCompile library.java.junit\n+    testCompile library.java.hamcrest_core\n+    testRuntimeOnly library.java.slf4j_jdk14\n+    testCompile project(path: \":runners:direct-java\", configuration: \"shadow\")\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNzIyNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n          \n          \n            \n             * <p>Prefer {@link TextIO} when not reading files with multi-line records or additional record metadata is not", "url": "https://github.com/apache/beam/pull/12924#discussion_r495017225", "createdAt": "2020-09-25T14:13:51Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxNzYzMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n          \n          \n            \n             * <p>{@link #read} returns a {@link PCollection} of {@link Row}s with schema {@link", "url": "https://github.com/apache/beam/pull/12924#discussion_r495017631", "createdAt": "2020-09-25T14:14:26Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxODQwMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n          \n          \n            \n             * ContextualTextIO.Read#withDelimiter})\n          \n          \n            \n             * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter via {@link\n          \n          \n            \n             * ContextualTextIO.Read#withDelimiter}).", "url": "https://github.com/apache/beam/pull/12924#discussion_r495018401", "createdAt": "2020-09-25T14:15:32Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAxOTU2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *      .withHasMultilineCSVRecords(true));\n          \n          \n            \n             *     .withHasMultilineCSVRecords(true));", "url": "https://github.com/apache/beam/pull/12924#discussion_r495019565", "createdAt": "2020-09-25T14:17:17Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyMDUwMw==", "bodyText": "Why would someone choose this over example 5?\nThey seem to both be watching for new files.", "url": "https://github.com/apache/beam/pull/12924#discussion_r495020503", "createdAt": "2020-09-25T14:18:43Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyMTU3Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n          \n          \n            \n             * <p>NOTE: When using the {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)}", "url": "https://github.com/apache/beam/pull/12924#discussion_r495021576", "createdAt": "2020-09-25T14:20:22Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyMjQ5Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * read from different offsets. For a large file this can result in lower performance.\n          \n          \n            \n             * read from different offsets. This limits parallelism to the number of files being read and will likely lower performance.", "url": "https://github.com/apache/beam/pull/12924#discussion_r495022496", "createdAt": "2020-09-25T14:21:41Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyNDQwOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n          \n          \n            \n                 * <p>This can be a local path (if running locally), or a remote path such as a Google Cloud Storage filename or", "url": "https://github.com/apache/beam/pull/12924#discussion_r495024408", "createdAt": "2020-09-25T14:24:35Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyNTEwOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n          \n          \n            \n                public Read withMultilineCSVRecords(Boolean withMultilineCSVRecords) {", "url": "https://github.com/apache/beam/pull/12924#discussion_r495025108", "createdAt": "2020-09-25T14:25:34Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyNzk0Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *      .setWithoutRecordNumMetadata(true));\n          \n          \n            \n             *     .setWithoutRecordNumMetadata());", "url": "https://github.com/apache/beam/pull/12924#discussion_r495027946", "createdAt": "2020-09-25T14:29:43Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAyODg2NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n          \n          \n            \n             * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n          \n          \n            \n             * introduces a shuffle step, which increases the resources used by the pipeline. <b>By default\n          \n          \n            \n             * withoutRecordNumMetadata is set to false requiring an additional grouping operation.</b>", "url": "https://github.com/apache/beam/pull/12924#discussion_r495028864", "createdAt": "2020-09-25T14:31:04Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzOTE1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n          \n          \n            \n                      input.getPipeline().apply(\"CreateSingletonPColl\", Create.of(Arrays.asList(1)));", "url": "https://github.com/apache/beam/pull/12924#discussion_r495039153", "createdAt": "2020-09-25T14:46:56Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 412}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzOTkwOQ==", "bodyText": "nit: ComputeRecordsBeforeEachRange -> ComputeNumRecordsBeforeEachRange", "url": "https://github.com/apache/beam/pull/12924#discussion_r495039909", "createdAt": "2020-09-25T14:48:07Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =\n+          singletonPcoll\n+              .apply(\n+                  \"ComputeRecordsBeforeRange\",\n+                  ParDo.of(new ComputeRecordsBeforeEachRange(rangeSizes))\n+                      .withSideInputs(rangeSizes))\n+              .apply(\"NumRecordsBeforeEachRangeAsView\", View.asMap());\n+\n+      return recordsGroupedByFileAndRange\n+          .apply(\n+              \"AssignLineNums\",\n+              ParDo.of(new AssignRecordNums(numRecordsBeforeEachRange))\n+                  .withSideInputs(numRecordsBeforeEachRange))\n+          .setRowSchema(RecordWithMetadata.getSchema());\n+    }\n+\n+    @VisibleForTesting\n+    static class AddFileNameAndRange extends DoFn<Row, KV<KV<String, Long>, Row>> {\n+      @ProcessElement\n+      public void processElement(\n+          @Element Row record, OutputReceiver<KV<KV<String, Long>, Row>> out) {\n+\n+        out.output(\n+            KV.of(\n+                KV.of(\n+                    record\n+                        .getLogicalTypeValue(RecordWithMetadata.RESOURCE_ID, ResourceId.class)\n+                        .toString(),\n+                    record.getInt64(RecordWithMetadata.RANGE_OFFSET)),\n+                record));\n+      }\n+    }\n+\n+    /**\n+     * Helper class for computing number of record in the File preceding the beginning of the Range\n+     * in this file.\n+     */\n+    @VisibleForTesting\n+    static class ComputeRecordsBeforeEachRange extends DoFn<Integer, KV<KV<String, Long>, Long>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 458}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA0MDkxNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    for (Map.Entry<KV<String, Long>, Long> entry : rangeSizesMap.entrySet()) {\n          \n          \n            \n                      sorted.put(entry.getKey(), entry.getValue());\n          \n          \n            \n                    }\n          \n          \n            \n                    sorted.putAll(rangeSizesMap);", "url": "https://github.com/apache/beam/pull/12924#discussion_r495040917", "createdAt": "2020-09-25T14:49:43Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =\n+          singletonPcoll\n+              .apply(\n+                  \"ComputeRecordsBeforeRange\",\n+                  ParDo.of(new ComputeRecordsBeforeEachRange(rangeSizes))\n+                      .withSideInputs(rangeSizes))\n+              .apply(\"NumRecordsBeforeEachRangeAsView\", View.asMap());\n+\n+      return recordsGroupedByFileAndRange\n+          .apply(\n+              \"AssignLineNums\",\n+              ParDo.of(new AssignRecordNums(numRecordsBeforeEachRange))\n+                  .withSideInputs(numRecordsBeforeEachRange))\n+          .setRowSchema(RecordWithMetadata.getSchema());\n+    }\n+\n+    @VisibleForTesting\n+    static class AddFileNameAndRange extends DoFn<Row, KV<KV<String, Long>, Row>> {\n+      @ProcessElement\n+      public void processElement(\n+          @Element Row record, OutputReceiver<KV<KV<String, Long>, Row>> out) {\n+\n+        out.output(\n+            KV.of(\n+                KV.of(\n+                    record\n+                        .getLogicalTypeValue(RecordWithMetadata.RESOURCE_ID, ResourceId.class)\n+                        .toString(),\n+                    record.getInt64(RecordWithMetadata.RANGE_OFFSET)),\n+                record));\n+      }\n+    }\n+\n+    /**\n+     * Helper class for computing number of record in the File preceding the beginning of the Range\n+     * in this file.\n+     */\n+    @VisibleForTesting\n+    static class ComputeRecordsBeforeEachRange extends DoFn<Integer, KV<KV<String, Long>, Long>> {\n+      private final PCollectionView<Map<KV<String, Long>, Long>> rangeSizes;\n+\n+      public ComputeRecordsBeforeEachRange(\n+          PCollectionView<Map<KV<String, Long>, Long>> rangeSizes) {\n+        this.rangeSizes = rangeSizes;\n+      }\n+\n+      // Add custom comparator as KV<K, V> is not comparable by default\n+      private static class FileRangeComparator<K extends Comparable<K>, V extends Comparable<V>>\n+          implements Comparator<KV<K, V>>, Serializable {\n+        @Override\n+        public int compare(KV<K, V> a, KV<K, V> b) {\n+          if (a.getKey().compareTo(b.getKey()) == 0) {\n+            return a.getValue().compareTo(b.getValue());\n+          }\n+          return a.getKey().compareTo(b.getKey());\n+        }\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext p) {\n+        // Get the Map Containing the size from side-input\n+        Map<KV<String, Long>, Long> rangeSizesMap = p.sideInput(rangeSizes);\n+\n+        // The FileRange Pair must be sorted\n+        SortedMap<KV<String, Long>, Long> sorted = new TreeMap<>(new FileRangeComparator<>());\n+\n+        // Initialize sorted map with values\n+        for (Map.Entry<KV<String, Long>, Long> entry : rangeSizesMap.entrySet()) {\n+          sorted.put(entry.getKey(), entry.getValue());\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 489}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA0MzIwMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                              \"ComputeRecordsBeforeRange\",\n          \n          \n            \n                              \"ComputeNumRecordsBeforeRange\",", "url": "https://github.com/apache/beam/pull/12924#discussion_r495043202", "createdAt": "2020-09-25T14:53:12Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =\n+          singletonPcoll\n+              .apply(\n+                  \"ComputeRecordsBeforeRange\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 423}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1MDA1OQ==", "bodyText": "We should be using a multimap PCollectionView here of filename -> KV<offset, numRecordsInOffset>\nThis will alleviate concerns about how much of the map we need to load into memory at any given time since we will do a sort per key (filename).\nIf you have 10s of thousands of files which possibly have 1000s of offset ranges the other format will take up too much memory.", "url": "https://github.com/apache/beam/pull/12924#discussion_r495050059", "createdAt": "2020-09-25T15:03:53Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          input.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n+       */\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 420}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA1Mjg5OA==", "bodyText": "The algorithm used with these side inputs is only safe if we are using a trigger that fires once (like the default trigger). We need to ensure that the windowing strategy is compatible with our algorithm and throw an error otherwise.", "url": "https://github.com/apache/beam/pull/12924#discussion_r495052898", "createdAt": "2020-09-25T15:08:34Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -0,0 +1,640 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.contextualtextio;\n+\n+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.CompressedSource;\n+import org.apache.beam.sdk.io.Compression;\n+import org.apache.beam.sdk.io.FileBasedSource;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;\n+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@link PTransform}s that read text files and collect contextual information of the elements in\n+ * the input.\n+ *\n+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not\n+ * required.\n+ *\n+ * <h2>Reading from text files</h2>\n+ *\n+ * <p>To read a {@link PCollection} from one or more text files, use {@code\n+ * ContextualTextIO.read()}. To instantiate a transform use {@link\n+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.\n+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use\n+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.\n+ *\n+ * <p>{@link #read} returns a {@link PCollection} of {@link Row} with schema {@link\n+ * RecordWithMetadata#getSchema()}, each corresponding to one line of an input UTF-8 text file\n+ * (split into lines delimited by '\\n', '\\r', '\\r\\n', or specified delimiter see {@link\n+ * ContextualTextIO.Read#withDelimiter})\n+ *\n+ * <h3>Filepattern expansion and watching</h3>\n+ *\n+ * <p>By default, the filepatterns are expanded only once. The combination of {@link\n+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow\n+ * streaming of new files matching the filepattern(s).\n+ *\n+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}\n+ * allows them in case the filepattern contains a glob wildcard character. Use {@link\n+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link\n+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure\n+ * this behavior.\n+ *\n+ * <p>Example 1: reading a file or filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // A simple Read of a file:\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read().from(\"/local/path/to/file.txt\"));\n+ * }</pre>\n+ *\n+ * <p>Example 2: reading a PCollection of filenames.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * // E.g. the filenames might be computed from other data in the pipeline, or\n+ * // read from a data source.\n+ * PCollection<String> filenames = ...;\n+ *\n+ * // Read all files in the collection.\n+ * PCollection<Row> records =\n+ *     filenames\n+ *         .apply(FileIO.matchAll())\n+ *         .apply(FileIO.readMatches())\n+ *         .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 3: streaming new files matching a filepattern.\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*\")\n+ *     .watchForNewFiles(\n+ *       // Check for new files every minute\n+ *       Duration.standardMinutes(1),\n+ *       // Stop watching the filepattern if no new files appear within an hour\n+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));\n+ * }</pre>\n+ *\n+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may\n+ * contain line breaks.\n+ *\n+ * <p>Example of such a file could be:\n+ *\n+ * <p>\"aaa\",\"b CRLF bb\",\"ccc\" CRLF zzz,yyy,xxx\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .withHasMultilineCSVRecords(true));\n+ * }</pre>\n+ *\n+ * <p>Example 5: reading while watching for new files\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(FileIO.match()\n+ *      .filepattern(\"filepattern\")\n+ *      .continuously(\n+ *        Duration.millis(100),\n+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))\n+ *      .apply(FileIO.readMatches())\n+ *      .apply(ContextualTextIO.readFiles());\n+ * }</pre>\n+ *\n+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n+ * Objects would still contain recordNums, but these recordNums would correspond to their positions\n+ * in their respective offsets rather than their positions within the entire file).\n+ *\n+ * <pre>{@code\n+ * Pipeline p = ...;\n+ *\n+ * PCollection<Row> records = p.apply(ContextualTextIO.read()\n+ *     .from(\"/local/path/to/files/*.csv\")\n+ *      .setWithoutRecordNumMetadata(true));\n+ * }</pre>\n+ *\n+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n+ * option, a single reader will be used to process the file, rather than multiple readers which can\n+ * read from different offsets. For a large file this can result in lower performance.\n+ *\n+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n+ * example, when when only filename metadata is required. Computing record positions currently\n+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ *\n+ * <h3>Reading a very large number of files</h3>\n+ *\n+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of\n+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better\n+ * performance and scalability. Note that it may decrease performance if the filepattern matches\n+ * only a small number of files.\n+ */\n+public class ContextualTextIO {\n+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;\n+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);\n+\n+  /**\n+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link\n+   * PCollection} containing one {@link Row element} for each line in the input files.\n+   */\n+  public static Read read() {\n+    return new AutoValue_ContextualTextIO_Read.Builder()\n+        .setCompression(Compression.AUTO)\n+        .setHintMatchesManyFiles(false)\n+        .setWithoutRecordNumMetadata(false)\n+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /**\n+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link\n+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.\n+   */\n+  public static ReadFiles readFiles() {\n+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()\n+        // 64MB is a reasonable value that allows to amortize the cost of opening files,\n+        // but is not so large as to exhaust a typical runner's maximum amount of output per\n+        // ProcessElement call.\n+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)\n+        .setHasMultilineCSVRecords(false)\n+        .build();\n+  }\n+\n+  /** Implementation of {@link #read}. */\n+  @AutoValue\n+  public abstract static class Read extends PTransform<PBegin, PCollection<Row>> {\n+    abstract @Nullable ValueProvider<String> getFilepattern();\n+\n+    abstract MatchConfiguration getMatchConfiguration();\n+\n+    abstract boolean getHintMatchesManyFiles();\n+\n+    abstract boolean getWithoutRecordNumMetadata();\n+\n+    abstract Compression getCompression();\n+\n+    abstract @Nullable Boolean getHasMultilineCSVRecords();\n+\n+    @SuppressWarnings(\"mutable\") // this returns an array that can be mutated by the caller\n+    abstract byte @Nullable [] getDelimiter();\n+\n+    abstract Builder toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setFilepattern(ValueProvider<String> filepattern);\n+\n+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);\n+\n+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);\n+\n+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);\n+\n+      abstract Builder setCompression(Compression compression);\n+\n+      abstract Builder setDelimiter(byte @Nullable [] delimiter);\n+\n+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);\n+\n+      abstract Read build();\n+    }\n+\n+    /**\n+     * Reads text from the file(s) with the given filename or filename pattern.\n+     *\n+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or\n+     * filename pattern of the form {@code \"gs://<bucket>/<filepath>\"} (if running locally or using\n+     * remote execution service).\n+     *\n+     * <p>Standard <a href=\"http://docs.oracle.com/javase/tutorial/essential/io/find.html\" >Java\n+     * Filesystem glob patterns</a> (\"*\", \"?\", \"[..]\") are supported.\n+     *\n+     * <p>If it is known that the filepattern will match a very large number of files (at least tens\n+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.\n+     */\n+    public Read from(String filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return from(StaticValueProvider.of(filepattern));\n+    }\n+\n+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */\n+    public Read from(ValueProvider<String> filepattern) {\n+      checkArgument(filepattern != null, \"filepattern can not be null\");\n+      return toBuilder().setFilepattern(filepattern).build();\n+    }\n+\n+    /** Sets the {@link MatchConfiguration}. */\n+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {\n+      return toBuilder().setMatchConfiguration(matchConfiguration).build();\n+    }\n+\n+    /**\n+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.\n+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).\n+     */\n+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {\n+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();\n+    }\n+\n+    /**\n+     * Reads from input sources using the specified compression type.\n+     *\n+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.\n+     */\n+    public Read withCompression(Compression compression) {\n+      return toBuilder().setCompression(compression).build();\n+    }\n+\n+    /**\n+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of\n+     * files.\n+     *\n+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves\n+     * performance for this case, but it may worsen performance if the filepattern matches only a\n+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will\n+     * happen less efficiently within individual files).\n+     */\n+    public Read withHintMatchesManyFiles() {\n+      return toBuilder().setHintMatchesManyFiles(true).build();\n+    }\n+\n+    /**\n+     * Allows the user to opt out of getting recordNums associated with each record.\n+     *\n+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each\n+     * record, which will increase the resources used by the pipeline.\n+     *\n+     * <p>Use this when metadata like fileNames are required and their position/order can be\n+     * ignored.\n+     */\n+    public Read withoutRecordNumMetadata() {\n+      return toBuilder().setWithoutRecordNumMetadata(true).build();\n+    }\n+\n+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */\n+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {\n+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));\n+    }\n+\n+    /** Set the custom delimiter to be used in place of the default ones ('\\r', '\\n' or '\\r\\n'). */\n+    public Read withDelimiter(byte[] delimiter) {\n+      checkArgument(delimiter != null, \"delimiter can not be null\");\n+      checkArgument(!isSelfOverlapping(delimiter), \"delimiter must not self-overlap\");\n+      return toBuilder().setDelimiter(delimiter).build();\n+    }\n+\n+    static boolean isSelfOverlapping(byte[] s) {\n+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty\n+      for (int i = 1; i < s.length - 1; ++i) {\n+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PBegin input) {\n+      checkNotNull(\n+          getFilepattern(), \"need to set the filepattern of a ContextualTextIO.Read transform\");\n+      PCollection<Row> records = null;\n+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {\n+        records = input.apply(\"Read\", org.apache.beam.sdk.io.Read.from(getSource()));\n+      } else {\n+        // All other cases go through FileIO + ReadFiles\n+        records =\n+            input\n+                .apply(\n+                    \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+                .apply(\"Match All\", FileIO.matchAll().withConfiguration(getMatchConfiguration()))\n+                .apply(\n+                    \"Read Matches\",\n+                    FileIO.readMatches()\n+                        .withCompression(getCompression())\n+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))\n+                .apply(\"Via ReadFiles\", readFiles().withDelimiter(getDelimiter()));\n+      }\n+\n+      // Check if the user decided to opt out of recordNums associated with records\n+      if (getWithoutRecordNumMetadata()) {\n+        return records;\n+      }\n+\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f7ebc5a6345e2f2d80794b6e27f3eb2caac2fa2"}, "originalPosition": 396}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1aaa1e9ac9991d16eb68342d90c93b05391af34c", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/1aaa1e9ac9991d16eb68342d90c93b05391af34c", "committedDate": "2020-09-27T00:24:28Z", "message": "Update sdks/java/io/contextual-text-io/build.gradle\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8404af268fd31adbdf75849d611e38734c27f10f", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/8404af268fd31adbdf75849d611e38734c27f10f", "committedDate": "2020-09-28T04:35:53Z", "message": "Update sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "75afb71b586491629efdbc540030e7094a19dea0", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/75afb71b586491629efdbc540030e7094a19dea0", "committedDate": "2020-09-27T00:42:43Z", "message": "Update sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}, "afterCommit": {"oid": "46fc4729775d1c5fe6232010a78151f2afc4e6f3", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/46fc4729775d1c5fe6232010a78151f2afc4e6f3", "committedDate": "2020-09-28T04:40:07Z", "message": "[BEAM-10124] Change default for RecordNum to be false.\nAdd Check for trigger\nChange module location"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNTkzNjYw", "url": "https://github.com/apache/beam/pull/12924#pullrequestreview-500593660", "createdAt": "2020-10-01T18:02:33Z", "commit": {"oid": "46fc4729775d1c5fe6232010a78151f2afc4e6f3"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxODowMjozM1rOHbVbIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxODowNTo0M1rOHbVhJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyNDYwOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * record positions currently introduces a shuffle step, which increases the resources used by the\n          \n          \n            \n             * record positions currently introduces a grouping step, which increases the resources used by the", "url": "https://github.com/apache/beam/pull/12924#discussion_r498424609", "createdAt": "2020-10-01T18:02:33Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -167,26 +177,26 @@\n  *      .apply(ContextualTextIO.readFiles());\n  * }</pre>\n  *\n- * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the\n- * Objects would still contain recordNums, but these recordNums would correspond to their positions\n- * in their respective offsets rather than their positions within the entire file).\n+ * <p>Example 6: reading with recordNum metadata. (the Objects still contain recordNums, but these\n+ * recordNums would correspond to their positions in their respective offsets rather than their\n+ * positions within the entire file).\n  *\n  * <pre>{@code\n  * Pipeline p = ...;\n  *\n  * PCollection<Row> records = p.apply(ContextualTextIO.read()\n  *     .from(\"/local/path/to/files/*.csv\")\n- *      .setWithoutRecordNumMetadata(true));\n+ *      .setWithRecordNumMetadata(true));\n  * }</pre>\n  *\n  * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this\n  * option, a single reader will be used to process the file, rather than multiple readers which can\n  * read from different offsets. For a large file this can result in lower performance.\n  *\n- * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for\n- * example, when when only filename metadata is required. Computing record positions currently\n- * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default\n- * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>\n+ * <p>NOTE: Use {@link Read#withRecordNumMetadata()} when recordNum metadata is required. Computing\n+ * record positions currently introduces a shuffle step, which increases the resources used by the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46fc4729775d1c5fe6232010a78151f2afc4e6f3"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyNTU2OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          \"getWithRecordNumMetadata only support the default trigger not. %s\", currentTrigger));\n          \n          \n            \n                          \"getWithRecordNumMetadata(true) only supports the default trigger not: %s\", currentTrigger));", "url": "https://github.com/apache/beam/pull/12924#discussion_r498425569", "createdAt": "2020-10-01T18:04:32Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -637,4 +619,68 @@ private CreateTextSourceFn(byte[] delimiter, boolean hasMultilineCSVRecords) {\n \n   /** Disable construction of utility class. */\n   private ContextualTextIO() {}\n+\n+  private static class ProcessRecordNumbers extends PTransform<PCollection<Row>, PCollection<Row>> {\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<Row> records) {\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      // This algorithm only works with triggers that fire once, for now only default trigger is\n+      // supported.\n+      Trigger currentTrigger = records.getWindowingStrategy().getTrigger();\n+\n+      Set<Trigger> allowedTriggers =\n+          ImmutableSet.of(\n+              Repeatedly.forever(AfterWatermark.pastEndOfWindow()), DefaultTrigger.of());\n+\n+      Preconditions.checkArgument(\n+          allowedTriggers.contains(currentTrigger),\n+          String.format(\n+              \"getWithRecordNumMetadata only support the default trigger not. %s\", currentTrigger));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46fc4729775d1c5fe6232010a78151f2afc4e6f3"}, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQyNjE1MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset\n          \n          \n            \n                   * After computing the number of lines before each range, we can find the line number in original file as numLinesBeforeOffset + lineNumInCurrentOffset", "url": "https://github.com/apache/beam/pull/12924#discussion_r498426150", "createdAt": "2020-10-01T18:05:43Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java", "diffHunk": "@@ -637,4 +619,68 @@ private CreateTextSourceFn(byte[] delimiter, boolean hasMultilineCSVRecords) {\n \n   /** Disable construction of utility class. */\n   private ContextualTextIO() {}\n+\n+  private static class ProcessRecordNumbers extends PTransform<PCollection<Row>, PCollection<Row>> {\n+\n+    @Override\n+    public PCollection<Row> expand(PCollection<Row> records) {\n+      /*\n+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.\n+       *\n+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.\n+       */\n+\n+      // This algorithm only works with triggers that fire once, for now only default trigger is\n+      // supported.\n+      Trigger currentTrigger = records.getWindowingStrategy().getTrigger();\n+\n+      Set<Trigger> allowedTriggers =\n+          ImmutableSet.of(\n+              Repeatedly.forever(AfterWatermark.pastEndOfWindow()), DefaultTrigger.of());\n+\n+      Preconditions.checkArgument(\n+          allowedTriggers.contains(currentTrigger),\n+          String.format(\n+              \"getWithRecordNumMetadata only support the default trigger not. %s\", currentTrigger));\n+\n+      PCollection<KV<KV<String, Long>, Row>> recordsGroupedByFileAndRange =\n+          records\n+              .apply(\"AddFileNameAndRange\", ParDo.of(new AddFileNameAndRange()))\n+              .setCoder(\n+                  KvCoder.of(\n+                      KvCoder.of(StringUtf8Coder.of(), BigEndianLongCoder.of()),\n+                      RowCoder.of(RecordWithMetadata.getSchema())));\n+\n+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =\n+          recordsGroupedByFileAndRange\n+              .apply(\"CountRecordsForEachFileRange\", Count.perKey())\n+              .apply(\"SizesAsView\", View.asMap());\n+\n+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines\n+      // before each Range\n+      PCollection<Integer> singletonPcoll =\n+          records.getPipeline().apply(\"CreateSingletonPcoll\", Create.of(Arrays.asList(1)));\n+\n+      /*\n+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file\n+       *\n+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46fc4729775d1c5fe6232010a78151f2afc4e6f3"}, "originalPosition": 327}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35d8567fd60058365122d0c4da925de765293134", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/35d8567fd60058365122d0c4da925de765293134", "committedDate": "2020-10-05T04:24:34Z", "message": "[BEAM-10124] Change default for RecordNum to be false.\nAdd Check for trigger\nChange module location\nChange to MultiMapView"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ae7e6ad302ccd118b78941c33fef403c74877a95", "author": {"user": {"login": "rezarokni", "name": "Reza Rokni"}}, "url": "https://github.com/apache/beam/commit/ae7e6ad302ccd118b78941c33fef403c74877a95", "committedDate": "2020-10-02T02:35:14Z", "message": "Update sdks/java/io/contextualtextio/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java\n\nCo-authored-by: Lukasz Cwik <lcwik@google.com>"}, "afterCommit": {"oid": "35d8567fd60058365122d0c4da925de765293134", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/35d8567fd60058365122d0c4da925de765293134", "committedDate": "2020-10-05T04:24:34Z", "message": "[BEAM-10124] Change default for RecordNum to be false.\nAdd Check for trigger\nChange module location\nChange to MultiMapView"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyMjg0ODM3", "url": "https://github.com/apache/beam/pull/12924#pullrequestreview-502284837", "createdAt": "2020-10-05T17:48:36Z", "commit": {"oid": "35d8567fd60058365122d0c4da925de765293134"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2527, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}