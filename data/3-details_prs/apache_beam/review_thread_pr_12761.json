{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc4MTM0NzI1", "number": 12761, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxOTozNDoyMFrOEgIP1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxOTozNDoyMFrOEgIP1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMTI1MDEzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery_file_loads.py", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxOTozNDoyMFrOHM0wBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxOTo1NzowN1rOHM1arw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIwOTIyMw==", "bodyText": "May be update the seed to _bq_uuid() function instead of concatenating two UUIDs ?\nAlso, I'm curious why the existing seed didn't work for this user.", "url": "https://github.com/apache/beam/pull/12761#discussion_r483209223", "createdAt": "2020-09-03T19:34:20Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery_file_loads.py", "diffHunk": "@@ -356,8 +359,13 @@ def process(self, element, job_name_prefix=None):\n       copy_from_reference.projectId = vp.RuntimeValueProvider.get_value(\n           'project', str, '')\n \n-    copy_job_name = '%s_%s' % (\n+    copy_job_name = '%s_%s_%s' % (\n         job_name_prefix,\n+        _bq_uuid(\n+            '%s:%s.%s' % (", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "87299757455e0c65b1d54232feba0016020de0c8"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIxNjY4Mw==", "bodyText": "I've made the change, and added a smallmodification to a test that reproduces this issue.", "url": "https://github.com/apache/beam/pull/12761#discussion_r483216683", "createdAt": "2020-09-03T19:49:48Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery_file_loads.py", "diffHunk": "@@ -356,8 +359,13 @@ def process(self, element, job_name_prefix=None):\n       copy_from_reference.projectId = vp.RuntimeValueProvider.get_value(\n           'project', str, '')\n \n-    copy_job_name = '%s_%s' % (\n+    copy_job_name = '%s_%s_%s' % (\n         job_name_prefix,\n+        _bq_uuid(\n+            '%s:%s.%s' % (", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIwOTIyMw=="}, "originalCommit": {"oid": "87299757455e0c65b1d54232feba0016020de0c8"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIyMDE0Mw==", "bodyText": "Addressing the q about why this did not work - the BQ job names are generated by joining a prefix with the hashof a table name. COPY jobs are necessary when we write to multiple temporary tables, and copy to a single destination table.\nBecause we were hashing the destination table name, ALL the copy jobs had the same destination, so they had the same hash, and thus the same job name - and this would cause a failure.", "url": "https://github.com/apache/beam/pull/12761#discussion_r483220143", "createdAt": "2020-09-03T19:57:07Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery_file_loads.py", "diffHunk": "@@ -356,8 +359,13 @@ def process(self, element, job_name_prefix=None):\n       copy_from_reference.projectId = vp.RuntimeValueProvider.get_value(\n           'project', str, '')\n \n-    copy_job_name = '%s_%s' % (\n+    copy_job_name = '%s_%s_%s' % (\n         job_name_prefix,\n+        _bq_uuid(\n+            '%s:%s.%s' % (", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIwOTIyMw=="}, "originalCommit": {"oid": "87299757455e0c65b1d54232feba0016020de0c8"}, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 447, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}