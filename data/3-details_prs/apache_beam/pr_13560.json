{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQwODE4NDk1", "number": 13560, "title": "Dataframe docs", "bodyText": "This is a new page of documentation for the DataFrames API. The code snippets are included inline. In a later commit, I'll create a snippet and a test for the wordcount example, but this should work for now.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-12-16T01:02:36Z", "url": "https://github.com/apache/beam/pull/13560", "merged": true, "mergeCommit": {"oid": "019db838942811b33b8492bb07a45816469ded3d"}, "closed": true, "closedAt": "2020-12-18T00:58:56Z", "author": {"login": "pcoet"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdmkHVWgH2gAyNTQwODE4NDk1OjBlNmU2Zjg2ZTNhODJjM2IyNDYyZWRkZDhmYzc2NzkwODJhNWNiOGQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdnNAI8AH2gAyNTQwODE4NDk1Ojc3ZDdhN2Q2M2NhYzQwNjMwNzcxNjdkODdhOGU1YmY2NDU3NjAzZjQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "0e6e6f86e3a82c3b2462eddd8fc7679082a5cb8d", "author": {"user": {"login": "pcoet", "name": "David Huntsperger"}}, "url": "https://github.com/apache/beam/commit/0e6e6f86e3a82c3b2462eddd8fc7679082a5cb8d", "committedDate": "2020-12-16T00:50:57Z", "message": "Created new page of docs for DataFrames DSL"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4f1d80f23613f58269a8edce65863eeb7117cd5", "author": {"user": {"login": "pcoet", "name": "David Huntsperger"}}, "url": "https://github.com/apache/beam/commit/e4f1d80f23613f58269a8edce65863eeb7117cd5", "committedDate": "2020-12-16T00:52:26Z", "message": "added DataFrames overview page"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54", "author": {"user": {"login": "pcoet", "name": "David Huntsperger"}}, "url": "https://github.com/apache/beam/commit/107ccc76f22b5bed96aa51b0c789b979aa44db54", "committedDate": "2020-12-16T00:57:02Z", "message": "Merge branch 'master' into dataframeDocs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMjk5MjUw", "url": "https://github.com/apache/beam/pull/13560#pullrequestreview-553299250", "createdAt": "2020-12-16T02:01:58Z", "commit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNzoyOToxNlrOIHQ1fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMToyOTo1M1rOIHghqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ4Njc4MQ==", "bodyText": "All of the DataframeTransform references should link to the pydoc: https://beam.apache.org/releases/pydoc/current/apache_beam.dataframe.transforms.html#apache_beam.dataframe.transforms.DataframeTransform", "url": "https://github.com/apache/beam/pull/13560#discussion_r544486781", "createdAt": "2020-12-16T17:29:16Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to `DataframeTransform`:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDczODcwMg==", "bodyText": "This should do the same aggregation as in the first example (sorry this is my faultt, I've been changing around these examples).\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              | beam.Select(trip_distance=lambda line: float(..),\n          \n          \n            \n                            passenger_count=lambda line: int(..))\n          \n          \n            \n              | DataframeTransform(lambda df: df[['passenger_count', 'trip_distance']].groupby('passenger_count').sum())\n          \n          \n            \n              | beam.Map(lambda row: f\"{row.passenger_count}: {row.trip_distance}\")\n          \n          \n            \n              | beam.Select(DOLocationID=lambda line: int(..),\n          \n          \n            \n                            passenger_count=lambda line: int(..))\n          \n          \n            \n              | DataframeTransform(lambda df: df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum())\n          \n          \n            \n              | beam.Map(lambda row: f\"{row.DOLocationID}: {row.passenger_count}\")", "url": "https://github.com/apache/beam/pull/13560#discussion_r544738702", "createdAt": "2020-12-17T01:15:50Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to `DataframeTransform`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(trip_distance=lambda line: float(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'trip_distance']].groupby('passenger_count').sum())\n+  | beam.Map(lambda row: f\"{row.passenger_count}: {row.trip_distance}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDczOTgzNw==", "bodyText": "\"DataframeTransform\" and \"SqlTransform\" should link to the javadoc/pydoc\nDataframeTransform: https://beam.apache.org/releases/pydoc/current/apache_beam.dataframe.transforms.html#apache_beam.dataframe.transforms.DataframeTransform\nSqlTransform (not sure how to handle the fact that we have two links here, maybe just use the Python one?)\nJava: https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/extensions/sql/SqlTransform.html\nPython: https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.sql.html#apache_beam.transforms.sql.SqlTransform", "url": "https://github.com/apache/beam/pull/13560#discussion_r544739837", "createdAt": "2020-12-17T01:18:54Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to `DataframeTransform`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(trip_distance=lambda line: float(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'trip_distance']].groupby('passenger_count').sum())\n+  | beam.Map(lambda row: f\"{row.passenger_count}: {row.trip_distance}\")\n+  ...\n+{{< /highlight >}}\n+\n+`DataframeTransform` is similar to `SqlTransform` from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0MDY4MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The Beam DataFrames API is deferred, like the rest of the Beam API. As a result, there are some limitations on what you can do with Beam DataFrames, compared to the standard Pandas implementation:\n          \n          \n            \n            The Beam DataFrame API is deferred, like the rest of the Beam API. As a result, there are some limitations on what you can do with Beam DataFrames, compared to the standard Pandas implementation:\n          \n      \n    \n    \n  \n\nElsewhere we have \"the Beam DataFrame API\" or \"Beam DataFrames\" (referencing the objects) but not \"Beam DataFrames API\"", "url": "https://github.com/apache/beam/pull/13560#discussion_r544740681", "createdAt": "2020-12-17T01:21:26Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to `DataframeTransform`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(trip_distance=lambda line: float(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'trip_distance']].groupby('passenger_count').sum())\n+  | beam.Map(lambda row: f\"{row.passenger_count}: {row.trip_distance}\")\n+  ...\n+{{< /highlight >}}\n+\n+`DataframeTransform` is similar to `SqlTransform` from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.\n+\n+`DataframeTransform` can accept and return multiple PCollections by name and by keyword, as shown in the following examples:\n+\n+{{< highlight py >}}\n+output = (pc1, pc2) | DataframeTransform(lambda df1, df2: ...)\n+\n+output = {'a': pc, ...} | DataframeTransform(lambda a, ...: ...)\n+\n+pc1, pc2 = {'a': pc} | DataframeTransform(lambda a: expr1, expr2)\n+\n+{...} = {a: pc} | DataframeTransform(lambda a: {...})\n+{{< /highlight >}}\n+\n+## Differences from standard Pandas {#differences_from_standard_pandas}\n+\n+The Beam DataFrames API is deferred, like the rest of the Beam API. As a result, there are some limitations on what you can do with Beam DataFrames, compared to the standard Pandas implementation:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0MTgxMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * **WontImplementError**: Indicates that this is an operation or argument that isn\u2019t supported because it\u2019s incompatible with the Beam model. The largest class of operations that raise this error are operations that are order sensitive, such as [shift](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html), [cummax](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummax.html), [cummin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummin.html), [head](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html), and [tail](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html#pandas.DataFrame.tail). These cannot be trivially mapped to Beam because PCollections, representing distributed datasets, are unordered.\n          \n          \n            \n            * **WontImplementError**: Indicates that this is an operation or argument that we do not intend to support because of one of the limitations listed above.\n          \n      \n    \n    \n  \n\nI think the detail about order-sensitive operations is good, but it should be moved up to the \"unordered\" limitation above.", "url": "https://github.com/apache/beam/pull/13560#discussion_r544741813", "createdAt": "2020-12-17T01:24:20Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to `DataframeTransform`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(trip_distance=lambda line: float(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'trip_distance']].groupby('passenger_count').sum())\n+  | beam.Map(lambda row: f\"{row.passenger_count}: {row.trip_distance}\")\n+  ...\n+{{< /highlight >}}\n+\n+`DataframeTransform` is similar to `SqlTransform` from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.\n+\n+`DataframeTransform` can accept and return multiple PCollections by name and by keyword, as shown in the following examples:\n+\n+{{< highlight py >}}\n+output = (pc1, pc2) | DataframeTransform(lambda df1, df2: ...)\n+\n+output = {'a': pc, ...} | DataframeTransform(lambda a, ...: ...)\n+\n+pc1, pc2 = {'a': pc} | DataframeTransform(lambda a: expr1, expr2)\n+\n+{...} = {a: pc} | DataframeTransform(lambda a: {...})\n+{{< /highlight >}}\n+\n+## Differences from standard Pandas {#differences_from_standard_pandas}\n+\n+The Beam DataFrames API is deferred, like the rest of the Beam API. As a result, there are some limitations on what you can do with Beam DataFrames, compared to the standard Pandas implementation:\n+\n+* Because all operations are deferred, the result of a given operation may not be available for control flow. For example, you can compute a sum, but you can't branch on the result.\n+* Result columns must be computable without access to the data. For example, you can\u2019t use [transpose](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html).\n+* PCollections in Beam are inherently unordered, so Pandas operations that are sensitive to the ordering of rows are unsupported.\n+\n+With Beam DataFrames, computation doesn\u2019t take place until the pipeline runs. Before that, only the shape or schema of the result is known, meaning that you can work with the names and types of the columns, but not the result data itself.\n+\n+There are a few common exceptions you may see when attempting to use certain Pandas operations:\n+\n+* **WontImplementError**: Indicates that this is an operation or argument that isn\u2019t supported because it\u2019s incompatible with the Beam model. The largest class of operations that raise this error are operations that are order sensitive, such as [shift](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html), [cummax](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummax.html), [cummin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummin.html), [head](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html), and [tail](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html#pandas.DataFrame.tail). These cannot be trivially mapped to Beam because PCollections, representing distributed datasets, are unordered.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0MjI2MQ==", "bodyText": "This may be better inlined:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * **NonParallelOperation**: Indicates that you\u2019re attempting a non-parallel operation outside of an `allow_non_parallel_operations` block. Some operations don't lend themselves to parallel computation. They can still be used, but must be guarded:\n          \n          \n            \n            ```py\n          \n          \n            \n            with beam.dataframe.allow_non_parallel_operations(True):\n          \n          \n            \n              ...\n          \n          \n            \n            ```\n          \n          \n            \n            * **NonParallelOperation**: Indicates that you\u2019re attempting a non-parallel operation outside of an `allow_non_parallel_operations` block. Some operations don't lend themselves to parallel computation. They can still be used, but must be guarded in a `with beam.dataframe.allow_non_parallel_operations(True)` block.", "url": "https://github.com/apache/beam/pull/13560#discussion_r544742261", "createdAt": "2020-12-17T01:25:44Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to `DataframeTransform`:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(trip_distance=lambda line: float(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'trip_distance']].groupby('passenger_count').sum())\n+  | beam.Map(lambda row: f\"{row.passenger_count}: {row.trip_distance}\")\n+  ...\n+{{< /highlight >}}\n+\n+`DataframeTransform` is similar to `SqlTransform` from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.\n+\n+`DataframeTransform` can accept and return multiple PCollections by name and by keyword, as shown in the following examples:\n+\n+{{< highlight py >}}\n+output = (pc1, pc2) | DataframeTransform(lambda df1, df2: ...)\n+\n+output = {'a': pc, ...} | DataframeTransform(lambda a, ...: ...)\n+\n+pc1, pc2 = {'a': pc} | DataframeTransform(lambda a: expr1, expr2)\n+\n+{...} = {a: pc} | DataframeTransform(lambda a: {...})\n+{{< /highlight >}}\n+\n+## Differences from standard Pandas {#differences_from_standard_pandas}\n+\n+The Beam DataFrames API is deferred, like the rest of the Beam API. As a result, there are some limitations on what you can do with Beam DataFrames, compared to the standard Pandas implementation:\n+\n+* Because all operations are deferred, the result of a given operation may not be available for control flow. For example, you can compute a sum, but you can't branch on the result.\n+* Result columns must be computable without access to the data. For example, you can\u2019t use [transpose](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html).\n+* PCollections in Beam are inherently unordered, so Pandas operations that are sensitive to the ordering of rows are unsupported.\n+\n+With Beam DataFrames, computation doesn\u2019t take place until the pipeline runs. Before that, only the shape or schema of the result is known, meaning that you can work with the names and types of the columns, but not the result data itself.\n+\n+There are a few common exceptions you may see when attempting to use certain Pandas operations:\n+\n+* **WontImplementError**: Indicates that this is an operation or argument that isn\u2019t supported because it\u2019s incompatible with the Beam model. The largest class of operations that raise this error are operations that are order sensitive, such as [shift](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html), [cummax](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummax.html), [cummin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cummin.html), [head](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html), and [tail](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html#pandas.DataFrame.tail). These cannot be trivially mapped to Beam because PCollections, representing distributed datasets, are unordered.\n+* **NotImplementedError**: Indicates this is an operation or argument that hasn\u2019t been implemented yet. Many Pandas operations are already available through Beam DataFrames, but there\u2019s still a long tail of unimplemented operations.\n+* **NonParallelOperation**: Indicates that you\u2019re attempting a non-parallel operation outside of an `allow_non_parallel_operations` block. Some operations don't lend themselves to parallel computation. They can still be used, but must be guarded:\n+```py\n+with beam.dataframe.allow_non_parallel_operations(True):\n+  ...\n+```", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0Mzg0OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {{< highlight py >}}\n          \n          \n            \n            <!-- TODO(BEAM-11480): Make these examples snippets -->\n          \n          \n            \n            {{< highlight py >}}\n          \n      \n    \n    \n  \n\nPlease drop a TODO here for this action (I can take it).", "url": "https://github.com/apache/beam/pull/13560#discussion_r544743849", "createdAt": "2020-12-17T01:29:53Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+{{< highlight py >}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "107ccc76f22b5bed96aa51b0c789b979aa44db54"}, "originalPosition": 56}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cda119a85d6718634ab5fbdc716e6877af936b8", "author": {"user": {"login": "pcoet", "name": "David Huntsperger"}}, "url": "https://github.com/apache/beam/commit/7cda119a85d6718634ab5fbdc716e6877af936b8", "committedDate": "2020-12-17T14:56:05Z", "message": "minor changes in response to feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MDA2MDYz", "url": "https://github.com/apache/beam/pull/13560#pullrequestreview-555006063", "createdAt": "2020-12-17T21:34:56Z", "commit": {"oid": "7cda119a85d6718634ab5fbdc716e6877af936b8"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMTozNDo1NlrOIIJr2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMTozNToyNlrOIIJszA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQxODIwMg==", "bodyText": "nit: this can be monospaced and a link\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [DataframeTransform][pydoc_dataframe_transform] is similar to [SqlTransform][pydoc_sql_transform] from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.\n          \n          \n            \n            [`DataframeTransform`][pydoc_dataframe_transform] is similar to [`SqlTransform`][pydoc_sql_transform] from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.", "url": "https://github.com/apache/beam/pull/13560#discussion_r545418202", "createdAt": "2020-12-17T21:34:56Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+<!-- TODO(BEAM-11480): Convert these examples to snippets -->\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to [DataframeTransform][pydoc_dataframe_transform]:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(DOLocationID=lambda line: int(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum())\n+  | beam.Map(lambda row: f\"{row.DOLocationID}: {row.passenger_count}\")\n+  ...\n+{{< /highlight >}}\n+\n+[DataframeTransform][pydoc_dataframe_transform] is similar to [SqlTransform][pydoc_sql_transform] from the [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) DSL. Where `SqlTransform` translates a SQL query to a PTransform, `DataframeTransform` is a PTransform that applies a function that takes and returns DataFrames. A `DataframeTransform` can be particularly useful if you have a stand-alone function that can be called both on Beam and on ordinary Pandas DataFrames.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cda119a85d6718634ab5fbdc716e6877af936b8"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQxODQ0NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It\u2019s also possible to use the DataFrame API by passing a function to [DataframeTransform][pydoc_dataframe_transform]:\n          \n          \n            \n            It\u2019s also possible to use the DataFrame API by passing a function to [`DataframeTransform`][pydoc_dataframe_transform]:", "url": "https://github.com/apache/beam/pull/13560#discussion_r545418444", "createdAt": "2020-12-17T21:35:26Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+<!-- TODO(BEAM-11480): Convert these examples to snippets -->\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to [DataframeTransform][pydoc_dataframe_transform]:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cda119a85d6718634ab5fbdc716e6877af936b8"}, "originalPosition": 86}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fe64e14a2c71cb9d773cc9b38cdc6eb460272b4", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/7fe64e14a2c71cb9d773cc9b38cdc6eb460272b4", "committedDate": "2020-12-17T21:36:29Z", "message": "Update website/www/site/content/en/documentation/dsls/dataframes/overview.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3d4bf2a502ffbc35f1cd52ecf96e35108714339", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/c3d4bf2a502ffbc35f1cd52ecf96e35108714339", "committedDate": "2020-12-17T21:36:35Z", "message": "Update website/www/site/content/en/documentation/dsls/dataframes/overview.md"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MDc0ODMy", "url": "https://github.com/apache/beam/pull/13560#pullrequestreview-555074832", "createdAt": "2020-12-17T23:41:55Z", "commit": {"oid": "c3d4bf2a502ffbc35f1cd52ecf96e35108714339"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzo0MTo1NVrOIINUJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzo0Mzo0MVrOIINW0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NzY3MA==", "bodyText": "We've updated this to only read a sample, so this caveat can be removed.", "url": "https://github.com/apache/beam/pull/13560#discussion_r545477670", "createdAt": "2020-12-17T23:41:55Z", "author": {"login": "robertwb"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3d4bf2a502ffbc35f1cd52ecf96e35108714339"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3ODM1NA==", "bodyText": "This should just be DataframeTransform(lambda df: df.groupby('DOLocationID').sum()) as the Select narrowed it down to just those two fields already.", "url": "https://github.com/apache/beam/pull/13560#discussion_r545478354", "createdAt": "2020-12-17T23:43:41Z", "author": {"login": "robertwb"}, "path": "website/www/site/content/en/documentation/dsls/dataframes/overview.md", "diffHunk": "@@ -0,0 +1,131 @@\n+---\n+type: languages\n+title: \"Beam DataFrames: Overview\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Beam DataFrames overview\n+\n+The Apache Beam Python SDK provides a DataFrame API for working with Pandas-like [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) objects. The feature lets you convert a PCollection to a DataFrame and then interact with the DataFrame using the standard methods available on the Pandas DataFrame API. The DataFrame API is built on top of the Pandas implementation, and Pandas DataFrame methods are invoked on subsets of the datasets in parallel. The big difference between Beam DataFrames and Pandas DataFrames is that operations are deferred by the Beam API, to support the Beam parallel processing model.\n+\n+You can think of Beam DataFrames as a domain-specific language (DSL) for Beam pipelines. Similar to [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), DataFrames is a DSL built into the Beam Python SDK. Using this DSL, you can create pipelines without referencing standard Beam constructs like [ParDo](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) or [CombinePerKey](https://beam.apache.org/documentation/transforms/python/aggregation/combineperkey/).\n+\n+The Beam DataFrame API is intended to provide access to a familiar programming interface within a Beam pipeline. In some cases, the DataFrame API can also improve pipeline efficiency by deferring to the highly efficient, vectorized Pandas implementation.\n+\n+## What is a DataFrame?\n+\n+If you\u2019re new to Pandas DataFrames, you can get started by reading [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html), which shows you how to import and work with the `pandas` package. Pandas is an open-source Python library for data manipulation and analysis. It provides data structures that simplify working with relational or labeled data. One of these data structures is the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), which contains two-dimensional tabular data and provides labeled rows and columns for the data.\n+\n+## Using DataFrames\n+\n+To use Beam DataFrames, you need to install Apache Beam version 2.26.0 or higher (for complete setup instructions, see the [Apache Beam Python SDK Quickstart](https://beam.apache.org/get-started/quickstart-py/)) and Pandas version 1.0 or higher. You can use DataFrames as shown in the following example, which reads New York City taxi data from a CSV file, performs a grouped aggregation, and writes the output back to CSV. Note that this example processes a large dataset and should be run in a distributed environment:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.io import read_csv\n+\n+with beam.Pipeline() as p:\n+  df = p | read_csv(\"gs://apache-beam-samples/nyc_taxi/misc/sample.csv\")\n+  agg = df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum()\n+  agg.to_csv('output')\n+{{< /highlight >}}\n+\n+Pandas is able to infer column names from the first row of the CSV data, which is where `passenger_count` and `DOLocationID` come from.\n+\n+In this example, the only traditional Beam type is the `Pipeline` instance. Otherwise the example is written completely with the DataFrame API. This is possible because the Beam DataFrame API includes its own IO operations (for example, `read_csv` and `to_csv`) based on the Pandas native implementations. `read_*` and `to_*` operations support file patterns and any Beam-compatible file system. The grouping is accomplished with a group-by-key, and arbitrary Pandas operations (in this case, `sum`) can be applied before the final write that occurs with `to_csv`.\n+\n+The Beam DataFrame API aims to be compatible with the native Pandas implementation, with a few caveats detailed below in [Differences from standard Pandas]({{< ref \"#differences_from_standard_pandas\" >}}).\n+\n+## Embedding DataFrames in a pipeline\n+\n+To use the DataFrames API in a larger pipeline, you can convert a PCollection to a DataFrame, process the DataFrame, and then convert the DataFrame back to a PCollection. In order to convert a PCollection to a DataFrame and back, you have to use PCollections that have [schemas](https://beam.apache.org/documentation/programming-guide/#what-is-a-schema) attached. A PCollection with a schema attached is also referred to as a *schema-aware PCollection*. To learn more about attaching a schema to a PCollection, see [Creating schemas](https://beam.apache.org/documentation/programming-guide/#creating-schemas).\n+\n+Here\u2019s an example that creates a schema-aware PCollection, converts it to a DataFrame using `to_dataframe`, processes the DataFrame, and then converts the DataFrame back to a PCollection using `to_pcollection`:\n+\n+<!-- TODO(BEAM-11480): Convert these examples to snippets -->\n+{{< highlight py >}}\n+from apache_beam.dataframe.convert import to_dataframe\n+from apache_beam.dataframe.convert import to_pcollection\n+...\n+    # Read the text file[pattern] into a PCollection.\n+    lines = p | 'Read' >> ReadFromText(known_args.input)\n+\n+    words = (\n+        lines\n+        | 'Split' >> beam.FlatMap(\n+            lambda line: re.findall(r'[\\w]+', line)).with_output_types(str)\n+        # Map to Row objects to generate a schema suitable for conversion\n+        # to a dataframe.\n+        | 'ToRows' >> beam.Map(lambda word: beam.Row(word=word)))\n+\n+    df = to_dataframe(words)\n+    df['count'] = 1\n+    counted = df.groupby('word').sum()\n+    counted.to_csv(known_args.output)\n+\n+    # Deferred DataFrames can also be converted back to schema'd PCollections\n+    counted_pc = to_pcollection(counted, include_indexes=True)\n+\n+    # Do something with counted_pc\n+    ...\n+{{< /highlight >}}\n+\n+You can [see the full example on GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_dataframe.py).\n+\n+It\u2019s also possible to use the DataFrame API by passing a function to [`DataframeTransform`][pydoc_dataframe_transform]:\n+\n+{{< highlight py >}}\n+from apache_beam.dataframe.transforms import DataframeTransform\n+\n+with beam.Pipeline() as p:\n+  ...\n+  | beam.Select(DOLocationID=lambda line: int(..),\n+                passenger_count=lambda line: int(..))\n+  | DataframeTransform(lambda df: df[['passenger_count', 'DOLocationID']].groupby('DOLocationID').sum())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3d4bf2a502ffbc35f1cd52ecf96e35108714339"}, "originalPosition": 95}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77d7a7d63cac4063077167d87a8e5bf6457603f4", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/77d7a7d63cac4063077167d87a8e5bf6457603f4", "committedDate": "2020-12-18T00:29:12Z", "message": "Address review comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4446, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}