{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYwNTY3MjIz", "number": 12440, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0Mjo0OFrOEUsuZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwMDo0Nzo1M1rOEWCgAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTM5NzQ4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/doctests.py", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0Mjo0OFrOG7C2WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjozNTo0N1rOG9GcVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA==", "bodyText": "It'd be nice to track how many actually triggered this error (and perhaps track the reasons why).", "url": "https://github.com/apache/beam/pull/12440#discussion_r464565848", "createdAt": "2020-08-03T17:42:48Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NDMzMg==", "bodyText": "Yeah that would be really good. Is it possible to get at that information though?", "url": "https://github.com/apache/beam/pull/12440#discussion_r466684332", "createdAt": "2020-08-06T21:02:04Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA=="}, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4NjA0Mg==", "bodyText": "We could record it here: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/dataframe/doctests.py#L242", "url": "https://github.com/apache/beam/pull/12440#discussion_r466686042", "createdAt": "2020-08-06T21:05:34Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA=="}, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyMTg3Nw==", "bodyText": "I found that I could override report_success to collect this", "url": "https://github.com/apache/beam/pull/12440#discussion_r466721877", "createdAt": "2020-08-06T22:35:47Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NTg0OA=="}, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTQwMTgzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/doctests.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0NDowNFrOG7C4-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMToxMDo0NlrOG9EZog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NjUyMQ==", "bodyText": "Worth printing the stats regardless of failure?", "url": "https://github.com/apache/beam/pull/12440#discussion_r464566521", "createdAt": "2020-08-03T17:44:04Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4ODQxOA==", "bodyText": "I added this because I wasn't sure how to accurately report the stats in the case of failures. In theory one of the wont implement tests could fail, then skipped/wont implement/passed/failed wouldn't be a partitioning of tries.\nIf there's a way to get specific information about the wont implement test cases we could de-dup it though.", "url": "https://github.com/apache/beam/pull/12440#discussion_r466688418", "createdAt": "2020-08-06T21:10:46Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NjUyMQ=="}, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTQwNzk5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/doctests.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0NTo0NFrOG7C8ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjozOToyNFrOG9Gg8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzQ1OQ==", "bodyText": "The way this is implemented, self.tries includes the skipped ones (which get mutated to pass). I suppose we could instead remove them from the examples list altogether.", "url": "https://github.com/apache/beam/pull/12440#discussion_r464567459", "createdAt": "2020-08-03T17:45:44Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return\n+    total_test_cases = self.skipped + self.tries", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyMzA1OQ==", "bodyText": "Good catch, I'm not sure what I was thinking here.", "url": "https://github.com/apache/beam/pull/12440#discussion_r466723059", "createdAt": "2020-08-06T22:39:24Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return\n+    total_test_cases = self.skipped + self.tries", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2NzQ1OQ=="}, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTQxNzkyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/doctests.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0ODo0NlrOG7DCkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0ODo0NlrOG7DCkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2ODk3OQ==", "bodyText": "Just because they're not won't implement, doesn't mean we will (e.g .they could be skipped, or they're already implemented in which case the future tense is odd). Maybe break this down as\nXxx total test cases.\n    Xxx skipped\n    Xxx won't implement\n        Yyy reason A\n        Yyy reason B\n        ...\n    Xxx passed", "url": "https://github.com/apache/beam/pull/12440#discussion_r464568979", "createdAt": "2020-08-03T17:48:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/doctests.py", "diffHunk": "@@ -290,34 +298,60 @@ def to_callable(cond):\n     super(BeamDataframeDoctestRunner, self).__init__(\n         checker=_DeferrredDataframeOutputChecker(self._test_env, use_beam),\n         **kwargs)\n+    self.skipped = 0\n+    self.wont_implement = 0\n \n   def run(self, test, **kwargs):\n     self._checker.reset()\n-    if test.name in self._skip:\n-      for example in test.examples:\n-        if any(should_skip(example) for should_skip in self._skip[test.name]):\n-          example.source = 'pass'\n-          example.want = ''\n     for example in test.examples:\n-      if example.exc_msg is None:\n+      if any(should_skip(example)\n+             for should_skip in self._skip.get(test.name, [])):\n+        example.source = 'pass'\n+        example.want = ''\n+        self.skipped += 1\n+      elif example.exc_msg is None and any(\n+          wont_implement(example)\n+          for wont_implement in self._wont_implement.get(test.name, [])):\n         # Don't fail doctests that raise this error.\n         example.exc_msg = (\n             'apache_beam.dataframe.frame_base.WontImplementError: ...')\n+        self.wont_implement += 1\n     with self._test_env.context():\n-      return super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      result = super(BeamDataframeDoctestRunner, self).run(test, **kwargs)\n+      return result\n \n   def fake_pandas_module(self):\n     return self._test_env.fake_pandas_module()\n \n+  def summarize(self):\n+    super(BeamDataframeDoctestRunner, self).summarize()\n+    if self.failures:\n+      return\n+    total_test_cases = self.skipped + self.tries\n+    will_implement = total_test_cases - self.wont_implement\n+    print(\"%d total test cases.\" % total_test_cases)\n+    print(\n+        \"%d will implement, %d won't implement.\" %", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69b1b391628860a75b63d0dc9b5f0153af8eb017"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNTQ1MDkxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/pandas_doctests_test.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwMDo0Nzo1M1rOG9Iyag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxOTo1MDo0NFrOG9l_0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc2MDI5OA==", "bodyText": "Maybe rename this to \"wont_implement_ok\"?", "url": "https://github.com/apache/beam/pull/12440#discussion_r466760298", "createdAt": "2020-08-07T00:47:53Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/pandas_doctests_test.py", "diffHunk": "@@ -90,6 +113,16 @@ def test_series_tests(self):\n     result = doctests.testmod(\n         pd.core.series,\n         use_beam=False,\n+        report=True,\n+        wont_implement={", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41830e56916ff583bbda6b299f62ebf8888995b2"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIzODg2Ng==", "bodyText": "Done!", "url": "https://github.com/apache/beam/pull/12440#discussion_r467238866", "createdAt": "2020-08-07T19:50:44Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/pandas_doctests_test.py", "diffHunk": "@@ -90,6 +113,16 @@ def test_series_tests(self):\n     result = doctests.testmod(\n         pd.core.series,\n         use_beam=False,\n+        report=True,\n+        wont_implement={", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc2MDI5OA=="}, "originalCommit": {"oid": "41830e56916ff583bbda6b299f62ebf8888995b2"}, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 778, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}