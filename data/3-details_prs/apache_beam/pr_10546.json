{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxMTkxMDU5", "number": 10546, "title": "[BEAM-9008] Add CassandraIO readAll method", "bodyText": "Edit: This PR adds a readAll method that returns a PTransform<PCollection<Read>, PCollection> to allow for more fine grained querying of a Cassandra compatible databases (Cassandra, Scylla etc).  Since each Read allows one to specify a Query and/or RingRange, a pipeline does not have to do a full table scan to retrieve data, it can now read a subset using this pattern.\nBecause Read allows the user to specify a Set, this allows even more flexibility, in that a user can decide to put N number of RingRanges (which each could be representing individual keys) in a single Read, this allows one to linearize queries, which is very important when doing a large number of individual queries so as to not overload single shards or cores.\nOld comment for posterity, not relevant to the current redesign after working with @iemejia: This PR adds a readAll method that returns a PTransform<PColl<List<RingRange>>, PCollection<T>> to allow for parallel querying of a small subset of a Cassandra databse.  In addition, it refactors the current method read that returns a PTransform<PBegin, PColl<T>> to be a ParDo based one, so we can share code between read and readAll.\nNote: One thing I wasn't sure about was how to abstract over AutoValue classes.  I'm much more familiar with Scala than Java and haven't used AutoValue before, so I wasn't sure if there was a better way than what I did with having a getCassandraConfig on both the static Read class as well as the ReadAll class.  Don't think I can use an interface since I don't want the methods public.  Open to suggestions here.\nThanks in advance for being patient with a large change and please let me know if I can change/add anything!\nR:@iemejia\n\n\n\nLang\nSDK\nApex\nDataflow\nFlink\nGearpump\nSamza\nSpark\n\n\n\n\nGo\n\n---\n---\n\n---\n---\n\n\n\nJava\n\n\n\n\n\n\n\n\n\nPython\n\n---\n\n\n---\n---\n\n\n\nXLang\n---\n---\n---\n\n---\n---\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-01-09T22:30:05Z", "url": "https://github.com/apache/beam/pull/10546", "merged": true, "mergeCommit": {"oid": "b6c62cabfeb7108f18a73c626d3bd242299b08d4"}, "closed": true, "closedAt": "2021-09-08T15:26:27Z", "author": {"login": "vmarquez"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb4xux-ABqjI5MzY2ODcyNjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABe8YC1sAFqTc0OTI5NTY0Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fc7380a6a16a5044da0cce94138730a1f0f3975b", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/fc7380a6a16a5044da0cce94138730a1f0f3975b", "committedDate": "2020-01-09T22:27:23Z", "message": "implementing CassandraIO.readAll"}, "afterCommit": {"oid": "e6e2a82264cab1bbbd6f6a2a8229ee9f89e61d8e", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/e6e2a82264cab1bbbd6f6a2a8229ee9f89e61d8e", "committedDate": "2020-01-09T22:30:55Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5d929a2c1de95ec7a1ba183cc5b8e08a9cb483a0", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/5d929a2c1de95ec7a1ba183cc5b8e08a9cb483a0", "committedDate": "2020-01-09T22:33:35Z", "message": "minor cleanup"}, "afterCommit": {"oid": "5679a3ffe38b628a5bef85062a1d435b7bcae798", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/5679a3ffe38b628a5bef85062a1d435b7bcae798", "committedDate": "2020-01-09T22:36:04Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5679a3ffe38b628a5bef85062a1d435b7bcae798", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/5679a3ffe38b628a5bef85062a1d435b7bcae798", "committedDate": "2020-01-09T22:36:04Z", "message": "implementing CassandraIO.readAll"}, "afterCommit": {"oid": "3110e0334332eef17ae498124876486181ed5196", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3110e0334332eef17ae498124876486181ed5196", "committedDate": "2020-01-10T19:16:25Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3110e0334332eef17ae498124876486181ed5196", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3110e0334332eef17ae498124876486181ed5196", "committedDate": "2020-01-10T19:16:25Z", "message": "implementing CassandraIO.readAll"}, "afterCommit": {"oid": "d85e6b95d64664af37148c792b35ff02abfd90f5", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/d85e6b95d64664af37148c792b35ff02abfd90f5", "committedDate": "2020-01-10T20:21:45Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d85e6b95d64664af37148c792b35ff02abfd90f5", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/d85e6b95d64664af37148c792b35ff02abfd90f5", "committedDate": "2020-01-10T20:21:45Z", "message": "implementing CassandraIO.readAll"}, "afterCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe", "committedDate": "2020-01-15T00:34:23Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxNzMwMzcw", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-341730370", "createdAt": "2020-01-13T09:55:38Z", "commit": {"oid": "d85e6b95d64664af37148c792b35ff02abfd90f5"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QwOTo1NTozOFrOFcxnHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMFQxNjoxMDo1NlrOFfjNwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTcxNzI3Ng==", "bodyText": "lowercase c", "url": "https://github.com/apache/beam/pull/10546#discussion_r365717276", "createdAt": "2020-01-13T09:55:38Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraConfig.java", "diffHunk": "@@ -0,0 +1,60 @@\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Session;\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+\n+@AutoValue\n+abstract class CassandraConfig<T> implements Serializable {\n+  @Nullable\n+  abstract ValueProvider<List<String>> hosts();\n+\n+  @Nullable\n+  abstract ValueProvider<String> query();\n+\n+  @Nullable\n+  abstract ValueProvider<Integer> port();\n+\n+  @Nullable\n+  abstract ValueProvider<String> keyspace();\n+\n+  @Nullable\n+  abstract ValueProvider<String> table();\n+\n+  @Nullable\n+  abstract ValueProvider<String> username();\n+\n+  @Nullable\n+  abstract ValueProvider<String> password();\n+\n+  @Nullable\n+  abstract ValueProvider<String> localDc();\n+\n+  @Nullable\n+  abstract ValueProvider<String> consistencyLevel();\n+\n+  @Nullable\n+  abstract SerializableFunction<Session, Mapper> mapperFactoryFn();\n+\n+  @Nullable\n+  abstract Class<T> entity();\n+\n+  public static <T> CassandraConfig<T> Create(ValueProvider<List<String>> hosts,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d85e6b95d64664af37148c792b35ff02abfd90f5"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5NzkzNA==", "bodyText": "Remove System.out.println", "url": "https://github.com/apache/beam/pull/10546#discussion_r367997934", "createdAt": "2020-01-17T15:37:30Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1146,4 +918,297 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T>\n+      extends PTransform<PCollection<RingRange>, PCollection<T>> {\n+    @Nullable\n+    abstract ValueProvider<List<String>> hosts();\n+\n+    @Nullable\n+    abstract ValueProvider<String> query();\n+\n+    @Nullable\n+    abstract ValueProvider<Integer> port();\n+\n+    @Nullable\n+    abstract ValueProvider<String> keyspace();\n+\n+    @Nullable\n+    abstract ValueProvider<String> table();\n+\n+    @Nullable\n+    abstract Class<T> entity();\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    @Nullable\n+    abstract ValueProvider<String> username();\n+\n+    @Nullable\n+    abstract ValueProvider<String> password();\n+\n+    @Nullable\n+    abstract ValueProvider<String> localDc();\n+\n+    @Nullable\n+    abstract ValueProvider<String> consistencyLevel();\n+\n+    @Nullable\n+    abstract ValueProvider<Integer> splitCount();\n+\n+    @Nullable\n+    abstract SerializableFunction<Session, Mapper> mapperFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<RingRange, Integer> groupingFn();\n+\n+    abstract Builder<T> builder();\n+\n+    /** Specify the hosts of the Apache Cassandra instances. */\n+    public ReadAll<T> withHosts(List<String> hosts) {\n+      checkArgument(hosts != null, \"hosts can not be null\");\n+      checkArgument(!hosts.isEmpty(), \"hosts can not be empty\");\n+      return withHosts(ValueProvider.StaticValueProvider.of(hosts));\n+    }\n+\n+    /** Specify the hosts of the Apache Cassandra instances. */\n+    public ReadAll<T> withHosts(ValueProvider<List<String>> hosts) {\n+      return builder().setHosts(hosts).build();\n+    }\n+\n+    /** Specify the port number of the Apache Cassandra instances. */\n+    public ReadAll<T> withPort(int port) {\n+      checkArgument(port > 0, \"port must be > 0, but was: %s\", port);\n+      return withPort(ValueProvider.StaticValueProvider.of(port));\n+    }\n+\n+    /** Specify the port number of the Apache Cassandra instances. */\n+    public ReadAll<T> withPort(ValueProvider<Integer> port) {\n+      return builder().setPort(port).build();\n+    }\n+\n+    /** Specify the Cassandra keyspace where to read data. */\n+    public ReadAll<T> withKeyspace(String keyspace) {\n+      checkArgument(keyspace != null, \"keyspace can not be null\");\n+      return withKeyspace(ValueProvider.StaticValueProvider.of(keyspace));\n+    }\n+\n+    /** Specify the Cassandra keyspace where to read data. */\n+    public ReadAll<T> withKeyspace(ValueProvider<String> keyspace) {\n+      return builder().setKeyspace(keyspace).build();\n+    }\n+\n+    /** Specify the Cassandra table where to read data. */\n+    public ReadAll<T> withTable(String table) {\n+      checkArgument(table != null, \"table can not be null\");\n+      return withTable(ValueProvider.StaticValueProvider.of(table));\n+    }\n+\n+    /** Specify the Cassandra table where to read data. */\n+    public ReadAll<T> withTable(ValueProvider<String> table) {\n+      return builder().setTable(table).build();\n+    }\n+\n+    /** Specify the query to read data. */\n+    public ReadAll<T> withQuery(String query) {\n+      checkArgument(query != null && query.length() > 0, \"query cannot be null\");\n+      return withQuery(ValueProvider.StaticValueProvider.of(query));\n+    }\n+\n+    /** Specify the query to read data. */\n+    public ReadAll<T> withQuery(ValueProvider<String> query) {\n+      return builder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * Specify the entity class (annotated POJO). The {@link CassandraIO} will read the data and\n+     * convert the data as entity instances. The {@link PCollection} resulting from the read will\n+     * contains entity elements.\n+     */\n+    public ReadAll<T> withEntity(Class<T> entity) {\n+      checkArgument(entity != null, \"entity can not be null\");\n+      return builder().setEntity(entity).build();\n+    }\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {\n+      checkArgument(coder != null, \"coder can not be null\");\n+      return builder().setCoder(coder).build();\n+    }\n+\n+    /** Specify the username for authentication. */\n+    public ReadAll<T> withUsername(String username) {\n+      checkArgument(username != null, \"username can not be null\");\n+      return withUsername(ValueProvider.StaticValueProvider.of(username));\n+    }\n+\n+    /** Specify the username for authentication. */\n+    public ReadAll<T> withUsername(ValueProvider<String> username) {\n+      return builder().setUsername(username).build();\n+    }\n+\n+    /** Specify the password used for authentication. */\n+    public ReadAll<T> withPassword(String password) {\n+      checkArgument(password != null, \"password can not be null\");\n+      return withPassword(ValueProvider.StaticValueProvider.of(password));\n+    }\n+\n+    /** Specify the password used for authentication. */\n+    public ReadAll<T> withPassword(ValueProvider<String> password) {\n+      return builder().setPassword(password).build();\n+    }\n+\n+    /** Specify the local DC used for the load balancing. */\n+    public ReadAll<T> withLocalDc(String localDc) {\n+      checkArgument(localDc != null, \"localDc can not be null\");\n+      return withLocalDc(ValueProvider.StaticValueProvider.of(localDc));\n+    }\n+\n+    /** Specify the local DC used for the load balancing. */\n+    public ReadAll<T> withLocalDc(ValueProvider<String> localDc) {\n+      return builder().setLocalDc(localDc).build();\n+    }\n+\n+    public ReadAll<T> withConsistencyLevel(String consistencyLevel) {\n+      checkArgument(consistencyLevel != null, \"consistencyLevel can not be null\");\n+      return withConsistencyLevel(ValueProvider.StaticValueProvider.of(consistencyLevel));\n+    }\n+\n+    public ReadAll<T> withConsistencyLevel(ValueProvider<String> consistencyLevel) {\n+      return builder().setConsistencyLevel(consistencyLevel).build();\n+    }\n+\n+    public ReadAll<T> withGroupingFn(SerializableFunction<RingRange, Integer> groupingFunction) {\n+      return builder().setGroupingFn(groupingFunction).build();\n+    }\n+\n+    public ReadAll<T> withSplitCount(ValueProvider<Integer> splitCount) {\n+      return builder().setSplitCount(splitCount).build();\n+    }\n+\n+    public ReadAll<T> withSlitCount(Integer splitCount) {\n+      checkArgument(splitCount != null, \"splitCount can not be null\");\n+      return withSplitCount(ValueProvider.StaticValueProvider.<Integer>of(splitCount));\n+    }\n+\n+    /**\n+     * A factory to create a specific {@link Mapper} for a given Cassandra Session. This is useful\n+     * to provide mappers that don't rely in Cassandra annotated objects.\n+     */\n+    public ReadAll<T> withMapperFactoryFn(SerializableFunction<Session, Mapper> mapperFactory) {\n+      checkArgument(\n+          mapperFactory != null,\n+          \"CassandraIO.withMapperFactory\" + \"(withMapperFactory) called with null value\");\n+      return builder().setMapperFactoryFn(mapperFactory).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PCollection<RingRange> input) {\n+      checkArgument((hosts() != null && port() != null), \"WithHosts() and withPort() are required\");\n+      checkArgument(keyspace() != null, \"withKeyspace() is required\");\n+      checkArgument(table() != null, \"withTable() is required\");\n+      checkArgument(entity() != null, \"withEntity() is required\");\n+      checkArgument(coder() != null, \"withCoder() is required\");\n+      checkArgument(groupingFn() != null, \"GroupingFn OR splitCount must be set\");\n+\n+      try (Cluster cluster =\n+          getCluster(hosts(), port(), username(), password(), localDc(), consistencyLevel())) {\n+        return input\n+            .apply(\n+                \"mapping for the grouping function\",\n+                MapElements.into(\n+                        TypeDescriptors.kvs(\n+                            TypeDescriptors.integers(), TypeDescriptor.of(RingRange.class)))\n+                    .via(rr -> KV.of(groupingFn().apply(rr), rr)))\n+            .apply(\"Grouping by grouping function\", GroupByKey.create())\n+            .apply(\n+                \"mapping to key\",\n+                MapElements.into(TypeDescriptors.iterables(TypeDescriptor.of(RingRange.class)))\n+                    .via(kv -> kv.getValue()))\n+            .apply(\"ParDo\", ParDo.of(new QueryFn<>(getCassandraConfig())))\n+            .setCoder(coder());\n+      }\n+    }\n+\n+    CassandraConfig<T> getCassandraConfig() {\n+      return CassandraConfig.create(\n+          hosts(),\n+          query(),\n+          port(),\n+          keyspace(),\n+          table(),\n+          username(),\n+          password(),\n+          localDc(),\n+          consistencyLevel(),\n+          mapperFactoryFn(),\n+          entity());\n+    }\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setHosts(ValueProvider<List<String>> hosts);\n+\n+      abstract Builder<T> setQuery(ValueProvider<String> query);\n+\n+      abstract Builder<T> setPort(ValueProvider<Integer> port);\n+\n+      abstract Builder<T> setKeyspace(ValueProvider<String> keyspace);\n+\n+      abstract Builder<T> setTable(ValueProvider<String> table);\n+\n+      abstract Builder<T> setEntity(Class<T> entity);\n+\n+      abstract Optional<Class<T>> entity();\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setUsername(ValueProvider<String> username);\n+\n+      abstract Builder<T> setPassword(ValueProvider<String> password);\n+\n+      abstract Builder<T> setLocalDc(ValueProvider<String> localDc);\n+\n+      abstract Builder<T> setConsistencyLevel(ValueProvider<String> consistencyLevel);\n+\n+      abstract Builder<T> setSplitCount(ValueProvider<Integer> splitCount);\n+\n+      abstract ValueProvider<Integer> splitCount();\n+\n+      abstract Builder<T> setMapperFactoryFn(SerializableFunction<Session, Mapper> mapperFactoryFn);\n+\n+      abstract Optional<SerializableFunction<Session, Mapper>> mapperFactoryFn();\n+\n+      abstract Builder<T> setGroupingFn(SerializableFunction<RingRange, Integer> groupingFn);\n+\n+      abstract Optional<SerializableFunction<RingRange, Integer>> groupingFn();\n+\n+      abstract ReadAll<T> autoBuild();\n+\n+      public ReadAll<T> build() {\n+        if (!mapperFactoryFn().isPresent() && entity().isPresent()) {\n+          setMapperFactoryFn(new DefaultObjectMapperFactory(entity().get()));\n+        }\n+        System.out.println(this.groupingFn());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 894}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5ODUzNA==", "bodyText": "remove public and rename to ReadFn since the goal here would be just to Read", "url": "https://github.com/apache/beam/pull/10546#discussion_r367998534", "createdAt": "2020-01-17T15:38:37Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/QueryFn.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueryFn<T> extends DoFn<Iterable<RingRange>, T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5OTAxMg==", "bodyText": "Replace CassandraIO.class with the new ReadFn.class", "url": "https://github.com/apache/beam/pull/10546#discussion_r367999012", "createdAt": "2020-01-17T15:39:36Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/QueryFn.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueryFn<T> extends DoFn<Iterable<RingRange>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CassandraIO.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5OTIzMg==", "bodyText": "Remove public", "url": "https://github.com/apache/beam/pull/10546#discussion_r367999232", "createdAt": "2020-01-17T15:40:03Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/QueryFn.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueryFn<T> extends DoFn<Iterable<RingRange>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CassandraIO.class);\n+\n+  private final CassandraConfig<T> read;\n+\n+  private transient Cluster cluster;\n+\n+  private transient Session session;\n+\n+  private String partitionKey;\n+\n+  public QueryFn(CassandraConfig<T> read) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5OTUxNw==", "bodyText": "nit: remove space", "url": "https://github.com/apache/beam/pull/10546#discussion_r367999517", "createdAt": "2020-01-17T15:40:35Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/QueryFn.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueryFn<T> extends DoFn<Iterable<RingRange>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CassandraIO.class);\n+\n+  private final CassandraConfig<T> read;\n+\n+  private transient Cluster cluster;\n+\n+  private transient Session session;\n+\n+  private String partitionKey;\n+\n+  public QueryFn(CassandraConfig<T> read) {\n+    this.read = read;\n+  }\n+\n+  @Setup\n+  public void setup() {\n+    this.cluster =\n+        CassandraIO.getCluster(\n+            read.hosts(),\n+            read.port(),\n+            read.username(),\n+            read.password(),\n+            read.localDc(),\n+            read.consistencyLevel());\n+    this.session = this.cluster.connect(read.keyspace().get());\n+    this.partitionKey =\n+        cluster.getMetadata().getKeyspace(read.keyspace().get()).getTable(read.table().get())\n+            .getPartitionKey().stream()\n+            .map(ColumnMetadata::getName)\n+            .collect(Collectors.joining(\",\"));\n+  }\n+\n+  public void teardown() {\n+    this.session.close();\n+    this.cluster.close();\n+  }\n+\n+  @ProcessElement\n+  public void processElement(@Element Iterable<RingRange> tokens, OutputReceiver<T> receiver) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAwMDk5Mw==", "bodyText": "Since RingRange may end up been compared for equality too can you please add equals/hashcode too. And add a test that tests that Serializability and equality works in Beam we have the SerializableUtils.ensureSerializableRoundTrip that should make it.", "url": "https://github.com/apache/beam/pull/10546#discussion_r368000993", "createdAt": "2020-01-17T15:43:22Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -17,10 +17,11 @@\n  */\n package org.apache.beam.sdk.io.cassandra;\n \n+import java.io.Serializable;\n import java.math.BigInteger;\n \n /** Models a Cassandra token range. */\n-final class RingRange {\n+final class RingRange implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAxMTMzMg==", "bodyText": "this can be the flatted version no? DoFn<RingRange, T> or actually the new Query objectr I mention above DoFn<CassandraQuery, T>.", "url": "https://github.com/apache/beam/pull/10546#discussion_r368011332", "createdAt": "2020-01-17T16:02:31Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/QueryFn.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueryFn<T> extends DoFn<Iterable<RingRange>, T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzk5ODUzNA=="}, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAxMzY0MA==", "bodyText": "Can you move this one to QueryFn since that's the only place where it is used.", "url": "https://github.com/apache/beam/pull/10546#discussion_r368013640", "createdAt": "2020-01-17T16:07:15Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -732,6 +505,35 @@ public T getCurrent() throws NoSuchElementException {\n     DELETE\n   }\n \n+  /**\n+   * Check if the current partitioner is the Murmur3 (default in Cassandra version newer than 2).\n+   */\n+  @VisibleForTesting\n+  static boolean isMurmur3Partitioner(Cluster cluster) {\n+    return MURMUR3PARTITIONER.equals(cluster.getMetadata().getPartitioner());\n+  }\n+\n+  static String generateRangeQuery(CassandraConfig spec, String partitionKey) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 552}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAxMzc4MA==", "bodyText": "Same for this one, move to QueryFn", "url": "https://github.com/apache/beam/pull/10546#discussion_r368013780", "createdAt": "2020-01-17T16:07:32Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -732,6 +505,35 @@ public T getCurrent() throws NoSuchElementException {\n     DELETE\n   }\n \n+  /**\n+   * Check if the current partitioner is the Murmur3 (default in Cassandra version newer than 2).\n+   */\n+  @VisibleForTesting\n+  static boolean isMurmur3Partitioner(Cluster cluster) {\n+    return MURMUR3PARTITIONER.equals(cluster.getMetadata().getPartitioner());\n+  }\n+\n+  static String generateRangeQuery(CassandraConfig spec, String partitionKey) {\n+    final String rangeFilter =\n+        Joiner.on(\" AND \")\n+            .skipNulls()\n+            .join(\n+                String.format(\"(token(%s) >= ?)\", partitionKey),\n+                String.format(\"(token(%s) < ?)\", partitionKey));\n+    final String query =\n+        (spec.query() == null)\n+            ? buildQuery(spec) + \" WHERE \" + rangeFilter\n+            : buildQuery(spec) + \" AND \" + rangeFilter;\n+    LOG.debug(\"CassandraIO generated query : {}\", query);\n+    return query;\n+  }\n+\n+  private static String buildQuery(CassandraConfig spec) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYxOTE3Mg==", "bodyText": "We should divide this object into two: One that has the parameters we need to query elements and other that has just the connection part. The goal is that the query part will be sufficient to do a read and to be splitted if necessary, with this we might have a richer set of options than just with RingRange for example to have ReadAll query multiple tables. This object probably should include: query, keyspace, table, entity and RingRange.", "url": "https://github.com/apache/beam/pull/10546#discussion_r368619172", "createdAt": "2020-01-20T15:55:43Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraConfig.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Session;\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+\n+@AutoValue\n+abstract class CassandraConfig<T> implements Serializable {\n+  @Nullable\n+  abstract ValueProvider<List<String>> hosts();\n+\n+  @Nullable\n+  abstract ValueProvider<String> query();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYyMjQ0NQ==", "bodyText": "For maintenance coherence in DoFn based IOs we follow a pattern where the first part Creates the required object to query (the one I mentioned above), the second splits the query into multiples queries for each partition via a SplitFn extends DoFn and then with the result of such splits we do a read equivalent to current QueryFn. Can you refactor this segment to behave like that (with the query object I mentioned above).", "url": "https://github.com/apache/beam/pull/10546#discussion_r368622445", "createdAt": "2020-01-20T16:01:31Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -325,7 +359,58 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      try (Cluster cluster =\n+          getCluster(hosts(), port(), username(), password(), localDc(), consistencyLevel())) {\n+        if (isMurmur3Partitioner(cluster)) {\n+          LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+          List<BigInteger> tokens =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYyMzk5Ng==", "bodyText": "We don't need this one anymore I think we can remove this class now as well as: CassandraIO.getRingFraction and CassandraIOTest.testRingFraction. Can you remove these too please.", "url": "https://github.com/apache/beam/pull/10546#discussion_r368623996", "createdAt": "2020-01-20T16:04:39Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -608,121 +470,32 @@ static double getRingFraction(List<TokenRange> tokenRanges) {\n       return ringFraction;\n     }\n \n-    /**\n-     * Check if the current partitioner is the Murmur3 (default in Cassandra version newer than 2).\n-     */\n-    @VisibleForTesting\n-    static boolean isMurmur3Partitioner(Cluster cluster) {\n-      return MURMUR3PARTITIONER.equals(cluster.getMetadata().getPartitioner());\n-    }\n-\n     /** Measure distance between two tokens. */\n     @VisibleForTesting\n     static BigInteger distance(BigInteger left, BigInteger right) {\n       return (right.compareTo(left) > 0)\n           ? right.subtract(left)\n           : right.subtract(left).add(SplitGenerator.getRangeSize(MURMUR3PARTITIONER));\n     }\n+  }\n \n-    /**\n-     * Represent a token range in Cassandra instance, wrapping the partition count, size and token\n-     * range.\n-     */\n-    @VisibleForTesting\n-    static class TokenRange {\n-      private final long partitionCount;\n-      private final long meanPartitionSize;\n-      private final BigInteger rangeStart;\n-      private final BigInteger rangeEnd;\n-\n-      TokenRange(\n-          long partitionCount, long meanPartitionSize, BigInteger rangeStart, BigInteger rangeEnd) {\n-        this.partitionCount = partitionCount;\n-        this.meanPartitionSize = meanPartitionSize;\n-        this.rangeStart = rangeStart;\n-        this.rangeEnd = rangeEnd;\n-      }\n-    }\n-\n-    private class CassandraReader extends BoundedSource.BoundedReader<T> {\n-      private final CassandraIO.CassandraSource<T> source;\n-      private Cluster cluster;\n-      private Session session;\n-      private Iterator<T> iterator;\n-      private T current;\n-\n-      CassandraReader(CassandraSource<T> source) {\n-        this.source = source;\n-      }\n-\n-      @Override\n-      public boolean start() {\n-        LOG.debug(\"Starting Cassandra reader\");\n-        cluster =\n-            getCluster(\n-                source.spec.hosts(),\n-                source.spec.port(),\n-                source.spec.username(),\n-                source.spec.password(),\n-                source.spec.localDc(),\n-                source.spec.consistencyLevel());\n-        session = cluster.connect(source.spec.keyspace().get());\n-        LOG.debug(\"Queries: \" + source.splitQueries);\n-        List<ResultSetFuture> futures = new ArrayList<>();\n-        for (String query : source.splitQueries) {\n-          futures.add(session.executeAsync(query));\n-        }\n-\n-        final Mapper<T> mapper = getMapper(session, source.spec.entity());\n-\n-        for (ResultSetFuture result : futures) {\n-          if (iterator == null) {\n-            iterator = mapper.map(result.getUninterruptibly());\n-          } else {\n-            iterator = Iterators.concat(iterator, mapper.map(result.getUninterruptibly()));\n-          }\n-        }\n-\n-        return advance();\n-      }\n-\n-      @Override\n-      public boolean advance() {\n-        if (iterator.hasNext()) {\n-          current = iterator.next();\n-          return true;\n-        }\n-        current = null;\n-        return false;\n-      }\n-\n-      @Override\n-      public void close() {\n-        LOG.debug(\"Closing Cassandra reader\");\n-        if (session != null) {\n-          session.close();\n-        }\n-        if (cluster != null) {\n-          cluster.close();\n-        }\n-      }\n-\n-      @Override\n-      public T getCurrent() throws NoSuchElementException {\n-        if (current == null) {\n-          throw new NoSuchElementException();\n-        }\n-        return current;\n-      }\n-\n-      @Override\n-      public CassandraIO.CassandraSource<T> getCurrentSource() {\n-        return source;\n-      }\n-\n-      private Mapper<T> getMapper(Session session, Class<T> enitity) {\n-        return source.spec.mapperFactoryFn().apply(session);\n-      }\n+  /**\n+   * Represent a token range in Cassandra instance, wrapping the partition count, size and token\n+   * range.\n+   */\n+  @VisibleForTesting\n+  static class TokenRange {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 525}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYyNjI5OA==", "bodyText": "Actually this should probably be done not for this class but for the CassandraQuery object", "url": "https://github.com/apache/beam/pull/10546#discussion_r368626298", "createdAt": "2020-01-20T16:09:18Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -17,10 +17,11 @@\n  */\n package org.apache.beam.sdk.io.cassandra;\n \n+import java.io.Serializable;\n import java.math.BigInteger;\n \n /** Models a Cassandra token range. */\n-final class RingRange {\n+final class RingRange implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODAwMDk5Mw=="}, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYyNjUxNQ==", "bodyText": "nit: remove space", "url": "https://github.com/apache/beam/pull/10546#discussion_r368626515", "createdAt": "2020-01-20T16:09:46Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/test/java/org/apache/beam/sdk/io/cassandra/CassandraIOTest.java", "diffHunk": "@@ -312,6 +298,52 @@ public void testRead() throws Exception {\n     pipeline.run();\n   }\n \n+  @Test\n+  public void testReadAll() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYyNjY0NQ==", "bodyText": "nit: remove space", "url": "https://github.com/apache/beam/pull/10546#discussion_r368626645", "createdAt": "2020-01-20T16:10:03Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/test/java/org/apache/beam/sdk/io/cassandra/CassandraIOTest.java", "diffHunk": "@@ -387,6 +420,7 @@ public Mapper apply(Session input) {\n \n     @Override\n     public Iterator map(ResultSet resultSet) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYyNzEzNg==", "bodyText": "Once we have the SplitFn we should probably test this, actually we had a lot of historic issues with Splitting on this connector so please pay attention to ensure we don't introduce regressions.", "url": "https://github.com/apache/beam/pull/10546#discussion_r368627136", "createdAt": "2020-01-20T16:10:56Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/test/java/org/apache/beam/sdk/io/cassandra/CassandraIOTest.java", "diffHunk": "@@ -466,40 +500,9 @@ public void testCustomMapperImplDelete() {\n     assertEquals(1, counter.intValue());\n   }\n \n-  @Test\n-  public void testSplit() throws Exception {\n-    PipelineOptions options = PipelineOptionsFactory.create();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3a2f83fc21aca9eeafca4f98a993c4e2a2838fe"}, "originalPosition": 227}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "129dc2a0d4c7b9c49b2401653ef6cd64763e021a", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/129dc2a0d4c7b9c49b2401653ef6cd64763e021a", "committedDate": "2020-02-21T05:46:43Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "30fddc24c38e37932ada8e3e145ed4879e35b38b", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/30fddc24c38e37932ada8e3e145ed4879e35b38b", "committedDate": "2020-03-30T23:08:53Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "30fddc24c38e37932ada8e3e145ed4879e35b38b", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/30fddc24c38e37932ada8e3e145ed4879e35b38b", "committedDate": "2020-03-30T23:08:53Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "3f9cc6c95601c655fdc11720a18c75c78d2d78a4", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3f9cc6c95601c655fdc11720a18c75c78d2d78a4", "committedDate": "2020-03-30T23:49:22Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3f9cc6c95601c655fdc11720a18c75c78d2d78a4", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3f9cc6c95601c655fdc11720a18c75c78d2d78a4", "committedDate": "2020-03-30T23:49:22Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "committedDate": "2020-03-31T01:49:47Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "committedDate": "2020-03-31T01:49:47Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "9f622c6f7c5c0962f996aa8074dfe11ad21b9f21", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/9f622c6f7c5c0962f996aa8074dfe11ad21b9f21", "committedDate": "2020-04-01T04:43:40Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9f622c6f7c5c0962f996aa8074dfe11ad21b9f21", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/9f622c6f7c5c0962f996aa8074dfe11ad21b9f21", "committedDate": "2020-04-01T04:43:40Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "committedDate": "2020-03-31T01:49:47Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/d9dd5d0f9be24dda4fa31c4642076e519c345aa2", "committedDate": "2020-03-31T01:49:47Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "3feafb0fcbc940693cdcb29db620e181cc1af864", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3feafb0fcbc940693cdcb29db620e181cc1af864", "committedDate": "2020-04-04T01:10:18Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3feafb0fcbc940693cdcb29db620e181cc1af864", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3feafb0fcbc940693cdcb29db620e181cc1af864", "committedDate": "2020-04-04T01:10:18Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/80f3ca7d7c53ac008ea25993d7061bfa626104e4", "committedDate": "2020-04-06T18:08:58Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwMzkwMDI2", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-390390026", "createdAt": "2020-04-08T23:06:31Z", "commit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMzowNjozMVrOGDEBiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMzoyMjo1NFrOGDEWRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2NDg0MA==", "bodyText": "We should produce a PCollection<Read> here instead and this splitting logic should be moved into the SplitFn function.", "url": "https://github.com/apache/beam/pull/10546#discussion_r405864840", "createdAt": "2020-04-08T23:06:31Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -318,15 +354,84 @@ private CassandraIO() {}\n       return builder().setMapperFactoryFn(mapperFactory).build();\n     }\n \n+    public Read<T> withRingRange(RingRange ringRange) {\n+      return withRingRange(ValueProvider.StaticValueProvider.of(ringRange));\n+    }\n+\n+    public Read<T> withRingRange(ValueProvider<RingRange> ringRange) {\n+      return builder().setRingRange(ringRange).build();\n+    }\n+\n     @Override\n     public PCollection<T> expand(PBegin input) {\n       checkArgument((hosts() != null && port() != null), \"WithHosts() and withPort() are required\");\n       checkArgument(keyspace() != null, \"withKeyspace() is required\");\n       checkArgument(table() != null, \"withTable() is required\");\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n+      try (Cluster cluster =\n+          getCluster(hosts(), port(), username(), password(), localDc(), consistencyLevel())) {\n+        Integer splitCount = cluster.getMetadata().getAllHosts().size();\n+        if (minNumberOfSplits() != null && minNumberOfSplits().get() != null) {\n+          splitCount = minNumberOfSplits().get();\n+        }\n+        ReadAll<T> readAll =\n+            CassandraIO.<T>readAll()\n+                .withCoder(this.coder())\n+                .withConsistencyLevel(this.consistencyLevel())\n+                .withEntity(this.entity())\n+                .withHosts(this.hosts())\n+                .withKeyspace(this.keyspace())\n+                .withLocalDc(this.localDc())\n+                .withPort(this.port())\n+                .withPassword(this.password())\n+                .withQuery(this.query())\n+                .withTable(this.table())\n+                .withUsername(this.username())\n+                .withSplitCount(splitCount)\n+                .withMapperFactoryFn(this.mapperFactoryFn());\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+        if (isMurmur3Partitioner(cluster)) {\n+          LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+          List<BigInteger> tokens =\n+              cluster.getMetadata().getTokenRanges().stream()\n+                  .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                  .collect(Collectors.toList());\n+\n+          SplitGenerator splitGenerator =\n+              new SplitGenerator(cluster.getMetadata().getPartitioner());\n+\n+          List<Read<T>> splits =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2NTg3Mw==", "bodyText": "private static class SplitFn<T> extends DoFn<Read<T>, Read<T>>\nNotice that in the second Read you should setup the RingRange so each individual Read can then be Read by a DoFn that knows how to read Read specifications with a RingRange.", "url": "https://github.com/apache/beam/pull/10546#discussion_r405865873", "createdAt": "2020-04-08T23:09:48Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -318,15 +354,84 @@ private CassandraIO() {}\n       return builder().setMapperFactoryFn(mapperFactory).build();\n     }\n \n+    public Read<T> withRingRange(RingRange ringRange) {\n+      return withRingRange(ValueProvider.StaticValueProvider.of(ringRange));\n+    }\n+\n+    public Read<T> withRingRange(ValueProvider<RingRange> ringRange) {\n+      return builder().setRingRange(ringRange).build();\n+    }\n+\n     @Override\n     public PCollection<T> expand(PBegin input) {\n       checkArgument((hosts() != null && port() != null), \"WithHosts() and withPort() are required\");\n       checkArgument(keyspace() != null, \"withKeyspace() is required\");\n       checkArgument(table() != null, \"withTable() is required\");\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n+      try (Cluster cluster =\n+          getCluster(hosts(), port(), username(), password(), localDc(), consistencyLevel())) {\n+        Integer splitCount = cluster.getMetadata().getAllHosts().size();\n+        if (minNumberOfSplits() != null && minNumberOfSplits().get() != null) {\n+          splitCount = minNumberOfSplits().get();\n+        }\n+        ReadAll<T> readAll =\n+            CassandraIO.<T>readAll()\n+                .withCoder(this.coder())\n+                .withConsistencyLevel(this.consistencyLevel())\n+                .withEntity(this.entity())\n+                .withHosts(this.hosts())\n+                .withKeyspace(this.keyspace())\n+                .withLocalDc(this.localDc())\n+                .withPort(this.port())\n+                .withPassword(this.password())\n+                .withQuery(this.query())\n+                .withTable(this.table())\n+                .withUsername(this.username())\n+                .withSplitCount(splitCount)\n+                .withMapperFactoryFn(this.mapperFactoryFn());\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+        if (isMurmur3Partitioner(cluster)) {\n+          LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+          List<BigInteger> tokens =\n+              cluster.getMetadata().getTokenRanges().stream()\n+                  .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                  .collect(Collectors.toList());\n+\n+          SplitGenerator splitGenerator =\n+              new SplitGenerator(cluster.getMetadata().getPartitioner());\n+\n+          List<Read<T>> splits =\n+              splitGenerator.generateSplits(splitCount, tokens).stream()\n+                  .map(rr -> CassandraIO.<T>read().withRingRange(rr))\n+                  .collect(Collectors.toList());\n+\n+          return input.apply(\"Creating splits\", Create.of(splits)).apply(\"readAll\", readAll);\n+\n+        } else {\n+          LOG.warn(\n+              \"Only Murmur3Partitioner is supported for splitting, using an unique source for \"\n+                  + \"the read\");\n+          String partitioner = cluster.getMetadata().getPartitioner();\n+          RingRange totalRingRange =\n+              new RingRange(\n+                  SplitGenerator.getRangeMin(partitioner), SplitGenerator.getRangeMax(partitioner));\n+          return input\n+              .apply(Create.of(CassandraIO.<T>read().withRingRange(totalRingRange)))\n+              .apply(readAll)\n+              .setCoder(coder());\n+        }\n+      }\n+    }\n+\n+    private static class SplitFn<T> extends DoFn<List<RingRange>, Read<T>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2NjAzMA==", "bodyText": "The split logic that is above should be here.", "url": "https://github.com/apache/beam/pull/10546#discussion_r405866030", "createdAt": "2020-04-08T23:10:19Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -318,15 +354,84 @@ private CassandraIO() {}\n       return builder().setMapperFactoryFn(mapperFactory).build();\n     }\n \n+    public Read<T> withRingRange(RingRange ringRange) {\n+      return withRingRange(ValueProvider.StaticValueProvider.of(ringRange));\n+    }\n+\n+    public Read<T> withRingRange(ValueProvider<RingRange> ringRange) {\n+      return builder().setRingRange(ringRange).build();\n+    }\n+\n     @Override\n     public PCollection<T> expand(PBegin input) {\n       checkArgument((hosts() != null && port() != null), \"WithHosts() and withPort() are required\");\n       checkArgument(keyspace() != null, \"withKeyspace() is required\");\n       checkArgument(table() != null, \"withTable() is required\");\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n+      try (Cluster cluster =\n+          getCluster(hosts(), port(), username(), password(), localDc(), consistencyLevel())) {\n+        Integer splitCount = cluster.getMetadata().getAllHosts().size();\n+        if (minNumberOfSplits() != null && minNumberOfSplits().get() != null) {\n+          splitCount = minNumberOfSplits().get();\n+        }\n+        ReadAll<T> readAll =\n+            CassandraIO.<T>readAll()\n+                .withCoder(this.coder())\n+                .withConsistencyLevel(this.consistencyLevel())\n+                .withEntity(this.entity())\n+                .withHosts(this.hosts())\n+                .withKeyspace(this.keyspace())\n+                .withLocalDc(this.localDc())\n+                .withPort(this.port())\n+                .withPassword(this.password())\n+                .withQuery(this.query())\n+                .withTable(this.table())\n+                .withUsername(this.username())\n+                .withSplitCount(splitCount)\n+                .withMapperFactoryFn(this.mapperFactoryFn());\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+        if (isMurmur3Partitioner(cluster)) {\n+          LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+          List<BigInteger> tokens =\n+              cluster.getMetadata().getTokenRanges().stream()\n+                  .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                  .collect(Collectors.toList());\n+\n+          SplitGenerator splitGenerator =\n+              new SplitGenerator(cluster.getMetadata().getPartitioner());\n+\n+          List<Read<T>> splits =\n+              splitGenerator.generateSplits(splitCount, tokens).stream()\n+                  .map(rr -> CassandraIO.<T>read().withRingRange(rr))\n+                  .collect(Collectors.toList());\n+\n+          return input.apply(\"Creating splits\", Create.of(splits)).apply(\"readAll\", readAll);\n+\n+        } else {\n+          LOG.warn(\n+              \"Only Murmur3Partitioner is supported for splitting, using an unique source for \"\n+                  + \"the read\");\n+          String partitioner = cluster.getMetadata().getPartitioner();\n+          RingRange totalRingRange =\n+              new RingRange(\n+                  SplitGenerator.getRangeMin(partitioner), SplitGenerator.getRangeMax(partitioner));\n+          return input\n+              .apply(Create.of(CassandraIO.<T>read().withRingRange(totalRingRange)))\n+              .apply(readAll)\n+              .setCoder(coder());\n+        }\n+      }\n+    }\n+\n+    private static class SplitFn<T> extends DoFn<List<RingRange>, Read<T>> {\n+      @ProcessElement\n+      public void processElement(@Element List<RingRange> input, OutputReceiver<Read<T>> output) {\n+        for (RingRange rr : input) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2NzUxMQ==", "bodyText": "This transform should not have any attribute / with method, those come from the Read in the input PCollection.", "url": "https://github.com/apache/beam/pull/10546#discussion_r405867511", "createdAt": "2020-04-08T23:14:54Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +887,344 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract ValueProvider<List<String>> hosts();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "originalPosition": 663}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2OTkxMQ==", "bodyText": "The key point here is to apply for every read the split function and produce subsequent reads with RingRanges that then are Reshuffled and passed to a ParDo with the ReadFn function.", "url": "https://github.com/apache/beam/pull/10546#discussion_r405869911", "createdAt": "2020-04-08T23:22:09Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +887,344 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract ValueProvider<List<String>> hosts();\n+\n+    @Nullable\n+    abstract ValueProvider<String> query();\n+\n+    @Nullable\n+    abstract ValueProvider<Integer> port();\n+\n+    @Nullable\n+    abstract ValueProvider<String> keyspace();\n+\n+    @Nullable\n+    abstract ValueProvider<String> table();\n+\n+    @Nullable\n+    abstract Class<T> entity();\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    @Nullable\n+    abstract ValueProvider<String> username();\n+\n+    @Nullable\n+    abstract ValueProvider<String> password();\n+\n+    @Nullable\n+    abstract ValueProvider<String> localDc();\n+\n+    @Nullable\n+    abstract ValueProvider<String> consistencyLevel();\n+\n+    @Nullable\n+    abstract ValueProvider<Integer> splitCount();\n+\n+    @Nullable\n+    abstract SerializableFunction<Session, Mapper> mapperFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<RingRange, Integer> groupingFn();\n+\n+    abstract Builder<T> builder();\n+\n+    /** Specify the hosts of the Apache Cassandra instances. */\n+    public ReadAll<T> withHosts(List<String> hosts) {\n+      checkArgument(hosts != null, \"hosts can not be null\");\n+      checkArgument(!hosts.isEmpty(), \"hosts can not be empty\");\n+      return withHosts(ValueProvider.StaticValueProvider.of(hosts));\n+    }\n+\n+    /** Specify the hosts of the Apache Cassandra instances. */\n+    public ReadAll<T> withHosts(ValueProvider<List<String>> hosts) {\n+      return builder().setHosts(hosts).build();\n+    }\n+\n+    /** Specify the port number of the Apache Cassandra instances. */\n+    public ReadAll<T> withPort(int port) {\n+      checkArgument(port > 0, \"port must be > 0, but was: %s\", port);\n+      return withPort(ValueProvider.StaticValueProvider.of(port));\n+    }\n+\n+    /** Specify the port number of the Apache Cassandra instances. */\n+    public ReadAll<T> withPort(ValueProvider<Integer> port) {\n+      return builder().setPort(port).build();\n+    }\n+\n+    /** Specify the Cassandra keyspace where to read data. */\n+    public ReadAll<T> withKeyspace(String keyspace) {\n+      checkArgument(keyspace != null, \"keyspace can not be null\");\n+      return withKeyspace(ValueProvider.StaticValueProvider.of(keyspace));\n+    }\n+\n+    /** Specify the Cassandra keyspace where to read data. */\n+    public ReadAll<T> withKeyspace(ValueProvider<String> keyspace) {\n+      return builder().setKeyspace(keyspace).build();\n+    }\n+\n+    /** Specify the Cassandra table where to read data. */\n+    public ReadAll<T> withTable(String table) {\n+      checkArgument(table != null, \"table can not be null\");\n+      return withTable(ValueProvider.StaticValueProvider.of(table));\n+    }\n+\n+    /** Specify the Cassandra table where to read data. */\n+    public ReadAll<T> withTable(ValueProvider<String> table) {\n+      return builder().setTable(table).build();\n+    }\n+\n+    /** Specify the query to read data. */\n+    public ReadAll<T> withQuery(String query) {\n+      checkArgument(query != null && query.length() > 0, \"query cannot be null\");\n+      return withQuery(ValueProvider.StaticValueProvider.of(query));\n+    }\n+\n+    /** Specify the query to read data. */\n+    public ReadAll<T> withQuery(ValueProvider<String> query) {\n+      return builder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * Specify the entity class (annotated POJO). The {@link CassandraIO} will read the data and\n+     * convert the data as entity instances. The {@link PCollection} resulting from the read will\n+     * contains entity elements.\n+     */\n+    public ReadAll<T> withEntity(Class<T> entity) {\n+      checkArgument(entity != null, \"entity can not be null\");\n+      return builder().setEntity(entity).build();\n+    }\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {\n+      checkArgument(coder != null, \"coder can not be null\");\n+      return builder().setCoder(coder).build();\n+    }\n+\n+    /** Specify the username for authentication. */\n+    public ReadAll<T> withUsername(String username) {\n+      checkArgument(username != null, \"username can not be null\");\n+      return withUsername(ValueProvider.StaticValueProvider.of(username));\n+    }\n+\n+    /** Specify the username for authentication. */\n+    public ReadAll<T> withUsername(ValueProvider<String> username) {\n+      return builder().setUsername(username).build();\n+    }\n+\n+    /** Specify the password used for authentication. */\n+    public ReadAll<T> withPassword(String password) {\n+      checkArgument(password != null, \"password can not be null\");\n+      return withPassword(ValueProvider.StaticValueProvider.of(password));\n+    }\n+\n+    /** Specify the password used for authentication. */\n+    public ReadAll<T> withPassword(ValueProvider<String> password) {\n+      return builder().setPassword(password).build();\n+    }\n+\n+    /** Specify the local DC used for the load balancing. */\n+    public ReadAll<T> withLocalDc(String localDc) {\n+      checkArgument(localDc != null, \"localDc can not be null\");\n+      return withLocalDc(ValueProvider.StaticValueProvider.of(localDc));\n+    }\n+\n+    /** Specify the local DC used for the load balancing. */\n+    public ReadAll<T> withLocalDc(ValueProvider<String> localDc) {\n+      return builder().setLocalDc(localDc).build();\n+    }\n+\n+    public ReadAll<T> withConsistencyLevel(String consistencyLevel) {\n+      checkArgument(consistencyLevel != null, \"consistencyLevel can not be null\");\n+      return withConsistencyLevel(ValueProvider.StaticValueProvider.of(consistencyLevel));\n+    }\n+\n+    public ReadAll<T> withConsistencyLevel(ValueProvider<String> consistencyLevel) {\n+      return builder().setConsistencyLevel(consistencyLevel).build();\n+    }\n+\n+    public ReadAll<T> withGroupingFn(SerializableFunction<RingRange, Integer> groupingFunction) {\n+      return builder().setGroupingFn(groupingFunction).build();\n+    }\n+\n+    public ReadAll<T> withSplitCount(ValueProvider<Integer> splitCount) {\n+      return builder().setSplitCount(splitCount).build();\n+    }\n+\n+    public ReadAll<T> withSplitCount(Integer splitCount) {\n+      checkArgument(splitCount != null, \"splitCount can not be null\");\n+      return withSplitCount(ValueProvider.StaticValueProvider.<Integer>of(splitCount));\n+    }\n+\n+    /**\n+     * A factory to create a specific {@link Mapper} for a given Cassandra Session. This is useful\n+     * to provide mappers that don't rely in Cassandra annotated objects.\n+     */\n+    public ReadAll<T> withMapperFactoryFn(SerializableFunction<Session, Mapper> mapperFactory) {\n+      checkArgument(\n+          mapperFactory != null,\n+          \"CassandraIO.withMapperFactory\" + \"(withMapperFactory) called with null value\");\n+      return builder().setMapperFactoryFn(mapperFactory).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PCollection<Read<T>> input) {\n+      checkArgument((hosts() != null && port() != null), \"WithHosts() and withPort() are required\");\n+      checkArgument(keyspace() != null, \"withKeyspace() is required\");\n+      checkArgument(table() != null, \"withTable() is required\");\n+      checkArgument(entity() != null, \"withEntity() is required\");\n+      checkArgument(coder() != null, \"withCoder() is required\");\n+      checkArgument(groupingFn() != null, \"GroupingFn OR splitCount must be set\");\n+      try (Cluster cluster =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "originalPosition": 852}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg3MDE0OQ==", "bodyText": "You don't need this ReadAll ref here, Read is enough, ReadAll is the one that delegates to a normal Read.", "url": "https://github.com/apache/beam/pull/10546#discussion_r405870149", "createdAt": "2020-04-08T23:22:54Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/ReadFn.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.io.cassandra.CassandraIO.Read;\n+import org.apache.beam.sdk.io.cassandra.CassandraIO.ReadAll;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class ReadFn<T> extends DoFn<Read<T>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFn.class);\n+\n+  private transient Cluster cluster;\n+\n+  private transient Session session;\n+\n+  private final ReadAll<T> readAll;\n+\n+  public ReadFn(ReadAll<T> readAll) {\n+    this.readAll = readAll;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4"}, "originalPosition": 46}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "80f3ca7d7c53ac008ea25993d7061bfa626104e4", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/80f3ca7d7c53ac008ea25993d7061bfa626104e4", "committedDate": "2020-04-06T18:08:58Z", "message": "Refactor to allow Read to use ReadAll, and ReadAll takes Read<T>"}, "afterCommit": {"oid": "291759e176412e3818192cfc3168dcba8275d563", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/291759e176412e3818192cfc3168dcba8275d563", "committedDate": "2020-04-09T17:13:49Z", "message": "removing some unused code for readAll"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMjcyMDA1", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-402272005", "createdAt": "2020-04-28T23:03:57Z", "commit": {"oid": "335808e814aa3bc68de3cd84e08e1af5fc3704cb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMzowMzo1N1rOGNqKQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMzowMzo1N1rOGNqKQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk3NTQyNg==", "bodyText": "This looks perfect!", "url": "https://github.com/apache/beam/pull/10546#discussion_r416975426", "createdAt": "2020-04-28T23:03:57Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -391,36 +393,57 @@ static Cluster getCluster(\n                 .withSplitCount(splitCount)\n                 .withMapperFactoryFn(this.mapperFactoryFn());\n \n-        if (isMurmur3Partitioner(cluster)) {\n-          LOG.info(\"Murmur3Partitioner detected, splitting\");\n-\n-          List<BigInteger> tokens =\n-              cluster.getMetadata().getTokenRanges().stream()\n-                  .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n-                  .collect(Collectors.toList());\n-\n-          SplitGenerator splitGenerator =\n-              new SplitGenerator(cluster.getMetadata().getPartitioner());\n-\n-          List<Read<T>> splits =\n-              splitGenerator.generateSplits(splitCount, tokens).stream()\n-                  .map(rr -> CassandraIO.<T>read().withRingRange(rr))\n-                  .collect(Collectors.toList());\n+        return input\n+            .apply(Create.of(this))\n+            .apply(ParDo.of(new SplitFn<T>()))\n+            .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+            .apply(Reshuffle.viaRandomKey())\n+            .apply(readAll);\n+      }\n+    }\n \n-          return input.apply(\"Creating splits\", Create.of(splits)).apply(\"readAll\", readAll);\n+    private static class SplitFn<T> extends DoFn<Read<T>, Read<T>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "335808e814aa3bc68de3cd84e08e1af5fc3704cb"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMjcyNzE5", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-402272719", "createdAt": "2020-04-28T23:05:40Z", "commit": {"oid": "335808e814aa3bc68de3cd84e08e1af5fc3704cb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMzowNTo0MFrOGNqMqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMzowNTo0MFrOGNqMqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk3NjA0Mw==", "bodyText": "Now we need to tackle this in two parts maybe, one is to implement the read with a ReadFn like method  and as a next step to get rid of all the methods on ReadAll to simplify it to its core.", "url": "https://github.com/apache/beam/pull/10546#discussion_r416976043", "createdAt": "2020-04-28T23:05:40Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -391,36 +393,57 @@ static Cluster getCluster(\n                 .withSplitCount(splitCount)\n                 .withMapperFactoryFn(this.mapperFactoryFn());\n \n-        if (isMurmur3Partitioner(cluster)) {\n-          LOG.info(\"Murmur3Partitioner detected, splitting\");\n-\n-          List<BigInteger> tokens =\n-              cluster.getMetadata().getTokenRanges().stream()\n-                  .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n-                  .collect(Collectors.toList());\n-\n-          SplitGenerator splitGenerator =\n-              new SplitGenerator(cluster.getMetadata().getPartitioner());\n-\n-          List<Read<T>> splits =\n-              splitGenerator.generateSplits(splitCount, tokens).stream()\n-                  .map(rr -> CassandraIO.<T>read().withRingRange(rr))\n-                  .collect(Collectors.toList());\n+        return input\n+            .apply(Create.of(this))\n+            .apply(ParDo.of(new SplitFn<T>()))\n+            .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+            .apply(Reshuffle.viaRandomKey())\n+            .apply(readAll);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "335808e814aa3bc68de3cd84e08e1af5fc3704cb"}, "originalPosition": 40}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "335808e814aa3bc68de3cd84e08e1af5fc3704cb", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/335808e814aa3bc68de3cd84e08e1af5fc3704cb", "committedDate": "2020-04-14T21:15:05Z", "message": "refactor read.expand to be more idiomatic to the solr code"}, "afterCommit": {"oid": "f2c78f5a7883d835756b3cff1c65a5c8d91c4a82", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/f2c78f5a7883d835756b3cff1c65a5c8d91c4a82", "committedDate": "2020-06-01T04:23:05Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f2c78f5a7883d835756b3cff1c65a5c8d91c4a82", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/f2c78f5a7883d835756b3cff1c65a5c8d91c4a82", "committedDate": "2020-06-01T04:23:05Z", "message": "implementing CassandraIO.readAll"}, "afterCommit": {"oid": "211c831f8912768d8dda93a877b2d13eb1634cda", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/211c831f8912768d8dda93a877b2d13eb1634cda", "committedDate": "2020-06-01T17:09:04Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "211c831f8912768d8dda93a877b2d13eb1634cda", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/211c831f8912768d8dda93a877b2d13eb1634cda", "committedDate": "2020-06-01T17:09:04Z", "message": "implementing CassandraIO.readAll"}, "afterCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/a116abe3c944420c007e6d4afc914bf29267ae8d", "committedDate": "2020-06-01T17:20:05Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyNjkxNjE1", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-422691615", "createdAt": "2020-06-02T13:45:41Z", "commit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzo0NTo0MlrOGdyWiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMToxNzo1OFrOGeETbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4Njg1Ng==", "bodyText": "private", "url": "https://github.com/apache/beam/pull/10546#discussion_r433886856", "createdAt": "2020-06-02T13:45:42Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/test/java/org/apache/beam/sdk/io/cassandra/CassandraIOTest.java", "diffHunk": "@@ -317,9 +302,70 @@ public void testRead() throws Exception {\n     PAssert.that(mapped.apply(\"Count occurrences per scientist\", Count.perKey()))\n         .satisfies(\n             input -> {\n+              int count = 0;\n               for (KV<String, Long> element : input) {\n+                count++;\n                 assertEquals(element.getKey(), NUM_ROWS / 10, element.getValue().longValue());\n               }\n+              assertEquals(11, count);\n+              return null;\n+            });\n+\n+    pipeline.run();\n+  }\n+\n+  CassandraIO.Read<Scientist> getReadWithRingRange(RingRange... rr) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5NjM3Mg==", "bodyText": "Can we move this method to the test class where it is used. I don't want to add Cassandra specific Metadata to the public API of RingRange with the hope this will help us evolve RingRange into a proper Restriction (future work out of the scope of this PR)\nCan you also please add a public static RingRange of(BigInteger start, BigInteger send) method and make the normal constructor private and refactor in every use.", "url": "https://github.com/apache/beam/pull/10546#discussion_r433896372", "createdAt": "2020-06-02T13:58:21Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -55,4 +58,9 @@ public boolean isWrapping() {\n   public String toString() {\n     return String.format(\"(%s,%s]\", start.toString(), end.toString());\n   }\n+\n+  public static RingRange fromEncodedKey(Metadata metadata, ByteBuffer... bb) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5Njk5NA==", "bodyText": "nit: For the tests can we assure that every reference to the tables and Scientist object usage follows this order: id, name, department.", "url": "https://github.com/apache/beam/pull/10546#discussion_r433896994", "createdAt": "2020-06-02T13:59:14Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/test/java/org/apache/beam/sdk/io/cassandra/CassandraIOTest.java", "diffHunk": "@@ -191,39 +186,44 @@ private static void insertData() throws Exception {\n     LOG.info(\"Create Cassandra tables\");\n     session.execute(\n         String.format(\n-            \"CREATE TABLE IF NOT EXISTS %s.%s(person_id int, person_name text, PRIMARY KEY\"\n-                + \"(person_id));\",\n+            \"CREATE TABLE IF NOT EXISTS %s.%s(person_department text, person_id int, person_name text, PRIMARY KEY\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5Nzg0OQ==", "bodyText": "why TEMP ?", "url": "https://github.com/apache/beam/pull/10546#discussion_r433897849", "createdAt": "2020-06-02T14:00:13Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/test/java/org/apache/beam/sdk/io/cassandra/CassandraIOTest.java", "diffHunk": "@@ -480,66 +527,22 @@ public void testCustomMapperImplDelete() {\n     assertEquals(1, counter.intValue());\n   }\n \n-  @Test\n-  public void testSplit() throws Exception {\n-    PipelineOptions options = PipelineOptionsFactory.create();\n-    CassandraIO.Read<Scientist> read =\n-        CassandraIO.<Scientist>read()\n-            .withHosts(Collections.singletonList(CASSANDRA_HOST))\n-            .withPort(cassandraPort)\n-            .withKeyspace(CASSANDRA_KEYSPACE)\n-            .withTable(CASSANDRA_TABLE)\n-            .withEntity(Scientist.class)\n-            .withCoder(SerializableCoder.of(Scientist.class));\n-\n-    // initialSource will be read without splitting (which does not happen in production)\n-    // so we need to provide splitQueries to avoid NPE in source.reader.start()\n-    String splitQuery = QueryBuilder.select().from(CASSANDRA_KEYSPACE, CASSANDRA_TABLE).toString();\n-    CassandraIO.CassandraSource<Scientist> initialSource =\n-        new CassandraIO.CassandraSource<>(read, Collections.singletonList(splitQuery));\n-    int desiredBundleSizeBytes = 2048;\n-    long estimatedSize = initialSource.getEstimatedSizeBytes(options);\n-    List<BoundedSource<Scientist>> splits = initialSource.split(desiredBundleSizeBytes, options);\n-    SourceTestUtils.assertSourcesEqualReferenceSource(initialSource, splits, options);\n-    float expectedNumSplitsloat =\n-        (float) initialSource.getEstimatedSizeBytes(options) / desiredBundleSizeBytes;\n-    long sum = 0;\n-\n-    for (BoundedSource<Scientist> subSource : splits) {\n-      sum += subSource.getEstimatedSizeBytes(options);\n-    }\n-\n-    // due to division and cast estimateSize != sum but will be close. Exact equals checked below\n-    assertEquals((long) (estimatedSize / splits.size()) * splits.size(), sum);\n-\n-    int expectedNumSplits = (int) Math.ceil(expectedNumSplitsloat);\n-    assertEquals(\"Wrong number of splits\", expectedNumSplits, splits.size());\n-    int emptySplits = 0;\n-    for (BoundedSource<Scientist> subSource : splits) {\n-      if (readFromSource(subSource, options).isEmpty()) {\n-        emptySplits += 1;\n-      }\n-    }\n-    assertThat(\n-        \"There are too many empty splits, parallelism is sub-optimal\",\n-        emptySplits,\n-        lessThan((int) (ACCEPTABLE_EMPTY_SPLITS_PERCENTAGE * splits.size())));\n-  }\n-\n   private List<Row> getRows(String table) {\n     ResultSet result =\n         session.execute(\n             String.format(\"select person_id,person_name from %s.%s\", CASSANDRA_KEYSPACE, table));\n     return result.all();\n   }\n \n+  // TEMP TEST", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk5NjE5MA==", "bodyText": "nit: move below we tend to preserve the lifecycle order of methods setup-startbundle-processElement-finishbundle-teardown", "url": "https://github.com/apache/beam/pull/10546#discussion_r433996190", "createdAt": "2020-06-02T16:03:39Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/ReadFn.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Iterator;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.io.cassandra.CassandraIO.Read;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class ReadFn<T> extends DoFn<Read<T>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFn.class);\n+\n+  private transient Cluster cluster;\n+\n+  private transient Session session;\n+\n+  private transient Read<T> lastRead;\n+\n+  @Teardown\n+  public void teardown() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAwNTQyOA==", "bodyText": "Remove", "url": "https://github.com/apache/beam/pull/10546#discussion_r434005428", "createdAt": "2020-06-02T16:17:04Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          // .apply(Reshuffle.viaRandomKey())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyMzA0MA==", "bodyText": "move down into the apply, it makes the code more readable.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434123040", "createdAt": "2020-06-02T19:23:05Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyNDEwNw==", "bodyText": "Move the Split as the first step of the ReadAll expansion so non advanced users (those who do not specify RingRange manually could get their code 'partitioned' correctly.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434124107", "createdAt": "2020-06-02T19:25:07Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyNjIzMg==", "bodyText": "\ud83d\udc4d for making it public now!", "url": "https://github.com/apache/beam/pull/10546#discussion_r434126232", "createdAt": "2020-06-02T19:28:55Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -17,10 +17,13 @@\n  */\n package org.apache.beam.sdk.io.cassandra;\n \n+import com.datastax.driver.core.Metadata;\n+import java.io.Serializable;\n import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n \n /** Models a Cassandra token range. */\n-final class RingRange {\n+public final class RingRange implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyNzE5Ng==", "bodyText": "We should add the equals() and hashCode() methods, those are now mandatory for the Set contract used in CassandraIO to be consistent.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434127196", "createdAt": "2020-06-02T19:30:47Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -17,10 +17,13 @@\n  */\n package org.apache.beam.sdk.io.cassandra;\n \n+import com.datastax.driver.core.Metadata;\n+import java.io.Serializable;\n import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n \n /** Models a Cassandra token range. */\n-final class RingRange {\n+public final class RingRange implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyNjIzMg=="}, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyOTI1OA==", "bodyText": "Please also add @Experimental(Kind.SOURCE_SINK) to the class.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434129258", "createdAt": "2020-06-02T19:34:34Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -17,10 +17,13 @@\n  */\n package org.apache.beam.sdk.io.cassandra;\n \n+import com.datastax.driver.core.Metadata;\n+import java.io.Serializable;\n import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n \n /** Models a Cassandra token range. */\n-final class RingRange {\n+public final class RingRange implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzNTk1OA==", "bodyText": "\ud83d\udc4f remove repeated and useless \ud83d\udc4f", "url": "https://github.com/apache/beam/pull/10546#discussion_r434135958", "createdAt": "2020-06-02T19:47:50Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -370,384 +488,16 @@ private CassandraIO() {}\n         return autoBuild();\n       }\n     }\n-  }\n-\n-  @VisibleForTesting\n-  static class CassandraSource<T> extends BoundedSource<T> {\n-    final Read<T> spec;\n-    final List<String> splitQueries;\n-    // split source ached size - can't be calculated when already split\n-    Long estimatedSize;\n-    private static final String MURMUR3PARTITIONER = \"org.apache.cassandra.dht.Murmur3Partitioner\";\n-\n-    CassandraSource(Read<T> spec, List<String> splitQueries) {\n-      this(spec, splitQueries, null);\n-    }\n-\n-    private CassandraSource(Read<T> spec, List<String> splitQueries, Long estimatedSize) {\n-      this.estimatedSize = estimatedSize;\n-      this.spec = spec;\n-      this.splitQueries = splitQueries;\n-    }\n-\n-    @Override\n-    public Coder<T> getOutputCoder() {\n-      return spec.coder();\n-    }\n-\n-    @Override\n-    public BoundedReader<T> createReader(PipelineOptions pipelineOptions) {\n-      return new CassandraReader(this);\n-    }\n-\n-    @Override\n-    public List<BoundedSource<T>> split(\n-        long desiredBundleSizeBytes, PipelineOptions pipelineOptions) {\n-      try (Cluster cluster =\n-          getCluster(\n-              spec.hosts(),\n-              spec.port(),\n-              spec.username(),\n-              spec.password(),\n-              spec.localDc(),\n-              spec.consistencyLevel())) {\n-        if (isMurmur3Partitioner(cluster)) {\n-          LOG.info(\"Murmur3Partitioner detected, splitting\");\n-          return splitWithTokenRanges(\n-              spec, desiredBundleSizeBytes, getEstimatedSizeBytes(pipelineOptions), cluster);\n-        } else {\n-          LOG.warn(\n-              \"Only Murmur3Partitioner is supported for splitting, using a unique source for \"\n-                  + \"the read\");\n-          return Collections.singletonList(\n-              new CassandraIO.CassandraSource<>(spec, Collections.singletonList(buildQuery(spec))));\n-        }\n-      }\n-    }\n-\n-    private static String buildQuery(Read spec) {\n-      return (spec.query() == null)\n-          ? String.format(\"SELECT * FROM %s.%s\", spec.keyspace().get(), spec.table().get())\n-          : spec.query().get().toString();\n-    }\n-\n-    /**\n-     * Compute the number of splits based on the estimated size and the desired bundle size, and\n-     * create several sources.\n-     */\n-    private List<BoundedSource<T>> splitWithTokenRanges(\n-        CassandraIO.Read<T> spec,\n-        long desiredBundleSizeBytes,\n-        long estimatedSizeBytes,\n-        Cluster cluster) {\n-      long numSplits =\n-          getNumSplits(desiredBundleSizeBytes, estimatedSizeBytes, spec.minNumberOfSplits());\n-      LOG.info(\"Number of desired splits is {}\", numSplits);\n-\n-      SplitGenerator splitGenerator = new SplitGenerator(cluster.getMetadata().getPartitioner());\n-      List<BigInteger> tokens =\n-          cluster.getMetadata().getTokenRanges().stream()\n-              .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n-              .collect(Collectors.toList());\n-      List<List<RingRange>> splits = splitGenerator.generateSplits(numSplits, tokens);\n-      LOG.info(\"{} splits were actually generated\", splits.size());\n-\n-      final String partitionKey =\n-          cluster.getMetadata().getKeyspace(spec.keyspace().get()).getTable(spec.table().get())\n-              .getPartitionKey().stream()\n-              .map(ColumnMetadata::getName)\n-              .collect(Collectors.joining(\",\"));\n-\n-      List<TokenRange> tokenRanges =\n-          getTokenRanges(cluster, spec.keyspace().get(), spec.table().get());\n-      final long estimatedSize = getEstimatedSizeBytesFromTokenRanges(tokenRanges) / splits.size();\n-\n-      List<BoundedSource<T>> sources = new ArrayList<>();\n-      for (List<RingRange> split : splits) {\n-        List<String> queries = new ArrayList<>();\n-        for (RingRange range : split) {\n-          if (range.isWrapping()) {\n-            // A wrapping range is one that overlaps from the end of the partitioner range and its\n-            // start (ie : when the start token of the split is greater than the end token)\n-            // We need to generate two queries here : one that goes from the start token to the end\n-            // of\n-            // the partitioner range, and the other from the start of the partitioner range to the\n-            // end token of the split.\n-            queries.add(generateRangeQuery(spec, partitionKey, range.getStart(), null));\n-            // Generation of the second query of the wrapping range\n-            queries.add(generateRangeQuery(spec, partitionKey, null, range.getEnd()));\n-          } else {\n-            queries.add(generateRangeQuery(spec, partitionKey, range.getStart(), range.getEnd()));\n-          }\n-        }\n-        sources.add(new CassandraIO.CassandraSource<>(spec, queries, estimatedSize));\n-      }\n-      return sources;\n-    }\n-\n-    private static String generateRangeQuery(\n-        Read spec, String partitionKey, BigInteger rangeStart, BigInteger rangeEnd) {\n-      final String rangeFilter =\n-          Joiner.on(\" AND \")\n-              .skipNulls()\n-              .join(\n-                  rangeStart == null\n-                      ? null\n-                      : String.format(\"(token(%s) >= %d)\", partitionKey, rangeStart),\n-                  rangeEnd == null\n-                      ? null\n-                      : String.format(\"(token(%s) < %d)\", partitionKey, rangeEnd));\n-      final String query =\n-          (spec.query() == null)\n-              ? buildQuery(spec) + \" WHERE \" + rangeFilter\n-              : buildQuery(spec) + \" AND \" + rangeFilter;\n-      LOG.debug(\"CassandraIO generated query : {}\", query);\n-      return query;\n-    }\n-\n-    private static long getNumSplits(\n-        long desiredBundleSizeBytes,\n-        long estimatedSizeBytes,\n-        @Nullable ValueProvider<Integer> minNumberOfSplits) {\n-      long numSplits =\n-          desiredBundleSizeBytes > 0 ? (estimatedSizeBytes / desiredBundleSizeBytes) : 1;\n-      if (numSplits <= 0) {\n-        LOG.warn(\"Number of splits is less than 0 ({}), fallback to 1\", numSplits);\n-        numSplits = 1;\n-      }\n-      return minNumberOfSplits != null ? Math.max(numSplits, minNumberOfSplits.get()) : numSplits;\n-    }\n-\n-    /**\n-     * Returns cached estimate for split or if missing calculate size for whole table. Highly\n-     * innacurate if query is specified.\n-     *\n-     * @param pipelineOptions\n-     * @return\n-     */\n-    @Override\n-    public long getEstimatedSizeBytes(PipelineOptions pipelineOptions) {\n-      if (estimatedSize != null) {\n-        return estimatedSize;\n-      } else {\n-        try (Cluster cluster =\n-            getCluster(\n-                spec.hosts(),\n-                spec.port(),\n-                spec.username(),\n-                spec.password(),\n-                spec.localDc(),\n-                spec.consistencyLevel())) {\n-          if (isMurmur3Partitioner(cluster)) {\n-            try {\n-              List<TokenRange> tokenRanges =\n-                  getTokenRanges(cluster, spec.keyspace().get(), spec.table().get());\n-              this.estimatedSize = getEstimatedSizeBytesFromTokenRanges(tokenRanges);\n-              return this.estimatedSize;\n-            } catch (Exception e) {\n-              LOG.warn(\"Can't estimate the size\", e);\n-              return 0L;\n-            }\n-          } else {\n-            LOG.warn(\"Only Murmur3 partitioner is supported, can't estimate the size\");\n-            return 0L;\n-          }\n-        }\n-      }\n-    }\n-\n-    @VisibleForTesting\n-    static long getEstimatedSizeBytesFromTokenRanges(List<TokenRange> tokenRanges) {\n-      long size = 0L;\n-      for (TokenRange tokenRange : tokenRanges) {\n-        size = size + tokenRange.meanPartitionSize * tokenRange.partitionCount;\n-      }\n-      return Math.round(size / getRingFraction(tokenRanges));\n-    }\n \n-    @Override\n-    public void populateDisplayData(DisplayData.Builder builder) {\n-      super.populateDisplayData(builder);\n-      if (spec.hosts() != null) {\n-        builder.add(DisplayData.item(\"hosts\", spec.hosts().toString()));\n-      }\n-      if (spec.port() != null) {\n-        builder.add(DisplayData.item(\"port\", spec.port()));\n-      }\n-      builder.addIfNotNull(DisplayData.item(\"keyspace\", spec.keyspace()));\n-      builder.addIfNotNull(DisplayData.item(\"table\", spec.table()));\n-      builder.addIfNotNull(DisplayData.item(\"username\", spec.username()));\n-      builder.addIfNotNull(DisplayData.item(\"localDc\", spec.localDc()));\n-      builder.addIfNotNull(DisplayData.item(\"consistencyLevel\", spec.consistencyLevel()));\n-    }\n     // ------------- CASSANDRA SOURCE UTIL METHODS ---------------//\n \n-    /**\n-     * Gets the list of token ranges that a table occupies on a give Cassandra node.\n-     *\n-     * <p>NB: This method is compatible with Cassandra 2.1.5 and greater.\n-     */\n-    private static List<TokenRange> getTokenRanges(Cluster cluster, String keyspace, String table) {\n-      try (Session session = cluster.newSession()) {\n-        ResultSet resultSet =\n-            session.execute(\n-                \"SELECT range_start, range_end, partitions_count, mean_partition_size FROM \"\n-                    + \"system.size_estimates WHERE keyspace_name = ? AND table_name = ?\",\n-                keyspace,\n-                table);\n-\n-        ArrayList<TokenRange> tokenRanges = new ArrayList<>();\n-        for (Row row : resultSet) {\n-          TokenRange tokenRange =\n-              new TokenRange(\n-                  row.getLong(\"partitions_count\"),\n-                  row.getLong(\"mean_partition_size\"),\n-                  new BigInteger(row.getString(\"range_start\")),\n-                  new BigInteger(row.getString(\"range_end\")));\n-          tokenRanges.add(tokenRange);\n-        }\n-        // The table may not contain the estimates yet\n-        // or have partitions_count and mean_partition_size fields = 0\n-        // if the data was just inserted and the amount of data in the table was small.\n-        // This is very common situation during tests,\n-        // when we insert a few rows and immediately query them.\n-        // However, for tiny data sets the lack of size estimates is not a problem at all,\n-        // because we don't want to split tiny data anyways.\n-        // Therefore, we're not issuing a warning if the result set was empty\n-        // or mean_partition_size and partitions_count = 0.\n-        return tokenRanges;\n-      }\n-    }\n-\n-    /** Compute the percentage of token addressed compared with the whole tokens in the cluster. */\n-    @VisibleForTesting\n-    static double getRingFraction(List<TokenRange> tokenRanges) {\n-      double ringFraction = 0;\n-      for (TokenRange tokenRange : tokenRanges) {\n-        ringFraction =\n-            ringFraction\n-                + (distance(tokenRange.rangeStart, tokenRange.rangeEnd).doubleValue()\n-                    / SplitGenerator.getRangeSize(MURMUR3PARTITIONER).doubleValue());\n-      }\n-      return ringFraction;\n-    }\n-\n-    /**\n-     * Check if the current partitioner is the Murmur3 (default in Cassandra version newer than 2).\n-     */\n-    @VisibleForTesting\n-    static boolean isMurmur3Partitioner(Cluster cluster) {\n-      return MURMUR3PARTITIONER.equals(cluster.getMetadata().getPartitioner());\n-    }\n-\n     /** Measure distance between two tokens. */\n     @VisibleForTesting\n     static BigInteger distance(BigInteger left, BigInteger right) {\n       return (right.compareTo(left) > 0)\n           ? right.subtract(left)\n           : right.subtract(left).add(SplitGenerator.getRangeSize(MURMUR3PARTITIONER));\n     }\n-\n-    /**\n-     * Represent a token range in Cassandra instance, wrapping the partition count, size and token\n-     * range.\n-     */\n-    @VisibleForTesting\n-    static class TokenRange {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 510}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0MTkyMQ==", "bodyText": "Better define Integer splitCount = read.minNumberOfSplits().get(); first, this will allow you to skip one server visit if it is already set up by the user.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434141921", "createdAt": "2020-06-02T19:59:23Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          // .apply(Reshuffle.viaRandomKey())\n+          .apply(readAll);\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+\n+        try (Cluster cluster =\n+            getCluster(\n+                read.hosts(),\n+                read.port(),\n+                read.username(),\n+                read.password(),\n+                read.localDc(),\n+                read.consistencyLevel())) {\n+          if (isMurmur3Partitioner(cluster)) {\n+            LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+            List<BigInteger> tokens =\n+                cluster.getMetadata().getTokenRanges().stream()\n+                    .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                    .collect(Collectors.toList());\n+            Integer splitCount = cluster.getMetadata().getAllHosts().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0MjQwMA==", "bodyText": "Since we need Metadata below and getMetadata() makes a synchronized operation maybe obtain Metadata once and reuse after.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434142400", "createdAt": "2020-06-02T20:00:14Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          // .apply(Reshuffle.viaRandomKey())\n+          .apply(readAll);\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+\n+        try (Cluster cluster =\n+            getCluster(\n+                read.hosts(),\n+                read.port(),\n+                read.username(),\n+                read.password(),\n+                read.localDc(),\n+                read.consistencyLevel())) {\n+          if (isMurmur3Partitioner(cluster)) {\n+            LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+            List<BigInteger> tokens =\n+                cluster.getMetadata().getTokenRanges().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NTkwMw==", "bodyText": "Do you think this logic is already covered by the new implementation or that we cannot end up having issues on wrapping ranges e.g. repeated data. I can barely remember why we went into these 'hacks' I expected this to be a responsability of SplitGenerator?", "url": "https://github.com/apache/beam/pull/10546#discussion_r434145903", "createdAt": "2020-06-02T20:07:27Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -370,384 +488,16 @@ private CassandraIO() {}\n         return autoBuild();\n       }\n     }\n-  }\n-\n-  @VisibleForTesting\n-  static class CassandraSource<T> extends BoundedSource<T> {\n-    final Read<T> spec;\n-    final List<String> splitQueries;\n-    // split source ached size - can't be calculated when already split\n-    Long estimatedSize;\n-    private static final String MURMUR3PARTITIONER = \"org.apache.cassandra.dht.Murmur3Partitioner\";\n-\n-    CassandraSource(Read<T> spec, List<String> splitQueries) {\n-      this(spec, splitQueries, null);\n-    }\n-\n-    private CassandraSource(Read<T> spec, List<String> splitQueries, Long estimatedSize) {\n-      this.estimatedSize = estimatedSize;\n-      this.spec = spec;\n-      this.splitQueries = splitQueries;\n-    }\n-\n-    @Override\n-    public Coder<T> getOutputCoder() {\n-      return spec.coder();\n-    }\n-\n-    @Override\n-    public BoundedReader<T> createReader(PipelineOptions pipelineOptions) {\n-      return new CassandraReader(this);\n-    }\n-\n-    @Override\n-    public List<BoundedSource<T>> split(\n-        long desiredBundleSizeBytes, PipelineOptions pipelineOptions) {\n-      try (Cluster cluster =\n-          getCluster(\n-              spec.hosts(),\n-              spec.port(),\n-              spec.username(),\n-              spec.password(),\n-              spec.localDc(),\n-              spec.consistencyLevel())) {\n-        if (isMurmur3Partitioner(cluster)) {\n-          LOG.info(\"Murmur3Partitioner detected, splitting\");\n-          return splitWithTokenRanges(\n-              spec, desiredBundleSizeBytes, getEstimatedSizeBytes(pipelineOptions), cluster);\n-        } else {\n-          LOG.warn(\n-              \"Only Murmur3Partitioner is supported for splitting, using a unique source for \"\n-                  + \"the read\");\n-          return Collections.singletonList(\n-              new CassandraIO.CassandraSource<>(spec, Collections.singletonList(buildQuery(spec))));\n-        }\n-      }\n-    }\n-\n-    private static String buildQuery(Read spec) {\n-      return (spec.query() == null)\n-          ? String.format(\"SELECT * FROM %s.%s\", spec.keyspace().get(), spec.table().get())\n-          : spec.query().get().toString();\n-    }\n-\n-    /**\n-     * Compute the number of splits based on the estimated size and the desired bundle size, and\n-     * create several sources.\n-     */\n-    private List<BoundedSource<T>> splitWithTokenRanges(\n-        CassandraIO.Read<T> spec,\n-        long desiredBundleSizeBytes,\n-        long estimatedSizeBytes,\n-        Cluster cluster) {\n-      long numSplits =\n-          getNumSplits(desiredBundleSizeBytes, estimatedSizeBytes, spec.minNumberOfSplits());\n-      LOG.info(\"Number of desired splits is {}\", numSplits);\n-\n-      SplitGenerator splitGenerator = new SplitGenerator(cluster.getMetadata().getPartitioner());\n-      List<BigInteger> tokens =\n-          cluster.getMetadata().getTokenRanges().stream()\n-              .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n-              .collect(Collectors.toList());\n-      List<List<RingRange>> splits = splitGenerator.generateSplits(numSplits, tokens);\n-      LOG.info(\"{} splits were actually generated\", splits.size());\n-\n-      final String partitionKey =\n-          cluster.getMetadata().getKeyspace(spec.keyspace().get()).getTable(spec.table().get())\n-              .getPartitionKey().stream()\n-              .map(ColumnMetadata::getName)\n-              .collect(Collectors.joining(\",\"));\n-\n-      List<TokenRange> tokenRanges =\n-          getTokenRanges(cluster, spec.keyspace().get(), spec.table().get());\n-      final long estimatedSize = getEstimatedSizeBytesFromTokenRanges(tokenRanges) / splits.size();\n-\n-      List<BoundedSource<T>> sources = new ArrayList<>();\n-      for (List<RingRange> split : splits) {\n-        List<String> queries = new ArrayList<>();\n-        for (RingRange range : split) {\n-          if (range.isWrapping()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 323}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0ODAzNQ==", "bodyText": "You do not need to rebuild the object the current output here should be simply read.withRingRanges(new HashSet<>(rr));", "url": "https://github.com/apache/beam/pull/10546#discussion_r434148035", "createdAt": "2020-06-02T20:11:27Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          // .apply(Reshuffle.viaRandomKey())\n+          .apply(readAll);\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+\n+        try (Cluster cluster =\n+            getCluster(\n+                read.hosts(),\n+                read.port(),\n+                read.username(),\n+                read.password(),\n+                read.localDc(),\n+                read.consistencyLevel())) {\n+          if (isMurmur3Partitioner(cluster)) {\n+            LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+            List<BigInteger> tokens =\n+                cluster.getMetadata().getTokenRanges().stream()\n+                    .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                    .collect(Collectors.toList());\n+            Integer splitCount = cluster.getMetadata().getAllHosts().size();\n+            if (read.minNumberOfSplits() != null && read.minNumberOfSplits().get() != null) {\n+              splitCount = read.minNumberOfSplits().get();\n+            }\n+\n+            SplitGenerator splitGenerator =\n+                new SplitGenerator(cluster.getMetadata().getPartitioner());\n+            splitGenerator\n+                .generateSplits(splitCount, tokens)\n+                .forEach(\n+                    rr ->\n+                        outputReceiver.output(\n+                            CassandraIO.<T>read()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0ODQyNw==", "bodyText": "same as above the output is the initial read with the modified RingRange otherwise you would need to copy/define all attributes.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434148427", "createdAt": "2020-06-02T20:12:14Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          // .apply(Reshuffle.viaRandomKey())\n+          .apply(readAll);\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+\n+        try (Cluster cluster =\n+            getCluster(\n+                read.hosts(),\n+                read.port(),\n+                read.username(),\n+                read.password(),\n+                read.localDc(),\n+                read.consistencyLevel())) {\n+          if (isMurmur3Partitioner(cluster)) {\n+            LOG.info(\"Murmur3Partitioner detected, splitting\");\n+\n+            List<BigInteger> tokens =\n+                cluster.getMetadata().getTokenRanges().stream()\n+                    .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                    .collect(Collectors.toList());\n+            Integer splitCount = cluster.getMetadata().getAllHosts().size();\n+            if (read.minNumberOfSplits() != null && read.minNumberOfSplits().get() != null) {\n+              splitCount = read.minNumberOfSplits().get();\n+            }\n+\n+            SplitGenerator splitGenerator =\n+                new SplitGenerator(cluster.getMetadata().getPartitioner());\n+            splitGenerator\n+                .generateSplits(splitCount, tokens)\n+                .forEach(\n+                    rr ->\n+                        outputReceiver.output(\n+                            CassandraIO.<T>read()\n+                                .withRingRanges(new HashSet<>(rr))\n+                                .withCoder(coder())\n+                                .withConsistencyLevel(consistencyLevel())\n+                                .withEntity(entity())\n+                                .withHosts(hosts())\n+                                .withKeyspace(keyspace())\n+                                .withLocalDc(localDc())\n+                                .withPort(port())\n+                                .withPassword(password())\n+                                .withQuery(query())\n+                                .withTable(table())\n+                                .withUsername(username())\n+                                .withMapperFactoryFn(mapperFactoryFn())));\n+          } else {\n+            LOG.warn(\n+                \"Only Murmur3Partitioner is supported for splitting, using an unique source for \"\n+                    + \"the read\");\n+            String partitioner = cluster.getMetadata().getPartitioner();\n+            RingRange totalRingRange =\n+                new RingRange(\n+                    SplitGenerator.getRangeMin(partitioner),\n+                    SplitGenerator.getRangeMax(partitioner));\n+            outputReceiver.output(\n+                CassandraIO.<T>read()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE1MTQwNg==", "bodyText": "I think we are missing here the case when the user already defines his RingRange  in the input read, what should we do in that case? my intutition tells me we should just pass the read as an output without changes, but if we want to split his decision we should have the RingRange input in account somehow.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434151406", "createdAt": "2020-06-02T20:18:13Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          // .apply(Reshuffle.viaRandomKey())\n+          .apply(readAll);\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+\n+        try (Cluster cluster =\n+            getCluster(\n+                read.hosts(),\n+                read.port(),\n+                read.username(),\n+                read.password(),\n+                read.localDc(),\n+                read.consistencyLevel())) {\n+          if (isMurmur3Partitioner(cluster)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE1MjQ0Nw==", "bodyText": "Since it seems the new split implementation does not use isWrapping() please remove that method from the RingRange class", "url": "https://github.com/apache/beam/pull/10546#discussion_r434152447", "createdAt": "2020-06-02T20:20:14Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -17,10 +17,13 @@\n  */\n package org.apache.beam.sdk.io.cassandra;\n \n+import com.datastax.driver.core.Metadata;\n+import java.io.Serializable;\n import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n \n /** Models a Cassandra token range. */\n-final class RingRange {\n+public final class RingRange implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyNjIzMg=="}, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE1MzQ2MQ==", "bodyText": "\"Reshuffle\"", "url": "https://github.com/apache/beam/pull/10546#discussion_r434153461", "createdAt": "2020-06-02T20:22:23Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +898,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {\n+      checkArgument(coder != null, \"coder can not be null\");\n+      return builder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PCollection<Read<T>> input) {\n+      checkArgument(coder() != null, \"withCoder() is required\");\n+      return input\n+          .apply(\"shuffle\", Reshuffle.viaRandomKey())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 687}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE1MzU1Mg==", "bodyText": "\"Read\"", "url": "https://github.com/apache/beam/pull/10546#discussion_r434153552", "createdAt": "2020-06-02T20:22:34Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +898,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {\n+      checkArgument(coder != null, \"coder can not be null\");\n+      return builder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PCollection<Read<T>> input) {\n+      checkArgument(coder() != null, \"withCoder() is required\");\n+      return input\n+          .apply(\"shuffle\", Reshuffle.viaRandomKey())\n+          .apply(\"read\", ParDo.of(new ReadFn<>()))\n+          .setCoder(this.coder());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 689}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE1NDA0Nw==", "bodyText": "Also add the \"Split\", name to the step", "url": "https://github.com/apache/beam/pull/10546#discussion_r434154047", "createdAt": "2020-06-02T20:23:35Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -326,7 +371,78 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.<T>readAll().withCoder(this.coder());\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(ParDo.of(new SplitFn()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEyNDEwNw=="}, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE4MDY4Nw==", "bodyText": "coder should come from the input read", "url": "https://github.com/apache/beam/pull/10546#discussion_r434180687", "createdAt": "2020-06-02T21:17:31Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +898,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {\n+      checkArgument(coder != null, \"coder can not be null\");\n+      return builder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PCollection<Read<T>> input) {\n+      checkArgument(coder() != null, \"withCoder() is required\");\n+      return input\n+          .apply(\"shuffle\", Reshuffle.viaRandomKey())\n+          .apply(\"read\", ParDo.of(new ReadFn<>()))\n+          .setCoder(this.coder());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 689}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE4MDk3Mg==", "bodyText": "I am not sure if we need this, I assume the coder comes form the `Read input.", "url": "https://github.com/apache/beam/pull/10546#discussion_r434180972", "createdAt": "2020-06-02T21:17:58Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +898,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 678}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4de78f8de85c7df93e4ea7a5c60e1f03ef150ff0", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/4de78f8de85c7df93e4ea7a5c60e1f03ef150ff0", "committedDate": "2020-06-03T16:25:50Z", "message": "minor changes requested"}, "afterCommit": {"oid": "cbd25ad65a2f87b6b9a6f8da5169b005af186ee6", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/cbd25ad65a2f87b6b9a6f8da5169b005af186ee6", "committedDate": "2020-06-08T17:00:17Z", "message": "minor changes requested"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3b47248f7c396bb1cf4260ac19950df85ed22af7", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/3b47248f7c396bb1cf4260ac19950df85ed22af7", "committedDate": "2020-07-12T23:57:14Z", "message": "make query without ringrange work"}, "afterCommit": {"oid": "4ab11508cfb4f037a1b8c86003b174ff6268c55d", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/4ab11508cfb4f037a1b8c86003b174ff6268c55d", "committedDate": "2020-07-16T00:30:22Z", "message": "make query without ringrange work"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4ab11508cfb4f037a1b8c86003b174ff6268c55d", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/4ab11508cfb4f037a1b8c86003b174ff6268c55d", "committedDate": "2020-07-16T00:30:22Z", "message": "make query without ringrange work"}, "afterCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/f3377d98ca9ed9988a866f3671fe1106fbb437bd", "committedDate": "2020-09-24T02:50:05Z", "message": "implementing CassandraIO.readAll"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MjIzMTkw", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-507223190", "createdAt": "2020-10-13T09:05:46Z", "commit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTowNTo0N1rOHgc1iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMTo0Njo1MlrOHg6qLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc4ODkzOQ==", "bodyText": "Please move this one inside of SplitFn.\nYou would probably need some hack like:\nPCollection<Read<T>> split = (PCollection<Read<T>>) input.apply(\"Split\", ParDo.of(new SplitFn()));\nreturn split\n      .apply(\"Reshuffle\", Reshuffle.viaRandomKey())\n      .apply(\"Read\", ParDo.of(new ReadFn<>()));", "url": "https://github.com/apache/beam/pull/10546#discussion_r503788939", "createdAt": "2020-10-13T09:05:47Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -128,11 +126,18 @@\n \n   private CassandraIO() {}\n \n+  private static final String MURMUR3PARTITIONER = \"org.apache.cassandra.dht.Murmur3Partitioner\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyMDk0Nw==", "bodyText": "I prefer to assign the result of getRingRanges to a Set and assign it to a variable to help ease potential debugging in the future.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504220947", "createdAt": "2020-10-13T20:00:38Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -373,7 +419,86 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.readAll();\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(\"Split\", ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          .apply(\"ReadAll\", readAll.withCoder(this.coder()));\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+        getRingRanges(read)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyMTI0Mw==", "bodyText": "return a Set (see comment above)", "url": "https://github.com/apache/beam/pull/10546#discussion_r504221243", "createdAt": "2020-10-13T20:01:10Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -373,7 +419,86 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.readAll();\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(\"Split\", ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          .apply(\"ReadAll\", readAll.withCoder(this.coder()));\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+        getRingRanges(read)\n+            .forEach(\n+                rr ->\n+                    outputReceiver.output(\n+                        CassandraIO.<T>read()\n+                            .withRingRanges(new HashSet<>(rr))\n+                            .withCoder(coder())\n+                            .withConsistencyLevel(consistencyLevel())\n+                            .withEntity(entity())\n+                            .withHosts(hosts())\n+                            .withKeyspace(keyspace())\n+                            .withLocalDc(localDc())\n+                            .withPort(port())\n+                            .withPassword(password())\n+                            .withQuery(query())\n+                            .withTable(table())\n+                            .withUsername(username())\n+                            .withMapperFactoryFn(mapperFactoryFn())));\n+      }\n+\n+      Stream<Set<RingRange>> getRingRanges(Read<T> read) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyNDYyMQ==", "bodyText": "If we change the type I suppose we can make this return simpler.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504224621", "createdAt": "2020-10-13T20:07:32Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -373,7 +419,86 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.readAll();\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(\"Split\", ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          .apply(\"ReadAll\", readAll.withCoder(this.coder()));\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {\n+\n+      @ProcessElement\n+      public void process(\n+          @Element CassandraIO.Read<T> read, OutputReceiver<Read<T>> outputReceiver) {\n+        getRingRanges(read)\n+            .forEach(\n+                rr ->\n+                    outputReceiver.output(\n+                        CassandraIO.<T>read()\n+                            .withRingRanges(new HashSet<>(rr))\n+                            .withCoder(coder())\n+                            .withConsistencyLevel(consistencyLevel())\n+                            .withEntity(entity())\n+                            .withHosts(hosts())\n+                            .withKeyspace(keyspace())\n+                            .withLocalDc(localDc())\n+                            .withPort(port())\n+                            .withPassword(password())\n+                            .withQuery(query())\n+                            .withTable(table())\n+                            .withUsername(username())\n+                            .withMapperFactoryFn(mapperFactoryFn())));\n+      }\n+\n+      Stream<Set<RingRange>> getRingRanges(Read<T> read) {\n+        if (read.ringRanges() == null || read.ringRanges().get() == null) {\n+          try (Cluster cluster =\n+              getCluster(\n+                  read.hosts(),\n+                  read.port(),\n+                  read.username(),\n+                  read.password(),\n+                  read.localDc(),\n+                  read.consistencyLevel())) {\n+            if (isMurmur3Partitioner(cluster)) {\n+              LOG.info(\"Murmur3Partitioner detected, splitting\");\n+              Integer splitCount;\n+              if (read.minNumberOfSplits() != null && read.minNumberOfSplits().get() != null) {\n+                splitCount = read.minNumberOfSplits().get();\n+              } else {\n+                splitCount = cluster.getMetadata().getAllHosts().size();\n+              }\n+              List<BigInteger> tokens =\n+                  cluster.getMetadata().getTokenRanges().stream()\n+                      .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n+                      .collect(Collectors.toList());\n+              SplitGenerator splitGenerator =\n+                  new SplitGenerator(cluster.getMetadata().getPartitioner());\n+\n+              return splitGenerator.generateSplits(splitCount, tokens).stream()\n+                  .map(l -> new HashSet<>(l));\n+\n+            } else {\n+              LOG.warn(\n+                  \"Only Murmur3Partitioner is supported for splitting, using an unique source for \"\n+                      + \"the read\");\n+              String partitioner = cluster.getMetadata().getPartitioner();\n+              RingRange totalRingRange =\n+                  RingRange.of(\n+                      SplitGenerator.getRangeMin(partitioner),\n+                      SplitGenerator.getRangeMax(partitioner));\n+              return Collections.<Set<RingRange>>singleton(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyNTU5Mw==", "bodyText": "Can we move this one into SplitFn too since it is not used in other places.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504225593", "createdAt": "2020-10-13T20:09:33Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -423,396 +550,20 @@ private CassandraIO() {}\n     }\n   }\n \n-  @VisibleForTesting\n-  static class CassandraSource<T> extends BoundedSource<T> {\n-    final Read<T> spec;\n-    final List<String> splitQueries;\n-    // split source ached size - can't be calculated when already split\n-    Long estimatedSize;\n-    private static final String MURMUR3PARTITIONER = \"org.apache.cassandra.dht.Murmur3Partitioner\";\n-\n-    CassandraSource(Read<T> spec, List<String> splitQueries) {\n-      this(spec, splitQueries, null);\n-    }\n-\n-    private CassandraSource(Read<T> spec, List<String> splitQueries, Long estimatedSize) {\n-      this.estimatedSize = estimatedSize;\n-      this.spec = spec;\n-      this.splitQueries = splitQueries;\n-    }\n-\n-    @Override\n-    public Coder<T> getOutputCoder() {\n-      return spec.coder();\n-    }\n-\n-    @Override\n-    public BoundedReader<T> createReader(PipelineOptions pipelineOptions) {\n-      return new CassandraReader(this);\n-    }\n-\n-    @Override\n-    public List<BoundedSource<T>> split(\n-        long desiredBundleSizeBytes, PipelineOptions pipelineOptions) {\n-      try (Cluster cluster =\n-          getCluster(\n-              spec.hosts(),\n-              spec.port(),\n-              spec.username(),\n-              spec.password(),\n-              spec.localDc(),\n-              spec.consistencyLevel(),\n-              spec.connectTimeout(),\n-              spec.readTimeout())) {\n-        if (isMurmur3Partitioner(cluster)) {\n-          LOG.info(\"Murmur3Partitioner detected, splitting\");\n-          return splitWithTokenRanges(\n-              spec, desiredBundleSizeBytes, getEstimatedSizeBytes(pipelineOptions), cluster);\n-        } else {\n-          LOG.warn(\n-              \"Only Murmur3Partitioner is supported for splitting, using a unique source for \"\n-                  + \"the read\");\n-          return Collections.singletonList(\n-              new CassandraIO.CassandraSource<>(spec, Collections.singletonList(buildQuery(spec))));\n-        }\n-      }\n-    }\n-\n-    private static String buildQuery(Read spec) {\n-      return (spec.query() == null)\n-          ? String.format(\"SELECT * FROM %s.%s\", spec.keyspace().get(), spec.table().get())\n-          : spec.query().get().toString();\n-    }\n-\n-    /**\n-     * Compute the number of splits based on the estimated size and the desired bundle size, and\n-     * create several sources.\n-     */\n-    private List<BoundedSource<T>> splitWithTokenRanges(\n-        CassandraIO.Read<T> spec,\n-        long desiredBundleSizeBytes,\n-        long estimatedSizeBytes,\n-        Cluster cluster) {\n-      long numSplits =\n-          getNumSplits(desiredBundleSizeBytes, estimatedSizeBytes, spec.minNumberOfSplits());\n-      LOG.info(\"Number of desired splits is {}\", numSplits);\n-\n-      SplitGenerator splitGenerator = new SplitGenerator(cluster.getMetadata().getPartitioner());\n-      List<BigInteger> tokens =\n-          cluster.getMetadata().getTokenRanges().stream()\n-              .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n-              .collect(Collectors.toList());\n-      List<List<RingRange>> splits = splitGenerator.generateSplits(numSplits, tokens);\n-      LOG.info(\"{} splits were actually generated\", splits.size());\n-\n-      final String partitionKey =\n-          cluster.getMetadata().getKeyspace(spec.keyspace().get()).getTable(spec.table().get())\n-              .getPartitionKey().stream()\n-              .map(ColumnMetadata::getName)\n-              .collect(Collectors.joining(\",\"));\n-\n-      List<TokenRange> tokenRanges =\n-          getTokenRanges(cluster, spec.keyspace().get(), spec.table().get());\n-      final long estimatedSize = getEstimatedSizeBytesFromTokenRanges(tokenRanges) / splits.size();\n-\n-      List<BoundedSource<T>> sources = new ArrayList<>();\n-      for (List<RingRange> split : splits) {\n-        List<String> queries = new ArrayList<>();\n-        for (RingRange range : split) {\n-          if (range.isWrapping()) {\n-            // A wrapping range is one that overlaps from the end of the partitioner range and its\n-            // start (ie : when the start token of the split is greater than the end token)\n-            // We need to generate two queries here : one that goes from the start token to the end\n-            // of\n-            // the partitioner range, and the other from the start of the partitioner range to the\n-            // end token of the split.\n-            queries.add(generateRangeQuery(spec, partitionKey, range.getStart(), null));\n-            // Generation of the second query of the wrapping range\n-            queries.add(generateRangeQuery(spec, partitionKey, null, range.getEnd()));\n-          } else {\n-            queries.add(generateRangeQuery(spec, partitionKey, range.getStart(), range.getEnd()));\n-          }\n-        }\n-        sources.add(new CassandraIO.CassandraSource<>(spec, queries, estimatedSize));\n-      }\n-      return sources;\n-    }\n-\n-    private static String generateRangeQuery(\n-        Read spec, String partitionKey, BigInteger rangeStart, BigInteger rangeEnd) {\n-      final String rangeFilter =\n-          Joiner.on(\" AND \")\n-              .skipNulls()\n-              .join(\n-                  rangeStart == null\n-                      ? null\n-                      : String.format(\"(token(%s) >= %d)\", partitionKey, rangeStart),\n-                  rangeEnd == null\n-                      ? null\n-                      : String.format(\"(token(%s) < %d)\", partitionKey, rangeEnd));\n-      final String query =\n-          (spec.query() == null)\n-              ? buildQuery(spec) + \" WHERE \" + rangeFilter\n-              : buildQuery(spec) + \" AND \" + rangeFilter;\n-      LOG.debug(\"CassandraIO generated query : {}\", query);\n-      return query;\n-    }\n-\n-    private static long getNumSplits(\n-        long desiredBundleSizeBytes,\n-        long estimatedSizeBytes,\n-        @Nullable ValueProvider<Integer> minNumberOfSplits) {\n-      long numSplits =\n-          desiredBundleSizeBytes > 0 ? (estimatedSizeBytes / desiredBundleSizeBytes) : 1;\n-      if (numSplits <= 0) {\n-        LOG.warn(\"Number of splits is less than 0 ({}), fallback to 1\", numSplits);\n-        numSplits = 1;\n-      }\n-      return minNumberOfSplits != null ? Math.max(numSplits, minNumberOfSplits.get()) : numSplits;\n-    }\n-\n-    /**\n-     * Returns cached estimate for split or if missing calculate size for whole table. Highly\n-     * innacurate if query is specified.\n-     *\n-     * @param pipelineOptions\n-     * @return\n-     */\n-    @Override\n-    public long getEstimatedSizeBytes(PipelineOptions pipelineOptions) {\n-      if (estimatedSize != null) {\n-        return estimatedSize;\n-      } else {\n-        try (Cluster cluster =\n-            getCluster(\n-                spec.hosts(),\n-                spec.port(),\n-                spec.username(),\n-                spec.password(),\n-                spec.localDc(),\n-                spec.consistencyLevel(),\n-                spec.connectTimeout(),\n-                spec.readTimeout())) {\n-          if (isMurmur3Partitioner(cluster)) {\n-            try {\n-              List<TokenRange> tokenRanges =\n-                  getTokenRanges(cluster, spec.keyspace().get(), spec.table().get());\n-              this.estimatedSize = getEstimatedSizeBytesFromTokenRanges(tokenRanges);\n-              return this.estimatedSize;\n-            } catch (Exception e) {\n-              LOG.warn(\"Can't estimate the size\", e);\n-              return 0L;\n-            }\n-          } else {\n-            LOG.warn(\"Only Murmur3 partitioner is supported, can't estimate the size\");\n-            return 0L;\n-          }\n-        }\n-      }\n-    }\n-\n-    @VisibleForTesting\n-    static long getEstimatedSizeBytesFromTokenRanges(List<TokenRange> tokenRanges) {\n-      long size = 0L;\n-      for (TokenRange tokenRange : tokenRanges) {\n-        size = size + tokenRange.meanPartitionSize * tokenRange.partitionCount;\n-      }\n-      return Math.round(size / getRingFraction(tokenRanges));\n-    }\n-\n-    @Override\n-    public void populateDisplayData(DisplayData.Builder builder) {\n-      super.populateDisplayData(builder);\n-      if (spec.hosts() != null) {\n-        builder.add(DisplayData.item(\"hosts\", spec.hosts().toString()));\n-      }\n-      if (spec.port() != null) {\n-        builder.add(DisplayData.item(\"port\", spec.port()));\n-      }\n-      builder.addIfNotNull(DisplayData.item(\"keyspace\", spec.keyspace()));\n-      builder.addIfNotNull(DisplayData.item(\"table\", spec.table()));\n-      builder.addIfNotNull(DisplayData.item(\"username\", spec.username()));\n-      builder.addIfNotNull(DisplayData.item(\"localDc\", spec.localDc()));\n-      builder.addIfNotNull(DisplayData.item(\"consistencyLevel\", spec.consistencyLevel()));\n-    }\n-    // ------------- CASSANDRA SOURCE UTIL METHODS ---------------//\n-\n-    /**\n-     * Gets the list of token ranges that a table occupies on a give Cassandra node.\n-     *\n-     * <p>NB: This method is compatible with Cassandra 2.1.5 and greater.\n-     */\n-    private static List<TokenRange> getTokenRanges(Cluster cluster, String keyspace, String table) {\n-      try (Session session = cluster.newSession()) {\n-        ResultSet resultSet =\n-            session.execute(\n-                \"SELECT range_start, range_end, partitions_count, mean_partition_size FROM \"\n-                    + \"system.size_estimates WHERE keyspace_name = ? AND table_name = ?\",\n-                keyspace,\n-                table);\n-\n-        ArrayList<TokenRange> tokenRanges = new ArrayList<>();\n-        for (Row row : resultSet) {\n-          TokenRange tokenRange =\n-              new TokenRange(\n-                  row.getLong(\"partitions_count\"),\n-                  row.getLong(\"mean_partition_size\"),\n-                  new BigInteger(row.getString(\"range_start\")),\n-                  new BigInteger(row.getString(\"range_end\")));\n-          tokenRanges.add(tokenRange);\n-        }\n-        // The table may not contain the estimates yet\n-        // or have partitions_count and mean_partition_size fields = 0\n-        // if the data was just inserted and the amount of data in the table was small.\n-        // This is very common situation during tests,\n-        // when we insert a few rows and immediately query them.\n-        // However, for tiny data sets the lack of size estimates is not a problem at all,\n-        // because we don't want to split tiny data anyways.\n-        // Therefore, we're not issuing a warning if the result set was empty\n-        // or mean_partition_size and partitions_count = 0.\n-        return tokenRanges;\n-      }\n-    }\n-\n-    /** Compute the percentage of token addressed compared with the whole tokens in the cluster. */\n-    @VisibleForTesting\n-    static double getRingFraction(List<TokenRange> tokenRanges) {\n-      double ringFraction = 0;\n-      for (TokenRange tokenRange : tokenRanges) {\n-        ringFraction =\n-            ringFraction\n-                + (distance(tokenRange.rangeStart, tokenRange.rangeEnd).doubleValue()\n-                    / SplitGenerator.getRangeSize(MURMUR3PARTITIONER).doubleValue());\n-      }\n-      return ringFraction;\n-    }\n-\n-    /**\n-     * Check if the current partitioner is the Murmur3 (default in Cassandra version newer than 2).\n-     */\n-    @VisibleForTesting\n-    static boolean isMurmur3Partitioner(Cluster cluster) {\n-      return MURMUR3PARTITIONER.equals(cluster.getMetadata().getPartitioner());\n-    }\n-\n-    /** Measure distance between two tokens. */\n-    @VisibleForTesting\n-    static BigInteger distance(BigInteger left, BigInteger right) {\n-      return (right.compareTo(left) > 0)\n-          ? right.subtract(left)\n-          : right.subtract(left).add(SplitGenerator.getRangeSize(MURMUR3PARTITIONER));\n-    }\n-\n-    /**\n-     * Represent a token range in Cassandra instance, wrapping the partition count, size and token\n-     * range.\n-     */\n-    @VisibleForTesting\n-    static class TokenRange {\n-      private final long partitionCount;\n-      private final long meanPartitionSize;\n-      private final BigInteger rangeStart;\n-      private final BigInteger rangeEnd;\n-\n-      TokenRange(\n-          long partitionCount, long meanPartitionSize, BigInteger rangeStart, BigInteger rangeEnd) {\n-        this.partitionCount = partitionCount;\n-        this.meanPartitionSize = meanPartitionSize;\n-        this.rangeStart = rangeStart;\n-        this.rangeEnd = rangeEnd;\n-      }\n-    }\n-\n-    private class CassandraReader extends BoundedSource.BoundedReader<T> {\n-      private final CassandraIO.CassandraSource<T> source;\n-      private Cluster cluster;\n-      private Session session;\n-      private Iterator<T> iterator;\n-      private T current;\n-\n-      CassandraReader(CassandraSource<T> source) {\n-        this.source = source;\n-      }\n-\n-      @Override\n-      public boolean start() {\n-        LOG.debug(\"Starting Cassandra reader\");\n-        cluster =\n-            getCluster(\n-                source.spec.hosts(),\n-                source.spec.port(),\n-                source.spec.username(),\n-                source.spec.password(),\n-                source.spec.localDc(),\n-                source.spec.consistencyLevel(),\n-                source.spec.connectTimeout(),\n-                source.spec.readTimeout());\n-        session = cluster.connect(source.spec.keyspace().get());\n-        LOG.debug(\"Queries: \" + source.splitQueries);\n-        List<ResultSetFuture> futures = new ArrayList<>();\n-        for (String query : source.splitQueries) {\n-          futures.add(session.executeAsync(query));\n-        }\n-\n-        final Mapper<T> mapper = getMapper(session, source.spec.entity());\n-\n-        for (ResultSetFuture result : futures) {\n-          if (iterator == null) {\n-            iterator = mapper.map(result.getUninterruptibly());\n-          } else {\n-            iterator = Iterators.concat(iterator, mapper.map(result.getUninterruptibly()));\n-          }\n-        }\n-\n-        return advance();\n-      }\n-\n-      @Override\n-      public boolean advance() {\n-        if (iterator.hasNext()) {\n-          current = iterator.next();\n-          return true;\n-        }\n-        current = null;\n-        return false;\n-      }\n-\n-      @Override\n-      public void close() {\n-        LOG.debug(\"Closing Cassandra reader\");\n-        if (session != null) {\n-          session.close();\n-        }\n-        if (cluster != null) {\n-          cluster.close();\n-        }\n-      }\n-\n-      @Override\n-      public T getCurrent() throws NoSuchElementException {\n-        if (current == null) {\n-          throw new NoSuchElementException();\n-        }\n-        return current;\n-      }\n-\n-      @Override\n-      public CassandraIO.CassandraSource<T> getCurrentSource() {\n-        return source;\n-      }\n-\n-      private Mapper<T> getMapper(Session session, Class<T> enitity) {\n-        return source.spec.mapperFactoryFn().apply(session);\n-      }\n-    }\n-  }\n-\n   /** Specify the mutation type: either write or delete. */\n   public enum MutationType {\n     WRITE,\n     DELETE\n   }\n \n+  /**\n+   * Check if the current partitioner is the Murmur3 (default in Cassandra version newer than 2).\n+   */\n+  @VisibleForTesting\n+  static boolean isMurmur3Partitioner(Cluster cluster) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 632}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyNjI0Nw==", "bodyText": "Right not so beautiful but well we have to live with this \ud83d\udc4d", "url": "https://github.com/apache/beam/pull/10546#discussion_r504226247", "createdAt": "2020-10-13T20:10:46Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1170,4 +898,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE4MDk3Mg=="}, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 678}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzMzI2OQ==", "bodyText": "Why we have a different getCluster method than the one for the Read, also why were socket options removed there? I think we can maybe move this one outside to a sort of CassandraUtils package, but that's optional, however the use of the same method should not be changed by this PR if there is not a strong reason to do so.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504233269", "createdAt": "2020-10-13T20:20:42Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -143,6 +148,36 @@ private CassandraIO() {}\n     return Write.<T>builder(MutationType.DELETE).build();\n   }\n \n+  /** Get a Cassandra cluster using hosts and port. */\n+  static Cluster getCluster(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNjczNA==", "bodyText": "What is the take on this? More concretely I really would like we could test corner cases of splitting like we had in the removed test methods like testEstimatedSizeBytesFromTokenRanges and the others.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504236734", "createdAt": "2020-10-13T20:24:18Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -370,384 +488,16 @@ private CassandraIO() {}\n         return autoBuild();\n       }\n     }\n-  }\n-\n-  @VisibleForTesting\n-  static class CassandraSource<T> extends BoundedSource<T> {\n-    final Read<T> spec;\n-    final List<String> splitQueries;\n-    // split source ached size - can't be calculated when already split\n-    Long estimatedSize;\n-    private static final String MURMUR3PARTITIONER = \"org.apache.cassandra.dht.Murmur3Partitioner\";\n-\n-    CassandraSource(Read<T> spec, List<String> splitQueries) {\n-      this(spec, splitQueries, null);\n-    }\n-\n-    private CassandraSource(Read<T> spec, List<String> splitQueries, Long estimatedSize) {\n-      this.estimatedSize = estimatedSize;\n-      this.spec = spec;\n-      this.splitQueries = splitQueries;\n-    }\n-\n-    @Override\n-    public Coder<T> getOutputCoder() {\n-      return spec.coder();\n-    }\n-\n-    @Override\n-    public BoundedReader<T> createReader(PipelineOptions pipelineOptions) {\n-      return new CassandraReader(this);\n-    }\n-\n-    @Override\n-    public List<BoundedSource<T>> split(\n-        long desiredBundleSizeBytes, PipelineOptions pipelineOptions) {\n-      try (Cluster cluster =\n-          getCluster(\n-              spec.hosts(),\n-              spec.port(),\n-              spec.username(),\n-              spec.password(),\n-              spec.localDc(),\n-              spec.consistencyLevel())) {\n-        if (isMurmur3Partitioner(cluster)) {\n-          LOG.info(\"Murmur3Partitioner detected, splitting\");\n-          return splitWithTokenRanges(\n-              spec, desiredBundleSizeBytes, getEstimatedSizeBytes(pipelineOptions), cluster);\n-        } else {\n-          LOG.warn(\n-              \"Only Murmur3Partitioner is supported for splitting, using a unique source for \"\n-                  + \"the read\");\n-          return Collections.singletonList(\n-              new CassandraIO.CassandraSource<>(spec, Collections.singletonList(buildQuery(spec))));\n-        }\n-      }\n-    }\n-\n-    private static String buildQuery(Read spec) {\n-      return (spec.query() == null)\n-          ? String.format(\"SELECT * FROM %s.%s\", spec.keyspace().get(), spec.table().get())\n-          : spec.query().get().toString();\n-    }\n-\n-    /**\n-     * Compute the number of splits based on the estimated size and the desired bundle size, and\n-     * create several sources.\n-     */\n-    private List<BoundedSource<T>> splitWithTokenRanges(\n-        CassandraIO.Read<T> spec,\n-        long desiredBundleSizeBytes,\n-        long estimatedSizeBytes,\n-        Cluster cluster) {\n-      long numSplits =\n-          getNumSplits(desiredBundleSizeBytes, estimatedSizeBytes, spec.minNumberOfSplits());\n-      LOG.info(\"Number of desired splits is {}\", numSplits);\n-\n-      SplitGenerator splitGenerator = new SplitGenerator(cluster.getMetadata().getPartitioner());\n-      List<BigInteger> tokens =\n-          cluster.getMetadata().getTokenRanges().stream()\n-              .map(tokenRange -> new BigInteger(tokenRange.getEnd().getValue().toString()))\n-              .collect(Collectors.toList());\n-      List<List<RingRange>> splits = splitGenerator.generateSplits(numSplits, tokens);\n-      LOG.info(\"{} splits were actually generated\", splits.size());\n-\n-      final String partitionKey =\n-          cluster.getMetadata().getKeyspace(spec.keyspace().get()).getTable(spec.table().get())\n-              .getPartitionKey().stream()\n-              .map(ColumnMetadata::getName)\n-              .collect(Collectors.joining(\",\"));\n-\n-      List<TokenRange> tokenRanges =\n-          getTokenRanges(cluster, spec.keyspace().get(), spec.table().get());\n-      final long estimatedSize = getEstimatedSizeBytesFromTokenRanges(tokenRanges) / splits.size();\n-\n-      List<BoundedSource<T>> sources = new ArrayList<>();\n-      for (List<RingRange> split : splits) {\n-        List<String> queries = new ArrayList<>();\n-        for (RingRange range : split) {\n-          if (range.isWrapping()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NTkwMw=="}, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 323}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzOTY4Ng==", "bodyText": "Any reason not to do this? It looks like a convenience method that if you depend in other place (outside of Beam) you can easily move there.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504239686", "createdAt": "2020-10-13T20:29:52Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/RingRange.java", "diffHunk": "@@ -55,4 +58,9 @@ public boolean isWrapping() {\n   public String toString() {\n     return String.format(\"(%s,%s]\", start.toString(), end.toString());\n   }\n+\n+  public static RingRange fromEncodedKey(Metadata metadata, ByteBuffer... bb) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg5NjM3Mg=="}, "originalCommit": {"oid": "a116abe3c944420c007e6d4afc914bf29267ae8d"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI0MjcxNQ==", "bodyText": "private static class SplitFn<T>", "url": "https://github.com/apache/beam/pull/10546#discussion_r504242715", "createdAt": "2020-10-13T20:35:32Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -373,7 +419,86 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.readAll();\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(\"Split\", ParDo.of(new SplitFn()))\n+          .setCoder(SerializableCoder.of(new TypeDescriptor<Read<T>>() {}))\n+          .apply(\"ReadAll\", readAll.withCoder(this.coder()));\n+    }\n+\n+    private class SplitFn extends DoFn<Read<T>, Read<T>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI1ODY5NA==", "bodyText": "Move this up before the withCoder method", "url": "https://github.com/apache/beam/pull/10546#discussion_r504258694", "createdAt": "2020-10-13T21:07:02Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1281,4 +1032,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();\n+\n+    /** Specify the {@link Coder} used to serialize the entity in the {@link PCollection}. */\n+    public ReadAll<T> withCoder(Coder<T> coder) {\n+      checkArgument(coder != null, \"coder can not be null\");\n+      return builder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PCollection<Read<T>> input) {\n+      checkArgument(coder() != null, \"withCoder() is required\");\n+      return input\n+          .apply(\"Reshuffle\", Reshuffle.viaRandomKey())\n+          .apply(\"Read\", ParDo.of(new ReadFn<>()))\n+          .setCoder(this.coder());\n+    }\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 672}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI2MTQ0Nw==", "bodyText": "Remove ReadAll.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504261447", "createdAt": "2020-10-13T21:12:34Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -1281,4 +1032,44 @@ private void waitForFuturesToFinish() throws ExecutionException, InterruptedExce\n       }\n     }\n   }\n+\n+  /**\n+   * A {@link PTransform} to read data from Apache Cassandra. See {@link CassandraIO} for more\n+   * information on usage and configuration.\n+   */\n+  @AutoValue\n+  public abstract static class ReadAll<T> extends PTransform<PCollection<Read<T>>, PCollection<T>> {\n+\n+    @Nullable\n+    abstract Coder<T> coder();\n+\n+    abstract ReadAll.Builder<T> builder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 654}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI3MzkxNg==", "bodyText": "Can you please move this Split into the ReadAll expand method. When not specified RingRanges like in the read() case we should split them on the ReadAll expansion. Of course if RingRanges are specified we will probably ignore recalculating them.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504273916", "createdAt": "2020-10-13T21:38:41Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/CassandraIO.java", "diffHunk": "@@ -373,7 +419,86 @@ private CassandraIO() {}\n       checkArgument(entity() != null, \"withEntity() is required\");\n       checkArgument(coder() != null, \"withCoder() is required\");\n \n-      return input.apply(org.apache.beam.sdk.io.Read.from(new CassandraSource<>(this, null)));\n+      ReadAll<T> readAll = CassandraIO.readAll();\n+\n+      return input\n+          .apply(Create.of(this))\n+          .apply(\"Split\", ParDo.of(new SplitFn()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI3NTE1MQ==", "bodyText": "This is still a bit messy, don't you think we can get something similar by using something like connection pooling and delegate this complexity to the driver (where it should reside)?\nhttps://docs.datastax.com/en/developer/java-driver/2.1/manual/pooling/\nNotice that I am not familiar with this but after a quicklook seems to be worth the look.", "url": "https://github.com/apache/beam/pull/10546#discussion_r504275151", "createdAt": "2020-10-13T21:41:17Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/ReadFn.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.io.cassandra.CassandraIO.Read;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class ReadFn<T> extends DoFn<Read<T>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFn.class);\n+\n+  private transient Cluster cluster;\n+\n+  private transient Session session;\n+\n+  private transient Read<T> lastRead;\n+\n+  @ProcessElement\n+  public void processElement(@Element Read<T> read, OutputReceiver<T> receiver) {\n+    Session session = getSession(read);\n+    Mapper<T> mapper = read.mapperFactoryFn().apply(session);\n+    String partitionKey =\n+        cluster.getMetadata().getKeyspace(read.keyspace().get()).getTable(read.table().get())\n+            .getPartitionKey().stream()\n+            .map(ColumnMetadata::getName)\n+            .collect(Collectors.joining(\",\"));\n+\n+    String query = generateRangeQuery(read, partitionKey, read.ringRanges() != null);\n+    PreparedStatement preparedStatement = session.prepare(query);\n+    Set<RingRange> ringRanges =\n+        read.ringRanges() == null ? Collections.<RingRange>emptySet() : read.ringRanges().get();\n+\n+    for (RingRange rr : ringRanges) {\n+      Token startToken = cluster.getMetadata().newToken(rr.getStart().toString());\n+      Token endToken = cluster.getMetadata().newToken(rr.getEnd().toString());\n+      ResultSet rs =\n+          session.execute(preparedStatement.bind().setToken(0, startToken).setToken(1, endToken));\n+      Iterator<T> iter = mapper.map(rs);\n+      while (iter.hasNext()) {\n+        T n = iter.next();\n+        receiver.output(n);\n+      }\n+    }\n+\n+    if (read.ringRanges() == null) {\n+      ResultSet rs = session.execute(preparedStatement.bind());\n+      Iterator<T> iter = mapper.map(rs);\n+      while (iter.hasNext()) {\n+        receiver.output(iter.next());\n+      }\n+    }\n+  }\n+\n+  @Teardown\n+  public void teardown() {\n+    if (session != null) {\n+      this.session.close();\n+    }\n+    if (cluster != null) {\n+      this.cluster.close();\n+    }\n+  }\n+\n+  private Session getSession(Read<T> read) {\n+    if (cluster == null || !reuseCluster(this.lastRead, read)) {\n+      this.cluster =\n+          CassandraIO.getCluster(\n+              read.hosts(),\n+              read.port(),\n+              read.username(),\n+              read.password(),\n+              read.localDc(),\n+              read.consistencyLevel());\n+    }\n+    if (session == null || !reuseSession(lastRead, read)) {\n+      this.session = this.cluster.connect(read.keyspace().get());\n+    }\n+    this.lastRead = read;\n+    return this.session;\n+  }\n+\n+  private static <T> boolean reuseCluster(Read<T> readA, Read<T> readB) {\n+    return readA != null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI3NzU1MQ==", "bodyText": "Remove if unused", "url": "https://github.com/apache/beam/pull/10546#discussion_r504277551", "createdAt": "2020-10-13T21:46:52Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/cassandra/src/main/java/org/apache/beam/sdk/io/cassandra/ReadFn.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.cassandra;\n+\n+import com.datastax.driver.core.Cluster;\n+import com.datastax.driver.core.ColumnMetadata;\n+import com.datastax.driver.core.PreparedStatement;\n+import com.datastax.driver.core.ResultSet;\n+import com.datastax.driver.core.Session;\n+import com.datastax.driver.core.Token;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.beam.sdk.io.cassandra.CassandraIO.Read;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class ReadFn<T> extends DoFn<Read<T>, T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFn.class);\n+\n+  private transient Cluster cluster;\n+\n+  private transient Session session;\n+\n+  private transient Read<T> lastRead;\n+\n+  @ProcessElement\n+  public void processElement(@Element Read<T> read, OutputReceiver<T> receiver) {\n+    Session session = getSession(read);\n+    Mapper<T> mapper = read.mapperFactoryFn().apply(session);\n+    String partitionKey =\n+        cluster.getMetadata().getKeyspace(read.keyspace().get()).getTable(read.table().get())\n+            .getPartitionKey().stream()\n+            .map(ColumnMetadata::getName)\n+            .collect(Collectors.joining(\",\"));\n+\n+    String query = generateRangeQuery(read, partitionKey, read.ringRanges() != null);\n+    PreparedStatement preparedStatement = session.prepare(query);\n+    Set<RingRange> ringRanges =\n+        read.ringRanges() == null ? Collections.<RingRange>emptySet() : read.ringRanges().get();\n+\n+    for (RingRange rr : ringRanges) {\n+      Token startToken = cluster.getMetadata().newToken(rr.getStart().toString());\n+      Token endToken = cluster.getMetadata().newToken(rr.getEnd().toString());\n+      ResultSet rs =\n+          session.execute(preparedStatement.bind().setToken(0, startToken).setToken(1, endToken));\n+      Iterator<T> iter = mapper.map(rs);\n+      while (iter.hasNext()) {\n+        T n = iter.next();\n+        receiver.output(n);\n+      }\n+    }\n+\n+    if (read.ringRanges() == null) {\n+      ResultSet rs = session.execute(preparedStatement.bind());\n+      Iterator<T> iter = mapper.map(rs);\n+      while (iter.hasNext()) {\n+        receiver.output(iter.next());\n+      }\n+    }\n+  }\n+\n+  @Teardown\n+  public void teardown() {\n+    if (session != null) {\n+      this.session.close();\n+    }\n+    if (cluster != null) {\n+      this.cluster.close();\n+    }\n+  }\n+\n+  private Session getSession(Read<T> read) {\n+    if (cluster == null || !reuseCluster(this.lastRead, read)) {\n+      this.cluster =\n+          CassandraIO.getCluster(\n+              read.hosts(),\n+              read.port(),\n+              read.username(),\n+              read.password(),\n+              read.localDc(),\n+              read.consistencyLevel());\n+    }\n+    if (session == null || !reuseSession(lastRead, read)) {\n+      this.session = this.cluster.connect(read.keyspace().get());\n+    }\n+    this.lastRead = read;\n+    return this.session;\n+  }\n+\n+  private static <T> boolean reuseCluster(Read<T> readA, Read<T> readB) {\n+    return readA != null\n+        && readA.hosts().get().equals(readB.hosts().get())\n+        && readA.port().get().equals(readB.port().get())\n+        && ((readA.username() != null && readA.username().equals(readB.username()))\n+            || (readA.username() == null && readB.username() == null))\n+        && ((readA.consistencyLevel() != null\n+                && readA.consistencyLevel().equals(readB.consistencyLevel()))\n+            || (readA.consistencyLevel() == null && readB.consistencyLevel() == null))\n+        && ((readA.localDc() != null && readA.localDc().equals(readB.consistencyLevel()))\n+            || (readA.localDc() == null && readB.localDc() == null));\n+  }\n+\n+  // TODO: Unit test\n+  private static <T> boolean reuseSession(Read<T> readA, Read<T> readB) {\n+    return (readA.keyspace() != null && readA.keyspace().equals(readB.keyspace()))\n+        || (readA.keyspace() == null && readB.keyspace() == null);\n+  }\n+\n+  /*\n+  private static String generateRangeQuery(Read<?> spec, String partitionKey) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3377d98ca9ed9988a866f3671fe1106fbb437bd"}, "originalPosition": 130}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "41dd82fae07af9b5fcafd75217af7bebbacde368", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/41dd82fae07af9b5fcafd75217af7bebbacde368", "committedDate": "2020-11-05T16:53:27Z", "message": "remove connection caching, will address in a separate PR"}, "afterCommit": {"oid": "8bb91d2b0fca16c5eae74773e247d9eab43a9a5f", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/8bb91d2b0fca16c5eae74773e247d9eab43a9a5f", "committedDate": "2021-02-16T17:46:45Z", "message": "moving split back to handle issue with query/ringranges"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e383ecbb4341394def66a6f729a40e8226f868e3", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/e383ecbb4341394def66a6f729a40e8226f868e3", "committedDate": "2021-06-23T00:26:38Z", "message": "connection sharing"}, "afterCommit": {"oid": "cf749f1b408894f5056821e52c1ee3fc9125f9cf", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/cf749f1b408894f5056821e52c1ee3fc9125f9cf", "committedDate": "2021-06-23T00:30:28Z", "message": "connection sharing"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzM2MjY3Mjc5", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-736267279", "createdAt": "2021-08-23T15:31:01Z", "commit": {"oid": "cf749f1b408894f5056821e52c1ee3fc9125f9cf"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e72e4d12094aa9fa8d80294b4f9d1617d50a13b3", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/e72e4d12094aa9fa8d80294b4f9d1617d50a13b3", "committedDate": "2021-08-25T05:16:44Z", "message": "flattening ring range splits returned by the split generator"}, "afterCommit": {"oid": "53e2f5efeff684842ae350e797a15e7ed7cdfc86", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/53e2f5efeff684842ae350e797a15e7ed7cdfc86", "committedDate": "2021-09-02T05:05:39Z", "message": "[BEAM-9008] adds CassandraIO.readAll"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e12fc33e55e23db9f2aee330039d16dace34f9aa", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/e12fc33e55e23db9f2aee330039d16dace34f9aa", "committedDate": "2021-09-07T22:31:14Z", "message": "[BEAM-9008] adds CassandraIO.readAll"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "53e2f5efeff684842ae350e797a15e7ed7cdfc86", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/53e2f5efeff684842ae350e797a15e7ed7cdfc86", "committedDate": "2021-09-02T05:05:39Z", "message": "[BEAM-9008] adds CassandraIO.readAll"}, "afterCommit": {"oid": "e12fc33e55e23db9f2aee330039d16dace34f9aa", "author": {"user": {"login": "vmarquez", "name": "Vincent Marquez"}}, "url": "https://github.com/apache/beam/commit/e12fc33e55e23db9f2aee330039d16dace34f9aa", "committedDate": "2021-09-07T22:31:14Z", "message": "[BEAM-9008] adds CassandraIO.readAll"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NzQ5Mjk1NjQ3", "url": "https://github.com/apache/beam/pull/10546#pullrequestreview-749295647", "createdAt": "2021-09-08T15:24:40Z", "commit": {"oid": "e12fc33e55e23db9f2aee330039d16dace34f9aa"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3706, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}