{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2NDk4NzM1", "number": 12202, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMzowNjozOFrOEMrwng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTo0ODo0MlrOEQr6fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzM1MzI2OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQyMzowNjozOFrOGu8IPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToxMDoyN1rOGveSBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg3MjgzMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            @AutoService(TableProvider.class)\n          \n          \n            \n            public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider {\n          \n          \n            \n            abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider {\n          \n      \n    \n    \n  \n\nI think this AutoService annotation is what's causing the Java PreCommit to fail. The AutoService annotation makes it so that a call ServiceLoader.load(TableProvider.class) will try to instantiate this class if it's in the classpath, and it's not possible to instantiate this since its abstract.\nSpecifically this is the ServiceLoader call that's biting you:\n\n  \n    \n      beam/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/BeamCalciteSchemaFactory.java\n    \n    \n        Lines 85 to 86\n      in\n      1a260cd\n    \n    \n    \n    \n\n        \n          \n           for (TableProvider provider : \n        \n\n        \n          \n               ServiceLoader.load(TableProvider.class, getClass().getClassLoader())) { \n        \n    \n  \n\n\nI think we should also make this package-private", "url": "https://github.com/apache/beam/pull/12202#discussion_r451872831", "createdAt": "2020-07-08T23:06:38Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.auto.service.AutoService;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #schemaCapableIOProvider} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+@AutoService(TableProvider.class)\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjMxNDg5Mw==", "bodyText": "The TableProvider stub classes for each IO are within IO specific packages. Since \"subpackages\" don't have access to package-private parent classes (as the tableproviderwrapper and the IO stub are within different packages), going package-private with the tableproviderwrapper doesn't seem to give access to the IO table provider stubs.", "url": "https://github.com/apache/beam/pull/12202#discussion_r452314893", "createdAt": "2020-07-09T15:44:47Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.auto.service.AutoService;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #schemaCapableIOProvider} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+@AutoService(TableProvider.class)\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg3MjgzMQ=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMjM5MA==", "bodyText": "Ah whoops! Yeah I guess the best we can do is mark it @Internal then.", "url": "https://github.com/apache/beam/pull/12202#discussion_r452432390", "createdAt": "2020-07-09T19:10:27Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.auto.service.AutoService;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #schemaCapableIOProvider} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+@AutoService(TableProvider.class)\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg3MjgzMQ=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzQ2Njk4OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDowMzowNlrOGu9K3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDowMzowNlrOGu9K3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg4OTg4Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n          \n          \n            \n            class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n          \n      \n    \n    \n  \n\nI think this can be package-private. It might also make sense to make it an inner class of SchemaIOTableProviderWrapper, but I'll leave that up to you", "url": "https://github.com/apache/beam/pull/12202#discussion_r451889886", "createdAt": "2020-07-09T00:03:06Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUwMzUxOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyMjowN1rOGu9f-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyMjowN1rOGu9f-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTI5MQ==", "bodyText": "Hmm this is actually something that will need to be different for each IO. Parquet and Avro are both bounded data sources, while pubsub is unbounded.\nCan you add this to the SchemaIO interface and plumb it through here?", "url": "https://github.com/apache/beam/pull/12202#discussion_r451895291", "createdAt": "2020-07-09T00:22:07Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUwNTcxOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyMzoyMlrOGu9hTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQxOToyMToxNFrOGveoNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTYyOA==", "bodyText": "This will also need to be different for avro/parquet vs. pubsub. It could just be determined from the same method on SchemaIO", "url": "https://github.com/apache/beam/pull/12202#discussion_r451895628", "createdAt": "2020-07-09T00:23:22Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }\n \n-    return begin\n-        .apply(\"ParquetIORead\", ParquetIO.read(AvroUtils.toAvroSchema(schema)).from(filePattern))\n-        .apply(\"GenericRecordToRow\", readConverter);\n+  @Override\n+  public Schema getSchema() {\n+    return schemaIO.schema();\n   }\n \n   @Override\n-  public PDone buildIOWriter(PCollection<Row> input) {\n-    throw new UnsupportedOperationException(\"Writing to a Parquet file is not supported\");\n+  public PCollection<Row> buildIOReader(PBegin begin) {\n+    PTransform<PBegin, PCollection<Row>> readerTransform = schemaIO.buildReader();\n+    return begin.apply(readerTransform);\n   }\n \n   @Override\n-  public PCollection.IsBounded isBounded() {\n-    return PCollection.IsBounded.BOUNDED;\n+  public POutput buildIOWriter(PCollection<Row> input) {\n+    PTransform<PCollection<Row>, POutput> writerTransform = schemaIO.buildWriter();\n+    return input.apply(writerTransform);\n   }\n \n   @Override\n   public BeamTableStatistics getTableStatistics(PipelineOptions options) {\n-    return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjMwODc0Ng==", "bodyText": "This relies on BeamTableStatistics which is not in the core directory. I could use schemaIO.isBounded() and instantiate the appropriate BeamTableStatistics within SchemaIOTableWrapper based on that value, which would work for pubsub/avro/parquet. But looking ahead, the getTableStatistics(..) method of other IOs such as seqgen and kafka rely on other methods within BeamTableStatistics. Do you think I should make a conversion class to BeamTableStatistics within core in anticipation of these issues or go ahead with using scheamIO.isBounded() regardless?", "url": "https://github.com/apache/beam/pull/12202#discussion_r452308746", "createdAt": "2020-07-09T15:35:52Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }\n \n-    return begin\n-        .apply(\"ParquetIORead\", ParquetIO.read(AvroUtils.toAvroSchema(schema)).from(filePattern))\n-        .apply(\"GenericRecordToRow\", readConverter);\n+  @Override\n+  public Schema getSchema() {\n+    return schemaIO.schema();\n   }\n \n   @Override\n-  public PDone buildIOWriter(PCollection<Row> input) {\n-    throw new UnsupportedOperationException(\"Writing to a Parquet file is not supported\");\n+  public PCollection<Row> buildIOReader(PBegin begin) {\n+    PTransform<PBegin, PCollection<Row>> readerTransform = schemaIO.buildReader();\n+    return begin.apply(readerTransform);\n   }\n \n   @Override\n-  public PCollection.IsBounded isBounded() {\n-    return PCollection.IsBounded.BOUNDED;\n+  public POutput buildIOWriter(PCollection<Row> input) {\n+    PTransform<PCollection<Row>, POutput> writerTransform = schemaIO.buildWriter();\n+    return input.apply(writerTransform);\n   }\n \n   @Override\n   public BeamTableStatistics getTableStatistics(PipelineOptions options) {\n-    return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTYyOA=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjMyNjE4MA==", "bodyText": "Great question. We should definitely keep the BeamTableStatistics in the SQL extensions for now, since it only has meaning for SQL.\nThe reason this exists is so we can do better when optimizing SQL queries - it's for \"cost-based optimization\" - here's a design doc about our implementation of it. The idea is we can be smarter with how we decide to perform a SQL query if we know how \"big\"  each data source is (or how \"fast\" in the case of an unbounded source). In theory this could be useful information in core Beam as well but we're a long way from doing that kind of optimization there.\nI think what we should do is have a default implementation that returns either UNBOUNDED_UNKNOWN or BOUNDED_UNKNOWN based on what SchemaIO tells us it is (which will be sufficient for avro/parquet/pubsub), and then other table providers can override it if they need to.", "url": "https://github.com/apache/beam/pull/12202#discussion_r452326180", "createdAt": "2020-07-09T16:02:11Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }\n \n-    return begin\n-        .apply(\"ParquetIORead\", ParquetIO.read(AvroUtils.toAvroSchema(schema)).from(filePattern))\n-        .apply(\"GenericRecordToRow\", readConverter);\n+  @Override\n+  public Schema getSchema() {\n+    return schemaIO.schema();\n   }\n \n   @Override\n-  public PDone buildIOWriter(PCollection<Row> input) {\n-    throw new UnsupportedOperationException(\"Writing to a Parquet file is not supported\");\n+  public PCollection<Row> buildIOReader(PBegin begin) {\n+    PTransform<PBegin, PCollection<Row>> readerTransform = schemaIO.buildReader();\n+    return begin.apply(readerTransform);\n   }\n \n   @Override\n-  public PCollection.IsBounded isBounded() {\n-    return PCollection.IsBounded.BOUNDED;\n+  public POutput buildIOWriter(PCollection<Row> input) {\n+    PTransform<PCollection<Row>, POutput> writerTransform = schemaIO.buildWriter();\n+    return input.apply(writerTransform);\n   }\n \n   @Override\n   public BeamTableStatistics getTableStatistics(PipelineOptions options) {\n-    return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTYyOA=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjMyOTUxNw==", "bodyText": "This is a little tricky though since the method is on BeamSqlTable, and not on TableProvider. One possible solution: make SchemaIOTableWrapper an inner (non-static) class of SchemaIOTableProviderWrapper. Then the method with the default logic can be on SchemaIOTableProviderWrapper, and it's sub-classes can override it if they need to.", "url": "https://github.com/apache/beam/pull/12202#discussion_r452329517", "createdAt": "2020-07-09T16:07:43Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }\n \n-    return begin\n-        .apply(\"ParquetIORead\", ParquetIO.read(AvroUtils.toAvroSchema(schema)).from(filePattern))\n-        .apply(\"GenericRecordToRow\", readConverter);\n+  @Override\n+  public Schema getSchema() {\n+    return schemaIO.schema();\n   }\n \n   @Override\n-  public PDone buildIOWriter(PCollection<Row> input) {\n-    throw new UnsupportedOperationException(\"Writing to a Parquet file is not supported\");\n+  public PCollection<Row> buildIOReader(PBegin begin) {\n+    PTransform<PBegin, PCollection<Row>> readerTransform = schemaIO.buildReader();\n+    return begin.apply(readerTransform);\n   }\n \n   @Override\n-  public PCollection.IsBounded isBounded() {\n-    return PCollection.IsBounded.BOUNDED;\n+  public POutput buildIOWriter(PCollection<Row> input) {\n+    PTransform<PCollection<Row>, POutput> writerTransform = schemaIO.buildWriter();\n+    return input.apply(writerTransform);\n   }\n \n   @Override\n   public BeamTableStatistics getTableStatistics(PipelineOptions options) {\n-    return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTYyOA=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM0NjUyMA==", "bodyText": "If the default logic is within TableProviderWrapper rather than TableWrapper, wouldn't it make sense to shift the SchemaIO.isBounded() method to the SchemaCapableIOProvider interface instead? The default logic within the tableproviderwrapper would otherwise rely on SchemaIOTableWrapper.isBounded()", "url": "https://github.com/apache/beam/pull/12202#discussion_r452346520", "createdAt": "2020-07-09T16:35:17Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }\n \n-    return begin\n-        .apply(\"ParquetIORead\", ParquetIO.read(AvroUtils.toAvroSchema(schema)).from(filePattern))\n-        .apply(\"GenericRecordToRow\", readConverter);\n+  @Override\n+  public Schema getSchema() {\n+    return schemaIO.schema();\n   }\n \n   @Override\n-  public PDone buildIOWriter(PCollection<Row> input) {\n-    throw new UnsupportedOperationException(\"Writing to a Parquet file is not supported\");\n+  public PCollection<Row> buildIOReader(PBegin begin) {\n+    PTransform<PBegin, PCollection<Row>> readerTransform = schemaIO.buildReader();\n+    return begin.apply(readerTransform);\n   }\n \n   @Override\n-  public PCollection.IsBounded isBounded() {\n-    return PCollection.IsBounded.BOUNDED;\n+  public POutput buildIOWriter(PCollection<Row> input) {\n+    PTransform<PCollection<Row>, POutput> writerTransform = schemaIO.buildWriter();\n+    return input.apply(writerTransform);\n   }\n \n   @Override\n   public BeamTableStatistics getTableStatistics(PipelineOptions options) {\n-    return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTYyOA=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzODA2OA==", "bodyText": "Yeah we could just make it a property on SchemaCapableIOProvider for now.", "url": "https://github.com/apache/beam/pull/12202#discussion_r452438068", "createdAt": "2020-07-09T19:21:14Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaIOTableWrapper.java", "diffHunk": "@@ -15,54 +15,63 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n \n import java.io.Serializable;\n-import org.apache.avro.generic.GenericRecord;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.SchemaBaseBeamTable;\n-import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.schemas.Schema;\n-import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.Row;\n+@Internal\n+@Experimental\n+/**\n+ * A generalized {@link Table} for IOs to create IO readers and writers.\n+ */\n+public class SchemaIOTableWrapper extends BaseBeamTable implements Serializable {\n+  protected final SchemaIO schemaIO;\n \n-/** {@link ParquetTable} is a {@link BeamSqlTable}. */\n-public class ParquetTable extends SchemaBaseBeamTable implements Serializable {\n-  private final String filePattern;\n+  private SchemaIOTableWrapper(SchemaIO schemaIO) {\n+    this.schemaIO = schemaIO;\n+  }\n \n-  public ParquetTable(Schema beamSchema, String filePattern) {\n-    super(beamSchema);\n-    this.filePattern = filePattern;\n+  static SchemaIOTableWrapper fromSchemaIO(SchemaIO schemaIO) {\n+    return new SchemaIOTableWrapper(schemaIO);\n   }\n \n   @Override\n-  public PCollection<Row> buildIOReader(PBegin begin) {\n-    PTransform<PCollection<GenericRecord>, PCollection<Row>> readConverter =\n-        GenericRecordReadConverter.builder().beamSchema(schema).build();\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.UNBOUNDED;\n+  }\n \n-    return begin\n-        .apply(\"ParquetIORead\", ParquetIO.read(AvroUtils.toAvroSchema(schema)).from(filePattern))\n-        .apply(\"GenericRecordToRow\", readConverter);\n+  @Override\n+  public Schema getSchema() {\n+    return schemaIO.schema();\n   }\n \n   @Override\n-  public PDone buildIOWriter(PCollection<Row> input) {\n-    throw new UnsupportedOperationException(\"Writing to a Parquet file is not supported\");\n+  public PCollection<Row> buildIOReader(PBegin begin) {\n+    PTransform<PBegin, PCollection<Row>> readerTransform = schemaIO.buildReader();\n+    return begin.apply(readerTransform);\n   }\n \n   @Override\n-  public PCollection.IsBounded isBounded() {\n-    return PCollection.IsBounded.BOUNDED;\n+  public POutput buildIOWriter(PCollection<Row> input) {\n+    PTransform<PCollection<Row>, POutput> writerTransform = schemaIO.buildWriter();\n+    return input.apply(writerTransform);\n   }\n \n   @Override\n   public BeamTableStatistics getTableStatistics(PipelineOptions options) {\n-    return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NTYyOA=="}, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUxMjQzOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/avro/AvroTableProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyNjo1NlrOGu9k-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyNjo1NlrOGu9k-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NjU3MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            @Internal\n          \n          \n            \n            @Experimental\n          \n          \n            \n            public class AvroTableProvider extends SchemaCapableIOTableProviderWrapper {\n          \n          \n            \n            @AutoService(TableProvider.class)\n          \n          \n            \n            public class AvroTableProvider extends SchemaCapableIOTableProviderWrapper {\n          \n      \n    \n    \n  \n\nWe can keep these annotations the same as they were, since this class should work exactly the same as it used to from the user perspective.", "url": "https://github.com/apache/beam/pull/12202#discussion_r451896570", "createdAt": "2020-07-09T00:26:56Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/avro/AvroTableProvider.java", "diffHunk": "@@ -38,15 +42,10 @@\n  * LOCATION '/tmp/persons.avro'\n  * }</pre>\n  */\n-@AutoService(TableProvider.class)\n-public class AvroTableProvider extends InMemoryMetaTableProvider {\n-  @Override\n-  public String getTableType() {\n-    return \"avro\";\n-  }\n-\n-  @Override\n-  public BeamSqlTable buildBeamSqlTable(Table table) {\n-    return new AvroTable(table.getName(), table.getSchema(), table.getLocation());\n+@Internal\n+@Experimental\n+public class AvroTableProvider extends SchemaCapableIOTableProviderWrapper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUxNTk4OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/parquet/ParquetTableProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyOTowMFrOGu9nDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDoyOTowMFrOGu9nDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5NzEwMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            @Internal\n          \n          \n            \n            @Experimental\n          \n          \n            \n            public class ParquetTableProvider extends SchemaCapableIOTableProviderWrapper {\n          \n          \n            \n            @AutoService(TableProvider.class)\n          \n          \n            \n            public class ParquetTableProvider extends SchemaCapableIOTableProviderWrapper {\n          \n      \n    \n    \n  \n\nHere as well", "url": "https://github.com/apache/beam/pull/12202#discussion_r451897103", "createdAt": "2020-07-09T00:29:00Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/parquet/ParquetTableProvider.java", "diffHunk": "@@ -38,15 +42,10 @@\n  * LOCATION '/home/admin/users.parquet'\n  * }</pre>\n  */\n-@AutoService(TableProvider.class)\n-public class ParquetTableProvider extends InMemoryMetaTableProvider {\n-  @Override\n-  public String getTableType() {\n-    return \"parquet\";\n-  }\n-\n-  @Override\n-  public BeamSqlTable buildBeamSqlTable(Table table) {\n-    return new ParquetTable(table.getSchema(), table.getLocation());\n+@Internal\n+@Experimental\n+public class ParquetTableProvider extends SchemaCapableIOTableProviderWrapper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUyNDkwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/GenericRecordReadConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDozMzo1NFrOGu9sCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDozMzo1NFrOGu9sCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5ODM3Ng==", "bodyText": "Can you make this class package-private? Users may be more tempted to use it now that it's outside of the SQL extensions.\nThis could be a generally useful transform, we may want to move it outside of the parquet package and make it public... but let's not do that now.", "url": "https://github.com/apache/beam/pull/12202#discussion_r451898376", "createdAt": "2020-07-09T00:33:54Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/GenericRecordReadConverter.java", "diffHunk": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n+package org.apache.beam.sdk.io.parquet;\n \n import com.google.auto.value.AutoValue;\n import java.io.Serializable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUzNTU5OnYy", "diffSide": "RIGHT", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDozOTo0OVrOGu9yAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDozOTo0OVrOGu9yAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTkwNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * An implementation of {@link SchemaCapableIOProvider} for reading and writing JSON payloads with\n          \n          \n            \n             * {@link AvroIO}.\n          \n          \n            \n             * An implementation of {@link SchemaCapableIOProvider} for reading and writing avro files with\n          \n          \n            \n             * {@link AvroIO}.", "url": "https://github.com/apache/beam/pull/12202#discussion_r451899905", "createdAt": "2020-07-09T00:39:49Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaCapableIOProvider} for reading and writing JSON payloads with\n+ * {@link AvroIO}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNzUzODQ1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDo0MToyN1rOGu9zsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMDo0MToyN1rOGu9zsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMDMzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * An implementation of {@link SchemaCapableIOProvider} for reading and writing JSON payloads with\n          \n          \n            \n             * {@link ParquetIO}.\n          \n          \n            \n             * An implementation of {@link SchemaCapableIOProvider} for reading and writing parquet files with\n          \n          \n            \n             * {@link ParquetIO}.", "url": "https://github.com/apache/beam/pull/12202#discussion_r451900338", "createdAt": "2020-07-09T00:41:27Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.parquet;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaCapableIOProvider} for reading and writing JSON payloads with\n+ * {@link ParquetIO}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60394123a7b9fd1a44b8c9c4860a489d0f911eda"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODk3NDQwOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMzoxODoxNlrOGzjdkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDo1MTozMVrOG0e0wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxMTU3MA==", "bodyText": "Table should be BeamSqlTable here.\nTable is actually a class for config, I think we should rename it to something like TableDefinition later... It is very confusing to me.", "url": "https://github.com/apache/beam/pull/12202#discussion_r456711570", "createdAt": "2020-07-17T23:18:16Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #getSchemaIOProvider()} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider\n+    implements Serializable {\n+  public abstract SchemaIOProvider getSchemaIOProvider();\n+\n+  @Override\n+  public String getTableType() {\n+    return getSchemaIOProvider().identifier();\n+  }\n+\n+  @Override\n+  public BeamSqlTable buildBeamSqlTable(Table tableDefinition) {\n+    JSONObject tableProperties = tableDefinition.getProperties();\n+\n+    try {\n+      RowJson.RowJsonDeserializer deserializer =\n+          RowJson.RowJsonDeserializer.forSchema(getSchemaIOProvider().configurationSchema())\n+              .withNullBehavior(RowJson.RowJsonDeserializer.NullBehavior.ACCEPT_MISSING_OR_NULL);\n+\n+      Row configurationRow =\n+          newObjectMapperWith(deserializer).readValue(tableProperties.toString(), Row.class);\n+\n+      SchemaIO schemaIO =\n+          getSchemaIOProvider()\n+              .from(tableDefinition.getLocation(), configurationRow, tableDefinition.getSchema());\n+\n+      return new SchemaIOTableWrapper(schemaIO);\n+    } catch (InvalidConfigurationException | InvalidSchemaException e) {\n+      throw new InvalidTableException(e.getMessage());\n+    } catch (JsonProcessingException e) {\n+      throw new AssertionError(\n+          \"Failed to re-parse TBLPROPERTIES JSON \" + tableProperties.toString());\n+    }\n+  }\n+\n+  private BeamTableStatistics getTableStatistics(PipelineOptions options) {\n+    if (isBounded().equals(PCollection.IsBounded.BOUNDED)) {\n+      return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    }\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;\n+  }\n+\n+  private PCollection.IsBounded isBounded() {\n+    return getSchemaIOProvider().isBounded();\n+  }\n+\n+  /** A generalized {@link Table} for IOs to create IO readers and writers. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY4NDE2Mg==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12202#discussion_r457684162", "createdAt": "2020-07-20T20:51:31Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #getSchemaIOProvider()} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider\n+    implements Serializable {\n+  public abstract SchemaIOProvider getSchemaIOProvider();\n+\n+  @Override\n+  public String getTableType() {\n+    return getSchemaIOProvider().identifier();\n+  }\n+\n+  @Override\n+  public BeamSqlTable buildBeamSqlTable(Table tableDefinition) {\n+    JSONObject tableProperties = tableDefinition.getProperties();\n+\n+    try {\n+      RowJson.RowJsonDeserializer deserializer =\n+          RowJson.RowJsonDeserializer.forSchema(getSchemaIOProvider().configurationSchema())\n+              .withNullBehavior(RowJson.RowJsonDeserializer.NullBehavior.ACCEPT_MISSING_OR_NULL);\n+\n+      Row configurationRow =\n+          newObjectMapperWith(deserializer).readValue(tableProperties.toString(), Row.class);\n+\n+      SchemaIO schemaIO =\n+          getSchemaIOProvider()\n+              .from(tableDefinition.getLocation(), configurationRow, tableDefinition.getSchema());\n+\n+      return new SchemaIOTableWrapper(schemaIO);\n+    } catch (InvalidConfigurationException | InvalidSchemaException e) {\n+      throw new InvalidTableException(e.getMessage());\n+    } catch (JsonProcessingException e) {\n+      throw new AssertionError(\n+          \"Failed to re-parse TBLPROPERTIES JSON \" + tableProperties.toString());\n+    }\n+  }\n+\n+  private BeamTableStatistics getTableStatistics(PipelineOptions options) {\n+    if (isBounded().equals(PCollection.IsBounded.BOUNDED)) {\n+      return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    }\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;\n+  }\n+\n+  private PCollection.IsBounded isBounded() {\n+    return getSchemaIOProvider().isBounded();\n+  }\n+\n+  /** A generalized {@link Table} for IOs to create IO readers and writers. */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxMTU3MA=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODk5ODUxOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubJsonTableProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMzozNDoyMlrOGzjqug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDo1MTo1OFrOG0e1mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxNDkzOA==", "bodyText": "Please add a TODO with JIRA link here such that people know this override is only a temporary solution, and we can remove it after the issue tracked by that JIRA is fixed. Like\n// TODO[BEAM-?????]: remove this override after TableProvider problem is fixed\n\nAnd we should do the same thing for all classes that extends SchemaCapableIOTableProviderWrapper", "url": "https://github.com/apache/beam/pull/12202#discussion_r456714938", "createdAt": "2020-07-17T23:34:22Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubJsonTableProvider.java", "diffHunk": "@@ -17,63 +17,32 @@\n  */\n package org.apache.beam.sdk.extensions.sql.meta.provider.pubsub;\n \n-import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n-\n-import com.alibaba.fastjson.JSONObject;\n-import com.fasterxml.jackson.core.JsonProcessingException;\n import com.google.auto.service.AutoService;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.annotations.Internal;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.Table;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.InMemoryMetaTableProvider;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.InvalidTableException;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.SchemaCapableIOTableProviderWrapper;\n import org.apache.beam.sdk.extensions.sql.meta.provider.TableProvider;\n import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n import org.apache.beam.sdk.io.gcp.pubsub.PubsubSchemaCapableIOProvider;\n-import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n-import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n-import org.apache.beam.sdk.schemas.io.SchemaIO;\n-import org.apache.beam.sdk.util.RowJson.RowJsonDeserializer;\n-import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n \n /**\n- * {@link TableProvider} for {@link PubsubIOJsonTable} which wraps {@link PubsubIO} for consumption\n- * by Beam SQL.\n+ * {@link TableProvider} for {@link PubsubIO} for consumption by Beam SQL.\n+ *\n+ * <p>Passes the {@link PubsubSchemaCapableIOProvider} to the generalized table provider wrapper,\n+ * {@link SchemaCapableIOTableProviderWrapper}, for Pubsub specific behavior.\n  */\n @Internal\n @Experimental\n @AutoService(TableProvider.class)\n-public class PubsubJsonTableProvider extends InMemoryMetaTableProvider {\n-\n+public class PubsubJsonTableProvider extends SchemaCapableIOTableProviderWrapper {\n   @Override\n-  public String getTableType() {\n-    return \"pubsub\";\n+  public SchemaIOProvider getSchemaIOProvider() {\n+    return new PubsubSchemaCapableIOProvider();\n   }\n \n   @Override\n-  public BeamSqlTable buildBeamSqlTable(Table tableDefinition) {\n-    JSONObject tableProperties = tableDefinition.getProperties();\n-    PubsubSchemaCapableIOProvider ioProvider = new PubsubSchemaCapableIOProvider();\n-\n-    try {\n-      RowJsonDeserializer deserializer =\n-          RowJsonDeserializer.forSchema(ioProvider.configurationSchema())\n-              .withNullBehavior(RowJsonDeserializer.NullBehavior.ACCEPT_MISSING_OR_NULL);\n-\n-      Row configurationRow =\n-          newObjectMapperWith(deserializer).readValue(tableProperties.toString(), Row.class);\n-\n-      SchemaIO pubsubSchemaIO =\n-          ioProvider.from(\n-              tableDefinition.getLocation(), configurationRow, tableDefinition.getSchema());\n-\n-      return PubsubIOJsonTable.fromSchemaIO(pubsubSchemaIO);\n-    } catch (InvalidConfigurationException | InvalidSchemaException e) {\n-      throw new InvalidTableException(e.getMessage());\n-    } catch (JsonProcessingException e) {\n-      throw new AssertionError(\n-          \"Failed to re-parse TBLPROPERTIES JSON \" + tableProperties.toString());\n-    }\n+  public String getTableType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY4NDM3Nw==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12202#discussion_r457684377", "createdAt": "2020-07-20T20:51:58Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/pubsub/PubsubJsonTableProvider.java", "diffHunk": "@@ -17,63 +17,32 @@\n  */\n package org.apache.beam.sdk.extensions.sql.meta.provider.pubsub;\n \n-import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n-\n-import com.alibaba.fastjson.JSONObject;\n-import com.fasterxml.jackson.core.JsonProcessingException;\n import com.google.auto.service.AutoService;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.annotations.Internal;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.Table;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.InMemoryMetaTableProvider;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.InvalidTableException;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.SchemaCapableIOTableProviderWrapper;\n import org.apache.beam.sdk.extensions.sql.meta.provider.TableProvider;\n import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n import org.apache.beam.sdk.io.gcp.pubsub.PubsubSchemaCapableIOProvider;\n-import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n-import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n-import org.apache.beam.sdk.schemas.io.SchemaIO;\n-import org.apache.beam.sdk.util.RowJson.RowJsonDeserializer;\n-import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n \n /**\n- * {@link TableProvider} for {@link PubsubIOJsonTable} which wraps {@link PubsubIO} for consumption\n- * by Beam SQL.\n+ * {@link TableProvider} for {@link PubsubIO} for consumption by Beam SQL.\n+ *\n+ * <p>Passes the {@link PubsubSchemaCapableIOProvider} to the generalized table provider wrapper,\n+ * {@link SchemaCapableIOTableProviderWrapper}, for Pubsub specific behavior.\n  */\n @Internal\n @Experimental\n @AutoService(TableProvider.class)\n-public class PubsubJsonTableProvider extends InMemoryMetaTableProvider {\n-\n+public class PubsubJsonTableProvider extends SchemaCapableIOTableProviderWrapper {\n   @Override\n-  public String getTableType() {\n-    return \"pubsub\";\n+  public SchemaIOProvider getSchemaIOProvider() {\n+    return new PubsubSchemaCapableIOProvider();\n   }\n \n   @Override\n-  public BeamSqlTable buildBeamSqlTable(Table tableDefinition) {\n-    JSONObject tableProperties = tableDefinition.getProperties();\n-    PubsubSchemaCapableIOProvider ioProvider = new PubsubSchemaCapableIOProvider();\n-\n-    try {\n-      RowJsonDeserializer deserializer =\n-          RowJsonDeserializer.forSchema(ioProvider.configurationSchema())\n-              .withNullBehavior(RowJsonDeserializer.NullBehavior.ACCEPT_MISSING_OR_NULL);\n-\n-      Row configurationRow =\n-          newObjectMapperWith(deserializer).readValue(tableProperties.toString(), Row.class);\n-\n-      SchemaIO pubsubSchemaIO =\n-          ioProvider.from(\n-              tableDefinition.getLocation(), configurationRow, tableDefinition.getSchema());\n-\n-      return PubsubIOJsonTable.fromSchemaIO(pubsubSchemaIO);\n-    } catch (InvalidConfigurationException | InvalidSchemaException e) {\n-      throw new InvalidTableException(e.getMessage());\n-    } catch (JsonProcessingException e) {\n-      throw new AssertionError(\n-          \"Failed to re-parse TBLPROPERTIES JSON \" + tableProperties.toString());\n-    }\n+  public String getTableType() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxNDkzOA=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0OTAwNTQ2OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMzozOTozNVrOGzjujw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDoxNzoyOFrOG0dxCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxNTkxOQ==", "bodyText": "We can make this class a static inner class by passing another parameter PCollection.IsBounded isBounded to the constructor and save it. This way we can also remove the 2 helper functions above (getTableStatistics and isBounded).", "url": "https://github.com/apache/beam/pull/12202#discussion_r456715919", "createdAt": "2020-07-17T23:39:35Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #getSchemaIOProvider()} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider\n+    implements Serializable {\n+  public abstract SchemaIOProvider getSchemaIOProvider();\n+\n+  @Override\n+  public String getTableType() {\n+    return getSchemaIOProvider().identifier();\n+  }\n+\n+  @Override\n+  public BeamSqlTable buildBeamSqlTable(Table tableDefinition) {\n+    JSONObject tableProperties = tableDefinition.getProperties();\n+\n+    try {\n+      RowJson.RowJsonDeserializer deserializer =\n+          RowJson.RowJsonDeserializer.forSchema(getSchemaIOProvider().configurationSchema())\n+              .withNullBehavior(RowJson.RowJsonDeserializer.NullBehavior.ACCEPT_MISSING_OR_NULL);\n+\n+      Row configurationRow =\n+          newObjectMapperWith(deserializer).readValue(tableProperties.toString(), Row.class);\n+\n+      SchemaIO schemaIO =\n+          getSchemaIOProvider()\n+              .from(tableDefinition.getLocation(), configurationRow, tableDefinition.getSchema());\n+\n+      return new SchemaIOTableWrapper(schemaIO);\n+    } catch (InvalidConfigurationException | InvalidSchemaException e) {\n+      throw new InvalidTableException(e.getMessage());\n+    } catch (JsonProcessingException e) {\n+      throw new AssertionError(\n+          \"Failed to re-parse TBLPROPERTIES JSON \" + tableProperties.toString());\n+    }\n+  }\n+\n+  private BeamTableStatistics getTableStatistics(PipelineOptions options) {\n+    if (isBounded().equals(PCollection.IsBounded.BOUNDED)) {\n+      return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    }\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;\n+  }\n+\n+  private PCollection.IsBounded isBounded() {\n+    return getSchemaIOProvider().isBounded();\n+  }\n+\n+  /** A generalized {@link Table} for IOs to create IO readers and writers. */\n+  private class SchemaIOTableWrapper extends BaseBeamTable {\n+    protected final SchemaIO schemaIO;\n+\n+    private SchemaIOTableWrapper(SchemaIO schemaIO) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2NjgyNQ==", "bodyText": "The reason why the two helper functions are there is because some IO Table Providers have logic that is more complex than the general form (ie TextTable), and we were planning on overriding those methods within the TableProvider of that IO.", "url": "https://github.com/apache/beam/pull/12202#discussion_r457666825", "createdAt": "2020-07-20T20:17:28Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #getSchemaIOProvider()} that is specific to\n+ * the IO.\n+ */\n+@Internal\n+@Experimental\n+public abstract class SchemaCapableIOTableProviderWrapper extends InMemoryMetaTableProvider\n+    implements Serializable {\n+  public abstract SchemaIOProvider getSchemaIOProvider();\n+\n+  @Override\n+  public String getTableType() {\n+    return getSchemaIOProvider().identifier();\n+  }\n+\n+  @Override\n+  public BeamSqlTable buildBeamSqlTable(Table tableDefinition) {\n+    JSONObject tableProperties = tableDefinition.getProperties();\n+\n+    try {\n+      RowJson.RowJsonDeserializer deserializer =\n+          RowJson.RowJsonDeserializer.forSchema(getSchemaIOProvider().configurationSchema())\n+              .withNullBehavior(RowJson.RowJsonDeserializer.NullBehavior.ACCEPT_MISSING_OR_NULL);\n+\n+      Row configurationRow =\n+          newObjectMapperWith(deserializer).readValue(tableProperties.toString(), Row.class);\n+\n+      SchemaIO schemaIO =\n+          getSchemaIOProvider()\n+              .from(tableDefinition.getLocation(), configurationRow, tableDefinition.getSchema());\n+\n+      return new SchemaIOTableWrapper(schemaIO);\n+    } catch (InvalidConfigurationException | InvalidSchemaException e) {\n+      throw new InvalidTableException(e.getMessage());\n+    } catch (JsonProcessingException e) {\n+      throw new AssertionError(\n+          \"Failed to re-parse TBLPROPERTIES JSON \" + tableProperties.toString());\n+    }\n+  }\n+\n+  private BeamTableStatistics getTableStatistics(PipelineOptions options) {\n+    if (isBounded().equals(PCollection.IsBounded.BOUNDED)) {\n+      return BeamTableStatistics.BOUNDED_UNKNOWN;\n+    }\n+    return BeamTableStatistics.UNBOUNDED_UNKNOWN;\n+  }\n+\n+  private PCollection.IsBounded isBounded() {\n+    return getSchemaIOProvider().isBounded();\n+  }\n+\n+  /** A generalized {@link Table} for IOs to create IO readers and writers. */\n+  private class SchemaIOTableWrapper extends BaseBeamTable {\n+    protected final SchemaIO schemaIO;\n+\n+    private SchemaIOTableWrapper(SchemaIO schemaIO) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxNTkxOQ=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0OTAwOTQwOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMzo0MjoyNFrOGzjwqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDo1MjozMVrOG0e2wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxNjQ1Ng==", "bodyText": "This line of comment belongs better as getSchemaIOProvider() method comment. Like:\n// Subclasses should provide the schemaIOProvider that is specific to its IO.\npublic abstract SchemaIOProvider getSchemaIOProvider();", "url": "https://github.com/apache/beam/pull/12202#discussion_r456716456", "createdAt": "2020-07-17T23:42:24Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #getSchemaIOProvider()} that is specific to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY4NDY3NA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12202#discussion_r457684674", "createdAt": "2020-07-20T20:52:31Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/SchemaCapableIOTableProviderWrapper.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.meta.provider;\n+\n+import static org.apache.beam.sdk.util.RowJsonUtils.newObjectMapperWith;\n+\n+import com.alibaba.fastjson.JSONObject;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.extensions.sql.impl.BeamTableStatistics;\n+import org.apache.beam.sdk.extensions.sql.meta.BaseBeamTable;\n+import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n+import org.apache.beam.sdk.extensions.sql.meta.Table;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.InvalidConfigurationException;\n+import org.apache.beam.sdk.schemas.io.InvalidSchemaException;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.RowJson;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * A general {@link TableProvider} for IOs for consumption by Beam SQL.\n+ *\n+ * <p>Can create child classes for IOs to pass {@link #getSchemaIOProvider()} that is specific to", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxNjQ1Ng=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0OTAzOTYwOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/parquet/ParquetTableProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOFQwMDowNDoxMVrOGzkBVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDo1MzozMFrOG0e4xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMDcyNw==", "bodyText": "Duplicate comment here.", "url": "https://github.com/apache/beam/pull/12202#discussion_r456720727", "createdAt": "2020-07-18T00:04:11Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/parquet/ParquetTableProvider.java", "diffHunk": "@@ -18,13 +18,20 @@\n package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n \n import com.google.auto.service.AutoService;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.Table;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.InMemoryMetaTableProvider;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.SchemaCapableIOTableProviderWrapper;\n import org.apache.beam.sdk.extensions.sql.meta.provider.TableProvider;\n+import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.io.parquet.ParquetSchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n \n /**\n- * {@link TableProvider} for {@link ParquetTable}.\n+ * {@link TableProvider} for {@link ParquetIO} for consumption by Beam SQL.\n+ *\n+ * <p>Passes the {@link ParquetSchemaCapableIOProvider} to the generalized table provider wrapper,\n+ * {@link SchemaCapableIOTableProviderWrapper}, for Parquet specific behavior.\n+ *\n+ * <p>Passes the {@link ParquetSchemaCapableIOProvider} to the generalized table provider wrapper,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY4NTE4OA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12202#discussion_r457685188", "createdAt": "2020-07-20T20:53:30Z", "author": {"login": "sclukas77"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/parquet/ParquetTableProvider.java", "diffHunk": "@@ -18,13 +18,20 @@\n package org.apache.beam.sdk.extensions.sql.meta.provider.parquet;\n \n import com.google.auto.service.AutoService;\n-import org.apache.beam.sdk.extensions.sql.meta.BeamSqlTable;\n-import org.apache.beam.sdk.extensions.sql.meta.Table;\n-import org.apache.beam.sdk.extensions.sql.meta.provider.InMemoryMetaTableProvider;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.SchemaCapableIOTableProviderWrapper;\n import org.apache.beam.sdk.extensions.sql.meta.provider.TableProvider;\n+import org.apache.beam.sdk.io.parquet.ParquetIO;\n+import org.apache.beam.sdk.io.parquet.ParquetSchemaCapableIOProvider;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n \n /**\n- * {@link TableProvider} for {@link ParquetTable}.\n+ * {@link TableProvider} for {@link ParquetIO} for consumption by Beam SQL.\n+ *\n+ * <p>Passes the {@link ParquetSchemaCapableIOProvider} to the generalized table provider wrapper,\n+ * {@link SchemaCapableIOTableProviderWrapper}, for Parquet specific behavior.\n+ *\n+ * <p>Passes the {@link ParquetSchemaCapableIOProvider} to the generalized table provider wrapper,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMDcyNw=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0OTA0NjMwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOFQwMDowOToyM1rOGzkE3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDo1Mjo1NlrOG0e3oQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMTYyOA==", "bodyText": "@Internal not necessary here. Please check in other places as well.", "url": "https://github.com/apache/beam/pull/12202#discussion_r456721628", "createdAt": "2020-07-18T00:09:23Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.parquet;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing parquet files with {@link\n+ * ParquetIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class ParquetSchemaCapableIOProvider implements SchemaIOProvider {\n+  /** Returns an id that uniquely represents this IO. */\n+  @Override\n+  public String identifier() {\n+    return \"parquet\";\n+  }\n+\n+  /**\n+   * Returns the expected schema of the configuration object. Note this is distinct from the schema\n+   * of the data source itself. No configuration expected for parquet.\n+   */\n+  @Override\n+  public Schema configurationSchema() {\n+    return Schema.builder().build();\n+  }\n+\n+  /**\n+   * Produce a SchemaIO given a String representing the data's location, the schema of the data that\n+   * resides there, and some IO-specific configuration object.\n+   */\n+  @Override\n+  public ParquetSchemaIO from(String location, Row configuration, Schema dataSchema) {\n+    return new ParquetSchemaIO(location, dataSchema);\n+  }\n+\n+  @Override\n+  public boolean requiresDataSchema() {\n+    return true;\n+  }\n+\n+  @Override\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.BOUNDED;\n+  }\n+\n+  /** An abstraction to create schema aware IOs. */\n+  @Internal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY4NDg5Nw==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12202#discussion_r457684897", "createdAt": "2020-07-20T20:52:56Z", "author": {"login": "sclukas77"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.parquet;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing parquet files with {@link\n+ * ParquetIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class ParquetSchemaCapableIOProvider implements SchemaIOProvider {\n+  /** Returns an id that uniquely represents this IO. */\n+  @Override\n+  public String identifier() {\n+    return \"parquet\";\n+  }\n+\n+  /**\n+   * Returns the expected schema of the configuration object. Note this is distinct from the schema\n+   * of the data source itself. No configuration expected for parquet.\n+   */\n+  @Override\n+  public Schema configurationSchema() {\n+    return Schema.builder().build();\n+  }\n+\n+  /**\n+   * Produce a SchemaIO given a String representing the data's location, the schema of the data that\n+   * resides there, and some IO-specific configuration object.\n+   */\n+  @Override\n+  public ParquetSchemaIO from(String location, Row configuration, Schema dataSchema) {\n+    return new ParquetSchemaIO(location, dataSchema);\n+  }\n+\n+  @Override\n+  public boolean requiresDataSchema() {\n+    return true;\n+  }\n+\n+  @Override\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.BOUNDED;\n+  }\n+\n+  /** An abstraction to create schema aware IOs. */\n+  @Internal", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMTYyOA=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0OTA1OTg2OnYy", "diffSide": "RIGHT", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/GenericRecordWriteConverter.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOFQwMDoyMDoyM1rOGzkMCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNjo0Mzo1MVrOG0WlKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMzQ2NA==", "bodyText": "This file is moved to the core module, I believe you want to move it to the io module?", "url": "https://github.com/apache/beam/pull/12202#discussion_r456723464", "createdAt": "2020-07-18T00:20:23Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/GenericRecordWriteConverter.java", "diffHunk": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.avro;\n+package org.apache.beam.sdk.io;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUyOTMyOA==", "bodyText": "The reason why I moved it here was to be in the same directory as AvroIO and AvroSchemaCapableIOProvider, which happened to be in core. Do you think I should move it to the io module regardless?", "url": "https://github.com/apache/beam/pull/12202#discussion_r457529328", "createdAt": "2020-07-20T16:12:20Z", "author": {"login": "sclukas77"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/GenericRecordWriteConverter.java", "diffHunk": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.avro;\n+package org.apache.beam.sdk.io;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMzQ2NA=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU0OTA5OA==", "bodyText": "I see. I wasn't aware that AvroIO is in core. Then moving it here make sense.", "url": "https://github.com/apache/beam/pull/12202#discussion_r457549098", "createdAt": "2020-07-20T16:43:51Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/GenericRecordWriteConverter.java", "diffHunk": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.extensions.sql.meta.provider.avro;\n+package org.apache.beam.sdk.io;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyMzQ2NA=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0OTA4MzQzOnYy", "diffSide": "RIGHT", "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOFQwMDo0MTowM1rOGzkYWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwODowNzo1MVrOHCIQdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyNjYxNg==", "bodyText": "This PR changes the behavior of this line, from setting the second parameter using table name to null. How will this affect the behavior of AvroIO?", "url": "https://github.com/apache/beam/pull/12202#discussion_r456726616", "createdAt": "2020-07-18T00:41:03Z", "author": {"login": "robinyqiu"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing Avro files with {@link\n+ * AvroIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class AvroSchemaCapableIOProvider implements SchemaIOProvider {\n+  /** Returns an id that uniquely represents this IO. */\n+  @Override\n+  public String identifier() {\n+    return \"avro\";\n+  }\n+\n+  /**\n+   * Returns the expected schema of the configuration object. Note this is distinct from the schema\n+   * of the data source itself. No configuration expected for Avro.\n+   */\n+  @Override\n+  public Schema configurationSchema() {\n+    return Schema.builder().build();\n+  }\n+\n+  /**\n+   * Produce a SchemaIO given a String representing the data's location, the schema of the data that\n+   * resides there, and some IO-specific configuration object.\n+   */\n+  @Override\n+  public AvroSchemaIO from(String location, Row configuration, Schema dataSchema) {\n+    return new AvroSchemaIO(location, dataSchema);\n+  }\n+\n+  @Override\n+  public boolean requiresDataSchema() {\n+    return true;\n+  }\n+\n+  @Override\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.BOUNDED;\n+  }\n+\n+  /** An abstraction to create schema aware IOs. */\n+  @Internal\n+  private static class AvroSchemaIO implements SchemaIO, Serializable {\n+    protected final Schema dataSchema;\n+    protected final String location;\n+\n+    private AvroSchemaIO(String location, Schema dataSchema) {\n+      this.dataSchema = dataSchema;\n+      this.location = location;\n+    }\n+\n+    @Override\n+    public Schema schema() {\n+      return dataSchema;\n+    }\n+\n+    @Override\n+    public PTransform<PBegin, PCollection<Row>> buildReader() {\n+      return new PTransform<PBegin, PCollection<Row>>() {\n+        @Override\n+        public PCollection<Row> expand(PBegin begin) {\n+          return begin\n+              .apply(\n+                  \"AvroIORead\",\n+                  AvroIO.readGenericRecords(AvroUtils.toAvroSchema(dataSchema, null, null))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1MTcwMA==", "bodyText": "We changed the table name to null here because the general SchemaCapableIOTableProviderWrapper does not pass the table.getName() value to AvroSchemaCapableIOProvider. Since the toAvroSchema(..) method here has a nullable name parameter that is used to create a record, it didn't seem imperative to maintaining functionality. @TheNeuralBit What do you think?", "url": "https://github.com/apache/beam/pull/12202#discussion_r457651700", "createdAt": "2020-07-20T19:47:56Z", "author": {"login": "sclukas77"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing Avro files with {@link\n+ * AvroIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class AvroSchemaCapableIOProvider implements SchemaIOProvider {\n+  /** Returns an id that uniquely represents this IO. */\n+  @Override\n+  public String identifier() {\n+    return \"avro\";\n+  }\n+\n+  /**\n+   * Returns the expected schema of the configuration object. Note this is distinct from the schema\n+   * of the data source itself. No configuration expected for Avro.\n+   */\n+  @Override\n+  public Schema configurationSchema() {\n+    return Schema.builder().build();\n+  }\n+\n+  /**\n+   * Produce a SchemaIO given a String representing the data's location, the schema of the data that\n+   * resides there, and some IO-specific configuration object.\n+   */\n+  @Override\n+  public AvroSchemaIO from(String location, Row configuration, Schema dataSchema) {\n+    return new AvroSchemaIO(location, dataSchema);\n+  }\n+\n+  @Override\n+  public boolean requiresDataSchema() {\n+    return true;\n+  }\n+\n+  @Override\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.BOUNDED;\n+  }\n+\n+  /** An abstraction to create schema aware IOs. */\n+  @Internal\n+  private static class AvroSchemaIO implements SchemaIO, Serializable {\n+    protected final Schema dataSchema;\n+    protected final String location;\n+\n+    private AvroSchemaIO(String location, Schema dataSchema) {\n+      this.dataSchema = dataSchema;\n+      this.location = location;\n+    }\n+\n+    @Override\n+    public Schema schema() {\n+      return dataSchema;\n+    }\n+\n+    @Override\n+    public PTransform<PBegin, PCollection<Row>> buildReader() {\n+      return new PTransform<PBegin, PCollection<Row>>() {\n+        @Override\n+        public PCollection<Row> expand(PBegin begin) {\n+          return begin\n+              .apply(\n+                  \"AvroIORead\",\n+                  AvroIO.readGenericRecords(AvroUtils.toAvroSchema(dataSchema, null, null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyNjYxNg=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3NzA3Ng==", "bodyText": "I think it is fine, when the name of the schema isn't specified we just call it \"topLevelRecord\": \n  \n    \n      beam/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/utils/AvroUtils.java\n    \n    \n         Line 327\n      in\n      f36250f\n    \n    \n    \n    \n\n        \n          \n           final String schemaName = Strings.isNullOrEmpty(name) ? \"topLevelRecord\" : name; \n        \n    \n  \n\n\nI don't think the name has any effect on the way we encode/decode the Avro records. (CC @iemejia in case I'm wrong)", "url": "https://github.com/apache/beam/pull/12202#discussion_r457777076", "createdAt": "2020-07-21T01:10:46Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing Avro files with {@link\n+ * AvroIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class AvroSchemaCapableIOProvider implements SchemaIOProvider {\n+  /** Returns an id that uniquely represents this IO. */\n+  @Override\n+  public String identifier() {\n+    return \"avro\";\n+  }\n+\n+  /**\n+   * Returns the expected schema of the configuration object. Note this is distinct from the schema\n+   * of the data source itself. No configuration expected for Avro.\n+   */\n+  @Override\n+  public Schema configurationSchema() {\n+    return Schema.builder().build();\n+  }\n+\n+  /**\n+   * Produce a SchemaIO given a String representing the data's location, the schema of the data that\n+   * resides there, and some IO-specific configuration object.\n+   */\n+  @Override\n+  public AvroSchemaIO from(String location, Row configuration, Schema dataSchema) {\n+    return new AvroSchemaIO(location, dataSchema);\n+  }\n+\n+  @Override\n+  public boolean requiresDataSchema() {\n+    return true;\n+  }\n+\n+  @Override\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.BOUNDED;\n+  }\n+\n+  /** An abstraction to create schema aware IOs. */\n+  @Internal\n+  private static class AvroSchemaIO implements SchemaIO, Serializable {\n+    protected final Schema dataSchema;\n+    protected final String location;\n+\n+    private AvroSchemaIO(String location, Schema dataSchema) {\n+      this.dataSchema = dataSchema;\n+      this.location = location;\n+    }\n+\n+    @Override\n+    public Schema schema() {\n+      return dataSchema;\n+    }\n+\n+    @Override\n+    public PTransform<PBegin, PCollection<Row>> buildReader() {\n+      return new PTransform<PBegin, PCollection<Row>>() {\n+        @Override\n+        public PCollection<Row> expand(PBegin begin) {\n+          return begin\n+              .apply(\n+                  \"AvroIORead\",\n+                  AvroIO.readGenericRecords(AvroUtils.toAvroSchema(dataSchema, null, null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyNjYxNg=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk5NDQ4NQ==", "bodyText": "Oups catching up on old stuff, AFAIR in theory it does but in practice it does not, because this is a part of the spec that is specified but nobody enforces specially the Java impl, and if the Java impl does not the others don't for compatibility reasons.", "url": "https://github.com/apache/beam/pull/12202#discussion_r471994485", "createdAt": "2020-08-18T08:07:51Z", "author": {"login": "iemejia"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.transforms.Convert;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing Avro files with {@link\n+ * AvroIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class AvroSchemaCapableIOProvider implements SchemaIOProvider {\n+  /** Returns an id that uniquely represents this IO. */\n+  @Override\n+  public String identifier() {\n+    return \"avro\";\n+  }\n+\n+  /**\n+   * Returns the expected schema of the configuration object. Note this is distinct from the schema\n+   * of the data source itself. No configuration expected for Avro.\n+   */\n+  @Override\n+  public Schema configurationSchema() {\n+    return Schema.builder().build();\n+  }\n+\n+  /**\n+   * Produce a SchemaIO given a String representing the data's location, the schema of the data that\n+   * resides there, and some IO-specific configuration object.\n+   */\n+  @Override\n+  public AvroSchemaIO from(String location, Row configuration, Schema dataSchema) {\n+    return new AvroSchemaIO(location, dataSchema);\n+  }\n+\n+  @Override\n+  public boolean requiresDataSchema() {\n+    return true;\n+  }\n+\n+  @Override\n+  public PCollection.IsBounded isBounded() {\n+    return PCollection.IsBounded.BOUNDED;\n+  }\n+\n+  /** An abstraction to create schema aware IOs. */\n+  @Internal\n+  private static class AvroSchemaIO implements SchemaIO, Serializable {\n+    protected final Schema dataSchema;\n+    protected final String location;\n+\n+    private AvroSchemaIO(String location, Schema dataSchema) {\n+      this.dataSchema = dataSchema;\n+      this.location = location;\n+    }\n+\n+    @Override\n+    public Schema schema() {\n+      return dataSchema;\n+    }\n+\n+    @Override\n+    public PTransform<PBegin, PCollection<Row>> buildReader() {\n+      return new PTransform<PBegin, PCollection<Row>>() {\n+        @Override\n+        public PCollection<Row> expand(PBegin begin) {\n+          return begin\n+              .apply(\n+                  \"AvroIORead\",\n+                  AvroIO.readGenericRecords(AvroUtils.toAvroSchema(dataSchema, null, null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyNjYxNg=="}, "originalCommit": {"oid": "d36671af68a40926bf0a3d2bdddd43579bcbc690"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTMyMTU5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTo0ODo0MlrOG0-c_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxODoxMDoxMlrOG1D_cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwMjM2NQ==", "bodyText": "could you remove the \"Capable\" from all of these class names now that we've switched to SchemaIOProvider?", "url": "https://github.com/apache/beam/pull/12202#discussion_r458202365", "createdAt": "2020-07-21T15:48:42Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.parquet;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing parquet files with {@link\n+ * ParquetIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class ParquetSchemaCapableIOProvider implements SchemaIOProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3df7054a2e7a81049a51ddf6a1d183dd99917c17"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI5MzEwNw==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12202#discussion_r458293107", "createdAt": "2020-07-21T18:10:12Z", "author": {"login": "sclukas77"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetSchemaCapableIOProvider.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.parquet;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.beam.sdk.annotations.Internal;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.io.SchemaIO;\n+import org.apache.beam.sdk.schemas.io.SchemaIOProvider;\n+import org.apache.beam.sdk.schemas.utils.AvroUtils;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.POutput;\n+import org.apache.beam.sdk.values.Row;\n+\n+/**\n+ * An implementation of {@link SchemaIOProvider} for reading and writing parquet files with {@link\n+ * ParquetIO}.\n+ */\n+@Internal\n+@AutoService(SchemaIOProvider.class)\n+public class ParquetSchemaCapableIOProvider implements SchemaIOProvider {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwMjM2NQ=="}, "originalCommit": {"oid": "3df7054a2e7a81049a51ddf6a1d183dd99917c17"}, "originalPosition": 40}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1107, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}