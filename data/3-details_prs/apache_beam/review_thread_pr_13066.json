{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAwODU4MzY2", "number": 13066, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzowNDoxOFrOEsOBEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxODozMlrOEusrFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODAyNDQ5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert.py", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzowNDoxOFrOHfawMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDowODoxMlrOHgLuSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcwNjIyNw==", "bodyText": "Note this uses a global cache which can be problematic in testing. I don't think we need to worry about pcollections being shared across pipelines though since the dataframe expressions should have a reference to the pipeline through the to_dataframe roots.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502706227", "createdAt": "2020-10-09T23:04:18Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,6 +68,9 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+TO_PCOLLECTION_CACHE = {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMjg4MA==", "bodyText": "I'm actually more worried about global caches in production than testing--the expressions themselves should not collide between pipelines.\nMaybe we could use a weakref.WeakKeyDictionary.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502712880", "createdAt": "2020-10-09T23:36:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,6 +68,9 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+TO_PCOLLECTION_CACHE = {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcwNjIyNw=="}, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxNzMxMg==", "bodyText": "Yeah agreed it could be a problem in production as well. WeakKeyDictionary is an interesting idea, I hadn't seen the weakref library before. I'll give that a shot.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502717312", "createdAt": "2020-10-10T00:02:21Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,6 +68,9 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+TO_PCOLLECTION_CACHE = {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcwNjIyNw=="}, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUwODU1NA==", "bodyText": "Updated this to use WeakValueDictionary for the caches, PTAL", "url": "https://github.com/apache/beam/pull/13066#discussion_r503508554", "createdAt": "2020-10-12T20:08:12Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,6 +68,9 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+TO_PCOLLECTION_CACHE = {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcwNjIyNw=="}, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODA3MzU0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzozODozMlrOHfbLmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDoxMDo1NFrOHgLyzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMzI0Mg==", "bodyText": "Put ()'s around ix, pc for better formatting.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502713242", "createdAt": "2020-10-09T23:38:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,21 +142,47 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(\n+                         (ix, df._expr) for ix, df in enumerate(new_dataframes))\n+                 )  # type: Dict[Any, pvalue.PCollection]\n+\n+  TO_PCOLLECTION_CACHE.update(\n+      {new_dataframes[ix]._expr: pc\n+       for ix, pc in new_results.items()})\n+\n+  raw_results = {\n+      ix: TO_PCOLLECTION_CACHE[df._expr]\n+      for ix, df in enumerate(dataframes)\n+  }\n \n   if yield_elements == \"schemas\":\n     results = {\n-        key: pc\n-        | \"Unbatch '%s'\" % dataframes[key]._expr._id >> schemas.UnbatchPandas(\n-            dataframes[key]._expr.proxy(), include_indexes=include_indexes)\n-        for (key, pc) in results.items()\n+        ix: _make_unbatched_pcoll(pc, dataframes[ix]._expr, include_indexes)\n+        for ix,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUwOTcxMQ==", "bodyText": "Done, thanks", "url": "https://github.com/apache/beam/pull/13066#discussion_r503509711", "createdAt": "2020-10-12T20:10:54Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,21 +142,47 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(\n+                         (ix, df._expr) for ix, df in enumerate(new_dataframes))\n+                 )  # type: Dict[Any, pvalue.PCollection]\n+\n+  TO_PCOLLECTION_CACHE.update(\n+      {new_dataframes[ix]._expr: pc\n+       for ix, pc in new_results.items()})\n+\n+  raw_results = {\n+      ix: TO_PCOLLECTION_CACHE[df._expr]\n+      for ix, df in enumerate(dataframes)\n+  }\n \n   if yield_elements == \"schemas\":\n     results = {\n-        key: pc\n-        | \"Unbatch '%s'\" % dataframes[key]._expr._id >> schemas.UnbatchPandas(\n-            dataframes[key]._expr.proxy(), include_indexes=include_indexes)\n-        for (key, pc) in results.items()\n+        ix: _make_unbatched_pcoll(pc, dataframes[ix]._expr, include_indexes)\n+        for ix,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMzI0Mg=="}, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODA3NjgwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzo0MDo0M1rOHfbNRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMDowNzozMFrOHgLtIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMzY3MA==", "bodyText": "Again, this grows without bound.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502713670", "createdAt": "2020-10-09T23:40:43Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,21 +142,47 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(\n+                         (ix, df._expr) for ix, df in enumerate(new_dataframes))\n+                 )  # type: Dict[Any, pvalue.PCollection]\n+\n+  TO_PCOLLECTION_CACHE.update(\n+      {new_dataframes[ix]._expr: pc\n+       for ix, pc in new_results.items()})\n+\n+  raw_results = {\n+      ix: TO_PCOLLECTION_CACHE[df._expr]\n+      for ix, df in enumerate(dataframes)\n+  }\n \n   if yield_elements == \"schemas\":\n     results = {\n-        key: pc\n-        | \"Unbatch '%s'\" % dataframes[key]._expr._id >> schemas.UnbatchPandas(\n-            dataframes[key]._expr.proxy(), include_indexes=include_indexes)\n-        for (key, pc) in results.items()\n+        ix: _make_unbatched_pcoll(pc, dataframes[ix]._expr, include_indexes)\n+        for ix,\n+        pc in raw_results.items()\n     }\n+  else:\n+    results = raw_results\n \n   if len(results) == 1 and not always_return_tuple:\n     return results[0]\n   else:\n     return tuple(value for key, value in sorted(results.items()))\n+\n+\n+memoize = functools.lru_cache(maxsize=None)\n+\n+\n+@memoize", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUwODI1OQ==", "bodyText": "Right I used the same approach for both.", "url": "https://github.com/apache/beam/pull/13066#discussion_r503508259", "createdAt": "2020-10-12T20:07:30Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,21 +142,47 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(\n+                         (ix, df._expr) for ix, df in enumerate(new_dataframes))\n+                 )  # type: Dict[Any, pvalue.PCollection]\n+\n+  TO_PCOLLECTION_CACHE.update(\n+      {new_dataframes[ix]._expr: pc\n+       for ix, pc in new_results.items()})\n+\n+  raw_results = {\n+      ix: TO_PCOLLECTION_CACHE[df._expr]\n+      for ix, df in enumerate(dataframes)\n+  }\n \n   if yield_elements == \"schemas\":\n     results = {\n-        key: pc\n-        | \"Unbatch '%s'\" % dataframes[key]._expr._id >> schemas.UnbatchPandas(\n-            dataframes[key]._expr.proxy(), include_indexes=include_indexes)\n-        for (key, pc) in results.items()\n+        ix: _make_unbatched_pcoll(pc, dataframes[ix]._expr, include_indexes)\n+        for ix,\n+        pc in raw_results.items()\n     }\n+  else:\n+    results = raw_results\n \n   if len(results) == 1 and not always_return_tuple:\n     return results[0]\n   else:\n     return tuple(value for key, value in sorted(results.items()))\n+\n+\n+memoize = functools.lru_cache(maxsize=None)\n+\n+\n+@memoize", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMzY3MA=="}, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3NDAwMjg1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxMDoxNVrOHjRG1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMDowMjo1N1rOHpOuIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0MjQ4NQ==", "bodyText": "Dict comprehension (for consistency)?", "url": "https://github.com/apache/beam/pull/13066#discussion_r506742485", "createdAt": "2020-10-16T22:10:15Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,19 +161,36 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr._id not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk5NDg0OA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/13066#discussion_r512994848", "createdAt": "2020-10-27T20:02:57Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,19 +161,36 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr._id not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0MjQ4NQ=="}, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3NDAwNjg5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxMjowOVrOHjRJHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjoyMjowN1rOHlGngQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0MzA3MA==", "bodyText": "Nice. Perhaps it's worth noting that the pipeline (indirectly) holds references to the transforms which keep both the collections and expressions alive. (Keeping the expressions alive is important to ensure their ids never get accidentally re-used.)", "url": "https://github.com/apache/beam/pull/13066#discussion_r506743070", "createdAt": "2020-10-16T22:12:09Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,7 +69,28 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+# PCollections generated by to_pcollection are memoized.\n+# WeakValueDictionary is used so the caches are cleaned up with the parent\n+# pipelines", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY2Nzc3Nw==", "bodyText": "Good idea, added that language", "url": "https://github.com/apache/beam/pull/13066#discussion_r508667777", "createdAt": "2020-10-20T16:22:07Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,7 +69,28 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+# PCollections generated by to_pcollection are memoized.\n+# WeakValueDictionary is used so the caches are cleaned up with the parent\n+# pipelines", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0MzA3MA=="}, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3NDAxNTMxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert_test.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxNjo1MFrOHjRONQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjoxMDowN1rOHlF-yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDM3Mw==", "bodyText": "This test (and below) seem largely copies of the test above, but we don't even need to run the pipeline to test what this is testing. Perhaps it'd be better to limit it to what we're trying to test for clarity, i.e. the ids are the same.", "url": "https://github.com/apache/beam/pull/13066#discussion_r506744373", "createdAt": "2020-10-16T22:16:50Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY1NzM1NQ==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/13066#discussion_r508657355", "createdAt": "2020-10-20T16:10:07Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDM3Mw=="}, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3NDAxNTU5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert_test.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxNjo1NlrOHjROYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjoxMDowMlrOHlF-mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDQxOQ==", "bodyText": "assertIs?", "url": "https://github.com/apache/beam/pull/13066#discussion_r506744419", "createdAt": "2020-10-16T22:16:56Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab)\n+      # Converting the same expressions should yeild the same pcolls\n+      pc_3a, pc_2a, pc_ab = convert.to_pcollection(df_3a, df_2a, df_ab)\n+\n+      self.assertEqual(id(pc_2a), id(pc_2a_))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY1NzMwNA==", "bodyText": "thanks, done", "url": "https://github.com/apache/beam/pull/13066#discussion_r508657304", "createdAt": "2020-10-20T16:10:02Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab)\n+      # Converting the same expressions should yeild the same pcolls\n+      pc_3a, pc_2a, pc_ab = convert.to_pcollection(df_3a, df_2a, df_ab)\n+\n+      self.assertEqual(id(pc_2a), id(pc_2a_))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDQxOQ=="}, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3NDAxODc3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/convert_test.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxODozMlrOHjRQOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjowOTo1NVrOHlF-Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDg4OA==", "bodyText": "Move these tests into the former, so you can also assert that to_pcollection(x, yield_elements='schema') != to_pcollection(x, yield_elements='pandas') (i.e. no accidental cross-cache contamination).", "url": "https://github.com/apache/beam/pull/13066#discussion_r506744888", "createdAt": "2020-10-16T22:18:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab)\n+      # Converting the same expressions should yeild the same pcolls\n+      pc_3a, pc_2a, pc_ab = convert.to_pcollection(df_3a, df_2a, df_ab)\n+\n+      self.assertEqual(id(pc_2a), id(pc_2a_))\n+      self.assertEqual(id(pc_ab), id(pc_ab_))\n+\n+      assert_that(pc_2a, equal_to(list(2 * a)), label='Check2a')\n+      assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n+      assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n+\n+  def test_convert_memoization_yield_pandas(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY1NzE3OA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/13066#discussion_r508657178", "createdAt": "2020-10-20T16:09:55Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab)\n+      # Converting the same expressions should yeild the same pcolls\n+      pc_3a, pc_2a, pc_ab = convert.to_pcollection(df_3a, df_2a, df_ab)\n+\n+      self.assertEqual(id(pc_2a), id(pc_2a_))\n+      self.assertEqual(id(pc_ab), id(pc_ab_))\n+\n+      assert_that(pc_2a, equal_to(list(2 * a)), label='Check2a')\n+      assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n+      assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n+\n+  def test_convert_memoization_yield_pandas(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDg4OA=="}, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 74}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2921, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}