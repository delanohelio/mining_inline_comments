{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxMzIzMTI1", "number": 11360, "reviewThreads": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMTozNzoxMFrODwk0QA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMToyMTowMFrOD9Hc1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMjYxNDQwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMTozNzoxMVrOGDqSxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNDozNzozN1rOGD8Rag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ5MTg0NQ==", "bodyText": "options.getSchema()", "url": "https://github.com/apache/beam/pull/11360#discussion_r406491845", "createdAt": "2020-04-09T21:37:11Z", "author": {"login": "takidau"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(int)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0bc349c20463acbe6250d2fa8ed22bb86e003572"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjc4NjQxMA==", "bodyText": "thanks!", "url": "https://github.com/apache/beam/pull/11360#discussion_r406786410", "createdAt": "2020-04-10T14:37:37Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(int)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ5MTg0NQ=="}, "originalCommit": {"oid": "0bc349c20463acbe6250d2fa8ed22bb86e003572"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODAwODgzOnYy", "diffSide": "RIGHT", "path": "CHANGES.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjozMzo1MlrOGLf03g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjozMzo1MlrOGLf03g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcwODk1OA==", "bodyText": "Please, move it to [2.22.0] - Unreleased section (I guess you need to rebase to have it).", "url": "https://github.com/apache/beam/pull/11360#discussion_r414708958", "createdAt": "2020-04-24T16:33:52Z", "author": {"login": "aromanenko-dev"}, "path": "CHANGES.md", "diffHunk": "@@ -28,7 +28,8 @@\n \n ## I/Os\n \n-* Support for X source added (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).\n+* Support for reading from Snowflake added (Java) ([BEAM-9722](https://issues.apache.org/jira/browse/BEAM-9722)).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODA1MjM0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjo0NDozMVrOGLgPAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxMDo1NjoyMlrOGWyAAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNTY1MQ==", "bodyText": "Does it establish a connection over a network?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414715651", "createdAt": "2020-04-24T16:44:31Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM0MTA2NA==", "bodyText": "Yes, it does. The reason is to early validate if connection parameters are valid", "url": "https://github.com/apache/beam/pull/11360#discussion_r415341064", "createdAt": "2020-04-26T15:57:21Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNTY1MQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTA1OA==", "bodyText": "Probably you should provide a withoutValidation() option so that users who submit a pipeline form a node that do not have access to the server can disable this.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191058", "createdAt": "2020-05-16T21:02:54Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNTY1MQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjU0MTA1Ng==", "bodyText": "That's good idea, added", "url": "https://github.com/apache/beam/pull/11360#discussion_r426541056", "createdAt": "2020-05-18T10:56:22Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNTY1MQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODA1NzM4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjo0NTo1MFrOGLgSKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjo0NTo1MFrOGLgSKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNjQ1Ng==", "bodyText": "Please, add a Javadoc for all \"with...\" exposed methods, since it's a part of public API of this IO", "url": "https://github.com/apache/beam/pull/11360#discussion_r414716456", "createdAt": "2020-04-24T16:45:50Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODA3MDI4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjo0OTowMVrOGLgZ1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjo0OTowMVrOGLgZ1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxODQyMA==", "bodyText": "nit \"are not allowed\"", "url": "https://github.com/apache/beam/pull/11360#discussion_r414718420", "createdAt": "2020-04-24T16:49:01Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 259}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODA4Mzg0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjo1MjoyMFrOGLgh8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMDo0NToxNFrOGUA0Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ==", "bodyText": "Will the temp directory be cleaned if pipeline was failed before?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414720499", "createdAt": "2020-04-24T16:52:20Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg0MDk3NQ==", "bodyText": "It seems that it doesn't at the moment. @kkucharc will share more info here", "url": "https://github.com/apache/beam/pull/11360#discussion_r420840975", "createdAt": "2020-05-06T14:34:48Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDk0MDUzNA==", "bodyText": "Yes, we checked and it doesn't. @aromanenko-dev do you think it should be provided? In case of testing, probably tests should take care of cleanup.", "url": "https://github.com/apache/beam/pull/11360#discussion_r420940534", "createdAt": "2020-05-06T16:50:19Z", "author": {"login": "kkucharc"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDk0ODkwMQ==", "bodyText": "I'm afraid that many failed pipelines could lead to wasting of used dick space in this case. It would be better to avoid such behavior, if possible.", "url": "https://github.com/apache/beam/pull/11360#discussion_r420948901", "createdAt": "2020-05-06T17:03:07Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTU4NjY2MA==", "bodyText": "While I generally agree that failing pipelines will eventually lead to lots of garbage on GCS (or other storage in future) I'm not sure how to ensure those files are deleted regardless of pipeline status.\nAFAIK there is nothing like @After or @Teardown for PTransform which we have in our IO. Using Wait transform gives us ability to remove files once data is read and seems reasonable choice.\nI've been checking how i.e. BigQueryIO is handling that case as it also needs to cleanup and they also have a cleanup transform that is called once all rows are read. I assume in their case cleanup also won't be run if something in-between fails.\nHow about filing JIRA issue for this case and try to look at solution in parallel to delivering other pieces of the Snowflake connector? @aromanenko-dev @kkucharc  WDYT?", "url": "https://github.com/apache/beam/pull/11360#discussion_r421586660", "createdAt": "2020-05-07T15:17:55Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjQxMw==", "bodyText": "This is a known issue with BQ source as well. Failed pipelines can leave temporary files behind. I'm afraid there is no good solution today. I think we need to introduce some sort of a generalized cleanup step to address this.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422436413", "createdAt": "2020-05-09T01:17:31Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE2OTkwOA==", "bodyText": "In this case, I'd suggest to add this into IO class Java doc to make users aware that such situation is possible and it will require manual procedure to clean temp dirs up.", "url": "https://github.com/apache/beam/pull/11360#discussion_r423169908", "createdAt": "2020-05-11T16:37:15Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYzODExNA==", "bodyText": "I've added a note here - would it be enough?", "url": "https://github.com/apache/beam/pull/11360#discussion_r423638114", "createdAt": "2020-05-12T10:45:14Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODEyNzcyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzowMzoyNlrOGLg8yQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzowMzoyNlrOGLg8yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyNzM2OQ==", "bodyText": "I believe, in general, COPY INTO can use the different external locations, like Amazon S3, Google Cloud Storage, or Microsoft Azure. So, please make it configurable for users even if this IO now  supports only GCS.", "url": "https://github.com/apache/beam/pull/11360#discussion_r414727369", "createdAt": "2020-04-24T17:03:26Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 268}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODE2MTMwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeServiceImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxMjowN1rOGLhQ6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxMjowN1rOGLhQ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczMjUyMA==", "bodyText": "Is it GCS specific implementation? I think we need to make it configurable which type of external location to use.", "url": "https://github.com/apache/beam/pull/11360#discussion_r414732520", "createdAt": "2020-04-24T17:12:07Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeServiceImpl.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import java.sql.Connection;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.util.function.Consumer;\n+import javax.sql.DataSource;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+\n+/**\n+ * Implemenation of {@link org.apache.beam.sdk.io.snowflake.SnowflakeService} used in production.\n+ */\n+public class SnowflakeServiceImpl implements SnowflakeService {\n+\n+  @Override\n+  public String copyIntoStage(\n+      SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+      String query,\n+      String table,\n+      String integrationName,\n+      String stagingBucketName,\n+      String tmpDirName)\n+      throws SQLException {\n+\n+    String from;\n+    if (query != null) {\n+      // Query must be surrounded with brackets\n+      from = String.format(\"(%s)\", query);\n+    } else {\n+      from = table;\n+    }\n+\n+    String externalLocation = String.format(\"gcs://%s/%s/\", stagingBucketName, tmpDirName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODE3MTg0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/credentials/package-info.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxNDo1OFrOGLhXNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxNDo1OFrOGLhXNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNDEzNQ==", "bodyText": "Add ASF license header", "url": "https://github.com/apache/beam/pull/11360#discussion_r414734135", "createdAt": "2020-04-24T17:14:58Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/credentials/package-info.java", "diffHunk": "@@ -0,0 +1,2 @@\n+/** Credentials for SnowflakeIO. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODE3MjQyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/package-info.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxNTowNVrOGLhXiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxNTowNVrOGLhXiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNDIxNw==", "bodyText": "Add ASF license header", "url": "https://github.com/apache/beam/pull/11360#discussion_r414734217", "createdAt": "2020-04-24T17:15:05Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/package-info.java", "diffHunk": "@@ -0,0 +1,2 @@\n+/** Snowflake IO transforms. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODE4MzE3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/FakeSnowflakeDatabase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxNzozNFrOGLhd9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxNjowMjowMFrOGMGePw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNTg2MQ==", "bodyText": "Are there any mock/fake already implemented snowflake databases that can be used for testing?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414735861", "createdAt": "2020-04-24T17:17:34Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/FakeSnowflakeDatabase.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import net.snowflake.client.jdbc.SnowflakeSQLException;\n+\n+/** Fake implementation of SnowFlake warehouse used in test code. */\n+public class FakeSnowflakeDatabase implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTM0MjE0Mw==", "bodyText": "There is no such thing AFAIK - we had to prepare fake ourselves", "url": "https://github.com/apache/beam/pull/11360#discussion_r415342143", "createdAt": "2020-04-26T16:02:00Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/FakeSnowflakeDatabase.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import net.snowflake.client.jdbc.SnowflakeSQLException;\n+\n+/** Fake implementation of SnowFlake warehouse used in test code. */\n+public class FakeSnowflakeDatabase implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNTg2MQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODE4Nzk4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/package-info.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxODo0NlrOGLhhCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxODo0NlrOGLhhCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNjY1MA==", "bodyText": "Add ASF license header", "url": "https://github.com/apache/beam/pull/11360#discussion_r414736650", "createdAt": "2020-04-24T17:18:46Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/package-info.java", "diffHunk": "@@ -0,0 +1,2 @@\n+/** Snowflake IO tests. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODczMDA1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxOTozOTo0NFrOGLme0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxNzowMTozOFrOGRcoIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgxODAwMA==", "bodyText": "Why not use Apache Beam's notion of a FileSystem?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414818000", "createdAt": "2020-04-24T19:39:44Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+/** Interface which defines common methods for cloud providers. */\n+public interface SnowflakeCloudProvider {\n+  void removeFiles(String bucketName, String pathOnBucket);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDk0ODAwMg==", "bodyText": "Thanks for suggestion @lukecwik . I changed it to GCSFileSystem.\nI also tried to change removeFiles in Fake implementation to use LocalFileSystem but I am a little bit concerned - LocalFileSystem doesn't match nested directories and fails on deleting not empty directory. That can cause that testing directory won't be cleaned and tests will become flaky.", "url": "https://github.com/apache/beam/pull/11360#discussion_r420948002", "createdAt": "2020-05-06T17:01:38Z", "author": {"login": "kkucharc"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+/** Interface which defines common methods for cloud providers. */\n+public interface SnowflakeCloudProvider {\n+  void removeFiles(String bucketName, String pathOnBucket);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgxODAwMA=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3ODczMTUxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/GCSProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxOTo0MDowM1rOGLmfmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxOTo0MDowM1rOGLmfmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgxODIwMg==", "bodyText": "Why not use the Apache Beam GCS filesystem?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414818202", "createdAt": "2020-04-24T19:40:03Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/GCSProvider.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import com.google.api.gax.paging.Page;\n+import com.google.cloud.storage.Blob;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageOptions;\n+import java.io.Serializable;\n+\n+public class GCSProvider implements SnowflakeCloudProvider, Serializable {\n+\n+  @Override\n+  public void removeFiles(String bucketName, String pathOnBucket) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyOTkzNjMzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo0NToxM1rOGS2kaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQwNjoyNTo1OFrOGVNqQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMTYwOA==", "bodyText": "Seems like all these methods are related to file handling. Can't we just use functionality offered by FileSystems class here ? If that is inadequate for some reason we can consider adding to that instead of introducing a new abstraction here as well.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422421608", "createdAt": "2020-05-08T23:45:13Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import java.io.IOException;\n+\n+/** Interface which defines common methods for cloud providers. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg5NzA4OA==", "bodyText": "Yes, you're right :) , I got rid of SnowfalkeCloudProvider with result of removing 200 lines of code. Thanks a lot @chamikaramj for spotting this", "url": "https://github.com/apache/beam/pull/11360#discussion_r424897088", "createdAt": "2020-05-14T06:25:58Z", "author": {"login": "purbanow"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import java.io.IOException;\n+\n+/** Interface which defines common methods for cloud providers. */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMTYwOA=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyOTkzNzk2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo0NjozNFrOGS2lVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMDo1MToxN1rOGUBAiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMTg0Ng==", "bodyText": "Is it always Google Could Storage or are other Beam FileSystem implementations supported as well ? (or can be supported in the future).", "url": "https://github.com/apache/beam/pull/11360#discussion_r422421846", "createdAt": "2020-05-08T23:46:34Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY0MTIyNQ==", "bodyText": "It's GCS currently, but in the future other cloud providers can be added. I've made changes to the docs - PTAL", "url": "https://github.com/apache/beam/pull/11360#discussion_r423641225", "createdAt": "2020-05-12T10:51:17Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMTg0Ng=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyOTk0MjY2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo1MDozMVrOGS2oCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo1MDozMVrOGS2oCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMjUzNg==", "bodyText": "I don't think implementation details are useful here (and probably detrimental here since we might change that at any point).", "url": "https://github.com/apache/beam/pull/11360#discussion_r422422536", "createdAt": "2020-05-08T23:50:31Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyOTk0NzE3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo1Mzo0MFrOGS2qig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMDo1MTowOFrOGUBAPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMzE3OA==", "bodyText": "GCS bucket name or is it something more generic ?", "url": "https://github.com/apache/beam/pull/11360#discussion_r422423178", "createdAt": "2020-05-08T23:53:40Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY0MTE1MA==", "bodyText": "GCS currently. Any of the Snowflake-supported cloud storages are possible in the future.", "url": "https://github.com/apache/beam/pull/11360#discussion_r423641150", "createdAt": "2020-05-12T10:51:08Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMzE3OA=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 256}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyOTk0ODA3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo1NDoxMlrOGS2q_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo1NDoxMlrOGS2q_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMzI5NA==", "bodyText": "Probably clarify what you mean by \"Storage Integration\" here.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422423294", "createdAt": "2020-05-08T23:54:12Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 265}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA0MDY4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMToxMzo0OVrOGS3cxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxMDo1MjozM1rOGUBC6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjAzNg==", "bodyText": "Coder to be used by the output PCollection generated by the source.\nBTW that's the default if a coder is not provided ? This does not have to be always configurable.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422436036", "createdAt": "2020-05-09T01:13:49Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY0MTgzMg==", "bodyText": "Thanks for the better description.\nThere is no default coder, user must always set coder - is this a problem?", "url": "https://github.com/apache/beam/pull/11360#discussion_r423641832", "createdAt": "2020-05-12T10:52:33Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjAzNg=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 283}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA0NDU0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMToxOTo0MlrOGS3e_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMToxNDo1N1rOGWcr7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjYwNw==", "bodyText": "Is there a reason to introduce two transforms here instead of having a single transform that converts Strings to type T ?", "url": "https://github.com/apache/beam/pull/11360#discussion_r422436607", "createdAt": "2020-05-09T01:19:42Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY0NDY2MA==", "bodyText": "Yes, the story behind it is to make connector more user-friendly.\nWe wanted to handle the CSV parsing for the user and left them only mapping from Object[] to T.\nOtherwise the user would have to do CSV line parsing on their own and face the same issues that we could fix for them (like having a comma in string) by setting proper delimiters and quoting when calling COPY statement.\nDo you think it's OK like this?", "url": "https://github.com/apache/beam/pull/11360#discussion_r423644660", "createdAt": "2020-05-12T10:57:55Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjYwNw=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTg1NQ==", "bodyText": "Sounds good.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191855", "createdAt": "2020-05-16T21:14:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjYwNw=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 396}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA1MTIxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMToyOTozMFrOGS3iyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMToyOTozMFrOGS3iyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNzU3OQ==", "bodyText": "Can we introduce just one create(SnowflakeCredentials credentials) method and fork based on the type of object passed in ?", "url": "https://github.com/apache/beam/pull/11360#discussion_r422437579", "createdAt": "2020-05-09T01:29:30Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        String csvLine = c.element();\n+        CSVParser parser = new CSVParserBuilder().withQuoteChar(CSV_QUOTE_CHAR.charAt(0)).build();\n+        String[] parts = parser.parseLine(csvLine);\n+        c.output(parts);\n+      }\n+    }\n+\n+    private static class MapStringArrayToUserDataFn<T> extends DoFn<String[], T> {\n+      private final CsvMapper<T> csvMapper;\n+\n+      public MapStringArrayToUserDataFn(CsvMapper<T> csvMapper) {\n+        this.csvMapper = csvMapper;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        context.output(csvMapper.mapRow(context.element()));\n+      }\n+    }\n+\n+    public static class CleanTmpFilesFromGcsFn extends DoFn<Object, Object> {\n+      private final String bucketName;\n+      private final String bucketPath;\n+      private final SnowflakeCloudProvider snowflakeCloudProvider;\n+\n+      public CleanTmpFilesFromGcsFn(\n+          String bucketName, String bucketPath, SnowflakeCloudProvider snowflakeCloudProvider) {\n+        this.bucketName = bucketName;\n+        this.bucketPath = bucketPath;\n+        this.snowflakeCloudProvider = snowflakeCloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        snowflakeCloudProvider.removeFiles(bucketName, bucketPath);\n+      }\n+    }\n+\n+    @Override\n+    public void populateDisplayData(DisplayData.Builder builder) {\n+      super.populateDisplayData(builder);\n+      if (getQuery() != null) {\n+        builder.add(DisplayData.item(\"query\", getQuery()));\n+      }\n+      if (getTable() != null) {\n+        builder.add(DisplayData.item(\"table\", getTable()));\n+      }\n+      builder.add(DisplayData.item(\"integrationName\", getIntegrationName()));\n+      builder.add(DisplayData.item(\"stagingBucketName\", getStagingBucketName()));\n+      builder.add(DisplayData.item(\"csvMapper\", getCsvMapper().getClass().getName()));\n+      builder.add(DisplayData.item(\"coder\", getCoder().getClass().getName()));\n+      if (getDataSourceProviderFn() instanceof HasDisplayData) {\n+        ((HasDisplayData) getDataSourceProviderFn()).populateDisplayData(builder);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A POJO describing a {@link DataSource}, providing all properties allowing to create a {@link\n+   * DataSource}.\n+   */\n+  @AutoValue\n+  public abstract static class DataSourceConfiguration implements Serializable {\n+    @Nullable\n+    public abstract String getUrl();\n+\n+    @Nullable\n+    public abstract String getUsername();\n+\n+    @Nullable\n+    public abstract String getPassword();\n+\n+    @Nullable\n+    public abstract PrivateKey getPrivateKey();\n+\n+    @Nullable\n+    public abstract String getOauthToken();\n+\n+    @Nullable\n+    public abstract String getDatabase();\n+\n+    @Nullable\n+    public abstract String getWarehouse();\n+\n+    @Nullable\n+    public abstract String getSchema();\n+\n+    @Nullable\n+    public abstract String getServerName();\n+\n+    @Nullable\n+    public abstract Integer getPortNumber();\n+\n+    @Nullable\n+    public abstract String getRole();\n+\n+    @Nullable\n+    public abstract Integer getLoginTimeout();\n+\n+    @Nullable\n+    public abstract Boolean getSsl();\n+\n+    @Nullable\n+    public abstract DataSource getDataSource();\n+\n+    abstract Builder builder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setUrl(String url);\n+\n+      abstract Builder setUsername(String username);\n+\n+      abstract Builder setPassword(String password);\n+\n+      abstract Builder setPrivateKey(PrivateKey privateKey);\n+\n+      abstract Builder setOauthToken(String oauthToken);\n+\n+      abstract Builder setDatabase(String database);\n+\n+      abstract Builder setWarehouse(String warehouse);\n+\n+      abstract Builder setSchema(String schema);\n+\n+      abstract Builder setServerName(String serverName);\n+\n+      abstract Builder setPortNumber(Integer portNumber);\n+\n+      abstract Builder setRole(String role);\n+\n+      abstract Builder setLoginTimeout(Integer loginTimeout);\n+\n+      abstract Builder setSsl(Boolean ssl);\n+\n+      abstract Builder setDataSource(DataSource dataSource);\n+\n+      abstract DataSourceConfiguration build();\n+    }\n+\n+    /**\n+     * Creates {@link DataSourceConfiguration} from existing instance of {@link DataSource}.\n+     *\n+     * @param dataSource - an instance of {@link DataSource}.\n+     */\n+    public static DataSourceConfiguration create(DataSource dataSource) {\n+      checkArgument(dataSource instanceof Serializable, \"dataSource must be Serializable\");\n+      return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n+          .setDataSource(dataSource)\n+          .build();\n+    }\n+\n+    /**\n+     * Creates {@link DataSourceConfiguration} from instance of {@link SnowflakeCredentials}.\n+     *\n+     * @param credentials - an instance of {@link SnowflakeCredentials}.\n+     */\n+    public static DataSourceConfiguration create(SnowflakeCredentials credentials) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 556}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDA2MDM3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo0MTo1N1rOGS3npA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNTozMjoxN1rOGU3X_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzODgyMA==", "bodyText": "Can we introduce tests to actually read data (from SnowFlakeFakeServiceImpl) and verify results. Please see DirectRunner-based BQ pipeline read tests.\nhttps://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOReadTest.java#L285\nAlso there's no need to manually call 'expand' in tests. We can just execute regular Beam pipelines that will perform the expansion.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422438820", "createdAt": "2020-05-09T01:41:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.read;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.beam.sdk.Pipeline.PipelineExecutionException;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.io.AvroGeneratedUser;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeCloudProvider;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeCloudProvider;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.unit.BatchTestPipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+@RunWith(JUnit4.class)\n+public class SnowflakeIOReadTest {\n+  public static final String FAKE_TABLE = \"FAKE_TABLE\";\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static BatchTestPipelineOptions options;\n+\n+  private static SnowflakeService snowflakeService;\n+  private static SnowflakeCloudProvider snowflakeCloudProvider;\n+\n+  private static String stagingBucketName;\n+  private static String integrationName;\n+\n+  private static List<GenericRecord> avroTestData;\n+\n+  @BeforeClass\n+  public static void setup() {\n+\n+    List<String> testData = Arrays.asList(\"Paul,51,red\", \"Jackson,41,green\");\n+\n+    avroTestData =\n+        ImmutableList.of(\n+            new AvroGeneratedUser(\"Paul\", 51, \"red\"),\n+            new AvroGeneratedUser(\"Jackson\", 41, \"green\"));\n+\n+    FakeSnowflakeDatabase.createTableWithElements(FAKE_TABLE, testData);\n+\n+    PipelineOptionsFactory.register(BatchTestPipelineOptions.class);\n+    options = TestPipeline.testingPipelineOptions().as(BatchTestPipelineOptions.class);\n+    options.setServerName(\"NULL.snowflakecomputing.com\");\n+    options.setStorageIntegration(\"STORAGE_INTEGRATION\");\n+    options.setStagingBucketName(\"BUCKET\");\n+\n+    stagingBucketName = options.getStagingBucketName();\n+    integrationName = options.getStorageIntegration();\n+\n+    dataSourceConfiguration =\n+        SnowflakeIO.DataSourceConfiguration.create(new FakeSnowflakeBasicDataSource())\n+            .withServerName(options.getServerName());\n+\n+    snowflakeService = new FakeSnowflakeServiceImpl();\n+    snowflakeCloudProvider = new FakeSnowflakeCloudProvider();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingStagingBucketName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withStagingBucketName() is required\");\n+\n+    SnowflakeIO.Read<GenericRecord> read =\n+        SnowflakeIO.<GenericRecord>read(snowflakeService, snowflakeCloudProvider)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema()));\n+\n+    read.expand(null);\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingIntegrationName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withIntegrationName() is required\");\n+\n+    SnowflakeIO.Read<GenericRecord> read =\n+        SnowflakeIO.<GenericRecord>read(snowflakeService, snowflakeCloudProvider)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema()));\n+\n+    read.expand(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUzMTk2NQ==", "bodyText": "Thank you @chamikaramj for pointing this. I changed  this tests so now it uses pipeline.run() instead of expand. The interesting thing is that I had to use TestRule similarly as in BQ tests (otherwise tests were failing with Outputs for non-root node SnowflakeIO.Read are null), do you know why is that? Is it something connected with \"clearing\" the pipeline for each test?\nIn case of reading data by SnowFlakeFakeServiceImpl there is a test testReadWithConfigIsProper (previously testConfigIsProper) L248 which does that. Do you think there should be more tests like this?", "url": "https://github.com/apache/beam/pull/11360#discussion_r424531965", "createdAt": "2020-05-13T15:32:17Z", "author": {"login": "kkucharc"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.read;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.beam.sdk.Pipeline.PipelineExecutionException;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.io.AvroGeneratedUser;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeCloudProvider;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeCloudProvider;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.unit.BatchTestPipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+@RunWith(JUnit4.class)\n+public class SnowflakeIOReadTest {\n+  public static final String FAKE_TABLE = \"FAKE_TABLE\";\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static BatchTestPipelineOptions options;\n+\n+  private static SnowflakeService snowflakeService;\n+  private static SnowflakeCloudProvider snowflakeCloudProvider;\n+\n+  private static String stagingBucketName;\n+  private static String integrationName;\n+\n+  private static List<GenericRecord> avroTestData;\n+\n+  @BeforeClass\n+  public static void setup() {\n+\n+    List<String> testData = Arrays.asList(\"Paul,51,red\", \"Jackson,41,green\");\n+\n+    avroTestData =\n+        ImmutableList.of(\n+            new AvroGeneratedUser(\"Paul\", 51, \"red\"),\n+            new AvroGeneratedUser(\"Jackson\", 41, \"green\"));\n+\n+    FakeSnowflakeDatabase.createTableWithElements(FAKE_TABLE, testData);\n+\n+    PipelineOptionsFactory.register(BatchTestPipelineOptions.class);\n+    options = TestPipeline.testingPipelineOptions().as(BatchTestPipelineOptions.class);\n+    options.setServerName(\"NULL.snowflakecomputing.com\");\n+    options.setStorageIntegration(\"STORAGE_INTEGRATION\");\n+    options.setStagingBucketName(\"BUCKET\");\n+\n+    stagingBucketName = options.getStagingBucketName();\n+    integrationName = options.getStorageIntegration();\n+\n+    dataSourceConfiguration =\n+        SnowflakeIO.DataSourceConfiguration.create(new FakeSnowflakeBasicDataSource())\n+            .withServerName(options.getServerName());\n+\n+    snowflakeService = new FakeSnowflakeServiceImpl();\n+    snowflakeCloudProvider = new FakeSnowflakeCloudProvider();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingStagingBucketName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withStagingBucketName() is required\");\n+\n+    SnowflakeIO.Read<GenericRecord> read =\n+        SnowflakeIO.<GenericRecord>read(snowflakeService, snowflakeCloudProvider)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema()));\n+\n+    read.expand(null);\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingIntegrationName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withIntegrationName() is required\");\n+\n+    SnowflakeIO.Read<GenericRecord> read =\n+        SnowflakeIO.<GenericRecord>read(snowflakeService, snowflakeCloudProvider)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema()));\n+\n+    read.expand(null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzODgyMA=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDEwNjM3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMDo1OTowNFrOGWcnqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMDo1OTowNFrOGWcnqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MDc2MQ==", "bodyText": "s/deleted/cleaned up", "url": "https://github.com/apache/beam/pull/11360#discussion_r426190761", "createdAt": "2020-05-16T20:59:04Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDEwNzc5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMTowMToxMlrOGWcoUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMTowMToxMlrOGWcoUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MDkyOQ==", "bodyText": "they may remain and will have to ...", "url": "https://github.com/apache/beam/pull/11360#discussion_r426190929", "createdAt": "2020-05-16T21:01:12Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDExMTAwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMTowNzo0NFrOGWcqGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxOTowNDoyOFrOGYZ9Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw==", "bodyText": "What will happen in case of a failure and a retry of a bundle ?\nI think you should add a Reshuffle step after CopyIntoStageFn() step to checkpoint the set of files (supported by most runners).\nAlso, make sure you have considered what would happen if CopyIntoStageFn() itself fails and retries. For example, BQ source tries to look for an already started export job before starting a new one.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191387", "createdAt": "2020-05-16T21:07:44Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc1MDYxOQ==", "bodyText": "If running COPY fails and it will be retried it will fail on any retry.\nIt's because the first attempt will put some files into staging bucket and another try will find out that staging location is not empty and it will fail.\nThere is a OVERWRITE option in Snowflake, telling that if there are files on cloud storage they will be overwritten, but we don't use it. The reason is simple - we're reading all data from this temporary location. If for any reason, on this location there will be garbage files, data will be malformed. I imagine it might happen that first attempt will put some files, and another attempt might have slightly different filenames and files will be appended. In this case we have partial results from first try and full results from second - we don't want that.\nThere is also no such thing as \"job\" in this case. It's just, simple, standalone \"plain old good sql query\" that is executed against snowflake. It is parallelised, but internally, on Snowflake side - from pipeline perspective it's one-off task. It doesn't have a job_id which status can be checked and resumed or waited for finish.\nI'd suggest to keep it as is - failed pipeline in this case might be better solution than broken data. WDYT?", "url": "https://github.com/apache/beam/pull/11360#discussion_r426750619", "createdAt": "2020-05-18T16:27:30Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzcxODc3Mw==", "bodyText": "It's better but I don't think it's a good user experience to fail the pipeline whenever a bundle is retried. Bundle retries are a usual part of the runner execution and source/sink transforms should be able to handle that.\nWhat is usually done in this case is to write data to a new temporary location each time the copy step is run. Then you add a Reshuffle write after the copy operation to checkpoint the results. This way you can guarantee that the set of files that are output to the next step are from a single execution and the job does not have to fail simply because a bundle is retried.\nWhen you cleanup you have to clean all temporary locations including retries.\nI'm fine if you don't want to handle this in the first try but let's at least add a TODO and a JIRA to fix this later.", "url": "https://github.com/apache/beam/pull/11360#discussion_r427718773", "createdAt": "2020-05-20T03:18:30Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk5Mzc5NA==", "bodyText": "Thanks @chamikaramj for insisting on this. The approach you suggested works perfectly and was easy to incorporate into our case.\nI've made changes that are creating a new sub-directory for each attempt of CopyIntoStageFn. If this Fn is retried, another subdirectory is created and if step succeeds, exactly this subdirectory will be passed to FileIO to further processing. In the end, all sub-directories are cleaned-up.\n\nI'm however not sure about Reshuffle. In our case output from CopyIntoStageFn is single String containing cloud storage path from where files should be read by FileIO. Does it make sense to reshuffle it? I've tested it and it works well with retries without reshuffle.\nAlso, Reshuffle is deprecated and might not work well in portability (if I understand docs properly) and we're aiming to use cross-language to support Python SDK.\nDo you think the Reshuffle step is needed here?", "url": "https://github.com/apache/beam/pull/11360#discussion_r427993794", "createdAt": "2020-05-20T13:07:02Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIxNTM0Mg==", "bodyText": "So you have following two steps that will get fused by the runner and run together.\nParDo(CopyIntoStageFn)+ParDo(ReadFiles)\nNow assume there's a failure when reading files. This will result in expensive CopyIntoStageFn from being executed again.\nReshuffle (for most runners) has the affect of breaking fusion and hence checkpointing results of ParDo(CopyIntoStageFn). So if there's a failure when reading, the ParDo(CopyIntoStageFn) step will not have to be re-run. Also this is a very inexpensive shuffle (we only send file-names through it).\nReshuffle is deprecated since its behavior across runners is not well defined. Some runners may consider it to be an identity operation while most runners will checkpoint results. I don't think it can be removed from the SDK anytime soon and many widely used connectors depend on it.", "url": "https://github.com/apache/beam/pull/11360#discussion_r428215342", "createdAt": "2020-05-20T18:19:27Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODI0NDI0Mg==", "bodyText": "Thanks @chamikaramj for detailed explanation. I've added Reshuffle step in latest commit", "url": "https://github.com/apache/beam/pull/11360#discussion_r428244242", "createdAt": "2020-05-20T19:04:28Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDExNTU2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMToxNjo0MlrOGWcsdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMToxNjo0MlrOGWcsdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTk4OQ==", "bodyText": "Probably specify MoveOptions.IGNORE_MISSING_FILES to not fail incase this bundle is retries after a failure (either the source or the destination should exist).\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/fs/MoveOptions.java#L30", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191989", "createdAt": "2020-05-16T21:16:42Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          stagingBucketDir,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(ParDo.of(new CleanTmpFilesFromGcsFn(stagingBucketDir)));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketDir;\n+      private final SnowflakeService snowflakeService;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketDir,\n+          SnowflakeService snowflakeService) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketDir = stagingBucketDir;\n+        this.snowflakeService = snowflakeService;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn, query, table, integrationName, stagingBucketDir);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        String csvLine = c.element();\n+        CSVParser parser = new CSVParserBuilder().withQuoteChar(CSV_QUOTE_CHAR.charAt(0)).build();\n+        String[] parts = parser.parseLine(csvLine);\n+        c.output(parts);\n+      }\n+    }\n+\n+    private static class MapStringArrayToUserDataFn<T> extends DoFn<String[], T> {\n+      private final CsvMapper<T> csvMapper;\n+\n+      public MapStringArrayToUserDataFn(CsvMapper<T> csvMapper) {\n+        this.csvMapper = csvMapper;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        context.output(csvMapper.mapRow(context.element()));\n+      }\n+    }\n+\n+    public static class CleanTmpFilesFromGcsFn extends DoFn<Object, Object> {\n+      private final String stagingBucketDir;\n+\n+      public CleanTmpFilesFromGcsFn(String stagingBucketDir) {\n+        this.stagingBucketDir = stagingBucketDir;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        String combinedPath = stagingBucketDir + \"*\";\n+        List<ResourceId> paths =\n+            FileSystems.match(combinedPath).metadata().stream()\n+                .map(metadata -> metadata.resourceId())\n+                .collect(Collectors.toList());\n+\n+        FileSystems.delete(paths);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 421}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1NDExNzk5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMToyMTowMFrOGWctpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMDoyNTowNVrOGXxSjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MjI5NA==", "bodyText": "Please make sure that all read paths are covered by tests.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426192294", "createdAt": "2020-05-16T21:21:00Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.read;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.beam.sdk.Pipeline.PipelineExecutionException;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.io.AvroGeneratedUser;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.unit.BatchTestPipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.TestRule;\n+import org.junit.runner.Description;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.junit.runners.model.Statement;\n+\n+@RunWith(JUnit4.class)\n+public class SnowflakeIOReadTest {\n+  public static final String FAKE_TABLE = \"FAKE_TABLE\";\n+\n+  @Rule public transient TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static BatchTestPipelineOptions options;\n+\n+  private static SnowflakeService snowflakeService;\n+\n+  private static String stagingBucketName;\n+  private static String integrationName;\n+\n+  private static List<GenericRecord> avroTestData;\n+\n+  private transient TemporaryFolder testFolder = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void setup() {\n+\n+    List<String> testData = Arrays.asList(\"Paul,51,red\", \"Jackson,41,green\");\n+\n+    avroTestData =\n+        ImmutableList.of(\n+            new AvroGeneratedUser(\"Paul\", 51, \"red\"),\n+            new AvroGeneratedUser(\"Jackson\", 41, \"green\"));\n+\n+    FakeSnowflakeDatabase.createTableWithElements(FAKE_TABLE, testData);\n+    PipelineOptionsFactory.register(BatchTestPipelineOptions.class);\n+    options = TestPipeline.testingPipelineOptions().as(BatchTestPipelineOptions.class);\n+    options.setServerName(\"NULL.snowflakecomputing.com\");\n+    options.setStorageIntegration(\"STORAGE_INTEGRATION\");\n+    options.setStagingBucketName(\"BUCKET\");\n+\n+    stagingBucketName = options.getStagingBucketName();\n+    integrationName = options.getStorageIntegration();\n+\n+    dataSourceConfiguration =\n+        SnowflakeIO.DataSourceConfiguration.create(new FakeSnowflakeBasicDataSource())\n+            .withServerName(options.getServerName());\n+\n+    snowflakeService = new FakeSnowflakeServiceImpl();\n+  }\n+\n+  @Rule\n+  public final transient TestRule folderThenPipeline =\n+      new TestRule() {\n+        @Override\n+        public Statement apply(final Statement base, final Description description) {\n+          Statement withPipeline =\n+              new Statement() {\n+                @Override\n+                public void evaluate() {\n+                  pipeline = TestPipeline.fromOptions(options);\n+                }\n+              };\n+          return testFolder.apply(withPipeline, description);\n+        }\n+      };\n+\n+  @Test\n+  public void testConfigIsMissingStagingBucketName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withStagingBucketName() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingIntegrationName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withIntegrationName() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingCsvMapper() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withCsvMapper() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingCoder() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withCoder() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper()));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingFromTableOrFromQuery() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"fromTable() or fromQuery() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingDataSourceConfiguration() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\n+        \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigContainsFromQueryAndFromTable() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"fromTable() and fromQuery() are not allowed together\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromQuery(\"\")\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testTableDoesntExist() {\n+    exceptionRule.expect(PipelineExecutionException.class);\n+    exceptionRule.expectMessage(\"SQL compilation error: Table does not exist\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(\"NON_EXIST\")\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testReadWithConfigIsProper() {\n+    PCollection<GenericRecord> items =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU3Nzk5OQ==", "bodyText": "I've just added tests for withQuery as it was missing and fixed tests in general", "url": "https://github.com/apache/beam/pull/11360#discussion_r427577999", "createdAt": "2020-05-19T20:25:05Z", "author": {"login": "DariuszAniszewski"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.read;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.beam.sdk.Pipeline.PipelineExecutionException;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.io.AvroGeneratedUser;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.unit.BatchTestPipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.TestRule;\n+import org.junit.runner.Description;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.junit.runners.model.Statement;\n+\n+@RunWith(JUnit4.class)\n+public class SnowflakeIOReadTest {\n+  public static final String FAKE_TABLE = \"FAKE_TABLE\";\n+\n+  @Rule public transient TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static BatchTestPipelineOptions options;\n+\n+  private static SnowflakeService snowflakeService;\n+\n+  private static String stagingBucketName;\n+  private static String integrationName;\n+\n+  private static List<GenericRecord> avroTestData;\n+\n+  private transient TemporaryFolder testFolder = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void setup() {\n+\n+    List<String> testData = Arrays.asList(\"Paul,51,red\", \"Jackson,41,green\");\n+\n+    avroTestData =\n+        ImmutableList.of(\n+            new AvroGeneratedUser(\"Paul\", 51, \"red\"),\n+            new AvroGeneratedUser(\"Jackson\", 41, \"green\"));\n+\n+    FakeSnowflakeDatabase.createTableWithElements(FAKE_TABLE, testData);\n+    PipelineOptionsFactory.register(BatchTestPipelineOptions.class);\n+    options = TestPipeline.testingPipelineOptions().as(BatchTestPipelineOptions.class);\n+    options.setServerName(\"NULL.snowflakecomputing.com\");\n+    options.setStorageIntegration(\"STORAGE_INTEGRATION\");\n+    options.setStagingBucketName(\"BUCKET\");\n+\n+    stagingBucketName = options.getStagingBucketName();\n+    integrationName = options.getStorageIntegration();\n+\n+    dataSourceConfiguration =\n+        SnowflakeIO.DataSourceConfiguration.create(new FakeSnowflakeBasicDataSource())\n+            .withServerName(options.getServerName());\n+\n+    snowflakeService = new FakeSnowflakeServiceImpl();\n+  }\n+\n+  @Rule\n+  public final transient TestRule folderThenPipeline =\n+      new TestRule() {\n+        @Override\n+        public Statement apply(final Statement base, final Description description) {\n+          Statement withPipeline =\n+              new Statement() {\n+                @Override\n+                public void evaluate() {\n+                  pipeline = TestPipeline.fromOptions(options);\n+                }\n+              };\n+          return testFolder.apply(withPipeline, description);\n+        }\n+      };\n+\n+  @Test\n+  public void testConfigIsMissingStagingBucketName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withStagingBucketName() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingIntegrationName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withIntegrationName() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingCsvMapper() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withCsvMapper() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingCoder() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withCoder() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper()));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingFromTableOrFromQuery() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"fromTable() or fromQuery() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingDataSourceConfiguration() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\n+        \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigContainsFromQueryAndFromTable() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"fromTable() and fromQuery() are not allowed together\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromQuery(\"\")\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testTableDoesntExist() {\n+    exceptionRule.expect(PipelineExecutionException.class);\n+    exceptionRule.expectMessage(\"SQL compilation error: Table does not exist\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(\"NON_EXIST\")\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testReadWithConfigIsProper() {\n+    PCollection<GenericRecord> items =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MjI5NA=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 245}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1451, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}