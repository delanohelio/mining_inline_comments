{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU3NDkxNDAx", "number": 12385, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNjo0NTowOFrOETwsCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDozOTozMVrOEqhwbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTU2MTA1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNjo0NTowOFrOG5rT0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo0MDo1NlrOG54Y4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMTYwMA==", "bodyText": "nit:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    request.config.option.test_pipeline_options\n          \n          \n            \n                    if request.config.option.test_pipeline_options else '')\n          \n          \n            \n                    request.config.option.test_pipeline_options or '')", "url": "https://github.com/apache/beam/pull/12385#discussion_r463131600", "createdAt": "2020-07-30T16:45:08Z", "author": {"login": "mxm"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NTg4OA==", "bodyText": "done", "url": "https://github.com/apache/beam/pull/12385#discussion_r463345888", "createdAt": "2020-07-31T00:40:56Z", "author": {"login": "ibzib"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMTYwMA=="}, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 194}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTU2NTYwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNjo0NjoyM1rOG5rWuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1MToyOFrOG54k7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjM0NA==", "bodyText": "Could this be problematic if one of the option values contains a space, e.g. as part of a json string for the environment config?", "url": "https://github.com/apache/beam/pull/12385#discussion_r463132344", "createdAt": "2020-07-30T16:46:23Z", "author": {"login": "mxm"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--flink_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NjQwMg==", "bodyText": "Good catch. I used shlex.split(...) instead.", "url": "https://github.com/apache/beam/pull/12385#discussion_r463346402", "createdAt": "2020-07-31T00:43:00Z", "author": {"login": "ibzib"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--flink_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjM0NA=="}, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0ODk3NQ==", "bodyText": "I also had to make many changes to preserve the correct string quoting. Tox in particular does something weird with quotes in its commands, so I had to double-escape the strings before passing to Tox and unescape them after.", "url": "https://github.com/apache/beam/pull/12385#discussion_r463348975", "createdAt": "2020-07-31T00:51:28Z", "author": {"login": "ibzib"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +53,380 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]\n+\n _LOGGER = logging.getLogger(__name__)\n \n Row = typing.NamedTuple(\"Row\", [(\"col1\", int), (\"col2\", unicode)])\n beam.coders.registry.register_coder(Row, beam.coders.RowCoder)\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.flink_runner_test \\\n-  #     --flink_job_server_jar=/path/to/job_server.jar \\\n-  #     --environment_type=docker \\\n-  #     --extra_experiments=beam_experiments \\\n-  #     [FlinkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--flink_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--streaming',\n-      default=False,\n-      action='store_true',\n-      help='Job type. batch or streaming')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback.')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  flink_job_server_jar = (\n-      known_args.flink_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:flink:%s:job-server:shadowJar' %\n-          FlinkRunnerOptions.PUBLISHED_FLINK_VERSIONS[-1]))\n-  streaming = known_args.streaming\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    conf_dir = None\n-    expansion_port = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-      if cls.conf_dir and exists(cls.conf_dir):\n-        _LOGGER.info(\"removing conf dir: %s\" % cls.conf_dir)\n-        rmtree(cls.conf_dir)\n-      super(FlinkRunnerTest, cls).tearDownClass()\n-\n-    @classmethod\n-    def _create_conf_dir(cls):\n-      \"\"\"Create (and save a static reference to) a \"conf dir\", used to provide\n-       metrics configs and verify metrics output\n-\n-       It gets cleaned up when the suite is done executing\"\"\"\n-\n-      if hasattr(cls, 'conf_dir'):\n-        cls.conf_dir = mkdtemp(prefix='flinktest-conf')\n-\n-        # path for a FileReporter to write metrics to\n-        cls.test_metrics_path = path.join(cls.conf_dir, 'test-metrics.txt')\n-\n-        # path to write Flink configuration to\n-        conf_path = path.join(cls.conf_dir, 'flink-conf.yaml')\n-        file_reporter = 'org.apache.beam.runners.flink.metrics.FileReporter'\n-        with open(conf_path, 'w') as f:\n-          f.write(\n-              linesep.join([\n-                  'metrics.reporters: file',\n-                  'metrics.reporter.file.class: %s' % file_reporter,\n-                  'metrics.reporter.file.path: %s' % cls.test_metrics_path,\n-                  'metrics.scope.operator: <operator_name>',\n-              ]))\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='flinktest')\n-\n-      cls._create_conf_dir()\n-      cls.expansion_port = expansion_port\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dorg.slf4j.simpleLogger.defaultLogLevel=warn',\n-            '-jar',\n-            flink_job_server_jar,\n-            '--flink-master',\n-            '[local]',\n-            '--flink-conf-dir',\n-            cls.conf_dir,\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    @classmethod\n-    def get_expansion_service(cls):\n-      # TODO Move expansion address resides into PipelineOptions\n-      return 'localhost:%s' % cls.expansion_port\n-\n-    def create_options(self):\n-      options = super(FlinkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      options._all_options['parallelism'] = 2\n-      options.view_as(PortableOptions).environment_type = (\n-          environment_type.upper())\n-      if environment_config:\n-        options.view_as(PortableOptions).environment_config = environment_config\n-\n-      if streaming:\n-        options.view_as(StandardOptions).streaming = True\n-      return options\n-\n-    # Can't read host files from within docker, read a \"local\" file there.\n-    def test_read(self):\n-      with self.create_pipeline() as p:\n-        lines = p | beam.io.ReadFromText('/etc/profile')\n-        assert_that(lines, lambda lines: len(lines) > 0)\n-\n-    def test_no_subtransform_composite(self):\n-      raise unittest.SkipTest(\"BEAM-4781\")\n \n-    def test_external_transform(self):\n+class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  conf_dir = None\n+  expansion_port = None\n+  flink_job_server_jar = None\n+\n+  def __init__(self, *args, **kwargs):\n+    super(FlinkRunnerTest, self).__init__(*args, **kwargs)\n+    self.environment_type = None\n+    self.environment_config = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--flink_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMjM0NA=="}, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTU3NDk2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/portability/spark_runner_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNjo0ODo0MlrOG5rcfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1MTozNlrOG54lGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMzgyMg==", "bodyText": "Same here, what about whitespace in the values?", "url": "https://github.com/apache/beam/pull/12385#discussion_r463133822", "createdAt": "2020-07-30T16:48:42Z", "author": {"login": "mxm"}, "path": "sdks/python/apache_beam/runners/portability/spark_runner_test.py", "diffHunk": "@@ -21,141 +21,158 @@\n \n import argparse\n import logging\n-import sys\n import unittest\n from shutil import rmtree\n from tempfile import mkdtemp\n \n-from apache_beam.options.pipeline_options import DebugOptions\n+import pytest\n+\n from apache_beam.options.pipeline_options import PortableOptions\n from apache_beam.runners.portability import job_server\n from apache_beam.runners.portability import portable_runner\n from apache_beam.runners.portability import portable_runner_test\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.spark_runner_test \\\n-  #     --spark_job_server_jar=/path/to/job_server.jar \\\n-  #     [SparkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--spark_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--environment_cache_millis',\n-      help='Environment cache TTL in milliseconds.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  spark_job_server_jar = (\n-      known_args.spark_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:spark:job-server:shadowJar'))\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  environment_cache_millis = known_args.environment_cache_millis\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class SparkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='sparktest')\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dbeam.spark.test.reuseSparkContext=true',\n-            '-jar',\n-            spark_job_server_jar,\n-            '--spark-master-url',\n-            'local',\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    def create_options(self):\n-      options = super(SparkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      portable_options = options.view_as(PortableOptions)\n-      portable_options.environment_type = environment_type.upper()\n-      if environment_config:\n-        portable_options.environment_config = environment_config\n-      if environment_cache_millis:\n-        portable_options.environment_cache_millis = environment_cache_millis\n-\n-      return options\n-\n-    def test_metrics(self):\n-      # Skip until Spark runner supports metrics.\n-      raise unittest.SkipTest(\"BEAM-7219\")\n-\n-    def test_sdf(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_with_watermark_tracking(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_with_sdf_initiated_checkpointing(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_synthetic_source(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_external_transforms(self):\n-      # Skip until Spark runner supports external transforms.\n-      raise unittest.SkipTest(\"BEAM-7232\")\n-\n-    def test_callbacks_with_exception(self):\n-      # Skip until Spark runner supports bundle finalization.\n-      raise unittest.SkipTest(\"BEAM-7233\")\n-\n-    def test_register_finalizations(self):\n-      # Skip until Spark runner supports bundle finalization.\n-      raise unittest.SkipTest(\"BEAM-7233\")\n-\n-    def test_flattened_side_input(self):\n-      # Blocked on support for transcoding\n-      # https://jira.apache.org/jira/browse/BEAM-7236\n-      super(SparkRunnerTest,\n-            self).test_flattened_side_input(with_transcoding=False)\n-\n-    # Inherits all other tests from PortableRunnerTest.\n+# Run as\n+#\n+# pytest spark_runner_test.py \\\n+#     [--test_pipeline_options \"--spark_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [SparkRunnerTest.test_method, ...]\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+\n+class SparkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  expansion_port = None\n+  spark_job_server_jar = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--spark_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTAxNg==", "bodyText": "done", "url": "https://github.com/apache/beam/pull/12385#discussion_r463349016", "createdAt": "2020-07-31T00:51:36Z", "author": {"login": "ibzib"}, "path": "sdks/python/apache_beam/runners/portability/spark_runner_test.py", "diffHunk": "@@ -21,141 +21,158 @@\n \n import argparse\n import logging\n-import sys\n import unittest\n from shutil import rmtree\n from tempfile import mkdtemp\n \n-from apache_beam.options.pipeline_options import DebugOptions\n+import pytest\n+\n from apache_beam.options.pipeline_options import PortableOptions\n from apache_beam.runners.portability import job_server\n from apache_beam.runners.portability import portable_runner\n from apache_beam.runners.portability import portable_runner_test\n \n-if __name__ == '__main__':\n-  # Run as\n-  #\n-  # python -m apache_beam.runners.portability.spark_runner_test \\\n-  #     --spark_job_server_jar=/path/to/job_server.jar \\\n-  #     [SparkRunnerTest.test_method, ...]\n-\n-  parser = argparse.ArgumentParser(add_help=True)\n-  parser.add_argument(\n-      '--spark_job_server_jar', help='Job server jar to submit jobs.')\n-  parser.add_argument(\n-      '--environment_type',\n-      default='loopback',\n-      help='Environment type. docker, process, or loopback')\n-  parser.add_argument('--environment_config', help='Environment config.')\n-  parser.add_argument(\n-      '--environment_cache_millis',\n-      help='Environment cache TTL in milliseconds.')\n-  parser.add_argument(\n-      '--extra_experiments',\n-      default=[],\n-      action='append',\n-      help='Beam experiments config.')\n-  known_args, args = parser.parse_known_args(sys.argv)\n-  sys.argv = args\n-\n-  spark_job_server_jar = (\n-      known_args.spark_job_server_jar or\n-      job_server.JavaJarJobServer.path_to_beam_jar(\n-          'runners:spark:job-server:shadowJar'))\n-  environment_type = known_args.environment_type.lower()\n-  environment_config = (\n-      known_args.environment_config if known_args.environment_config else None)\n-  environment_cache_millis = known_args.environment_cache_millis\n-  extra_experiments = known_args.extra_experiments\n-\n-  # This is defined here to only be run when we invoke this file explicitly.\n-  class SparkRunnerTest(portable_runner_test.PortableRunnerTest):\n-    _use_grpc = True\n-    _use_subprocesses = True\n-\n-    @classmethod\n-    def _subprocess_command(cls, job_port, expansion_port):\n-      # will be cleaned up at the end of this method, and recreated and used by\n-      # the job server\n-      tmp_dir = mkdtemp(prefix='sparktest')\n-\n-      try:\n-        return [\n-            'java',\n-            '-Dbeam.spark.test.reuseSparkContext=true',\n-            '-jar',\n-            spark_job_server_jar,\n-            '--spark-master-url',\n-            'local',\n-            '--artifacts-dir',\n-            tmp_dir,\n-            '--job-port',\n-            str(job_port),\n-            '--artifact-port',\n-            '0',\n-            '--expansion-port',\n-            str(expansion_port),\n-        ]\n-      finally:\n-        rmtree(tmp_dir)\n-\n-    @classmethod\n-    def get_runner(cls):\n-      return portable_runner.PortableRunner()\n-\n-    def create_options(self):\n-      options = super(SparkRunnerTest, self).create_options()\n-      options.view_as(\n-          DebugOptions).experiments = ['beam_fn_api'] + extra_experiments\n-      portable_options = options.view_as(PortableOptions)\n-      portable_options.environment_type = environment_type.upper()\n-      if environment_config:\n-        portable_options.environment_config = environment_config\n-      if environment_cache_millis:\n-        portable_options.environment_cache_millis = environment_cache_millis\n-\n-      return options\n-\n-    def test_metrics(self):\n-      # Skip until Spark runner supports metrics.\n-      raise unittest.SkipTest(\"BEAM-7219\")\n-\n-    def test_sdf(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_with_watermark_tracking(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_with_sdf_initiated_checkpointing(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_sdf_synthetic_source(self):\n-      # Skip until Spark runner supports SDF.\n-      raise unittest.SkipTest(\"BEAM-7222\")\n-\n-    def test_external_transforms(self):\n-      # Skip until Spark runner supports external transforms.\n-      raise unittest.SkipTest(\"BEAM-7232\")\n-\n-    def test_callbacks_with_exception(self):\n-      # Skip until Spark runner supports bundle finalization.\n-      raise unittest.SkipTest(\"BEAM-7233\")\n-\n-    def test_register_finalizations(self):\n-      # Skip until Spark runner supports bundle finalization.\n-      raise unittest.SkipTest(\"BEAM-7233\")\n-\n-    def test_flattened_side_input(self):\n-      # Blocked on support for transcoding\n-      # https://jira.apache.org/jira/browse/BEAM-7236\n-      super(SparkRunnerTest,\n-            self).test_flattened_side_input(with_transcoding=False)\n-\n-    # Inherits all other tests from PortableRunnerTest.\n+# Run as\n+#\n+# pytest spark_runner_test.py \\\n+#     [--test_pipeline_options \"--spark_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [SparkRunnerTest.test_method, ...]\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+\n+class SparkRunnerTest(portable_runner_test.PortableRunnerTest):\n+  _use_grpc = True\n+  _use_subprocesses = True\n+\n+  expansion_port = None\n+  spark_job_server_jar = None\n+\n+  @pytest.fixture(autouse=True)\n+  def parse_options(self, request):\n+    test_pipeline_options = (\n+        request.config.option.test_pipeline_options\n+        if request.config.option.test_pipeline_options else '')\n+    parser = argparse.ArgumentParser(add_help=True)\n+    parser.add_argument(\n+        '--spark_job_server_jar',\n+        help='Job server jar to submit jobs.',\n+        action='store')\n+    parser.add_argument(\n+        '--environment_type',\n+        default='LOOPBACK',\n+        choices=['DOCKER', 'PROCESS', 'LOOPBACK'],\n+        help='Set the environment type for running user code. DOCKER runs '\n+        'user code in a container. PROCESS runs user code in '\n+        'automatically started processes. LOOPBACK runs user code on '\n+        'the same process that originally submitted the job.')\n+    parser.add_argument(\n+        '--environment_config',\n+        help='Set environment configuration for running the user code.\\n For '\n+        'DOCKER: Url for the docker image.\\n For PROCESS: json of the '\n+        'form {\"os\": \"<OS>\", \"arch\": \"<ARCHITECTURE>\", \"command\": '\n+        '\"<process to execute>\", \"env\":{\"<Environment variables 1>\": '\n+        '\"<ENV_VAL>\"} }. All fields in the json are optional except '\n+        'command.')\n+    known_args, unknown_args = parser.parse_known_args(\n+        test_pipeline_options.split())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzEzMzgyMg=="}, "originalCommit": {"oid": "f0f2ec0ac24f41221a816d9eea6d79e515626f8a"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5NjQwNTYxOnYy", "diffSide": "RIGHT", "path": "sdks/python/scripts/run_pytest.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQyMjo0MDozOVrOG6Ysjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNjowNTozMFrOHbRaTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3NTIxNQ==", "bodyText": "Can you instead check if test-pipeline-options is set? I believe that's how all the other ITs are skipped (see TestPipeline code).", "url": "https://github.com/apache/beam/pull/12385#discussion_r463875215", "createdAt": "2020-07-31T22:40:39Z", "author": {"login": "udim"}, "path": "sdks/python/scripts/run_pytest.sh", "diffHunk": "@@ -29,10 +29,16 @@ posargs=$2\n \n # Run with pytest-xdist and without.\n pytest -o junit_suite_name=${envname} \\\n-  --junitxml=pytest_${envname}.xml -m 'not no_xdist' -n 6 --pyargs ${posargs}\n+  --junitxml=pytest_${envname}.xml -m 'not no_xdist' -n 6 \\\n+  --ignore=apache_beam/runners/portability/flink_runner_test.py \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25713d8cf88113bf84dad82f038bdc0122d27db0"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM1ODg2MQ==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12385#discussion_r498358861", "createdAt": "2020-10-01T16:05:30Z", "author": {"login": "ibzib"}, "path": "sdks/python/scripts/run_pytest.sh", "diffHunk": "@@ -29,10 +29,16 @@ posargs=$2\n \n # Run with pytest-xdist and without.\n pytest -o junit_suite_name=${envname} \\\n-  --junitxml=pytest_${envname}.xml -m 'not no_xdist' -n 6 --pyargs ${posargs}\n+  --junitxml=pytest_${envname}.xml -m 'not no_xdist' -n 6 \\\n+  --ignore=apache_beam/runners/portability/flink_runner_test.py \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg3NTIxNQ=="}, "originalCommit": {"oid": "25713d8cf88113bf84dad82f038bdc0122d27db0"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDI1NTg1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDoyMDoxN1rOHcx8dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDoyMzoyNVrOHcx_Xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0MDQ3MA==", "bodyText": "--test-pipeline-options", "url": "https://github.com/apache/beam/pull/12385#discussion_r499940470", "createdAt": "2020-10-06T00:20:17Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +54,386 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7219836bd567531287e880e3aa5b5d3a24e21122"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0MTIxNA==", "bodyText": "Oh yeah, --test_pipeline_options is now required (even though it should be possible to leave it empty). Boyuan, would you mind filing a PR to fix this?", "url": "https://github.com/apache/beam/pull/12385#discussion_r499941214", "createdAt": "2020-10-06T00:23:25Z", "author": {"login": "ibzib"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +54,386 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0MDQ3MA=="}, "originalCommit": {"oid": "7219836bd567531287e880e3aa5b5d3a24e21122"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDI4NzE2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDozOTozMVrOHcyO3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwMDozOTozMVrOHcyO3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTk0NTE4MA==", "bodyText": "And the test filter here doesn't work for me properly. The working version for me is\npytest flink_runner_test.py::TestClass:test_case --test-pipeline-options \"--flink_job_server_jar=XXX --environment_type=XXX \"", "url": "https://github.com/apache/beam/pull/12385#discussion_r499945180", "createdAt": "2020-10-06T00:39:31Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/portability/flink_runner_test.py", "diffHunk": "@@ -53,361 +54,386 @@\n from apache_beam.transforms import userstate\n from apache_beam.transforms.sql import SqlTransform\n \n+# Run as\n+#\n+# pytest flink_runner_test.py \\\n+#     [--test_pipeline_options \"--flink_job_server_jar=/path/to/job_server.jar \\\n+#                               --environment_type=DOCKER\"] \\\n+#     [FlinkRunnerTest.test_method, ...]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7219836bd567531287e880e3aa5b5d3a24e21122"}, "originalPosition": 26}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 965, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}