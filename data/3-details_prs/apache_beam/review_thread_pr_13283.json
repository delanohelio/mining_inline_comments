{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE3MDU5NTUw", "number": 13283, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNzo1OToxNVrOE30Igw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxOTozNjoyN1rOE5pRhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2OTYxMjgzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/ptransform.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNzo1OToxNVrOHxYTzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxOToxNjoxNFrOHxa1lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MDU1Nw==", "bodyText": "Ditto. Not sure why we need this update given that tests should already pass for Dataflow.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521540557", "createdAt": "2020-11-11T17:59:15Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/transforms/ptransform.py", "diffHunk": "@@ -695,9 +695,9 @@ def from_runner_api(cls,\n     # type: (...) -> Optional[PTransform]\n     if proto is None or proto.spec is None or not proto.spec.urn:\n       return None\n-    parameter_type, constructor = cls._known_urns[proto.spec.urn]\n \n     try:\n+      parameter_type, constructor = cls._known_urns[proto.spec.urn]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU4MTk3NA==", "bodyText": "It was for fixing Java xlang pipelines but the fix is still incomplete anyway. Will revert this.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521581974", "createdAt": "2020-11-11T19:16:14Z", "author": {"login": "ihji"}, "path": "sdks/python/apache_beam/transforms/ptransform.py", "diffHunk": "@@ -695,9 +695,9 @@ def from_runner_api(cls,\n     # type: (...) -> Optional[PTransform]\n     if proto is None or proto.spec is None or not proto.spec.urn:\n       return None\n-    parameter_type, constructor = cls._known_urns[proto.spec.urn]\n \n     try:\n+      parameter_type, constructor = cls._known_urns[proto.spec.urn]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MDU1Nw=="}, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2OTYxMzMwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNzo1OToyMVrOHxYUFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMjo1OToxOFrOHxhWMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MDYyOQ==", "bodyText": "Why do we need to update dataflow_runner for the test ?\nExisting tests should already work for dataflow_runner without these updates.\nDuplicates environments are handled here when submitting the Dataflow job request: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py#L303", "url": "https://github.com/apache/beam/pull/13283#discussion_r521540629", "createdAt": "2020-11-11T17:59:21Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -542,19 +593,32 @@ def run_pipeline(self, pipeline, options):\n       # TODO(chamikara): remove following pipeline and pipeline proto recreation\n       # after portable job submission path is fully in place.\n       from apache_beam import Pipeline\n-      pipeline = Pipeline.from_runner_api(\n+      pipeline, src_context = Pipeline.from_runner_api(\n           self.proto_pipeline,\n           pipeline.runner,\n           options,\n+          return_context=True,\n           allow_proto_holders=True)\n \n       # Pipelines generated from proto do not have output set to PDone set for\n       # leaf elements.\n       pipeline.visit(self._set_pdone_visitor(pipeline))\n \n+      from apache_beam.runners import pipeline_context\n+      dst_context = pipeline_context.PipelineContext(\n+          component_id_map=pipeline.component_id_map,\n+          default_environment=self._default_environment)\n+\n+      # Copy external environments to prevent dangling environment ids\n+      pipeline.visit(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU4MDg2MA==", "bodyText": "Python pipeline using python external transform doesn't work without handling dangling environment ids.\nIt's not because of duplicated environments. It's because of missing environments. Dangling environment IDs are generated when 1) PTransform URN from external environment is known (therefore RunnerAPIPTransformHolder won't work as expected) 2) PipelineContext from rehydration process is not used in the second proto conversion. Python external transforms in Python pipelines are rehydrated to AppliedPTransform since external PTransform URN is known to the SDK however AppliedPTransform only keeps an environment id not environment itself. Environments are saved separately in PipelineContext but we dropped the PipelineContext from rehydration in the second proto conversion.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521580860", "createdAt": "2020-11-11T19:14:00Z", "author": {"login": "ihji"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -542,19 +593,32 @@ def run_pipeline(self, pipeline, options):\n       # TODO(chamikara): remove following pipeline and pipeline proto recreation\n       # after portable job submission path is fully in place.\n       from apache_beam import Pipeline\n-      pipeline = Pipeline.from_runner_api(\n+      pipeline, src_context = Pipeline.from_runner_api(\n           self.proto_pipeline,\n           pipeline.runner,\n           options,\n+          return_context=True,\n           allow_proto_holders=True)\n \n       # Pipelines generated from proto do not have output set to PDone set for\n       # leaf elements.\n       pipeline.visit(self._set_pdone_visitor(pipeline))\n \n+      from apache_beam.runners import pipeline_context\n+      dst_context = pipeline_context.PipelineContext(\n+          component_id_map=pipeline.component_id_map,\n+          default_environment=self._default_environment)\n+\n+      # Copy external environments to prevent dangling environment ids\n+      pipeline.visit(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MDYyOQ=="}, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYxODAyNg==", "bodyText": "\"RunnerAPIPTransformHolder\" is expected to go away soon (when we enable portable job submission). Do you think we can skip this test (that use Python external transforms in a Python pipeline) for now and try without these updates when we have portable job submission ?", "url": "https://github.com/apache/beam/pull/13283#discussion_r521618026", "createdAt": "2020-11-11T20:25:45Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -542,19 +593,32 @@ def run_pipeline(self, pipeline, options):\n       # TODO(chamikara): remove following pipeline and pipeline proto recreation\n       # after portable job submission path is fully in place.\n       from apache_beam import Pipeline\n-      pipeline = Pipeline.from_runner_api(\n+      pipeline, src_context = Pipeline.from_runner_api(\n           self.proto_pipeline,\n           pipeline.runner,\n           options,\n+          return_context=True,\n           allow_proto_holders=True)\n \n       # Pipelines generated from proto do not have output set to PDone set for\n       # leaf elements.\n       pipeline.visit(self._set_pdone_visitor(pipeline))\n \n+      from apache_beam.runners import pipeline_context\n+      dst_context = pipeline_context.PipelineContext(\n+          component_id_map=pipeline.component_id_map,\n+          default_environment=self._default_environment)\n+\n+      # Copy external environments to prevent dangling environment ids\n+      pipeline.visit(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MDYyOQ=="}, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY4ODYyNQ==", "bodyText": "Make sense. Updated to enable PythonUsingJava and PythonUsingSql only.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521688625", "createdAt": "2020-11-11T22:59:18Z", "author": {"login": "ihji"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -542,19 +593,32 @@ def run_pipeline(self, pipeline, options):\n       # TODO(chamikara): remove following pipeline and pipeline proto recreation\n       # after portable job submission path is fully in place.\n       from apache_beam import Pipeline\n-      pipeline = Pipeline.from_runner_api(\n+      pipeline, src_context = Pipeline.from_runner_api(\n           self.proto_pipeline,\n           pipeline.runner,\n           options,\n+          return_context=True,\n           allow_proto_holders=True)\n \n       # Pipelines generated from proto do not have output set to PDone set for\n       # leaf elements.\n       pipeline.visit(self._set_pdone_visitor(pipeline))\n \n+      from apache_beam.runners import pipeline_context\n+      dst_context = pipeline_context.PipelineContext(\n+          component_id_map=pipeline.component_id_map,\n+          default_environment=self._default_environment)\n+\n+      # Copy external environments to prevent dangling environment ids\n+      pipeline.visit(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MDYyOQ=="}, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2OTYxNTc4OnYy", "diffSide": "RIGHT", "path": "sdks/python/setup.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxODowMDowMlrOHxYVpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxOTo1Njo1MlrOHxcJUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MTAyOQ==", "bodyText": "This probably should be a separate PR.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521541029", "createdAt": "2020-11-11T18:00:02Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/setup.py", "diffHunk": "@@ -256,70 +256,71 @@ def run(self):\n       'Python %s.%s. You may encounter bugs or missing features.' % (\n           sys.version_info.major, sys.version_info.minor))\n \n-setuptools.setup(\n-    name=PACKAGE_NAME,\n-    version=PACKAGE_VERSION,\n-    description=PACKAGE_DESCRIPTION,\n-    long_description=PACKAGE_LONG_DESCRIPTION,\n-    url=PACKAGE_URL,\n-    download_url=PACKAGE_DOWNLOAD_URL,\n-    author=PACKAGE_AUTHOR,\n-    author_email=PACKAGE_EMAIL,\n-    packages=setuptools.find_packages(),\n-    package_data={'apache_beam': [\n-        '*/*.pyx', '*/*/*.pyx', '*/*.pxd', '*/*/*.pxd', '*/*.h', '*/*/*.h',\n-        'testing/data/*.yaml', 'portability/api/*.yaml']},\n-    ext_modules=cythonize([\n-        'apache_beam/**/*.pyx',\n-        'apache_beam/coders/coder_impl.py',\n-        'apache_beam/metrics/cells.py',\n-        'apache_beam/metrics/execution.py',\n-        'apache_beam/runners/common.py',\n-        'apache_beam/runners/worker/logger.py',\n-        'apache_beam/runners/worker/opcounters.py',\n-        'apache_beam/runners/worker/operations.py',\n-        'apache_beam/transforms/cy_combiners.py',\n-        'apache_beam/utils/counters.py',\n-        'apache_beam/utils/windowed_value.py',\n-    ]),\n-    install_requires=REQUIRED_PACKAGES,\n-    python_requires=python_requires,\n-    test_suite='nose.collector',\n-    # BEAM-8840: Do NOT use tests_require or setup_requires.\n-    extras_require={\n-        'docs': ['Sphinx>=1.5.2,<2.0'],\n-        'test': REQUIRED_TEST_PACKAGES,\n-        'gcp': GCP_REQUIREMENTS,\n-        'interactive': INTERACTIVE_BEAM,\n-        'interactive_test': INTERACTIVE_BEAM_TEST,\n-        'aws': AWS_REQUIREMENTS,\n-        'azure': AZURE_REQUIREMENTS\n-    },\n-    zip_safe=False,\n-    # PyPI package information.\n-    classifiers=[\n-        'Intended Audience :: End Users/Desktop',\n-        'License :: OSI Approved :: Apache Software License',\n-        'Operating System :: POSIX :: Linux',\n-        'Programming Language :: Python :: 3.6',\n-        'Programming Language :: Python :: 3.7',\n-        'Programming Language :: Python :: 3.8',\n-        # When updating vesion classifiers, also update version warnings\n-        # above and in apache_beam/__init__.py.\n-        'Topic :: Software Development :: Libraries',\n-        'Topic :: Software Development :: Libraries :: Python Modules',\n-    ],\n-    license='Apache License, Version 2.0',\n-    keywords=PACKAGE_KEYWORDS,\n-    entry_points={\n-        'nose.plugins.0.10': [\n-            'beam_test_plugin = test_config:BeamTestPlugin',\n-        ]},\n-    cmdclass={\n-        'build_py': generate_protos_first(build_py),\n-        'develop': generate_protos_first(develop),\n-        'egg_info': generate_protos_first(egg_info),\n-        'test': generate_protos_first(test),\n-        'mypy': generate_protos_first(mypy),\n-    },\n-)\n+if __name__ == '__main__':", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYwMzQxMA==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521603410", "createdAt": "2020-11-11T19:56:52Z", "author": {"login": "ihji"}, "path": "sdks/python/setup.py", "diffHunk": "@@ -256,70 +256,71 @@ def run(self):\n       'Python %s.%s. You may encounter bugs or missing features.' % (\n           sys.version_info.major, sys.version_info.minor))\n \n-setuptools.setup(\n-    name=PACKAGE_NAME,\n-    version=PACKAGE_VERSION,\n-    description=PACKAGE_DESCRIPTION,\n-    long_description=PACKAGE_LONG_DESCRIPTION,\n-    url=PACKAGE_URL,\n-    download_url=PACKAGE_DOWNLOAD_URL,\n-    author=PACKAGE_AUTHOR,\n-    author_email=PACKAGE_EMAIL,\n-    packages=setuptools.find_packages(),\n-    package_data={'apache_beam': [\n-        '*/*.pyx', '*/*/*.pyx', '*/*.pxd', '*/*/*.pxd', '*/*.h', '*/*/*.h',\n-        'testing/data/*.yaml', 'portability/api/*.yaml']},\n-    ext_modules=cythonize([\n-        'apache_beam/**/*.pyx',\n-        'apache_beam/coders/coder_impl.py',\n-        'apache_beam/metrics/cells.py',\n-        'apache_beam/metrics/execution.py',\n-        'apache_beam/runners/common.py',\n-        'apache_beam/runners/worker/logger.py',\n-        'apache_beam/runners/worker/opcounters.py',\n-        'apache_beam/runners/worker/operations.py',\n-        'apache_beam/transforms/cy_combiners.py',\n-        'apache_beam/utils/counters.py',\n-        'apache_beam/utils/windowed_value.py',\n-    ]),\n-    install_requires=REQUIRED_PACKAGES,\n-    python_requires=python_requires,\n-    test_suite='nose.collector',\n-    # BEAM-8840: Do NOT use tests_require or setup_requires.\n-    extras_require={\n-        'docs': ['Sphinx>=1.5.2,<2.0'],\n-        'test': REQUIRED_TEST_PACKAGES,\n-        'gcp': GCP_REQUIREMENTS,\n-        'interactive': INTERACTIVE_BEAM,\n-        'interactive_test': INTERACTIVE_BEAM_TEST,\n-        'aws': AWS_REQUIREMENTS,\n-        'azure': AZURE_REQUIREMENTS\n-    },\n-    zip_safe=False,\n-    # PyPI package information.\n-    classifiers=[\n-        'Intended Audience :: End Users/Desktop',\n-        'License :: OSI Approved :: Apache Software License',\n-        'Operating System :: POSIX :: Linux',\n-        'Programming Language :: Python :: 3.6',\n-        'Programming Language :: Python :: 3.7',\n-        'Programming Language :: Python :: 3.8',\n-        # When updating vesion classifiers, also update version warnings\n-        # above and in apache_beam/__init__.py.\n-        'Topic :: Software Development :: Libraries',\n-        'Topic :: Software Development :: Libraries :: Python Modules',\n-    ],\n-    license='Apache License, Version 2.0',\n-    keywords=PACKAGE_KEYWORDS,\n-    entry_points={\n-        'nose.plugins.0.10': [\n-            'beam_test_plugin = test_config:BeamTestPlugin',\n-        ]},\n-    cmdclass={\n-        'build_py': generate_protos_first(build_py),\n-        'develop': generate_protos_first(develop),\n-        'egg_info': generate_protos_first(egg_info),\n-        'test': generate_protos_first(test),\n-        'mypy': generate_protos_first(mypy),\n-    },\n-)\n+if __name__ == '__main__':", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTU0MTAyOQ=="}, "originalCommit": {"oid": "46b5cb623ed485c03999601d2f764a82d30e7b5c"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MDY1NTgwOnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMzozMzoyM1rOHxiLog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDozNTowMVrOHyNXcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMjMwNg==", "bodyText": "Please remove experiments \"beam_fn_api\" and \"unified_worker\" and add \"use_runner_v2\"", "url": "https://github.com/apache/beam/pull/13283#discussion_r521702306", "createdAt": "2020-11-11T23:33:23Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/build.gradle", "diffHunk": "@@ -312,6 +313,37 @@ task validatesRunnerStreaming {\n   ))\n }\n \n+createCrossLanguageValidatesRunnerTask(\n+  startJobServer: buildAndPushDockerContainer,\n+  cleanupJobServer: cleanUpDockerImages,\n+  classpath: configurations.validatesRunner,\n+  numParallelTests: Integer.MAX_VALUE,\n+  needsSdkLocation: true,\n+  pythonPipelineOptions: [\n+    \"--runner=TestDataflowRunner\",\n+    \"--project=${dataflowProject}\",\n+    \"--region=${dataflowRegion}\",\n+    \"--sdk_harness_container_image_overrides=.*java.*,${dockerImageContainer}:${dockerTag}\",\n+    \"--experiments=beam_fn_api\",\n+    \"--experiments=use_unified_worker\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwOTg0MA==", "bodyText": "done.", "url": "https://github.com/apache/beam/pull/13283#discussion_r522409840", "createdAt": "2020-11-12T20:35:01Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/build.gradle", "diffHunk": "@@ -312,6 +313,37 @@ task validatesRunnerStreaming {\n   ))\n }\n \n+createCrossLanguageValidatesRunnerTask(\n+  startJobServer: buildAndPushDockerContainer,\n+  cleanupJobServer: cleanUpDockerImages,\n+  classpath: configurations.validatesRunner,\n+  numParallelTests: Integer.MAX_VALUE,\n+  needsSdkLocation: true,\n+  pythonPipelineOptions: [\n+    \"--runner=TestDataflowRunner\",\n+    \"--project=${dataflowProject}\",\n+    \"--region=${dataflowRegion}\",\n+    \"--sdk_harness_container_image_overrides=.*java.*,${dockerImageContainer}:${dockerTag}\",\n+    \"--experiments=beam_fn_api\",\n+    \"--experiments=use_unified_worker\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMjMwNg=="}, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MDY1NTk3OnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMzozMzozMVrOHxiLxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDozNToxMlrOHyNX4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMjM0Mg==", "bodyText": "Ditto.", "url": "https://github.com/apache/beam/pull/13283#discussion_r521702342", "createdAt": "2020-11-11T23:33:31Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/build.gradle", "diffHunk": "@@ -312,6 +313,37 @@ task validatesRunnerStreaming {\n   ))\n }\n \n+createCrossLanguageValidatesRunnerTask(\n+  startJobServer: buildAndPushDockerContainer,\n+  cleanupJobServer: cleanUpDockerImages,\n+  classpath: configurations.validatesRunner,\n+  numParallelTests: Integer.MAX_VALUE,\n+  needsSdkLocation: true,\n+  pythonPipelineOptions: [\n+    \"--runner=TestDataflowRunner\",\n+    \"--project=${dataflowProject}\",\n+    \"--region=${dataflowRegion}\",\n+    \"--sdk_harness_container_image_overrides=.*java.*,${dockerImageContainer}:${dockerTag}\",\n+    \"--experiments=beam_fn_api\",\n+    \"--experiments=use_unified_worker\",\n+  ],\n+  javaPipelineOptions: [\n+    \"--runner=TestDataflowRunner\",\n+    \"--project=${dataflowProject}\",\n+    \"--region=${dataflowRegion}\",\n+    \"--tempRoot=${dataflowValidatesTempRoot}\",\n+    //\"--sdkHarnessContainerImageOverrides=.*java.*,${dockerImageContainer}:${dockerTag}\",\n+    \"--experiments=beam_fn_api,use_unified_worker\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQwOTk1NA==", "bodyText": "done.", "url": "https://github.com/apache/beam/pull/13283#discussion_r522409954", "createdAt": "2020-11-12T20:35:12Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/build.gradle", "diffHunk": "@@ -312,6 +313,37 @@ task validatesRunnerStreaming {\n   ))\n }\n \n+createCrossLanguageValidatesRunnerTask(\n+  startJobServer: buildAndPushDockerContainer,\n+  cleanupJobServer: cleanUpDockerImages,\n+  classpath: configurations.validatesRunner,\n+  numParallelTests: Integer.MAX_VALUE,\n+  needsSdkLocation: true,\n+  pythonPipelineOptions: [\n+    \"--runner=TestDataflowRunner\",\n+    \"--project=${dataflowProject}\",\n+    \"--region=${dataflowRegion}\",\n+    \"--sdk_harness_container_image_overrides=.*java.*,${dockerImageContainer}:${dockerTag}\",\n+    \"--experiments=beam_fn_api\",\n+    \"--experiments=use_unified_worker\",\n+  ],\n+  javaPipelineOptions: [\n+    \"--runner=TestDataflowRunner\",\n+    \"--project=${dataflowProject}\",\n+    \"--region=${dataflowRegion}\",\n+    \"--tempRoot=${dataflowValidatesTempRoot}\",\n+    //\"--sdkHarnessContainerImageOverrides=.*java.*,${dockerImageContainer}:${dockerTag}\",\n+    \"--experiments=beam_fn_api,use_unified_worker\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMjM0Mg=="}, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MDY2NDkxOnYy", "diffSide": "RIGHT", "path": ".test-infra/jenkins/job_PostCommit_CrossLanguageValidatesRunner_Dataflow.groovy", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMzozNTozOVrOHxiRhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwMDo0ODo0N1rOHyU9eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMzgxNQ==", "bodyText": "Can we add Kafka as well ?", "url": "https://github.com/apache/beam/pull/13283#discussion_r521703815", "createdAt": "2020-11-11T23:35:39Z", "author": {"login": "chamikaramj"}, "path": ".test-infra/jenkins/job_PostCommit_CrossLanguageValidatesRunner_Dataflow.groovy", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonJobProperties as commonJobProperties\n+import PostcommitJobBuilder\n+\n+// This job runs the suite of ValidatesRunner tests against the Dataflow runner.\n+PostcommitJobBuilder.postCommitJob('beam_PostCommit_XVR_Dataflow',\n+    'Run XVR_Dataflow PostCommit', 'Dataflow CrossLanguageValidatesRunner Tests', this) {\n+      description('Runs the CrossLanguageValidatesRunner suite on the Dataflow runner.')\n+\n+      // Set common parameters.\n+      commonJobProperties.setTopLevelMainJobProperties(delegate)\n+\n+      // Publish all test results to Jenkins\n+      publishers {\n+        archiveJunit('**/nosetests*.xml')\n+      }\n+\n+      // Gradle goals for this job.\n+      // TODO: We only support Python pipeline using Java external transforms at the moment.\n+      // Enable testing for Java pipeline using Python external transforms when BEAM-11203\n+      // is implemented.\n+      steps {\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.6 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.6')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.8 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.8')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA SQL TRANSFORMS USING PYTHON 3.8 ***\"')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMDk0MA==", "bodyText": "I think it's not necessary. Kafka xlang test is already a part of python postcommit tests: https://github.com/apache/beam/blob/master/sdks/python/test-suites/portable/common.gradle#L177", "url": "https://github.com/apache/beam/pull/13283#discussion_r522410940", "createdAt": "2020-11-12T20:37:07Z", "author": {"login": "ihji"}, "path": ".test-infra/jenkins/job_PostCommit_CrossLanguageValidatesRunner_Dataflow.groovy", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonJobProperties as commonJobProperties\n+import PostcommitJobBuilder\n+\n+// This job runs the suite of ValidatesRunner tests against the Dataflow runner.\n+PostcommitJobBuilder.postCommitJob('beam_PostCommit_XVR_Dataflow',\n+    'Run XVR_Dataflow PostCommit', 'Dataflow CrossLanguageValidatesRunner Tests', this) {\n+      description('Runs the CrossLanguageValidatesRunner suite on the Dataflow runner.')\n+\n+      // Set common parameters.\n+      commonJobProperties.setTopLevelMainJobProperties(delegate)\n+\n+      // Publish all test results to Jenkins\n+      publishers {\n+        archiveJunit('**/nosetests*.xml')\n+      }\n+\n+      // Gradle goals for this job.\n+      // TODO: We only support Python pipeline using Java external transforms at the moment.\n+      // Enable testing for Java pipeline using Python external transforms when BEAM-11203\n+      // is implemented.\n+      steps {\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.6 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.6')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.8 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.8')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA SQL TRANSFORMS USING PYTHON 3.8 ***\"')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMzgxNQ=="}, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1MDk0NA==", "bodyText": "That only runs Kafka on Flink though right?\nand thank you for adding the SQL task :) it looks like it's working:", "url": "https://github.com/apache/beam/pull/13283#discussion_r522450944", "createdAt": "2020-11-12T21:56:13Z", "author": {"login": "TheNeuralBit"}, "path": ".test-infra/jenkins/job_PostCommit_CrossLanguageValidatesRunner_Dataflow.groovy", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonJobProperties as commonJobProperties\n+import PostcommitJobBuilder\n+\n+// This job runs the suite of ValidatesRunner tests against the Dataflow runner.\n+PostcommitJobBuilder.postCommitJob('beam_PostCommit_XVR_Dataflow',\n+    'Run XVR_Dataflow PostCommit', 'Dataflow CrossLanguageValidatesRunner Tests', this) {\n+      description('Runs the CrossLanguageValidatesRunner suite on the Dataflow runner.')\n+\n+      // Set common parameters.\n+      commonJobProperties.setTopLevelMainJobProperties(delegate)\n+\n+      // Publish all test results to Jenkins\n+      publishers {\n+        archiveJunit('**/nosetests*.xml')\n+      }\n+\n+      // Gradle goals for this job.\n+      // TODO: We only support Python pipeline using Java external transforms at the moment.\n+      // Enable testing for Java pipeline using Python external transforms when BEAM-11203\n+      // is implemented.\n+      steps {\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.6 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.6')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.8 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.8')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA SQL TRANSFORMS USING PYTHON 3.8 ***\"')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMzgxNQ=="}, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ1Mzk4Mg==", "bodyText": "Oh, that's right. Good catch \ud83d\ude04\nWill take a look if we can easily add Kafka xlang test too.", "url": "https://github.com/apache/beam/pull/13283#discussion_r522453982", "createdAt": "2020-11-12T22:02:24Z", "author": {"login": "ihji"}, "path": ".test-infra/jenkins/job_PostCommit_CrossLanguageValidatesRunner_Dataflow.groovy", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonJobProperties as commonJobProperties\n+import PostcommitJobBuilder\n+\n+// This job runs the suite of ValidatesRunner tests against the Dataflow runner.\n+PostcommitJobBuilder.postCommitJob('beam_PostCommit_XVR_Dataflow',\n+    'Run XVR_Dataflow PostCommit', 'Dataflow CrossLanguageValidatesRunner Tests', this) {\n+      description('Runs the CrossLanguageValidatesRunner suite on the Dataflow runner.')\n+\n+      // Set common parameters.\n+      commonJobProperties.setTopLevelMainJobProperties(delegate)\n+\n+      // Publish all test results to Jenkins\n+      publishers {\n+        archiveJunit('**/nosetests*.xml')\n+      }\n+\n+      // Gradle goals for this job.\n+      // TODO: We only support Python pipeline using Java external transforms at the moment.\n+      // Enable testing for Java pipeline using Python external transforms when BEAM-11203\n+      // is implemented.\n+      steps {\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.6 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.6')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.8 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.8')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA SQL TRANSFORMS USING PYTHON 3.8 ***\"')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMzgxNQ=="}, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUzNDI2NQ==", "bodyText": "We can't simply adding xlang Kafkaio test here since it needs a separate Kafka cluster setup on Dataflow (on Flink it uses locally launched Kafka cluster). Will merge without xlang Kafkaio test.", "url": "https://github.com/apache/beam/pull/13283#discussion_r522534265", "createdAt": "2020-11-13T00:48:47Z", "author": {"login": "ihji"}, "path": ".test-infra/jenkins/job_PostCommit_CrossLanguageValidatesRunner_Dataflow.groovy", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonJobProperties as commonJobProperties\n+import PostcommitJobBuilder\n+\n+// This job runs the suite of ValidatesRunner tests against the Dataflow runner.\n+PostcommitJobBuilder.postCommitJob('beam_PostCommit_XVR_Dataflow',\n+    'Run XVR_Dataflow PostCommit', 'Dataflow CrossLanguageValidatesRunner Tests', this) {\n+      description('Runs the CrossLanguageValidatesRunner suite on the Dataflow runner.')\n+\n+      // Set common parameters.\n+      commonJobProperties.setTopLevelMainJobProperties(delegate)\n+\n+      // Publish all test results to Jenkins\n+      publishers {\n+        archiveJunit('**/nosetests*.xml')\n+      }\n+\n+      // Gradle goals for this job.\n+      // TODO: We only support Python pipeline using Java external transforms at the moment.\n+      // Enable testing for Java pipeline using Python external transforms when BEAM-11203\n+      // is implemented.\n+      steps {\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.6 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.6')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA EXTERNAL TRANSFORMS USING PYTHON 3.8 ***\"')\n+        gradle {\n+          rootBuildScriptDir(commonJobProperties.checkoutDir)\n+          tasks(':runners:google-cloud-dataflow-java:validatesCrossLanguageRunnerPythonUsingJava')\n+          commonJobProperties.setGradleSwitches(delegate)\n+          switches('-PpythonVersion=3.8')\n+        }\n+        shell('echo \"*** RUN CROSS-LANGUAGE DATAFLOW PYTHON WITH JAVA SQL TRANSFORMS USING PYTHON 3.8 ***\"')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwMzgxNQ=="}, "originalCommit": {"oid": "d9908749f14d62e160f840e82c05c8975680d4e7"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4ODgwNTE5OnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/build.gradle", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxOTozNjoyN1rOH0OXug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxOTozNjoyN1rOH0OXug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDUyMzQ1MA==", "bodyText": "This task makes all Dataflow runner v2 tests fail because the sdk container is cleaned up as soon as it is built. The cause is that in the common configuration, you have\nconfig.startJobServer.finalizedBy config.cleanupJobServer\nThis will change the task dependency incorrectly.\nFiled jira here: https://issues.apache.org/jira/browse/BEAM-11270\ncc: @tysonjh", "url": "https://github.com/apache/beam/pull/13283#discussion_r524523450", "createdAt": "2020-11-16T19:36:27Z", "author": {"login": "boyuanzz"}, "path": "runners/google-cloud-dataflow-java/build.gradle", "diffHunk": "@@ -312,6 +313,36 @@ task validatesRunnerStreaming {\n   ))\n }\n \n+createCrossLanguageValidatesRunnerTask(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c988f81e5540a330300c7209b31a18bf046849"}, "originalPosition": 12}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2730, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}