{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4MjIwOTYx", "number": 13292, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToxODoxMlrOE6TYKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToyMzowOVrOE6TcUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTcwMzQ1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/util.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToxODoxMlrOH1SB0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNzo1OTo0OFrOH17PHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMTk1NA==", "bodyText": "Should this be tagged experimental similar to the Java change?", "url": "https://github.com/apache/beam/pull/13292#discussion_r525631954", "createdAt": "2020-11-18T01:18:12Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMwNzEwMg==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/13292#discussion_r526307102", "createdAt": "2020-11-18T17:59:48Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMTk1NA=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTcwNDMxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/util.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToxODozN1rOH1SCWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxODo0MzowM1rOH185_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMjA4OQ==", "bodyText": "How does this work? When/how runner does this load balancing?", "url": "https://github.com/apache/beam/pull/13292#discussion_r525632089", "createdAt": "2020-11-18T01:18:37Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMwOTEzMQ==", "bodyText": "In Dataflow, the load balancing is done inside Windmill during the shuffle before the GroupIntoBatchesDoFn. Basically Windmill can re-distribute the input element to different workers and assign them different shard ids accordingly.\nDifferent runner may choose different strategies by overriding the default implementation here. We will add the transform to the runner api (in follow-up PRs) so runners are able to recognize the transform and do something special if they want.", "url": "https://github.com/apache/beam/pull/13292#discussion_r526309131", "createdAt": "2020-11-18T18:02:59Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMjA4OQ=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMyNDQ1Nw==", "bodyText": "For my understanding. So, windmill understands the coding for shardedkey and the implementation here is the default implementation for cases where runner does not do this overriding?\nIf this is correct, could you update the comment to explain this a bit more to the reader?", "url": "https://github.com/apache/beam/pull/13292#discussion_r526324457", "createdAt": "2020-11-18T18:27:03Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMjA4OQ=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMzNDQ2Mw==", "bodyText": "Yes that is totally correct. I updated the documentation. How does it sound now?", "url": "https://github.com/apache/beam/pull/13292#discussion_r526334463", "createdAt": "2020-11-18T18:43:03Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMjA4OQ=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTcxMjU4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/util.py", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToyMjoxNVrOH1SHGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOToxOToxMFrOH1-QYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzMwNA==", "bodyText": "Why is the process id needed?\nNote that there is only 1 python process per container, and since all containers start the same way it is very likely that pid for each python process in their own containers might even be the same.\nIf you need to distinguish per worker, you may need some concept of worker id (which does not exist in Beam).", "url": "https://github.com/apache/beam/pull/13292#discussion_r525633304", "createdAt": "2020-11-18T01:22:15Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMxMDM2Ng==", "bodyText": "I see. The pid is probably not a good choice then. I changed it to use uuid instead, analogous to the Java implementation:\n\n  \n    \n      beam/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java\n    \n    \n         Line 96\n      in\n      a016ba5\n    \n    \n    \n    \n\n        \n          \n           private static final long uuid = UUID.randomUUID().getMostSignificantBits();", "url": "https://github.com/apache/beam/pull/13292#discussion_r526310366", "createdAt": "2020-11-18T18:04:54Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzMwNA=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMyMzE5Mg==", "bodyText": "Yes both UUID and making it analogous to Java are good ideas. For my understanding what is the purpose of this key selection? Are we trying to by default to keep key space equal to the size of whole set of threads across all workers?", "url": "https://github.com/apache/beam/pull/13292#discussion_r526323192", "createdAt": "2020-11-18T18:25:06Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzMwNA=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMzNDQ4Nw==", "bodyText": "The parallelism of theGroupIntoBatchesDoFn is tied to the number of keys (due to the per-key state semantics and the implementation of keyed state management). So choosing one shard per key (i.e., without key sharding) effectively means that we can not have more parallelism than the number of input keys. We are trying to by default spread the input elements to all available threads across workers, which is definitely not ideal but slightly better than no sharding.", "url": "https://github.com/apache/beam/pull/13292#discussion_r526334487", "createdAt": "2020-11-18T18:43:06Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzMwNA=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM1NjU3Ng==", "bodyText": "Sounds good. The updated comment also better explains this.", "url": "https://github.com/apache/beam/pull/13292#discussion_r526356576", "createdAt": "2020-11-18T19:19:10Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzMwNA=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTcxNDExOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/util.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToyMzowOVrOH1SIFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxODowNDo1MlrOH17bzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzU1Nw==", "bodyText": "Maybe use key_value instead of x ?", "url": "https://github.com/apache/beam/pull/13292#discussion_r525633557", "createdAt": "2020-11-18T01:23:09Z", "author": {"login": "aaltay"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()\n+\n+    def expand(self, pcoll):\n+      sharded_pcoll = pcoll | Map(\n+          lambda x: (", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMxMDM0OA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/13292#discussion_r526310348", "createdAt": "2020-11-18T18:04:52Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -780,6 +783,48 @@ def expand(self, pcoll):\n             self.max_buffering_duration_secs,\n             self.clock))\n \n+  @typehints.with_input_types(Tuple[K, V])\n+  @typehints.with_output_types(Tuple[K, Iterable[V]])\n+  class WithShardedKey(PTransform):\n+    \"\"\"A GroupIntoBatches transform that outputs batched elements associated\n+    with sharded input keys.\n+\n+    The sharding is determined by the runner to balance the load during the\n+    execution time. By default, it spreads the input elements with the same key\n+    to all available threads executing the transform.\n+    \"\"\"\n+    def __init__(self, batch_size, max_buffering_duration_secs=None):\n+      \"\"\"Create a new GroupIntoBatches.WithShardedKey.\n+\n+      Arguments:\n+        batch_size: (required) How many elements should be in a batch\n+        max_buffering_duration_secs: (optional) How long in seconds at most an\n+          incomplete batch of elements is allowed to be buffered in the states.\n+          The duration must be a positive second duration and should be given as\n+          an int or float.\n+      \"\"\"\n+      self.batch_size = batch_size\n+\n+      if max_buffering_duration_secs is not None:\n+        assert max_buffering_duration_secs > 0, (\n+            'max buffering duration should be a positive value')\n+      self.max_buffering_duration_secs = max_buffering_duration_secs\n+\n+    _pid = os.getpid()\n+\n+    def expand(self, pcoll):\n+      sharded_pcoll = pcoll | Map(\n+          lambda x: (", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMzU1Nw=="}, "originalCommit": {"oid": "eb4f057bd21111d7a39d4af1aef56f8e76c53afb"}, "originalPosition": 64}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2742, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}