{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ0NjI5NDA5", "number": 13605, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoxNTo1NlrOFKnjhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoyMTo0N1rOFKnlKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2Njc4MTUwOnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoxNTo1NlrOINae4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMDo0NDo0MlrOIOtPOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjI5MA==", "bodyText": "Is there a util function that we can depend on (similar to [1] for Python) introduce of checking these properties separately for runner v2 which could lead to inconsistencies ?\n[1] https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py#L1039", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936290", "createdAt": "2021-01-03T00:15:56Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MjE1NA==", "bodyText": "add utility function to check runner v2 flags", "url": "https://github.com/apache/beam/pull/13605#discussion_r552292154", "createdAt": "2021-01-06T00:44:42Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjI5MA=="}, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2Njc4NDE0OnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoxOToxNFrOINaf-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMDo0NzoxOVrOIOtSMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjU2OA==", "bodyText": "It might make sense to de-dupe here in-case same Docker URL gets added twice (we can generalize this when Dataflow supports multiple environments with the same Docker URL hence please add a TODO).", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936568", "createdAt": "2021-01-03T00:19:14Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MjkxNA==", "bodyText": "We are collecting the urls in an immutable set so there should be only one entry with the same container url.", "url": "https://github.com/apache/beam/pull/13605#discussion_r552292914", "createdAt": "2021-01-06T00:47:19Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjU2OA=="}, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2Njc4NDkyOnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoyMDoyNlrOINagTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMDo0NjoxOVrOIOtRHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY1Mw==", "bodyText": "Do we need any special handling for pipeline SDK similar to following for Python ?\nhttps://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/dataflow/internal/apiclient.py#L285", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936653", "createdAt": "2021-01-03T00:20:26Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());\n+      }\n+      List<SdkHarnessContainerImage> sdkContainerList =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MjYzOQ==", "bodyText": "Pipeline SDK container is passed as a parameter and to be the first entry added to the container url set (as same as the python code).", "url": "https://github.com/apache/beam/pull/13605#discussion_r552292639", "createdAt": "2021-01-06T00:46:19Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());\n+      }\n+      List<SdkHarnessContainerImage> sdkContainerList =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY1Mw=="}, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2Njc4NTI5OnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoyMTowOFrOINagew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQwMDo0ODoyNVrOIOtTYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY5OQ==", "bodyText": "For any Python environments, we should set 'useSingleCorePerContainer' to true for efficiency.", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936699", "createdAt": "2021-01-03T00:21:08Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());\n+      }\n+      List<SdkHarnessContainerImage> sdkContainerList =\n+          sdkContainerUrlSetBuilder.build().stream()\n+              .map(\n+                  (String url) -> {\n+                    SdkHarnessContainerImage image = new SdkHarnessContainerImage();\n+                    image.setContainerImage(url);\n+                    return image;\n+                  })\n+              .collect(Collectors.toList());\n+      for (WorkerPool workerPool : newJob.getEnvironment().getWorkerPools()) {\n+        workerPool.setSdkHarnessContainerImages(sdkContainerList);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5MzIxOQ==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/13605#discussion_r552293219", "createdAt": "2021-01-06T00:48:25Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1186,6 +1190,45 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     return dataflowPipelineJob;\n   }\n \n+  static void configureSdkHarnessContainerImages(\n+      DataflowPipelineOptions options,\n+      RunnerApi.Pipeline pipelineProto,\n+      Job newJob,\n+      String workerHarnessContainerImage) {\n+    if (hasExperiment(options, \"use_runner_v2\") || hasExperiment(options, \"use_unified_worker\")) {\n+      ImmutableSet.Builder<String> sdkContainerUrlSetBuilder = ImmutableSet.builder();\n+      sdkContainerUrlSetBuilder.add(workerHarnessContainerImage);\n+      for (Map.Entry<String, RunnerApi.Environment> entry :\n+          pipelineProto.getComponents().getEnvironmentsMap().entrySet()) {\n+        if (!BeamUrns.getUrn(RunnerApi.StandardEnvironments.Environments.DOCKER)\n+            .equals(entry.getValue().getUrn())) {\n+          throw new RuntimeException(\n+              \"Dataflow can only execute pipeline steps in Docker environments: \"\n+                  + entry.getValue().getUrn());\n+        }\n+        RunnerApi.DockerPayload dockerPayload;\n+        try {\n+          dockerPayload = RunnerApi.DockerPayload.parseFrom(entry.getValue().getPayload());\n+        } catch (InvalidProtocolBufferException e) {\n+          throw new RuntimeException(\"Error parsing docker payload.\", e);\n+        }\n+        sdkContainerUrlSetBuilder.add(dockerPayload.getContainerImage());\n+      }\n+      List<SdkHarnessContainerImage> sdkContainerList =\n+          sdkContainerUrlSetBuilder.build().stream()\n+              .map(\n+                  (String url) -> {\n+                    SdkHarnessContainerImage image = new SdkHarnessContainerImage();\n+                    image.setContainerImage(url);\n+                    return image;\n+                  })\n+              .collect(Collectors.toList());\n+      for (WorkerPool workerPool : newJob.getEnvironment().getWorkerPools()) {\n+        workerPool.setSdkHarnessContainerImages(sdkContainerList);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjY5OQ=="}, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2Njc4NTcwOnYy", "diffSide": "RIGHT", "path": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDoyMTo0N1rOINagqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMzoxOToyOVrOIQEU8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjc0Ng==", "bodyText": "Please also try out a Dataflow GCE job (or existing ITs).", "url": "https://github.com/apache/beam/pull/13605#discussion_r550936746", "createdAt": "2021-01-03T00:21:47Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java", "diffHunk": "@@ -1342,6 +1347,57 @@ public void testTransformTranslator() throws IOException {\n     assertTrue(transform.translated);\n   }\n \n+  @Test\n+  public void testSdkHarnessConfiguration() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjI5NTYwNw==", "bodyText": "The change applies to all existing ITs with use_runner_v2 flag (for example, runner v2 validate runner tests should fail if something goes wrong with this commit). Do you think we need a separate test only for this change?", "url": "https://github.com/apache/beam/pull/13605#discussion_r552295607", "createdAt": "2021-01-06T00:51:19Z", "author": {"login": "ihji"}, "path": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java", "diffHunk": "@@ -1342,6 +1347,57 @@ public void testTransformTranslator() throws IOException {\n     assertTrue(transform.translated);\n   }\n \n+  @Test\n+  public void testSdkHarnessConfiguration() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjc0Ng=="}, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxOTAyNQ==", "bodyText": "No existing Runner v2 ITs should be adequate. Thanks.", "url": "https://github.com/apache/beam/pull/13605#discussion_r553719025", "createdAt": "2021-01-08T03:19:29Z", "author": {"login": "chamikaramj"}, "path": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java", "diffHunk": "@@ -1342,6 +1347,57 @@ public void testTransformTranslator() throws IOException {\n     assertTrue(transform.translated);\n   }\n \n+  @Test\n+  public void testSdkHarnessConfiguration() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNjc0Ng=="}, "originalCommit": {"oid": "c9f868adfe3ebec23b59a4401694826e7e714bec"}, "originalPosition": 34}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2438, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}