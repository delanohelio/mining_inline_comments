{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NjYwMDY3", "number": 12572, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjo1OTozNFrOEoLZ9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo0NzoxM1rOEop5FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNTY1MzY1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjo1OTozNFrOHZHi4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMzoxOToxOVrOHaGebw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwMDA2Ng==", "bodyText": "Why do we need Reshuffle here?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496100066", "createdAt": "2020-09-28T16:59:34Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1620,14 +1635,43 @@ public void processElement(\n       CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n-      Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n-      PCollection<KafkaRecord<K, V>> output =\n-          input.apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this))).setCoder(outputCoder);\n-      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n-      if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n-        throw new IllegalStateException(\"Offset committed is not supported yet\");\n+      Coder<KafkaRecord<K, V>> recordCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+\n+      try {\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> outputWithDescriptor =\n+            input\n+                .apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this)))\n+                .setCoder(\n+                    KvCoder.of(\n+                        input\n+                            .getPipeline()\n+                            .getSchemaRegistry()\n+                            .getSchemaCoder(KafkaSourceDescriptor.class),\n+                        recordCoder));\n+        if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+          outputWithDescriptor =\n+              outputWithDescriptor\n+                  .apply(Reshuffle.viaRandomKey())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEzMTExOQ==", "bodyText": "The Reshuffle is used as a persistent layer which help us to guarantee that we will not re-read records priori to the committed offset.", "url": "https://github.com/apache/beam/pull/12572#discussion_r497131119", "createdAt": "2020-09-29T23:19:19Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1620,14 +1635,43 @@ public void processElement(\n       CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n-      Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n-      PCollection<KafkaRecord<K, V>> output =\n-          input.apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this))).setCoder(outputCoder);\n-      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n-      if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n-        throw new IllegalStateException(\"Offset committed is not supported yet\");\n+      Coder<KafkaRecord<K, V>> recordCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+\n+      try {\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> outputWithDescriptor =\n+            input\n+                .apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this)))\n+                .setCoder(\n+                    KvCoder.of(\n+                        input\n+                            .getPipeline()\n+                            .getSchemaRegistry()\n+                            .getSchemaCoder(KafkaSourceDescriptor.class),\n+                        recordCoder));\n+        if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+          outputWithDescriptor =\n+              outputWithDescriptor\n+                  .apply(Reshuffle.viaRandomKey())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwMDA2Ng=="}, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDYyMzMzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo0MToxN1rOHZ3e-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1ODo1M1rOHaBsbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NTQ5OA==", "bodyText": "Is it a possible state that there are no bootstrap servers defined?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496885498", "createdAt": "2020-09-29T16:41:17Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1Mjc4Mg==", "bodyText": "Yes, that's possible since we no longer force the user to provide bootstrap servers when constructing transforms. The bootstrap servers can also come from source descriptor.", "url": "https://github.com/apache/beam/pull/12572#discussion_r497052782", "createdAt": "2020-09-29T20:58:53Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NTQ5OA=="}, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDYzMzcwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo0Mzo1MFrOHZ3lJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMTo1NzozOVrOHaDocg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NzA3OA==", "bodyText": "How many uniq keys per bundle are expected? Only one (because of Max.longsPerKey()) on previous step?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496887078", "createdAt": "2020-09-29T16:43:50Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA4NDUzMA==", "bodyText": "How many uniq keys after Max.longsPerKey()  per bundle depends on the runner implementation. For dataflow specific, a bundle may contains many keys. The key I'm using here is a KafkaSourceDescriptor, which represents a unique Kafka connection(a topic + a partition + bootstrap servers).", "url": "https://github.com/apache/beam/pull/12572#discussion_r497084530", "createdAt": "2020-09-29T21:57:39Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NzA3OA=="}, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDY0ODUzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo0NzoxM1rOHZ3t9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxMDo0NTo1MlrOHaXxsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA==", "bodyText": "Could you elaborate a bit why Window is hardcoded to 5 mins?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496889334", "createdAt": "2020-09-29T16:47:13Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);\n+      Map<String, Object> config = new HashMap<>(currentConfig);\n+      if (description.getBootStrapServers() != null\n+          && description.getBootStrapServers().size() > 0) {\n+        config.put(\n+            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n+            String.join(\",\", description.getBootStrapServers()));\n+      }\n+      return config;\n+    }\n+  }\n+\n+  @Override\n+  public PCollection<Void> expand(PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> input) {\n+    try {\n+      return input\n+          .apply(\n+              MapElements.into(new TypeDescriptor<KV<KafkaSourceDescriptor, Long>>() {})\n+                  .via(element -> KV.of(element.getKey(), element.getValue().getOffset())))\n+          .setCoder(\n+              KvCoder.of(\n+                  input\n+                      .getPipeline()\n+                      .getSchemaRegistry()\n+                      .getSchemaCoder(KafkaSourceDescriptor.class),\n+                  VarLongCoder.of()))\n+          .apply(Window.into(FixedWindows.of(Duration.standardMinutes(5))))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyNTMxOQ==", "bodyText": "That's my expectation of the time interval for committing. The reason for committing offset is to have a good start point when we restart the pipeline, so it not requires a real-time commtting. Do you have any suggestion on this time?", "url": "https://github.com/apache/beam/pull/12572#discussion_r497125319", "createdAt": "2020-09-29T23:11:22Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);\n+      Map<String, Object> config = new HashMap<>(currentConfig);\n+      if (description.getBootStrapServers() != null\n+          && description.getBootStrapServers().size() > 0) {\n+        config.put(\n+            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n+            String.join(\",\", description.getBootStrapServers()));\n+      }\n+      return config;\n+    }\n+  }\n+\n+  @Override\n+  public PCollection<Void> expand(PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> input) {\n+    try {\n+      return input\n+          .apply(\n+              MapElements.into(new TypeDescriptor<KV<KafkaSourceDescriptor, Long>>() {})\n+                  .via(element -> KV.of(element.getKey(), element.getValue().getOffset())))\n+          .setCoder(\n+              KvCoder.of(\n+                  input\n+                      .getPipeline()\n+                      .getSchemaRegistry()\n+                      .getSchemaCoder(KafkaSourceDescriptor.class),\n+                  VarLongCoder.of()))\n+          .apply(Window.into(FixedWindows.of(Duration.standardMinutes(5))))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA=="}, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQxNDU3OA==", "bodyText": "Thanks. Not for now, let's keep it as \"it is\".  I'm just thinking if a user would need to be able to configure this or not.", "url": "https://github.com/apache/beam/pull/12572#discussion_r497414578", "createdAt": "2020-09-30T10:45:52Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);\n+      Map<String, Object> config = new HashMap<>(currentConfig);\n+      if (description.getBootStrapServers() != null\n+          && description.getBootStrapServers().size() > 0) {\n+        config.put(\n+            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n+            String.join(\",\", description.getBootStrapServers()));\n+      }\n+      return config;\n+    }\n+  }\n+\n+  @Override\n+  public PCollection<Void> expand(PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> input) {\n+    try {\n+      return input\n+          .apply(\n+              MapElements.into(new TypeDescriptor<KV<KafkaSourceDescriptor, Long>>() {})\n+                  .via(element -> KV.of(element.getKey(), element.getValue().getOffset())))\n+          .setCoder(\n+              KvCoder.of(\n+                  input\n+                      .getPipeline()\n+                      .getSchemaRegistry()\n+                      .getSchemaCoder(KafkaSourceDescriptor.class),\n+                  VarLongCoder.of()))\n+          .apply(Window.into(FixedWindows.of(Duration.standardMinutes(5))))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA=="}, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 122}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 689, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}