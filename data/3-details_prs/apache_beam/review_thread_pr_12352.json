{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1NTEyNDYw", "number": 12352, "reviewThreads": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzowMzozOVrOERiaAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNzozOToxMVrOEaFZZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODI0OTYwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzowMzozOVrOG2TmRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzowMzozOVrOG2TmRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU5NzM4MA==", "bodyText": "You can just do if consumers here.\nAs discussed, there may be more than one consumer, so worth thinking about what to do in that case.", "url": "https://github.com/apache/beam/pull/12352#discussion_r459597380", "createdAt": "2020-07-23T17:03:39Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -202,6 +205,17 @@ def __init__(\n     self._sample_counter = 0\n     self._next_sample = 0\n \n+    if consumers and len(consumers):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d8c8c3fc3f1e5913109bc1d92bcc3e6d4a1a8b9"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjA5Mzc1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/common.py", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTozNDoyNlrOG7JeJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODowNTowNlrOG-Zmmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NDM0Mg==", "bodyText": "This check is slow. Let's guard this or remove it (as it will still fail below).", "url": "https://github.com/apache/beam/pull/12352#discussion_r464674342", "createdAt": "2020-08-03T21:34:26Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/common.py", "diffHunk": "@@ -1340,6 +1342,17 @@ def process_outputs(\n         self.per_element_output_counter.add_input(0)\n       return\n \n+    if isinstance(results, (dict, str, unicode, bytes)):\n+      results_type = type(results).__name__\n+      raise TypeCheckError(\n+          'Returning a %s from a ParDo or FlatMap is '\n+          'discouraged. Please use list(\"%s\") if you really '\n+          'want this behavior.' % (results_type, results))\n+    elif not isinstance(results, collections.Iterable):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE0NTE3Ng==", "bodyText": "Out of curiosity, why is this check slow? Also, if we do remove it, is there a way to still give a relevant error message when it fails below?", "url": "https://github.com/apache/beam/pull/12352#discussion_r466145176", "createdAt": "2020-08-06T04:53:18Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/common.py", "diffHunk": "@@ -1340,6 +1342,17 @@ def process_outputs(\n         self.per_element_output_counter.add_input(0)\n       return\n \n+    if isinstance(results, (dict, str, unicode, bytes)):\n+      results_type = type(results).__name__\n+      raise TypeCheckError(\n+          'Returning a %s from a ParDo or FlatMap is '\n+          'discouraged. Please use list(\"%s\") if you really '\n+          'want this behavior.' % (results_type, results))\n+    elif not isinstance(results, collections.Iterable):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NDM0Mg=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NDM3OA==", "bodyText": "Not yet resolved?", "url": "https://github.com/apache/beam/pull/12352#discussion_r468084378", "createdAt": "2020-08-10T18:05:06Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/common.py", "diffHunk": "@@ -1340,6 +1342,17 @@ def process_outputs(\n         self.per_element_output_counter.add_input(0)\n       return\n \n+    if isinstance(results, (dict, str, unicode, bytes)):\n+      results_type = type(results).__name__\n+      raise TypeCheckError(\n+          'Returning a %s from a ParDo or FlatMap is '\n+          'discouraged. Please use list(\"%s\") if you really '\n+          'want this behavior.' % (results_type, results))\n+    elif not isinstance(results, collections.Iterable):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NDM0Mg=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjEwMTM0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/options/pipeline_options.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTozNzoyNlrOG7Ji7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTozNzoyNlrOG7Ji7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NTU2Ng==", "bodyText": "This should say \"only supported with portable runners (including the direct runner).\"", "url": "https://github.com/apache/beam/pull/12352#discussion_r464675566", "createdAt": "2020-08-03T21:37:26Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/options/pipeline_options.py", "diffHunk": "@@ -466,6 +466,13 @@ def _add_argparse_args(cls, parser):\n         help='Enable type checking at pipeline execution '\n         'time. NOTE: only supported with the '\n         'DirectRunner')\n+    parser.add_argument(\n+        '--performance_runtime_type_check',\n+        default=False,\n+        action='store_true',\n+        help='Enable faster type checking via sampling at pipeline execution '\n+        'time. NOTE: only supported with the '", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjExMjU2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/bundle_processor.py", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo0MTo0MVrOG7Jp0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjo1Mjo1MlrOG9Gydw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NzMzMQ==", "bodyText": "Generally it's preferable not to pass something big like self, but rather just the subset of information that's required here (e.g. the dict of type hint sources to type hints). This will also be needed for the cases that different outputs have different types.", "url": "https://github.com/apache/beam/pull/12352#discussion_r464677331", "createdAt": "2020-08-03T21:41:41Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/bundle_processor.py", "diffHunk": "@@ -188,7 +188,8 @@ def __init__(self,\n             self.name_context.step_name,\n             0,\n             next(iter(itervalues(consumers))),\n-            self.windowed_coder)\n+            self.windowed_coder,\n+            self)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5MTEzNQ==", "bodyText": "Okay, I refactored it to pass only the necessary data. Also, I thought self was just a reference to the class instance so it wouldn't be expensive to pass it around - is this not the case?", "url": "https://github.com/apache/beam/pull/12352#discussion_r466191135", "createdAt": "2020-08-06T07:09:53Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/bundle_processor.py", "diffHunk": "@@ -188,7 +188,8 @@ def __init__(self,\n             self.name_context.step_name,\n             0,\n             next(iter(itervalues(consumers))),\n-            self.windowed_coder)\n+            self.windowed_coder,\n+            self)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NzMzMQ=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyNzU0Mw==", "bodyText": "When I said \"big\" I meant in terms of surface area. (You're right that there's no performance concern here.) E.g. it's easier to reason about the code when we see that the callee is interested in the type hints rather than the whole operation.", "url": "https://github.com/apache/beam/pull/12352#discussion_r466727543", "createdAt": "2020-08-06T22:52:52Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/bundle_processor.py", "diffHunk": "@@ -188,7 +188,8 @@ def __init__(self,\n             self.name_context.step_name,\n             0,\n             next(iter(itervalues(consumers))),\n-            self.windowed_coder)\n+            self.windowed_coder,\n+            self)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NzMzMQ=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjEyMzk1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/operations.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo0NjoxMlrOG7Jwwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNjowNjoxMlrOG8khEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTEwNg==", "bodyText": "Where is this used?", "url": "https://github.com/apache/beam/pull/12352#discussion_r464679106", "createdAt": "2020-08-03T21:46:12Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/operations.py", "diffHunk": "@@ -238,6 +247,7 @@ def __init__(self,\n     self.execution_context = None  # type: Optional[ExecutionContext]\n     self.consumers = collections.defaultdict(\n         list)  # type: DefaultDict[int, List[Operation]]\n+    self.producer = None", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2NjAzNQ==", "bodyText": "Ah thanks, I'm not actually updating the value in the child class, DoOperation, so let me get rid of that and directly pass None", "url": "https://github.com/apache/beam/pull/12352#discussion_r466166035", "createdAt": "2020-08-06T06:06:12Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/operations.py", "diffHunk": "@@ -238,6 +247,7 @@ def __init__(self,\n     self.execution_context = None  # type: Optional[ExecutionContext]\n     self.consumers = collections.defaultdict(\n         list)  # type: DefaultDict[int, List[Operation]]\n+    self.producer = None", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3OTEwNg=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjEzMzgzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1MDowNFrOG7J2xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzoxMzoxOFrOG8mJvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MDY0NA==", "bodyText": "What if this transform has a type hint and the DoFn itself has a type hint? Will we check both?\nI would probably create the dictionary of {type hint source string: type hint} right here, pre-packaged and ready to be used directly from the worker.", "url": "https://github.com/apache/beam/pull/12352#discussion_r464680644", "createdAt": "2020-08-03T21:50:04Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,71 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo):\n+      if not self._in_combine:\n+        transform.fn._full_label = applied_transform.full_label\n+        self.store_type_hints(transform)\n+\n+  def store_type_hints(self, transform):\n+    type_hints = transform.get_type_hints()\n+\n+    input_types = None\n+    if type_hints.input_types:\n+      normal_hints, kwarg_hints = type_hints.input_types\n+\n+      if kwarg_hints:\n+        input_types = kwarg_hints\n+      if normal_hints:\n+        input_types = normal_hints\n+\n+    output_types = None\n+    if type_hints.output_types:\n+      normal_hints, kwarg_hints = type_hints.output_types\n+\n+      if kwarg_hints:\n+        output_types = kwarg_hints\n+      if normal_hints:\n+        output_types = normal_hints\n+\n+    try:\n+      argspec = inspect.getfullargspec(transform.fn._process_argspec_fn())\n+      if len(argspec.args):\n+        arg_index = 0\n+        if argspec.args[0] == 'self':\n+          arg_index = 1\n+        transform.fn._runtime_parameter_name = argspec.args[arg_index]\n+        if isinstance(input_types, dict):\n+          input_types = (input_types[argspec.args[arg_index]], )\n+    except TypeError:\n+      pass\n+\n+    if input_types and len(input_types):\n+      input_types = input_types[0]\n+\n+    if output_types and len(output_types):\n+      output_types = output_types[0]\n+\n+    transform.fn._runtime_type_hints = type_hints._replace(\n+        input_types=input_types, output_types=output_types)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5MjgyOQ==", "bodyText": "Through debugging, I've noticed that the type hints are always stored in the transforms and not the DoFn's for some reason. Do you know of a case where the DoFn stores the type hint?\n\n\nSounds good, moved (most of) the pre-processing computation here", "url": "https://github.com/apache/beam/pull/12352#discussion_r466192829", "createdAt": "2020-08-06T07:13:18Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,71 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo):\n+      if not self._in_combine:\n+        transform.fn._full_label = applied_transform.full_label\n+        self.store_type_hints(transform)\n+\n+  def store_type_hints(self, transform):\n+    type_hints = transform.get_type_hints()\n+\n+    input_types = None\n+    if type_hints.input_types:\n+      normal_hints, kwarg_hints = type_hints.input_types\n+\n+      if kwarg_hints:\n+        input_types = kwarg_hints\n+      if normal_hints:\n+        input_types = normal_hints\n+\n+    output_types = None\n+    if type_hints.output_types:\n+      normal_hints, kwarg_hints = type_hints.output_types\n+\n+      if kwarg_hints:\n+        output_types = kwarg_hints\n+      if normal_hints:\n+        output_types = normal_hints\n+\n+    try:\n+      argspec = inspect.getfullargspec(transform.fn._process_argspec_fn())\n+      if len(argspec.args):\n+        arg_index = 0\n+        if argspec.args[0] == 'self':\n+          arg_index = 1\n+        transform.fn._runtime_parameter_name = argspec.args[arg_index]\n+        if isinstance(input_types, dict):\n+          input_types = (input_types[argspec.args[arg_index]], )\n+    except TypeError:\n+      pass\n+\n+    if input_types and len(input_types):\n+      input_types = input_types[0]\n+\n+    if output_types and len(output_types):\n+      output_types = output_types[0]\n+\n+    transform.fn._runtime_type_hints = type_hints._replace(\n+        input_types=input_types, output_types=output_types)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MDY0NA=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjEzNjY2OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/sdks/python-type-safety.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1MTowNFrOG7J4YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjo1MzoyOVrOG9GzPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTA1Ng==", "bodyText": "Why is this Python 3 only? (Not that Python 2 is going to be around for long...)", "url": "https://github.com/apache/beam/pull/12352#discussion_r464681056", "createdAt": "2020-08-03T21:51:04Z", "author": {"login": "robertwb"}, "path": "website/www/site/content/en/documentation/sdks/python-type-safety.md", "diffHunk": "@@ -210,7 +210,21 @@ However, if you enable runtime type checking, the code is guaranteed to fail at\n {{< code_sample \"sdks/python/apache_beam/examples/snippets/snippets_test.py\" type_hints_runtime_on >}}\n {{< /highlight >}}\n \n-Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines.\n+Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines. See the following section for a quicker, production-friendly alternative.\n+\n+### Faster Runtime Type Checking\n+You can enable faster, sampling-based runtime type checking by setting the pipeline option `performance_runtime_type_check` to `True`.\n+\n+The is a Python 3 only feature that works by runtime type checking a small subset of values, called a sample, using optimized Cython code.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5MzY4MA==", "bodyText": "We use inspect.getfullargspec in typecheck.py L320 which is Py3 only. It could be refactored to support Py2, but like you say, it's not going to be around for much longer so it may not be worth the effort", "url": "https://github.com/apache/beam/pull/12352#discussion_r465293680", "createdAt": "2020-08-04T19:53:55Z", "author": {"login": "saavannanavati"}, "path": "website/www/site/content/en/documentation/sdks/python-type-safety.md", "diffHunk": "@@ -210,7 +210,21 @@ However, if you enable runtime type checking, the code is guaranteed to fail at\n {{< code_sample \"sdks/python/apache_beam/examples/snippets/snippets_test.py\" type_hints_runtime_on >}}\n {{< /highlight >}}\n \n-Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines.\n+Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines. See the following section for a quicker, production-friendly alternative.\n+\n+### Faster Runtime Type Checking\n+You can enable faster, sampling-based runtime type checking by setting the pipeline option `performance_runtime_type_check` to `True`.\n+\n+The is a Python 3 only feature that works by runtime type checking a small subset of values, called a sample, using optimized Cython code.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTA1Ng=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyNzc0MA==", "bodyText": "Ack. Sounds good to keep it as Python 3 only.", "url": "https://github.com/apache/beam/pull/12352#discussion_r466727740", "createdAt": "2020-08-06T22:53:29Z", "author": {"login": "robertwb"}, "path": "website/www/site/content/en/documentation/sdks/python-type-safety.md", "diffHunk": "@@ -210,7 +210,21 @@ However, if you enable runtime type checking, the code is guaranteed to fail at\n {{< code_sample \"sdks/python/apache_beam/examples/snippets/snippets_test.py\" type_hints_runtime_on >}}\n {{< /highlight >}}\n \n-Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines.\n+Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines. See the following section for a quicker, production-friendly alternative.\n+\n+### Faster Runtime Type Checking\n+You can enable faster, sampling-based runtime type checking by setting the pipeline option `performance_runtime_type_check` to `True`.\n+\n+The is a Python 3 only feature that works by runtime type checking a small subset of values, called a sample, using optimized Cython code.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTA1Ng=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjEzODM0OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/sdks/python-type-safety.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1MTo0MVrOG7J5XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1MTo0MVrOG7J5XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTMwOA==", "bodyText": "Rather than enumerating the transforms here, I would just say that it doesn't run on combining operations.", "url": "https://github.com/apache/beam/pull/12352#discussion_r464681308", "createdAt": "2020-08-03T21:51:41Z", "author": {"login": "robertwb"}, "path": "website/www/site/content/en/documentation/sdks/python-type-safety.md", "diffHunk": "@@ -210,7 +210,21 @@ However, if you enable runtime type checking, the code is guaranteed to fail at\n {{< code_sample \"sdks/python/apache_beam/examples/snippets/snippets_test.py\" type_hints_runtime_on >}}\n {{< /highlight >}}\n \n-Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines.\n+Note that because runtime type checks are done for each `PCollection` element, enabling this feature may incur a significant performance penalty. It is therefore recommended that runtime type checks are disabled for production pipelines. See the following section for a quicker, production-friendly alternative.\n+\n+### Faster Runtime Type Checking\n+You can enable faster, sampling-based runtime type checking by setting the pipeline option `performance_runtime_type_check` to `True`.\n+\n+The is a Python 3 only feature that works by runtime type checking a small subset of values, called a sample, using optimized Cython code.\n+\n+Currently, this feature does not support runtime type checking for side inputs or combine operations. Specifically, this feature will not runtime type check the following transforms:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjE0MTUyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/pipeline.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1Mjo0OVrOG7J7Kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwNTozMjoxMFrOG-o7ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTc3MA==", "bodyText": "Why not?", "url": "https://github.com/apache/beam/pull/12352#discussion_r464681770", "createdAt": "2020-08-03T21:52:49Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/pipeline.py", "diffHunk": "@@ -520,10 +520,26 @@ def run(self, test_runner_api='AUTO'):\n             self._options,\n             allow_proto_holders=True).run(False)\n \n+      if (self._options.view_as(TypeOptions).runtime_type_check and\n+          self._options.view_as(TypeOptions).performance_runtime_type_check):\n+        raise RuntimeError(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMwMjc1MQ==", "bodyText": "I don't see a use case. The existing runtime_type_check flag does everything that performance_runtime_type_check does but more, since it runtime type-checks everything instead of sampling. If a user turns on both, they lose performance in 2 ways with no added benefit coming from the performance_runtime_type_check. What do you think?", "url": "https://github.com/apache/beam/pull/12352#discussion_r465302751", "createdAt": "2020-08-04T20:11:56Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/pipeline.py", "diffHunk": "@@ -520,10 +520,26 @@ def run(self, test_runner_api='AUTO'):\n             self._options,\n             allow_proto_holders=True).run(False)\n \n+      if (self._options.view_as(TypeOptions).runtime_type_check and\n+          self._options.view_as(TypeOptions).performance_runtime_type_check):\n+        raise RuntimeError(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTc3MA=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyNjczMQ==", "bodyText": "I think eventually we'll want to consolidate the two. I suppose we can prohibit setting both if we want.", "url": "https://github.com/apache/beam/pull/12352#discussion_r466726731", "createdAt": "2020-08-06T22:50:20Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/pipeline.py", "diffHunk": "@@ -520,10 +520,26 @@ def run(self, test_runner_api='AUTO'):\n             self._options,\n             allow_proto_holders=True).run(False)\n \n+      if (self._options.view_as(TypeOptions).runtime_type_check and\n+          self._options.view_as(TypeOptions).performance_runtime_type_check):\n+        raise RuntimeError(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTc3MA=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMzNTU1NQ==", "bodyText": "Sounds good", "url": "https://github.com/apache/beam/pull/12352#discussion_r468335555", "createdAt": "2020-08-11T05:32:10Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/pipeline.py", "diffHunk": "@@ -520,10 +520,26 @@ def run(self, test_runner_api='AUTO'):\n             self._options,\n             allow_proto_holders=True).run(False)\n \n+      if (self._options.view_as(TypeOptions).runtime_type_check and\n+          self._options.view_as(TypeOptions).performance_runtime_type_check):\n+        raise RuntimeError(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MTc3MA=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjE1MTc2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1Njo0NlrOG7KBSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNjoxNDowMlrOG8krFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzMzNg==", "bodyText": "Move this logic into the visitor, rather than doing graph inspection on the workers. This will also allow us to consolidate all type checks into a single dict, rather than having redundant code for producer/consumer.", "url": "https://github.com/apache/beam/pull/12352#discussion_r464683336", "createdAt": "2020-08-03T21:56:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -202,6 +209,37 @@ def __init__(\n     self._sample_counter = 0\n     self._next_sample = 0\n \n+    self.producer_type_hints = None\n+    self.producer_full_label = None\n+    self.producer_parameter_name = None\n+\n+    if producer and hasattr(producer, 'spec') and hasattr(producer.spec,\n+                                                          'serialized_fn'):\n+      fns = pickler.loads(producer.spec.serialized_fn)\n+      if fns:\n+        if hasattr(fns[0], '_runtime_type_hints'):\n+          self.producer_type_hints = fns[0]._runtime_type_hints\n+        if hasattr(fns[0], '_full_label'):\n+          self.producer_full_label = fns[0]._full_label\n+        if hasattr(fns[0], '_runtime_parameter_name'):\n+          self.producer_parameter_name = fns[0]._runtime_parameter_name\n+\n+    self.consumer_type_hints = []\n+    self.consumer_full_labels = []\n+    self.consumer_parameter_names = []\n+\n+    if consumers:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2ODU5Nw==", "bodyText": "I moved as much logic as possible to the visitor (and removed the duplicated code) but kept the producer/consumer distinction because that determines whether we type check using the output types (for the producer) versus the input types (for the consumer)", "url": "https://github.com/apache/beam/pull/12352#discussion_r466168597", "createdAt": "2020-08-06T06:14:02Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -202,6 +209,37 @@ def __init__(\n     self._sample_counter = 0\n     self._next_sample = 0\n \n+    self.producer_type_hints = None\n+    self.producer_full_label = None\n+    self.producer_parameter_name = None\n+\n+    if producer and hasattr(producer, 'spec') and hasattr(producer.spec,\n+                                                          'serialized_fn'):\n+      fns = pickler.loads(producer.spec.serialized_fn)\n+      if fns:\n+        if hasattr(fns[0], '_runtime_type_hints'):\n+          self.producer_type_hints = fns[0]._runtime_type_hints\n+        if hasattr(fns[0], '_full_label'):\n+          self.producer_full_label = fns[0]._full_label\n+        if hasattr(fns[0], '_runtime_parameter_name'):\n+          self.producer_parameter_name = fns[0]._runtime_parameter_name\n+\n+    self.consumer_type_hints = []\n+    self.consumer_full_labels = []\n+    self.consumer_parameter_names = []\n+\n+    if consumers:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzMzNg=="}, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjE1MzYwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1NzozMFrOG7KCXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1NzozMFrOG7KCXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzYxNA==", "bodyText": "This logic belongs in the ParDoOperator (which I think already has a deserialized fn in hand).", "url": "https://github.com/apache/beam/pull/12352#discussion_r464683614", "createdAt": "2020-08-03T21:57:30Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -202,6 +209,37 @@ def __init__(\n     self._sample_counter = 0\n     self._next_sample = 0\n \n+    self.producer_type_hints = None\n+    self.producer_full_label = None\n+    self.producer_parameter_name = None\n+\n+    if producer and hasattr(producer, 'spec') and hasattr(producer.spec,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6177c9b164ec15879d34bb1f3236faa31a2edd0b"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNTI0MzYxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/bundle_processor.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjo1OTowNFrOG9G6fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMjo1OTowNFrOG9G6fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjcyOTU5OQ==", "bodyText": "This should just be None (or better, omitted) for operations without specs. We should probably just have a method on ParDoOperation to extract the type hints  as this is the one case we support.", "url": "https://github.com/apache/beam/pull/12352#discussion_r466729599", "createdAt": "2020-08-06T22:59:04Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/bundle_processor.py", "diffHunk": "@@ -188,7 +189,8 @@ def __init__(self,\n             self.name_context.step_name,\n             0,\n             next(iter(itervalues(consumers))),\n-            self.windowed_coder)\n+            self.windowed_coder,\n+            get_perf_runtime_type_hints(self))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "108e86960028af97e8b5d725e26593df77f0abc5"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNTM2MzI4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMzo1OTozNFrOG9IBAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODowNzozMlrOG-Zr1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NzY0OA==", "bodyText": "TODO: During graph construction, don't add any None constraints. This will allow us to remove this check", "url": "https://github.com/apache/beam/pull/12352#discussion_r466747648", "createdAt": "2020-08-06T23:59:34Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in self.output_type_constraints.items():\n+      parameter_name, constraint = type_constraint_tuple\n+      if constraint is not None:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NTcxOQ==", "bodyText": "Yes, please do that.", "url": "https://github.com/apache/beam/pull/12352#discussion_r468085719", "createdAt": "2020-08-10T18:07:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in self.output_type_constraints.items():\n+      parameter_name, constraint = type_constraint_tuple\n+      if constraint is not None:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NzY0OA=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDUxMjQ4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODowNzoxMVrOG-ZrJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQwNzowNDoyNFrOHBQK-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NTU0Mw==", "bodyText": "I don't think we want to mess with this. (Also, if we do, let's push this logic up into the visitor where we decide the label.)", "url": "https://github.com/apache/beam/pull/12352#discussion_r468085543", "createdAt": "2020-08-10T18:07:11Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in self.output_type_constraints.items():\n+      parameter_name, constraint = type_constraint_tuple\n+      if constraint is not None:\n+        try:\n+          _check_instance_type(constraint, value, parameter_name, verbose=True)\n+        except TypeCheckError as e:\n+          if not transform_label.startswith('ParDo'):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMyOTY0Mw==", "bodyText": "Okay, I pushed the logic up. The reason is so we stay consistent with how the existing error message is formatted.", "url": "https://github.com/apache/beam/pull/12352#discussion_r468329643", "createdAt": "2020-08-11T05:09:57Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in self.output_type_constraints.items():\n+      parameter_name, constraint = type_constraint_tuple\n+      if constraint is not None:\n+        try:\n+          _check_instance_type(constraint, value, parameter_name, verbose=True)\n+        except TypeCheckError as e:\n+          if not transform_label.startswith('ParDo'):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NTU0Mw=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTcyNQ==", "bodyText": "Oh... Yeah, changing all the error messages would be a pain, even if they're wrong now. Can you file a JIRA and drop a TODO to fix that then?", "url": "https://github.com/apache/beam/pull/12352#discussion_r470939725", "createdAt": "2020-08-15T05:16:05Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in self.output_type_constraints.items():\n+      parameter_name, constraint = type_constraint_tuple\n+      if constraint is not None:\n+        try:\n+          _check_instance_type(constraint, value, parameter_name, verbose=True)\n+        except TypeCheckError as e:\n+          if not transform_label.startswith('ParDo'):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NTU0Mw=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA3NTU3OQ==", "bodyText": "Sure - who should I assign the TODO to?", "url": "https://github.com/apache/beam/pull/12352#discussion_r471075579", "createdAt": "2020-08-16T07:04:24Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in self.output_type_constraints.items():\n+      parameter_name, constraint = type_constraint_tuple\n+      if constraint is not None:\n+        try:\n+          _check_instance_type(constraint, value, parameter_name, verbose=True)\n+        except TypeCheckError as e:\n+          if not transform_label.startswith('ParDo'):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4NTU0Mw=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDUzMzUxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/operations.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxMzozOFrOG-Z4Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwNToxMToyMVrOG-omAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4ODkwMw==", "bodyText": "This isn't correct, these type hints should not be used for this counter. Make this an optional argument defaulting to None and don't pass it here.", "url": "https://github.com/apache/beam/pull/12352#discussion_r468088903", "createdAt": "2020-08-10T18:13:38Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/operations.py", "diffHunk": "@@ -600,7 +627,8 @@ def _read_side_inputs(self, tags_and_types):\n           self.name_context.step_name,\n           view_options['coder'],\n           i,\n-          suffix='side-input')\n+          suffix='side-input',\n+          producer_type_hints=get_perf_runtime_type_hints(self))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMyOTk4Ng==", "bodyText": "Sounds good", "url": "https://github.com/apache/beam/pull/12352#discussion_r468329986", "createdAt": "2020-08-11T05:11:21Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/runners/worker/operations.py", "diffHunk": "@@ -600,7 +627,8 @@ def _read_side_inputs(self, tags_and_types):\n           self.name_context.step_name,\n           view_options['coder'],\n           i,\n-          suffix='side-input')\n+          suffix='side-input',\n+          producer_type_hints=get_perf_runtime_type_hints(self))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4ODkwMw=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDU0MDA2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxNTozMVrOG-Z8Xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwNTowNToyM1rOG-ogGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4OTk1MA==", "bodyText": "Why could this fail?", "url": "https://github.com/apache/beam/pull/12352#discussion_r468089950", "createdAt": "2020-08-10T18:15:31Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,95 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {\n+          applied_transform.full_label: self.get_output_type_hints(transform)\n+      }\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      if (hasattr(producer, 'transform') and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMyODQ3Mg==", "bodyText": "This used to error out with Impulse objects but adding the new method _add_type_constraint_from_consumer to PTransform fixed it", "url": "https://github.com/apache/beam/pull/12352#discussion_r468328472", "createdAt": "2020-08-11T05:05:23Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,95 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {\n+          applied_transform.full_label: self.get_output_type_hints(transform)\n+      }\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      if (hasattr(producer, 'transform') and", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA4OTk1MA=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDU0NTMyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxNjo1NVrOG-Z_aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwNTozMTowOFrOG-o6fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5MDcyOA==", "bodyText": "Cleaner to check isinstance(transform, core.ParDo).\nEven better would be to add a method like _add_producer_type_constraint to PTransform, default implementation empty, overridden in ParDo.", "url": "https://github.com/apache/beam/pull/12352#discussion_r468090728", "createdAt": "2020-08-10T18:16:55Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,95 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {\n+          applied_transform.full_label: self.get_output_type_hints(transform)\n+      }\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      if (hasattr(producer, 'transform') and\n+          hasattr(producer.transform, 'fn') and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMzNTIzMA==", "bodyText": "Awesome, added it", "url": "https://github.com/apache/beam/pull/12352#discussion_r468335230", "createdAt": "2020-08-11T05:31:08Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,95 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {\n+          applied_transform.full_label: self.get_output_type_hints(transform)\n+      }\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      if (hasattr(producer, 'transform') and\n+          hasattr(producer.transform, 'fn') and", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5MDcyOA=="}, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDU0NjkxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxNzoyNlrOG-aAZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxNzoyNlrOG-aAZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5MDk4MQ==", "bodyText": "We would want to add this regardless of the dict already being present, right? Perhaps with the above method that'd be simpler.", "url": "https://github.com/apache/beam/pull/12352#discussion_r468090981", "createdAt": "2020-08-10T18:17:26Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,95 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {\n+          applied_transform.full_label: self.get_output_type_hints(transform)\n+      }\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      if (hasattr(producer, 'transform') and\n+          hasattr(producer.transform, 'fn') and\n+          hasattr(producer.transform.fn, '_runtime_output_constraints')):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNDU0ODgxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxODowMFrOG-aBhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxODoxODowMFrOG-aBhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA5MTI2OA==", "bodyText": "Style: in Beam we avoid backslash line brakes. You can use yapf to break your lines for you. (If needed, add ()'s)>", "url": "https://github.com/apache/beam/pull/12352#discussion_r468091268", "createdAt": "2020-08-10T18:18:00Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,95 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {\n+          applied_transform.full_label: self.get_output_type_hints(transform)\n+      }\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      if (hasattr(producer, 'transform') and\n+          hasattr(producer.transform, 'fn') and\n+          hasattr(producer.transform.fn, '_runtime_output_constraints')):\n+        producer = producer.transform.fn\n+        producer._runtime_output_constraints[applied_transform.full_label] \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70e22b0a2685e29bc56d38d03c77808a52530e96"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAwMzg3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/snippets/snippets.py", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo0NDozMFrOHBHvNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMzowNjozMVrOHDfBPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzM5Ng==", "bodyText": "Why this change? Is encoded actually a list sometimes? (In which case, how did it ever pass?)", "url": "https://github.com/apache/beam/pull/12352#discussion_r470937396", "createdAt": "2020-08-15T04:44:30Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/snippets/snippets.py", "diffHunk": "@@ -689,8 +689,9 @@ def examples_wordcount_streaming(argv):\n \n     output = (\n         lines\n-        | 'DecodeUnicode' >>\n-        beam.FlatMap(lambda encoded: encoded.decode('utf-8'))\n+        | 'DecodeUnicode' >> beam.FlatMap(\n+            lambda encoded:\n+            (encoded if isinstance(encoded, list) else encoded.decode('utf-8')))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA3NTc0Mg==", "bodyText": "We changed a test to use a list instead of bytes, since it was written incorrectly", "url": "https://github.com/apache/beam/pull/12352#discussion_r471075742", "createdAt": "2020-08-16T07:06:24Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/examples/snippets/snippets.py", "diffHunk": "@@ -689,8 +689,9 @@ def examples_wordcount_streaming(argv):\n \n     output = (\n         lines\n-        | 'DecodeUnicode' >>\n-        beam.FlatMap(lambda encoded: encoded.decode('utf-8'))\n+        | 'DecodeUnicode' >> beam.FlatMap(\n+            lambda encoded:\n+            (encoded if isinstance(encoded, list) else encoded.decode('utf-8')))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzM5Ng=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIwODEyNQ==", "bodyText": "What I'm really asking was why was the test wrong? Isn't a list the wrong thing to pass to the next operation, ExtractWords, which uses re.findall?", "url": "https://github.com/apache/beam/pull/12352#discussion_r473208125", "createdAt": "2020-08-19T17:35:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/snippets/snippets.py", "diffHunk": "@@ -689,8 +689,9 @@ def examples_wordcount_streaming(argv):\n \n     output = (\n         lines\n-        | 'DecodeUnicode' >>\n-        beam.FlatMap(lambda encoded: encoded.decode('utf-8'))\n+        | 'DecodeUnicode' >> beam.FlatMap(\n+            lambda encoded:\n+            (encoded if isinstance(encoded, list) else encoded.decode('utf-8')))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzM5Ng=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI5NDYyNA==", "bodyText": "Oh yeah I see. The test is wrong now, but it was also wrong before because it returned an output value of type bytes, which we now throw an error for in process_outputs of common.py. We had an email thread about this earlier which I'll send a reply to for more context.", "url": "https://github.com/apache/beam/pull/12352#discussion_r473294624", "createdAt": "2020-08-19T20:19:07Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/examples/snippets/snippets.py", "diffHunk": "@@ -689,8 +689,9 @@ def examples_wordcount_streaming(argv):\n \n     output = (\n         lines\n-        | 'DecodeUnicode' >>\n-        beam.FlatMap(lambda encoded: encoded.decode('utf-8'))\n+        | 'DecodeUnicode' >> beam.FlatMap(\n+            lambda encoded:\n+            (encoded if isinstance(encoded, list) else encoded.decode('utf-8')))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzM5Ng=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQxNTk5OA==", "bodyText": "This should be changed to beam.Map(lambda encoded: encoded.decode('utf-8')) and the test values below changed back to bytes.", "url": "https://github.com/apache/beam/pull/12352#discussion_r473415998", "createdAt": "2020-08-19T23:06:31Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/snippets/snippets.py", "diffHunk": "@@ -689,8 +689,9 @@ def examples_wordcount_streaming(argv):\n \n     output = (\n         lines\n-        | 'DecodeUnicode' >>\n-        beam.FlatMap(lambda encoded: encoded.decode('utf-8'))\n+        | 'DecodeUnicode' >> beam.FlatMap(\n+            lambda encoded:\n+            (encoded if isinstance(encoded, list) else encoded.decode('utf-8')))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzM5Ng=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAwNTcyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo0NzoyMVrOHBHwBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxODoxNzoyOFrOHDTwTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzYwNg==", "bodyText": "This seems incorrect for Map(...) or Composite/ParDo(...). I don't think we should add a ParDo(...) if it's not already there (in which case we could drop this whole try-catch).", "url": "https://github.com/apache/beam/pull/12352#discussion_r470937606", "createdAt": "2020-08-15T04:47:21Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in (\n+            self.output_type_constraints.items()):\n+      parameter_name, constraint = type_constraint_tuple\n+      try:\n+        _check_instance_type(constraint, value, parameter_name, verbose=True)\n+      except TypeCheckError as e:\n+        if not transform_label.startswith('ParDo'):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIzMTQzNg==", "bodyText": "Drop a TODO(BEAM-10710) here as well.", "url": "https://github.com/apache/beam/pull/12352#discussion_r473231436", "createdAt": "2020-08-19T18:17:28Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/opcounters.py", "diffHunk": "@@ -224,8 +230,25 @@ def _observable_callback_inner(value, is_encoded=False):\n \n     return _observable_callback_inner\n \n+  def type_check(self, value):\n+    # type: (any, bool) -> None\n+    for transform_label, type_constraint_tuple in (\n+            self.output_type_constraints.items()):\n+      parameter_name, constraint = type_constraint_tuple\n+      try:\n+        _check_instance_type(constraint, value, parameter_name, verbose=True)\n+      except TypeCheckError as e:\n+        if not transform_label.startswith('ParDo'):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzYwNg=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAwODg2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/worker/operations.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo1MjoyM1rOHBHxbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo1MjoyM1rOHBHxbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzNzk2NQ==", "bodyText": "Omit this line. Different operations could choose to store them in different places (e.g. not all operations even have a serialized_fn).\nAlso, the line about \"This is only overridden by DoOperation's.\" is liable to go out of date (but unlikely to be updated) if we provide this anywhere else, and isn't important to this interface, so can be removed as well.\nWhat you might want to document, however, is the type of the return value.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470937965", "createdAt": "2020-08-15T04:52:23Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/worker/operations.py", "diffHunk": "@@ -452,6 +476,14 @@ def str_internal(self, is_recursive=False):\n \n     return '<%s %s>' % (printable_name, ', '.join(printable_fields))\n \n+  def _get_runtime_performance_hints(self):\n+    \"\"\"Returns any type hints required for performance runtime type-checking.\n+    These type hints are stored in the operation's spec's serialized_fn.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxMTkxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/ptransform.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo1NzowNVrOHBHyzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo1NzowNVrOHBHyzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODMxOQ==", "bodyText": "Likewise, omit this comment about ParDo.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938319", "createdAt": "2020-08-15T04:57:05Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/transforms/ptransform.py", "diffHunk": "@@ -702,6 +702,14 @@ def to_runner_api_pickled(self, unused_context):\n   def runner_api_requires_keyed_input(self):\n     return False\n \n+  def _add_type_constraint_from_consumer(self, full_label, input_type_hints):\n+    \"\"\"Adds a consumer transform's input type hints to our output type\n+    constraints, which is used during performance runtime type-checking.\n+\n+    This is only overridden by ParDo's.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxMzQ4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/core.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo1OTozMlrOHBHzhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNDo1OTozMlrOHBHzhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODUwMw==", "bodyText": "gettattr doesn't add the value if it's not present, you'll have to do\nif not hasattr(...):\n    self.fn._runtime_output_constraints = {}\nself.fn._runtime_output_constraints[...] = ...\n\n(I think this should be caught by tests, could you be sure you have a test that fails before fixing this?)", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938503", "createdAt": "2020-08-15T04:59:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/transforms/core.py", "diffHunk": "@@ -1465,6 +1465,11 @@ def get_restriction_coder(self):\n     from apache_beam.runners.common import DoFnSignature\n     return DoFnSignature(self.fn).get_restriction_coder()\n \n+  def _add_type_constraint_from_consumer(self, full_label, input_type_hints):\n+    output_constraints = getattr(self.fn, '_runtime_output_constraints', {})", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxNDM0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/core.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowMDozNVrOHBHz6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowMDozNVrOHBHz6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODYwMQ==", "bodyText": "Oh, I see you do it here. I think the logic would be clearer as above.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938601", "createdAt": "2020-08-15T05:00:35Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/transforms/core.py", "diffHunk": "@@ -1465,6 +1465,11 @@ def get_restriction_coder(self):\n     from apache_beam.runners.common import DoFnSignature\n     return DoFnSignature(self.fn).get_restriction_coder()\n \n+  def _add_type_constraint_from_consumer(self, full_label, input_type_hints):\n+    output_constraints = getattr(self.fn, '_runtime_output_constraints', {})\n+    output_constraints[full_label] = input_type_hints\n+    self.fn._runtime_output_constraints = output_constraints", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxNDk2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/core.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowMTo1OFrOHBH0Nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQyMzo0MzoyMlrOHBWNGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODY3OQ==", "bodyText": "You're recording that the type hint comes from this transform, but are you recording that it comes from the input of this transform? (E.g. if I get a failure about transform B's type hint being violated, it may not be clear if it was its input type or output type that was bad.)", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938679", "createdAt": "2020-08-15T05:01:58Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/transforms/core.py", "diffHunk": "@@ -1465,6 +1465,11 @@ def get_restriction_coder(self):\n     from apache_beam.runners.common import DoFnSignature\n     return DoFnSignature(self.fn).get_restriction_coder()\n \n+  def _add_type_constraint_from_consumer(self, full_label, input_type_hints):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE3NDQyNw==", "bodyText": "Yes, we store this information during get_input_type_hints which produces the data that's passed to _add_type_constraint_from_consumer.\nI added a test called test_downstream_input_type_hint_error_has_descriptive_error_msg to confirm this behavior, but let me know if that doesn't cover the case you're talking about.", "url": "https://github.com/apache/beam/pull/12352#discussion_r471174427", "createdAt": "2020-08-16T23:43:22Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/transforms/core.py", "diffHunk": "@@ -1465,6 +1465,11 @@ def get_restriction_coder(self):\n     from apache_beam.runners.common import DoFnSignature\n     return DoFnSignature(self.fn).get_restriction_coder()\n \n+  def _add_type_constraint_from_consumer(self, full_label, input_type_hints):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODY3OQ=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxNTc0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowMzowMFrOHBH0kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowMzowMFrOHBH0kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODc2OA==", "bodyText": "This will overwrite any typehint constraints that may have been set earlier. Instead, use the same method.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938768", "createdAt": "2020-08-15T05:03:00Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxNjAyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowMzoyNlrOHBH0rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQyMTo0MTo0MFrOHBVc-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODc5OQ==", "bodyText": "Don't add this prefix.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938799", "createdAt": "2020-08-15T05:03:26Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE2MjEwNA==", "bodyText": "Okay, I'll remove it. With regards to the JIRA issue filed from our conversation about this, I'll move the TODO to where 'ParDo' is currently being prefixed for other error messages here.", "url": "https://github.com/apache/beam/pull/12352#discussion_r471162104", "createdAt": "2020-08-16T21:41:40Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODc5OQ=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxNjM4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowNDoxOVrOHBH03g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowNDoxOVrOHBH03g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzODg0Ng==", "bodyText": "Nit: move this line into the if block.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470938846", "createdAt": "2020-08-15T05:04:19Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAxOTU5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowOToxNFrOHBH2Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQyMjo1OToyOFrOHBV6cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTIzMQ==", "bodyText": "This does not need to be guarded by if isinstance(transform, core.ParDo), we can do it for all transforms. (Actually, the same could be said of the output type hints if we eliminated the direct access to _runtime_output_constraints as mentioned above.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470939231", "createdAt": "2020-08-15T05:09:14Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTM0Nw==", "bodyText": "If this results in test failures due to more (incorrect) type hints being applied, feel free to drop a TODO here for future work.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470939347", "createdAt": "2020-08-15T05:10:31Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTIzMQ=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE2NTA1NA==", "bodyText": "It fails for Impulse, GroupByKey, Flatten and a few other transforms. Sounds good, added a TODO for now.", "url": "https://github.com/apache/beam/pull/12352#discussion_r471165054", "createdAt": "2020-08-16T22:11:57Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTIzMQ=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE2OTY0OQ==", "bodyText": "Nevermind, fixed it :)", "url": "https://github.com/apache/beam/pull/12352#discussion_r471169649", "createdAt": "2020-08-16T22:59:28Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTIzMQ=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAyMDA4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowOTo0N1rOHBH2lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNTowOTo0N1rOHBH2lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTI4NQ==", "bodyText": "Also visit this transform to get its typehints, if any?", "url": "https://github.com/apache/beam/pull/12352#discussion_r470939285", "createdAt": "2020-08-15T05:09:47Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAyMTMyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToxMTozN1rOHBH3Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToxMTozN1rOHBH3Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTQyMg==", "bodyText": "This is the one bit that might need to be guarded by DoFn.", "url": "https://github.com/apache/beam/pull/12352#discussion_r470939422", "createdAt": "2020-08-15T05:11:37Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      input_type_hints = self.get_input_type_hints(transform)\n+      if input_type_hints:\n+        producer.transform._add_type_constraint_from_consumer(\n+            full_label, input_type_hints)\n+\n+  def get_input_type_hints(self, transform):\n+    type_hints = transform.get_type_hints()\n+\n+    input_types = None\n+    if type_hints.input_types:\n+      normal_hints, kwarg_hints = type_hints.input_types\n+      if kwarg_hints:\n+        input_types = kwarg_hints\n+      if normal_hints:\n+        input_types = normal_hints\n+\n+    parameter_name = 'Unknown Parameter'\n+    try:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzAyMTU2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwNToxMTo1MlrOHBH3Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQyMTo1Mzo1MVrOHBVhnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTQ1MQ==", "bodyText": "Why would TypeError be raised?", "url": "https://github.com/apache/beam/pull/12352#discussion_r470939451", "createdAt": "2020-08-15T05:11:52Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      input_type_hints = self.get_input_type_hints(transform)\n+      if input_type_hints:\n+        producer.transform._add_type_constraint_from_consumer(\n+            full_label, input_type_hints)\n+\n+  def get_input_type_hints(self, transform):\n+    type_hints = transform.get_type_hints()\n+\n+    input_types = None\n+    if type_hints.input_types:\n+      normal_hints, kwarg_hints = type_hints.input_types\n+      if kwarg_hints:\n+        input_types = kwarg_hints\n+      if normal_hints:\n+        input_types = normal_hints\n+\n+    parameter_name = 'Unknown Parameter'\n+    try:\n+      argspec = inspect.getfullargspec(transform.fn._process_argspec_fn())\n+      if len(argspec.args):\n+        arg_index = 0\n+        if argspec.args[0] == 'self':\n+          arg_index = 1\n+        parameter_name = argspec.args[arg_index]\n+        if isinstance(input_types, dict):\n+          input_types = (input_types[argspec.args[arg_index]], )\n+    except TypeError:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE2MzI5NQ==", "bodyText": "This is for when an unsupported callable is passed to inspect.getfullargspec. I updated the code so this is more clear.", "url": "https://github.com/apache/beam/pull/12352#discussion_r471163295", "createdAt": "2020-08-16T21:53:51Z", "author": {"login": "saavannanavati"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +268,89 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+\n+  _in_combine = False\n+  combine_classes = (\n+      core.CombineFn,\n+      core.CombinePerKey,\n+      core.CombineValuesDoFn,\n+      core.CombineValues,\n+      core.CombineGlobally)\n+\n+  def enter_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = True\n+\n+  def leave_composite_transform(self, applied_transform):\n+    if isinstance(applied_transform.transform, self.combine_classes):\n+      self._in_combine = False\n+\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    if isinstance(transform, core.ParDo) and not self._in_combine:\n+      # Prefix label with 'ParDo' if necessary\n+      full_label = applied_transform.full_label\n+      if not full_label.startswith('ParDo'):\n+        full_label = 'ParDo(%s)' % full_label\n+\n+      # Store output type hints in current transform\n+      transform.fn._runtime_output_constraints = {}\n+      output_type_hints = self.get_output_type_hints(transform)\n+      if output_type_hints:\n+        transform.fn._runtime_output_constraints[full_label] = (\n+            output_type_hints)\n+\n+      # Store input type hints in producer transform\n+      producer = applied_transform.inputs[0].producer\n+      input_type_hints = self.get_input_type_hints(transform)\n+      if input_type_hints:\n+        producer.transform._add_type_constraint_from_consumer(\n+            full_label, input_type_hints)\n+\n+  def get_input_type_hints(self, transform):\n+    type_hints = transform.get_type_hints()\n+\n+    input_types = None\n+    if type_hints.input_types:\n+      normal_hints, kwarg_hints = type_hints.input_types\n+      if kwarg_hints:\n+        input_types = kwarg_hints\n+      if normal_hints:\n+        input_types = normal_hints\n+\n+    parameter_name = 'Unknown Parameter'\n+    try:\n+      argspec = inspect.getfullargspec(transform.fn._process_argspec_fn())\n+      if len(argspec.args):\n+        arg_index = 0\n+        if argspec.args[0] == 'self':\n+          arg_index = 1\n+        parameter_name = argspec.args[arg_index]\n+        if isinstance(input_types, dict):\n+          input_types = (input_types[argspec.args[arg_index]], )\n+    except TypeError:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkzOTQ1MQ=="}, "originalCommit": {"oid": "8ba15de2bead40991f2fb16a56e0cd8b4f0171ba"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1Nzg2ODU1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/typehints/typecheck.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNzozOToxMVrOHDSctg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNzozOToxMVrOHDSctg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIxMDAzOA==", "bodyText": "Use transform._add_type_constraint_from_consumer rather than direct access here as well. Then you can get rid of hasattr(transform, 'fn') and just write\noutput_type_hints = self.get_output_type_hints(transform)\nif output_type_hints:\n  transform._add_type_constraint_from_consumer(output_type_hints)", "url": "https://github.com/apache/beam/pull/12352#discussion_r473210038", "createdAt": "2020-08-19T17:39:11Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/typehints/typecheck.py", "diffHunk": "@@ -265,3 +269,74 @@ def visit_transform(self, applied_transform):\n                 transform.get_type_hints(),\n                 applied_transform.full_label),\n             applied_transform.full_label)\n+\n+\n+class PerformanceTypeCheckVisitor(pipeline.PipelineVisitor):\n+  def visit_transform(self, applied_transform):\n+    transform = applied_transform.transform\n+    full_label = applied_transform.full_label\n+\n+    # Store output type hints in current transform\n+    if hasattr(transform, 'fn'):\n+      if not hasattr(transform.fn, '_runtime_output_constraints'):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a1fd8352a7be42499dee0b03432926038926609e"}, "originalPosition": 53}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 932, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}