{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0MjQzMTAx", "number": 10839, "reviewThreads": {"totalCount": 53, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTozNjoxNFrOD865ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNTozMzowM1rOECnfKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjA2MTc0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTozNjoxNFrOGWJ52Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToxOTozMVrOGXzBmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NDEyMQ==", "bodyText": "this can be static constant.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425884121", "createdAt": "2020-05-15T15:36:14Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwNjQyNg==", "bodyText": "fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427606426", "createdAt": "2020-05-19T21:19:31Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NDEyMQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjA2ODk4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTozODoyMVrOGWJ-lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMToyOTowNFrOGcHZ2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NTMzNA==", "bodyText": "Maybe we should interpret continuationToken as cursor  here ? both are strings and are effectively same thing -  opaque string that let's you resume iteration.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425885334", "createdAt": "2020-05-15T15:38:21Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwNzUyMg==", "bodyText": "I think we need to have a broader discussion about this but the reason I don't want to make that an option is because I don't want users to think they have to provide a continuationToken to get the next page in changefeed (the implementation by default has the ability to get every page lazily). In changefeed the cursor is just used to initialize the changefeed object correctly as opposed to continually be fed to get the next page (this is what users are most used to doing). Does that make sense?_", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427607522", "createdAt": "2020-05-19T21:21:44Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NTMzNA=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3NDY3NQ==", "bodyText": "which is not far away from one of applications of continuation token.\nnormally user doesn't touch it. however, if the enumeration is pretty long they might be saving the token when error occur and just use it later to resume - i.e. use saved token to initialize iterable and let it continue without handholding the token.\nIf we choose to overload the meaning of continuation token then we need to make sure to add javadoc explaining how this work here.\nBtw. looks like in .NET PR continuationToken == cursor. See\nhttps://github.com/Azure/azure-sdk-for-net/pull/11692/files#diff-c69bdde9b907d209a3c6315196fdb137\nand\nhttps://github.com/seanmcc-msft/azure-sdk-for-net/blob/feature/storage/changeFeed/sdk/storage/Azure.Storage.Blobs.ChangeFeed/README.md#resume-with-cursor", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r428074675", "createdAt": "2020-05-20T14:49:21Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NTMzNA=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNDYxOA==", "bodyText": "discussed offline, we will throw for now and add in functionality later if required", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432134618", "createdAt": "2020-05-28T21:29:04Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NTMzNA=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjA3NDM4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTozOTo1MlrOGWKCCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMToyOTozNFrOGcHa5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg==", "bodyText": "I'd rather throw if preferredPageSize is too big. May spare surprise for someone trying to understand what's going on.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425886216", "createdAt": "2020-05-15T15:39:52Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwODM2MA==", "bodyText": "This preferred page size is kinda made up. We can have a discussion about this as well. I wanted the experience to be consistent with other service list calls where it uses the logic I used.\nIf the request does not specify maxResultsPerPage or specifies a value greater than 5,000, the server will return up to 5,000 items.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427608360", "createdAt": "2020-05-19T21:23:27Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwODYyNw==", "bodyText": "We can also add this to documentation if we want to go down this route - or we can let a user specify anything", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427608627", "createdAt": "2020-05-19T21:23:56Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA2OTYxOQ==", "bodyText": "If server does that then we probably can just drop Integer.min and let the server do this.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r428069619", "createdAt": "2020-05-20T14:43:01Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE1NjY5OQ==", "bodyText": "By the service does it - I mean for existing listBlobs/listContainers APIs. This number here is not restricted by the service", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r428156699", "createdAt": "2020-05-20T16:41:19Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNDg4Ng==", "bodyText": ".NET is also using this behavior since it's likely to be what users are used to", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432134886", "createdAt": "2020-05-28T21:29:34Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjA5ODExOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo0NjoxOVrOGWKRFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOToyNToyNVrOGcDU0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MDA3MQ==", "bodyText": "I think client is more of dependency required to create these things rather than a parameter (I guess we use same client to create all of these things). If so then this should rather be passed in constructor to this factory.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425890071", "createdAt": "2020-05-15T15:46:19Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxFactory.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * Factory class for {@link BlobChangefeedPagedFlux}.\n+ */\n+class BlobChangefeedPagedFluxFactory {\n+\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Creates a default instance of the BlobChangefeedPagedFluxFactory.\n+     */\n+    BlobChangefeedPagedFluxFactory() {\n+        this.changefeedFactory = new ChangefeedFactory();\n+    }\n+\n+    /**\n+     * Creates a BlobChangefeedPagedFluxFactory with the designated factories.\n+     */\n+    BlobChangefeedPagedFluxFactory(ChangefeedFactory changefeedFactory) {\n+        this.changefeedFactory = changefeedFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param client The {@link BlobContainerAsyncClient changefeed client}.\n+     * @param startTime The {@link OffsetDateTime start time}.\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    BlobChangefeedPagedFlux getBlobChangefeedPagedFlux(BlobContainerAsyncClient client, OffsetDateTime startTime,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA2Nzc5NA==", "bodyText": "Changed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432067794", "createdAt": "2020-05-28T19:25:25Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxFactory.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * Factory class for {@link BlobChangefeedPagedFlux}.\n+ */\n+class BlobChangefeedPagedFluxFactory {\n+\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Creates a default instance of the BlobChangefeedPagedFluxFactory.\n+     */\n+    BlobChangefeedPagedFluxFactory() {\n+        this.changefeedFactory = new ChangefeedFactory();\n+    }\n+\n+    /**\n+     * Creates a BlobChangefeedPagedFluxFactory with the designated factories.\n+     */\n+    BlobChangefeedPagedFluxFactory(ChangefeedFactory changefeedFactory) {\n+        this.changefeedFactory = changefeedFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param client The {@link BlobContainerAsyncClient changefeed client}.\n+     * @param startTime The {@link OffsetDateTime start time}.\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    BlobChangefeedPagedFlux getBlobChangefeedPagedFlux(BlobContainerAsyncClient client, OffsetDateTime startTime,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MDA3MQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEwMDU1OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedIterable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo0NzowNFrOGWKSyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToyNDo1N1rOGXzMFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MDUwNQ==", "bodyText": "Do we need this type?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425890505", "createdAt": "2020-05-15T15:47:04Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedIterable.java", "diffHunk": "@@ -0,0 +1,26 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedIterable;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedIterable} for Changefeed where the continuation token type is\n+ * {@link String}, the element type is {@link BlobChangefeedEvent}, and the page type is\n+ * {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedIterable extends ContinuablePagedIterable<String,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwOTEwOQ==", "bodyText": "This is one of those things where we're trying to be consistent. This seems to be the way even other services do this", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427609109", "createdAt": "2020-05-19T21:24:57Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedIterable.java", "diffHunk": "@@ -0,0 +1,26 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedIterable;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedIterable} for Changefeed where the continuation token type is\n+ * {@link String}, the element type is {@link BlobChangefeedEvent}, and the page type is\n+ * {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedIterable extends ContinuablePagedIterable<String,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MDUwNQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEwNDY5OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedResponse.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo0ODoxN1rOGWKVbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMToyOTo1OVrOGcHbyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTE4Mw==", "bodyText": "Oh, nice. So here we say that continuation token is cursor. Then we should honor it back as continuation token in the other place were we now throw.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425891183", "createdAt": "2020-05-15T15:48:17Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedResponse.java", "diffHunk": "@@ -0,0 +1,64 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePage;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents a page returned in BlobChangefeed.\n+ *\n+ * <p>A {@link BlobChangefeedPagedResponse} consists of {@link BlobChangefeedEvent} elements and {@link String}\n+ * continuation token (known as a cursor in Changefeed). </p>\n+ *\n+ * <p>A cursor can be used to re-initialize a BlobChangefeed to point to the next expected page. </p>\n+ *\n+ * @see BlobChangefeedPagedFlux\n+ * @see BlobChangefeedPagedIterable\n+ */\n+public class BlobChangefeedPagedResponse implements ContinuablePage<String, BlobChangefeedEvent> {\n+\n+    private final List<BlobChangefeedEvent> events;\n+    private final ChangefeedCursor cursor;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedPagedFlux}\n+     * @param events A {@link List} of {@link BlobChangefeedEvent BlobChangefeedEvents}.\n+     * @param cursor A {@link ChangefeedCursor cursor}.\n+     */\n+    BlobChangefeedPagedResponse(List<BlobChangefeedEvent> events, ChangefeedCursor cursor) {\n+        this.events = events;\n+        this.cursor = cursor;\n+    }\n+\n+    /**\n+     * @inheritDoc\n+     */\n+    public IterableStream<BlobChangefeedEvent> getElements() {\n+        return new IterableStream<>(this.events);\n+    }\n+\n+    /**\n+     * Gets a {@link List} of elements in the page.\n+     *\n+     * @return A {@link List} containing the elements in the page.\n+     */\n+    public List<BlobChangefeedEvent> getValue() {\n+        return this.events;\n+    }\n+\n+    /**\n+     * Gets a reference to the next page, should you want to re-initialize the BlobChangefeed.\n+     *\n+     * @return The {@link String cursor} that references the next page.\n+     */\n+    public String getContinuationToken() {\n+        /* Serialize the cursor and return it to the user as a String. */\n+        return cursor.serialize();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwOTk0MA==", "bodyText": "See my comment above. Maybe this method (inherited from pagedresponse) should throw instead and we make a getCursor method? Depends on the route we choose to take", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427609940", "createdAt": "2020-05-19T21:26:35Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedResponse.java", "diffHunk": "@@ -0,0 +1,64 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePage;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents a page returned in BlobChangefeed.\n+ *\n+ * <p>A {@link BlobChangefeedPagedResponse} consists of {@link BlobChangefeedEvent} elements and {@link String}\n+ * continuation token (known as a cursor in Changefeed). </p>\n+ *\n+ * <p>A cursor can be used to re-initialize a BlobChangefeed to point to the next expected page. </p>\n+ *\n+ * @see BlobChangefeedPagedFlux\n+ * @see BlobChangefeedPagedIterable\n+ */\n+public class BlobChangefeedPagedResponse implements ContinuablePage<String, BlobChangefeedEvent> {\n+\n+    private final List<BlobChangefeedEvent> events;\n+    private final ChangefeedCursor cursor;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedPagedFlux}\n+     * @param events A {@link List} of {@link BlobChangefeedEvent BlobChangefeedEvents}.\n+     * @param cursor A {@link ChangefeedCursor cursor}.\n+     */\n+    BlobChangefeedPagedResponse(List<BlobChangefeedEvent> events, ChangefeedCursor cursor) {\n+        this.events = events;\n+        this.cursor = cursor;\n+    }\n+\n+    /**\n+     * @inheritDoc\n+     */\n+    public IterableStream<BlobChangefeedEvent> getElements() {\n+        return new IterableStream<>(this.events);\n+    }\n+\n+    /**\n+     * Gets a {@link List} of elements in the page.\n+     *\n+     * @return A {@link List} containing the elements in the page.\n+     */\n+    public List<BlobChangefeedEvent> getValue() {\n+        return this.events;\n+    }\n+\n+    /**\n+     * Gets a reference to the next page, should you want to re-initialize the BlobChangefeed.\n+     *\n+     * @return The {@link String cursor} that references the next page.\n+     */\n+    public String getContinuationToken() {\n+        /* Serialize the cursor and return it to the user as a String. */\n+        return cursor.serialize();\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTE4Mw=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA2ODYwMw==", "bodyText": "yes, both should throw or both should work.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r428068603", "createdAt": "2020-05-20T14:41:45Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedResponse.java", "diffHunk": "@@ -0,0 +1,64 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePage;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents a page returned in BlobChangefeed.\n+ *\n+ * <p>A {@link BlobChangefeedPagedResponse} consists of {@link BlobChangefeedEvent} elements and {@link String}\n+ * continuation token (known as a cursor in Changefeed). </p>\n+ *\n+ * <p>A cursor can be used to re-initialize a BlobChangefeed to point to the next expected page. </p>\n+ *\n+ * @see BlobChangefeedPagedFlux\n+ * @see BlobChangefeedPagedIterable\n+ */\n+public class BlobChangefeedPagedResponse implements ContinuablePage<String, BlobChangefeedEvent> {\n+\n+    private final List<BlobChangefeedEvent> events;\n+    private final ChangefeedCursor cursor;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedPagedFlux}\n+     * @param events A {@link List} of {@link BlobChangefeedEvent BlobChangefeedEvents}.\n+     * @param cursor A {@link ChangefeedCursor cursor}.\n+     */\n+    BlobChangefeedPagedResponse(List<BlobChangefeedEvent> events, ChangefeedCursor cursor) {\n+        this.events = events;\n+        this.cursor = cursor;\n+    }\n+\n+    /**\n+     * @inheritDoc\n+     */\n+    public IterableStream<BlobChangefeedEvent> getElements() {\n+        return new IterableStream<>(this.events);\n+    }\n+\n+    /**\n+     * Gets a {@link List} of elements in the page.\n+     *\n+     * @return A {@link List} containing the elements in the page.\n+     */\n+    public List<BlobChangefeedEvent> getValue() {\n+        return this.events;\n+    }\n+\n+    /**\n+     * Gets a reference to the next page, should you want to re-initialize the BlobChangefeed.\n+     *\n+     * @return The {@link String cursor} that references the next page.\n+     */\n+    public String getContinuationToken() {\n+        /* Serialize the cursor and return it to the user as a String. */\n+        return cursor.serialize();\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTE4Mw=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNTExMw==", "bodyText": "Based on offline conversation, we are keeping this", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432135113", "createdAt": "2020-05-28T21:29:59Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedResponse.java", "diffHunk": "@@ -0,0 +1,64 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePage;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents a page returned in BlobChangefeed.\n+ *\n+ * <p>A {@link BlobChangefeedPagedResponse} consists of {@link BlobChangefeedEvent} elements and {@link String}\n+ * continuation token (known as a cursor in Changefeed). </p>\n+ *\n+ * <p>A cursor can be used to re-initialize a BlobChangefeed to point to the next expected page. </p>\n+ *\n+ * @see BlobChangefeedPagedFlux\n+ * @see BlobChangefeedPagedIterable\n+ */\n+public class BlobChangefeedPagedResponse implements ContinuablePage<String, BlobChangefeedEvent> {\n+\n+    private final List<BlobChangefeedEvent> events;\n+    private final ChangefeedCursor cursor;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedPagedFlux}\n+     * @param events A {@link List} of {@link BlobChangefeedEvent BlobChangefeedEvents}.\n+     * @param cursor A {@link ChangefeedCursor cursor}.\n+     */\n+    BlobChangefeedPagedResponse(List<BlobChangefeedEvent> events, ChangefeedCursor cursor) {\n+        this.events = events;\n+        this.cursor = cursor;\n+    }\n+\n+    /**\n+     * @inheritDoc\n+     */\n+    public IterableStream<BlobChangefeedEvent> getElements() {\n+        return new IterableStream<>(this.events);\n+    }\n+\n+    /**\n+     * Gets a {@link List} of elements in the page.\n+     *\n+     * @return A {@link List} containing the elements in the page.\n+     */\n+    public List<BlobChangefeedEvent> getValue() {\n+        return this.events;\n+    }\n+\n+    /**\n+     * Gets a reference to the next page, should you want to re-initialize the BlobChangefeed.\n+     *\n+     * @return The {@link String cursor} that references the next page.\n+     */\n+    public String getContinuationToken() {\n+        /* Serialize the cursor and return it to the user as a String. */\n+        return cursor.serialize();\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTE4Mw=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEwOTYwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo0OTo0M1rOGWKYiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMTozMToxNVrOGXzYXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTk3OQ==", "bodyText": "What does getSetupMono mean ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425891979", "createdAt": "2020-05-15T15:49:43Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,88 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.http.rest.ResponseBase;\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /*TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range ->\n+            client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.getSetupMono(range, options, requestConditions, downloadFunc,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMjI1NQ==", "bodyText": "Renamed to downloadFirstChunk. This method is meant to download the first chunk then get information such as the size of the blob to download the rest of it.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427612255", "createdAt": "2020-05-19T21:31:15Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,88 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.http.rest.ResponseBase;\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /*TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range ->\n+            client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.getSetupMono(range, options, requestConditions, downloadFunc,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTk3OQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjExNTg2OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo1MToyMVrOGWKcgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzowNDo0NFrOGfPfzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Mjk5NQ==", "bodyText": "same here about client being dependency rather than parameter.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425892995", "createdAt": "2020-05-15T15:51:21Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+\n+/**\n+ * Factory class for {@link BlobLazyDownloader}.\n+ */\n+class BlobLazyDownloaderFactory {\n+\n+    /**\n+     * Gets a new instance of a BlobLazyDownloader.\n+     *\n+     * @param client The blob client.\n+     * @param blockSize The block size to download.\n+     * @param offset The offset to start downloading from.\n+     * @return {@link BlobLazyDownloader}\n+     */\n+    BlobLazyDownloader getBlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MjY4OQ==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432082689", "createdAt": "2020-05-28T19:51:20Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+\n+/**\n+ * Factory class for {@link BlobLazyDownloader}.\n+ */\n+class BlobLazyDownloaderFactory {\n+\n+    /**\n+     * Gets a new instance of a BlobLazyDownloader.\n+     *\n+     * @param client The blob client.\n+     * @param blockSize The block size to download.\n+     * @param offset The offset to start downloading from.\n+     * @return {@link BlobLazyDownloader}\n+     */\n+    BlobLazyDownloader getBlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Mjk5NQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM0MzcwOQ==", "bodyText": "did you make any change? I can still see the old diff here.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435343709", "createdAt": "2020-06-04T15:23:06Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+\n+/**\n+ * Factory class for {@link BlobLazyDownloader}.\n+ */\n+class BlobLazyDownloaderFactory {\n+\n+    /**\n+     * Gets a new instance of a BlobLazyDownloader.\n+     *\n+     * @param client The blob client.\n+     * @param blockSize The block size to download.\n+     * @param offset The offset to start downloading from.\n+     * @return {@link BlobLazyDownloader}\n+     */\n+    BlobLazyDownloader getBlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Mjk5NQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQxMjk0Mw==", "bodyText": "my bad I missed this one", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435412943", "createdAt": "2020-06-04T17:04:44Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+\n+/**\n+ * Factory class for {@link BlobLazyDownloader}.\n+ */\n+class BlobLazyDownloaderFactory {\n+\n+    /**\n+     * Gets a new instance of a BlobLazyDownloader.\n+     *\n+     * @param client The blob client.\n+     * @param blockSize The block size to download.\n+     * @param offset The offset to start downloading from.\n+     * @return {@link BlobLazyDownloader}\n+     */\n+    BlobLazyDownloader getBlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Mjk5NQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEyNzAzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChangefeedFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo1NDoyN1rOGWKj8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MTo0OVrOGcEQdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NDg5OQ==", "bodyText": "same feedback about client being parameter.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425894899", "createdAt": "2020-05-15T15:54:27Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChangefeedFactory.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * Factory class for {@link ChangefeedFactory}.\n+ */\n+class ChangefeedFactory {\n+\n+    private final SegmentFactory segmentFactory;\n+\n+    /**\n+     * Creates a default instance of the ChangefeedFactory.\n+     */\n+    ChangefeedFactory() {\n+        this.segmentFactory = new SegmentFactory();\n+    }\n+\n+    /**\n+     * Creates a SegmentFactory with the designated factories.\n+     */\n+    ChangefeedFactory(SegmentFactory segmentFactory) {\n+        this.segmentFactory = segmentFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of a Changefeed.\n+     */\n+    Changefeed getChangefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzA2Mg==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432083062", "createdAt": "2020-05-28T19:51:49Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChangefeedFactory.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * Factory class for {@link ChangefeedFactory}.\n+ */\n+class ChangefeedFactory {\n+\n+    private final SegmentFactory segmentFactory;\n+\n+    /**\n+     * Creates a default instance of the ChangefeedFactory.\n+     */\n+    ChangefeedFactory() {\n+        this.segmentFactory = new SegmentFactory();\n+    }\n+\n+    /**\n+     * Creates a SegmentFactory with the designated factories.\n+     */\n+    ChangefeedFactory(SegmentFactory segmentFactory) {\n+        this.segmentFactory = segmentFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of a Changefeed.\n+     */\n+    Changefeed getChangefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NDg5OQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEyOTE5OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Chunk.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo1NTowM1rOGWKlUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxOToyNzowMlrOGbaC4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTI0OQ==", "bodyText": "do we want to validate other parameters ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425895249", "createdAt": "2020-05-15T15:55:03Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Chunk.java", "diffHunk": "@@ -0,0 +1,56 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import reactor.core.publisher.Flux;\n+\n+/**\n+ * A class that represents a Chunk in Changefeed.\n+ *\n+ * A chunk is an append blob that contains avro encoded changefeed events.\n+ */\n+class Chunk {\n+\n+    private final String chunkPath; /* Chunk path. */\n+    private final ChangefeedCursor shardCursor; /* Cursor associated with parent shard. */\n+    private final AvroReader avroReader;\n+\n+    /**\n+     * Creates a new Chunk.\n+     */\n+    Chunk(String chunkPath, ChangefeedCursor shardCursor, AvroReader avroReader) {\n+        StorageImplUtils.assertNotNull(\"avroReader\", avroReader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5MTQ1Ng==", "bodyText": "Originally didnt cause I did it in the ChunkFactory but I see no harm in adding this here too", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431391456", "createdAt": "2020-05-27T19:27:02Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Chunk.java", "diffHunk": "@@ -0,0 +1,56 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import reactor.core.publisher.Flux;\n+\n+/**\n+ * A class that represents a Chunk in Changefeed.\n+ *\n+ * A chunk is an append blob that contains avro encoded changefeed events.\n+ */\n+class Chunk {\n+\n+    private final String chunkPath; /* Chunk path. */\n+    private final ChangefeedCursor shardCursor; /* Cursor associated with parent shard. */\n+    private final AvroReader avroReader;\n+\n+    /**\n+     * Creates a new Chunk.\n+     */\n+    Chunk(String chunkPath, ChangefeedCursor shardCursor, AvroReader avroReader) {\n+        StorageImplUtils.assertNotNull(\"avroReader\", avroReader);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTI0OQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEzMTUzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo1NTo0MVrOGWKm5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTozMDoxM1rOGcHcMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTY1NQ==", "bodyText": "same feedback on client", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425895655", "createdAt": "2020-05-15T15:55:41Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {\n+        this.avroReaderFactory = new AvroReaderFactory();\n+        this.blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+    }\n+\n+    /**\n+     * Creates a ChunkFactory with the designated factories.\n+     */\n+    ChunkFactory(AvroReaderFactory avroReaderFactory, BlobLazyDownloaderFactory blobLazyDownloaderFactory) {\n+        this.avroReaderFactory = avroReaderFactory;\n+        this.blobLazyDownloaderFactory = blobLazyDownloaderFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of a Chunk.\n+     *\n+     * @param client The changefeed container client.\n+     * @param chunkPath The path to the chunk blob.\n+     * @param shardCursor The parent shard cursor.\n+     * @param blockOffset The offset of the block to start reading from. If 0, this indicates we should read the whole\n+     *                    avro file from the beginning.\n+     * @param objectBlockIndex The index of the last object in the block that was returned to the user.\n+     * @return {@link Chunk}\n+     */\n+    Chunk getChunk(BlobContainerAsyncClient client, String chunkPath, ChangefeedCursor shardCursor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNTIxOA==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432135218", "createdAt": "2020-05-28T21:30:13Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {\n+        this.avroReaderFactory = new AvroReaderFactory();\n+        this.blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+    }\n+\n+    /**\n+     * Creates a ChunkFactory with the designated factories.\n+     */\n+    ChunkFactory(AvroReaderFactory avroReaderFactory, BlobLazyDownloaderFactory blobLazyDownloaderFactory) {\n+        this.avroReaderFactory = avroReaderFactory;\n+        this.blobLazyDownloaderFactory = blobLazyDownloaderFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of a Chunk.\n+     *\n+     * @param client The changefeed container client.\n+     * @param chunkPath The path to the chunk blob.\n+     * @param shardCursor The parent shard cursor.\n+     * @param blockOffset The offset of the block to start reading from. If 0, this indicates we should read the whole\n+     *                    avro file from the beginning.\n+     * @param objectBlockIndex The index of the last object in the block that was returned to the user.\n+     * @return {@link Chunk}\n+     */\n+    Chunk getChunk(BlobContainerAsyncClient client, String chunkPath, ChangefeedCursor shardCursor,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTY1NQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEzMzMwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo1NjoxMlrOGWKoDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMTozMDoyN1rOGXzW0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTk0OQ==", "bodyText": "isn't changefeed format locked by us?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425895949", "createdAt": "2020-05-15T15:56:12Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMTg1Ng==", "bodyText": "By that comment I was referring to the chunk size (1MB). I placed it poorly. The performance of the downloads can vary so in case a customer is like - this is super slow, we can allow them to increase that threshold or decrease to improve perf depending on their situation. I think it would be a nice to have kind of thing", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427611856", "createdAt": "2020-05-19T21:30:27Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTk0OQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjEzODQzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo1NzozN1rOGWKrWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTozMDoyN1rOGcHcmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Njc5Mw==", "bodyText": "Do we need parameter-less constructors? I can imagine we could create all these factories in some top level class and use pure DI from there.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425896793", "createdAt": "2020-05-15T15:57:37Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5OTg2MA==", "bodyText": "I think this makes it easier to reason about, don't you think?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431399860", "createdAt": "2020-05-27T19:42:59Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Njc5Mw=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNTMyMw==", "bodyText": "fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432135323", "createdAt": "2020-05-28T21:30:27Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Njc5Mw=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE1NDg0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxTest.groovy", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowMjozM1rOGWK2Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTozMDo1MlrOGcHdSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTYxNQ==", "bodyText": "What is HelperSpec ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425899615", "createdAt": "2020-05-15T16:02:33Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxTest.groovy", "diffHunk": "@@ -0,0 +1,244 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+\n+import java.time.OffsetDateTime\n+\n+import static org.mockito.ArgumentMatchers.any\n+import static org.mockito.Mockito.*\n+\n+class BlobChangefeedPagedFluxTest extends HelperSpec {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMzk5OQ==", "bodyText": "related to comment below", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427613999", "createdAt": "2020-05-19T21:35:03Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxTest.groovy", "diffHunk": "@@ -0,0 +1,244 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+\n+import java.time.OffsetDateTime\n+\n+import static org.mockito.ArgumentMatchers.any\n+import static org.mockito.Mockito.*\n+\n+class BlobChangefeedPagedFluxTest extends HelperSpec {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTYxNQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNTQ5Nw==", "bodyText": "changed name and removed inheritance", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432135497", "createdAt": "2020-05-28T21:30:52Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxTest.groovy", "diffHunk": "@@ -0,0 +1,244 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+\n+import java.time.OffsetDateTime\n+\n+import static org.mockito.ArgumentMatchers.any\n+import static org.mockito.Mockito.*\n+\n+class BlobChangefeedPagedFluxTest extends HelperSpec {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTYxNQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE2NDcwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/HelperSpec.groovy", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowNTozMVrOGWK8lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToyODoyNlrOGXzTIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTIwNw==", "bodyText": "Should this be MockedChangeFeedSpec or something more concrete?\nhttps://softwareengineering.stackexchange.com/questions/247267/what-is-a-helper-is-it-a-design-pattern-is-it-an-algorithm", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425901207", "createdAt": "2020-05-15T16:05:31Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/HelperSpec.groovy", "diffHunk": "@@ -0,0 +1,68 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.core.util.FluxUtil\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventType\n+import com.azure.storage.blob.models.BlobType\n+import reactor.core.publisher.Flux\n+import spock.lang.Specification\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.AsynchronousFileChannel\n+import java.nio.file.Path\n+import java.nio.file.Paths\n+import java.nio.file.StandardOpenOption\n+import java.time.OffsetDateTime\n+\n+class HelperSpec extends Specification {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDkxMw==", "bodyText": "I think I may be able to get rid of this class entirely. I'll look into that as I re-review the tests", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427610913", "createdAt": "2020-05-19T21:28:26Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/HelperSpec.groovy", "diffHunk": "@@ -0,0 +1,68 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.core.util.FluxUtil\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventType\n+import com.azure.storage.blob.models.BlobType\n+import reactor.core.publisher.Flux\n+import spock.lang.Specification\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.AsynchronousFileChannel\n+import java.nio.file.Path\n+import java.nio.file.Paths\n+import java.nio.file.StandardOpenOption\n+import java.time.OffsetDateTime\n+\n+class HelperSpec extends Specification {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTIwNw=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE2NjgyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowNjowN1rOGWK94g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMToyMTo0NFrOGWUNqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTUzOA==", "bodyText": "what does it do?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425901538", "createdAt": "2020-05-15T16:06:07Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker", "diffHunk": "@@ -0,0 +1 @@\n+mock-maker-inline", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1MzAzMg==", "bodyText": "I looked up how to make Mockito work with Groovy and this is what came of it.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r426053032", "createdAt": "2020-05-15T21:21:44Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker", "diffHunk": "@@ -0,0 +1 @@\n+mock-maker-inline", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTUzOA=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjE2OTI4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/implementation/util/ChunkedDownloadUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowNjo1NlrOGWK_dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToyNzoyM1rOGXzRSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTk0Mg==", "bodyText": "Should this be called downloadFirstChunk?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425901942", "createdAt": "2020-05-15T16:06:56Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/implementation/util/ChunkedDownloadUtils.java", "diffHunk": "@@ -0,0 +1,116 @@\n+package com.azure.storage.blob.implementation.util;\n+\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.util.function.Tuple3;\n+\n+import java.util.function.Function;\n+\n+import static java.lang.StrictMath.toIntExact;\n+\n+/**\n+ * This class provides helper methods for lazy/chunked download.\n+ *\n+ * RESERVED FOR INTERNAL USE.\n+ */\n+public class ChunkedDownloadUtils {\n+\n+    /*\n+    Download the first chunk. Construct a Mono which will emit the total count for calculating the number of chunks,\n+    access conditions containing the etag to lock on, and the response from downloading the first chunk.\n+     */\n+    public static Mono<Tuple3<Long, BlobRequestConditions, BlobDownloadAsyncResponse>> getSetupMono(BlobRange range,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDQ0Mw==", "bodyText": "Yeah I can rename - I like that name.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r427610443", "createdAt": "2020-05-19T21:27:23Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/implementation/util/ChunkedDownloadUtils.java", "diffHunk": "@@ -0,0 +1,116 @@\n+package com.azure.storage.blob.implementation.util;\n+\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.util.function.Tuple3;\n+\n+import java.util.function.Function;\n+\n+import static java.lang.StrictMath.toIntExact;\n+\n+/**\n+ * This class provides helper methods for lazy/chunked download.\n+ *\n+ * RESERVED FOR INTERNAL USE.\n+ */\n+public class ChunkedDownloadUtils {\n+\n+    /*\n+    Download the first chunk. Construct a Mono which will emit the total count for calculating the number of chunks,\n+    access conditions containing the etag to lock on, and the response from downloading the first chunk.\n+     */\n+    public static Mono<Tuple3<Long, BlobRequestConditions, BlobDownloadAsyncResponse>> getSetupMono(BlobRange range,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTk0Mg=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDUyNTcxOnYy", "diffSide": "RIGHT", "path": "sdk/parents/azure-client-sdk-parent/pom.xml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjozODoxNVrOGb9mBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzoxMDowNlrOGd7l_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3Mzg5NQ==", "bodyText": "If the package is changed from internal.avro to implementation.avro this won't be needed as implementation is always excluded from Javadocs.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431973895", "createdAt": "2020-05-28T16:38:15Z", "author": {"login": "alzimmermsft"}, "path": "sdk/parents/azure-client-sdk-parent/pom.xml", "diffHunk": "@@ -534,6 +534,7 @@\n               com.azure.core.test*:\n               com.azure.endtoend*:\n               com.azure.perf*\n+              com.azure.storage.internal.avro*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA2Mzg1MQ==", "bodyText": "I think all the code is in com.azure.storage.internal.avro.implementation\nShould that be safe as well?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432063851", "createdAt": "2020-05-28T19:17:44Z", "author": {"login": "gapra-msft"}, "path": "sdk/parents/azure-client-sdk-parent/pom.xml", "diffHunk": "@@ -534,6 +534,7 @@\n               com.azure.core.test*:\n               com.azure.endtoend*:\n               com.azure.perf*\n+              com.azure.storage.internal.avro*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3Mzg5NQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAzODI3MQ==", "bodyText": "Yup, that should be safe.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434038271", "createdAt": "2020-06-02T17:10:06Z", "author": {"login": "alzimmermsft"}, "path": "sdk/parents/azure-client-sdk-parent/pom.xml", "diffHunk": "@@ -534,6 +534,7 @@\n               com.azure.core.test*:\n               com.azure.endtoend*:\n               com.azure.perf*\n+              com.azure.storage.internal.avro*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3Mzg5NQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDUzNDgxOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedClientBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjo0MDozNFrOGb9r0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOToxODo1OFrOGcDHyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NTM3Ng==", "bodyText": "Would version ever be null? If it is ever null that is likely an issue where the default isn't being set elsewhere.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431975376", "createdAt": "2020-05-28T16:40:34Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedClientBuilder.java", "diffHunk": "@@ -0,0 +1,73 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClientBuilder;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceVersion;\n+\n+/**\n+ * This class provides a fluent builder API to help aid the configuration and instantiation of\n+ * {@link BlobChangefeedClient BlobChangefeedClients} and {@link BlobChangefeedAsyncClient BlobChangefeedAsyncClients}\n+ * when {@link #buildClient() buildClient} and {@link #buildAsyncClient() buildAsyncClient} are called respectively.\n+ */\n+@ServiceClientBuilder(serviceClients = {BlobChangefeedClient.class, BlobChangefeedAsyncClient.class})\n+public final class BlobChangefeedClientBuilder {\n+\n+    private final String accountUrl;\n+    private final HttpPipeline pipeline;\n+    private final BlobServiceVersion version;\n+\n+    /**\n+     * Constructs the {@link BlobChangefeedClientBuilder} from the URL and pipeline of the {@link BlobServiceClient}.\n+     *\n+     * @param client {@link BlobServiceClient} whose properties are used to configure the builder.\n+     */\n+    public BlobChangefeedClientBuilder(BlobServiceClient client) {\n+        this.accountUrl = client.getAccountUrl();\n+        this.pipeline = client.getHttpPipeline();\n+        this.version = client.getServiceVersion();\n+    }\n+\n+    /**\n+     * Constructs the {@link BlobChangefeedClientBuilder} from from the URL and pipeline of the\n+     * {@link BlobServiceAsyncClient}.\n+     *\n+     * @param client {@link BlobServiceClient} whose properties are used to configure the builder.\n+     */\n+    public BlobChangefeedClientBuilder(BlobServiceAsyncClient client) {\n+        this.accountUrl = client.getAccountUrl();\n+        this.pipeline = client.getHttpPipeline();\n+        this.version = client.getServiceVersion();\n+    }\n+\n+    /**\n+     * Creates a {@link BlobChangefeedClient}.\n+     *\n+     * <p><strong>Code sample</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedClientBuilder#buildClient}\n+     *\n+     * @return a {@link BlobChangefeedClient} created from the configurations in this builder.\n+     */\n+    public BlobChangefeedClient buildClient() {\n+        return new BlobChangefeedClient(buildAsyncClient());\n+    }\n+\n+    /**\n+     * Creates a {@link BlobChangefeedAsyncClient}.\n+     *\n+     * <p><strong>Code sample</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedClientBuilder#buildAsyncClient}\n+     *\n+     * @return a {@link BlobChangefeedAsyncClient} created from the configurations in this builder.\n+     */\n+    public BlobChangefeedAsyncClient buildAsyncClient() {\n+        BlobServiceVersion serviceVersion = version != null ? version : BlobServiceVersion.getLatest();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA2NDQ1Ng==", "bodyText": "Yeah you're right. This should be correctly populated by the Service Client", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432064456", "createdAt": "2020-05-28T19:18:58Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedClientBuilder.java", "diffHunk": "@@ -0,0 +1,73 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClientBuilder;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceVersion;\n+\n+/**\n+ * This class provides a fluent builder API to help aid the configuration and instantiation of\n+ * {@link BlobChangefeedClient BlobChangefeedClients} and {@link BlobChangefeedAsyncClient BlobChangefeedAsyncClients}\n+ * when {@link #buildClient() buildClient} and {@link #buildAsyncClient() buildAsyncClient} are called respectively.\n+ */\n+@ServiceClientBuilder(serviceClients = {BlobChangefeedClient.class, BlobChangefeedAsyncClient.class})\n+public final class BlobChangefeedClientBuilder {\n+\n+    private final String accountUrl;\n+    private final HttpPipeline pipeline;\n+    private final BlobServiceVersion version;\n+\n+    /**\n+     * Constructs the {@link BlobChangefeedClientBuilder} from the URL and pipeline of the {@link BlobServiceClient}.\n+     *\n+     * @param client {@link BlobServiceClient} whose properties are used to configure the builder.\n+     */\n+    public BlobChangefeedClientBuilder(BlobServiceClient client) {\n+        this.accountUrl = client.getAccountUrl();\n+        this.pipeline = client.getHttpPipeline();\n+        this.version = client.getServiceVersion();\n+    }\n+\n+    /**\n+     * Constructs the {@link BlobChangefeedClientBuilder} from from the URL and pipeline of the\n+     * {@link BlobServiceAsyncClient}.\n+     *\n+     * @param client {@link BlobServiceClient} whose properties are used to configure the builder.\n+     */\n+    public BlobChangefeedClientBuilder(BlobServiceAsyncClient client) {\n+        this.accountUrl = client.getAccountUrl();\n+        this.pipeline = client.getHttpPipeline();\n+        this.version = client.getServiceVersion();\n+    }\n+\n+    /**\n+     * Creates a {@link BlobChangefeedClient}.\n+     *\n+     * <p><strong>Code sample</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedClientBuilder#buildClient}\n+     *\n+     * @return a {@link BlobChangefeedClient} created from the configurations in this builder.\n+     */\n+    public BlobChangefeedClient buildClient() {\n+        return new BlobChangefeedClient(buildAsyncClient());\n+    }\n+\n+    /**\n+     * Creates a {@link BlobChangefeedAsyncClient}.\n+     *\n+     * <p><strong>Code sample</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedClientBuilder#buildAsyncClient}\n+     *\n+     * @return a {@link BlobChangefeedAsyncClient} created from the configurations in this builder.\n+     */\n+    public BlobChangefeedAsyncClient buildAsyncClient() {\n+        BlobServiceVersion serviceVersion = version != null ? version : BlobServiceVersion.getLatest();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NTM3Ng=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDU0MTQxOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjo0MjoyOFrOGb9wOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1ODo0N1rOGcEl9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NjUwNA==", "bodyText": "Do you want to extend ContinuablePagedFlux or ContinuablePagedFluxCore? The latter contains some default implementations.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431976504", "createdAt": "2020-05-28T16:42:28Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4ODU2Nw==", "bodyText": "I didnt want to extend PagedFluxCore since it had some continuation state behavior that doesnt mix well with Changefeed.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432088567", "createdAt": "2020-05-28T19:58:47Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NjUwNA=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDU0Nzk4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjo0NDoyN1rOGb90qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjo0NDoyN1rOGb90qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NzY0Mg==", "bodyText": "Could use FluxUtil.fluxError(ClientLogger, RuntimeException) to log this error.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431977642", "createdAt": "2020-05-28T16:44:27Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDU2MDQ4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjo0ODowN1rOGb986Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTozNDo0MlrOGcHkUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3OTc1Mw==", "bodyText": "Do we want to do a min here? If preferredPageSize was actually passed shouldn't that take precedence over DEFAULT_PAGE_SIZE.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431979753", "createdAt": "2020-05-28T16:48:07Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzNzI5OQ==", "bodyText": "See this discussion #10839 (comment)\nI wanted to make the behavior of this the same as what customers get with service calls. If this seems wrong and we have a good reason why I can change it", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432137299", "createdAt": "2020-05-28T21:34:42Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3OTc1Mw=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDYwNjcxOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzowMTowNFrOGb-bOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTozODo1NVrOGcHr0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4NzUxNQ==", "bodyText": "Given these values are ignored, could we use then and thenMany?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431987515", "createdAt": "2020-05-28T17:01:04Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEzOTIxNw==", "bodyText": "Originally they werent playing well with my error logic but it seems to work now.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432139217", "createdAt": "2020-05-28T21:38:55Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4NzUxNQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDYxNDMyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzowMjozOFrOGb-gJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxNjozMzoyMFrOGdRnvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4ODc3NQ==", "bodyText": "Any chance this class could be used in multiple threads at once? Setting these class level variables could result in timing issues.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431988775", "createdAt": "2020-05-28T17:02:38Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return Mono.error(new RuntimeException(\"Changefeed has not been enabled for this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();\n+                    JsonNode jsonNode = objectMapper.readTree(json);\n+                    this.lastConsumable = OffsetDateTime.parse(jsonNode.get(\"lastConsumable\").asText());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY1NjYyNA==", "bodyText": "Changfeed wasnt designed to be threadsafe just cause it's so dependent on all these states.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432656624", "createdAt": "2020-05-29T18:15:41Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return Mono.error(new RuntimeException(\"Changefeed has not been enabled for this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();\n+                    JsonNode jsonNode = objectMapper.readTree(json);\n+                    this.lastConsumable = OffsetDateTime.parse(jsonNode.get(\"lastConsumable\").asText());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4ODc3NQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM1MDU4OQ==", "bodyText": "Now that I'm looking at it again though - I'm gonna check to see if this is the only one that is stored like that and used later - cause I think the rest of the classes arent state dependent. I should be able to just pass lastConsumable down and make this part thread safe", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r433350589", "createdAt": "2020-06-01T16:33:20Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return Mono.error(new RuntimeException(\"Changefeed has not been enabled for this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();\n+                    JsonNode jsonNode = objectMapper.readTree(json);\n+                    this.lastConsumable = OffsetDateTime.parse(jsonNode.get(\"lastConsumable\").asText());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4ODc3NQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDYxOTcwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzowMzozOFrOGb-jiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzowMzozOFrOGb-jiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4OTY0MQ==", "bodyText": "* <p>", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431989641", "createdAt": "2020-05-28T17:03:38Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDY4ODQ0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzoyMzoyM1rOGb_Qbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMDowMTo1N1rOGdd0Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAwMTEzNQ==", "bodyText": "Let me know if I understand this incorrectly.\nSo the structure of these paths would be similar to the following\n\"$blobchangefeed/log/00/2019/02/22/1810/\",\n\"$blobchangefeed/log/01/2019/02/22/1810/\"\n\nThis is checking if we hit a chunk with a matching cursor to what a customer passed such as 2019/02.\nCould this logic be simplified, with a small overhead in perf, to just check chunkPath.startsWith(userCursor.getChunkPath()) and that be the only logic needed to be checked.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432001135", "createdAt": "2020-05-28T17:23:23Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events. */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(client, chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */\n+            return chunks.filter(chunkPath -> {\n+                if (pass.get()) {\n+                    return true;\n+                } else {\n+                    /* If we hit the chunk specified in the user cursor, set pass to true and pass this chunk\n+                       and any subsequent chunks through. */\n+                    if (userCursor.getChunkPath().equals(chunkPath)) {\n+                        pass.set(true); /* This allows us to pass subsequent chunks through.*/\n+                        return true; /* This allows us to pass this chunk through. */\n+                    } else {\n+                        return false;\n+                    }\n+                }\n+            });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MDI3NA==", "bodyText": "So shards will be of the form $blobchangefeed/log/00/2019/02/22/1810/, $blobchangefeed/log/01/2019/02/22/1810/\nAnd the chunks under them will be of the form\n$blobchangefeed/log/00/2019/02/22/1810/0000.avro,\n$blobchangefeed/log/00/2019/02/22/1810/0001.avro,  $blobchangefeed/log/00/2019/02/22/1810/0002.avro, etc\nThe user cursor will maybe have stored something that looks like chunkPath = $blobchangefeed/log/00/2019/02/22/1810/0001.avro.\nand ideally the filter will return $blobchangefeed/log/00/2019/02/22/1810/0001.avro, and $blobchangefeed/log/00/2019/02/22/1810/0002.avro\nSo I dont think startsWith would be too applicable here.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r433550274", "createdAt": "2020-06-02T00:01:41Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events. */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(client, chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */\n+            return chunks.filter(chunkPath -> {\n+                if (pass.get()) {\n+                    return true;\n+                } else {\n+                    /* If we hit the chunk specified in the user cursor, set pass to true and pass this chunk\n+                       and any subsequent chunks through. */\n+                    if (userCursor.getChunkPath().equals(chunkPath)) {\n+                        pass.set(true); /* This allows us to pass subsequent chunks through.*/\n+                        return true; /* This allows us to pass this chunk through. */\n+                    } else {\n+                        return false;\n+                    }\n+                }\n+            });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAwMTEzNQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MDMzOA==", "bodyText": "I'll add a comment with the structure of the paths", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r433550338", "createdAt": "2020-06-02T00:01:57Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events. */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(client, chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */\n+            return chunks.filter(chunkPath -> {\n+                if (pass.get()) {\n+                    return true;\n+                } else {\n+                    /* If we hit the chunk specified in the user cursor, set pass to true and pass this chunk\n+                       and any subsequent chunks through. */\n+                    if (userCursor.getChunkPath().equals(chunkPath)) {\n+                        pass.set(true); /* This allows us to pass subsequent chunks through.*/\n+                        return true; /* This allows us to pass this chunk through. */\n+                    } else {\n+                        return false;\n+                    }\n+                }\n+            });", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAwMTEzNQ=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDc2MTIyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/DownloadUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzo0NDoyOVrOGb__VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzo0NDoyOVrOGb__VQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAxMzE0MQ==", "bodyText": "Would watch out on the usage of ByteBuffer.array as this will throw if the ByteBuffer is a DirectByteBuffer. May want to add in using FluxUtil.byteBufferToArray.\nsb.append(new String(FluxUtil.byteBufferToArray(buffer), StandardCharsets.UTF_8));", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432013141", "createdAt": "2020-05-28T17:44:29Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/DownloadUtils.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.util;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.charset.StandardCharsets;\n+\n+public class DownloadUtils {\n+\n+    /**\n+     * Reduces a Flux of ByteBuffer into a Mono of String\n+     */\n+    public static Mono<String> downloadToString(BlobContainerAsyncClient client, String blobPath) {\n+        return client.getBlobAsyncClient(blobPath)\n+            .download()\n+            .reduce(new StringBuilder(), (sb, buffer) -> {\n+                sb.append(new String(buffer.array(), StandardCharsets.UTF_8));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MDc3NTgxOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/TimeUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzo0ODo1MlrOGcAI2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMToxNjo1OVrOGcHCqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAxNTU3OA==", "bodyText": "Given a segment path idx/segments/2020/05/28/1047/meta.json why do we always set minute to 0? Instead should we set it to the following\nsplitPath.length < 6 ? 0 : Integer.parseInt(splitPath[5]) % 100 /* minute */", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432015578", "createdAt": "2020-05-28T17:48:52Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/TimeUtils.java", "diffHunk": "@@ -0,0 +1,122 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.util;\n+\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+\n+public class TimeUtils {\n+\n+    /**\n+     * Converts a path to an OffsetDateTime.\n+     * <p>For example,\n+     * <p>Segment path : idx/segments/1601/01/01/0000/meta.json\n+     * <p>OffsetDateTime : year - 1601, month - 01, day - 01, hour - 00, minute - 00\n+     * <p>OR\n+     * <p>Year path : idx/segments/1601/\n+     * <p>OffsetDateTime : year - 1601, month - 00, day - 00, hour - 00, minute - 00\n+     *\n+     * @param path The path to convert.\n+     * @return The time associated with the path.\n+     */\n+    public static OffsetDateTime convertPathToTime(String path) {\n+        if (path == null) {\n+            return null;\n+        }\n+        String[] splitPath = path.split(\"/\");\n+\n+        return OffsetDateTime.of(\n+            Integer.parseInt(splitPath[2]), /* year */\n+            splitPath.length < 4 ? 1 : Integer.parseInt(splitPath[3]), /* month */\n+            splitPath.length < 5 ? 1 : Integer.parseInt(splitPath[4]), /* day */\n+            splitPath.length < 6 ? 0 : Integer.parseInt(splitPath[5]) / 100, /* hour */\n+            0, /* minute */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEyODY4MQ==", "bodyText": "In changefeed a segment is a representation of an hour, so I just set min and sec to 0, 0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432128681", "createdAt": "2020-05-28T21:16:59Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/TimeUtils.java", "diffHunk": "@@ -0,0 +1,122 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.util;\n+\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+\n+public class TimeUtils {\n+\n+    /**\n+     * Converts a path to an OffsetDateTime.\n+     * <p>For example,\n+     * <p>Segment path : idx/segments/1601/01/01/0000/meta.json\n+     * <p>OffsetDateTime : year - 1601, month - 01, day - 01, hour - 00, minute - 00\n+     * <p>OR\n+     * <p>Year path : idx/segments/1601/\n+     * <p>OffsetDateTime : year - 1601, month - 00, day - 00, hour - 00, minute - 00\n+     *\n+     * @param path The path to convert.\n+     * @return The time associated with the path.\n+     */\n+    public static OffsetDateTime convertPathToTime(String path) {\n+        if (path == null) {\n+            return null;\n+        }\n+        String[] splitPath = path.split(\"/\");\n+\n+        return OffsetDateTime.of(\n+            Integer.parseInt(splitPath[2]), /* year */\n+            splitPath.length < 4 ? 1 : Integer.parseInt(splitPath[3]), /* month */\n+            splitPath.length < 5 ? 1 : Integer.parseInt(splitPath[4]), /* day */\n+            splitPath.length < 6 ? 0 : Integer.parseInt(splitPath[5]) / 100, /* hour */\n+            0, /* minute */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAxNTU3OA=="}, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTE4MTkyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MDo0MlrOGcENOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxOTowMVrOGcIsCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MjIzMg==", "bodyText": "Why specify the scheduler here?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432082232", "createdAt": "2020-05-28T19:50:42Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4NjQwMQ==", "bodyText": "It looks like we specified the scheduler in the original download to file code that you refactored. Weird. I don't recall why we would've done that.\nIn that case, why use immediate here instead of elastic?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432086401", "createdAt": "2020-05-28T19:56:06Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MjIzMg=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NTY1OQ==", "bodyText": "Changing to elastic and I'll remove the Schedulers method param", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432155659", "createdAt": "2020-05-28T22:19:01Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MjIzMg=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTMxOTIyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDoyODoxN1rOGcFkSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoyMDowNVrOGcIthg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNDUyMQ==", "bodyText": "You might be able to factor out the lambda into your ChunkedDownloadUtils class and then here you concatMap() and in downloadToFile we flatMap()?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432104521", "createdAt": "2020-05-28T20:28:17Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;\n+\n+                BlobDownloadAsyncResponse initialResponse = setupTuple3.getT3();\n+                return Flux.range(0, numChunks)\n+                    .concatMap(chunkNum -> { /* TODO (gapra) : This was the biggest difference - downloadToFile does", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NjAzOA==", "bodyText": "Yeah I think that's the best bet", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432156038", "createdAt": "2020-05-28T22:20:05Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;\n+\n+                BlobDownloadAsyncResponse initialResponse = setupTuple3.getT3();\n+                return Flux.range(0, numChunks)\n+                    .concatMap(chunkNum -> { /* TODO (gapra) : This was the biggest difference - downloadToFile does", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNDUyMQ=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTY1MjA2OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoyNDo0NlrOGcI0GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoyNDo0NlrOGcI0GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NzcyMA==", "bodyText": "This type doesn't seem like it's actually lazy to me. While it is sequential, it will still continue to issue download requests as long as the subscription to the data is not cancelled. I haven't seen yet where this is being used, but it will probably need to apply back pressure in order to actually slow down requests and make it lazy.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432157720", "createdAt": "2020-05-28T22:24:46Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMzY4NDY3OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzoyMTo0NlrOGd8AFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMjowMDozNlrOGevGRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA0NDk0OA==", "bodyText": "Would invert this check FINALIZED.equals(status.asText()). It'll be safer as its known to never NPE.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434044948", "createdAt": "2020-06-02T17:21:46Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "diffHunk": "@@ -0,0 +1,120 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Represents a Segment in Changefeed.\n+ *\n+ * A segment is a blob that points to a manifest file.\n+ * The segment manifest file (meta.json) shows the path of the change feed files for that segment in the\n+ * chunkFilePaths property. (Note: these chunkFilePaths are really shardPaths in this implementation.)\n+ * The chunkFilePaths property looks something like this.\n+ * { ...\n+ * \"chunkFilePaths\": [\n+ *         \"$blobchangefeed/log/00/2019/02/22/1810/\",\n+ *         \"$blobchangefeed/log/01/2019/02/22/1810/\"\n+ *     ],\n+ * ...}\n+ */\n+class Segment {\n+\n+    private final ClientLogger logger = new ClientLogger(Segment.class);\n+\n+    private static final String CHUNK_FILE_PATHS = \"chunkFilePaths\";\n+    private static final String STATUS = \"status\";\n+    private static final String FINALIZED = \"Finalized\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String segmentPath; /* Segment manifest location. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with parent changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ShardFactory shardFactory;\n+\n+    /**\n+     * Creates a new Segment.\n+     */\n+    Segment(BlobContainerAsyncClient client, String segmentPath, ChangefeedCursor cfCursor,\n+        ChangefeedCursor userCursor, ShardFactory shardFactory) {\n+        this.client = client;\n+        this.segmentPath = segmentPath;\n+        this.cfCursor = cfCursor;\n+        this.userCursor = userCursor;\n+        this.shardFactory = shardFactory;\n+    }\n+\n+    /**\n+     * Get all the events for the Segment.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* Download JSON manifest file. */\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(client, segmentPath)\n+            .flatMap(this::parseJson)\n+            /* Parse the JSON for shards. */\n+            .flatMapMany(this::getShards)\n+            /* Get all events for each shard. */\n+            .concatMap(Shard::getEvents);\n+    }\n+\n+    private Mono<JsonNode> parseJson(String json) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+        try {\n+            JsonNode jsonNode = objectMapper.readTree(json);\n+            return Mono.just(jsonNode);\n+        } catch (IOException e) {\n+            return FluxUtil.monoError(logger, new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private Flux<Shard> getShards(JsonNode node) {\n+\n+        /* Determine if the segment is finalized.\n+           If the segment is not finalized, do not return events for this segment. */\n+        JsonNode status = node.get(STATUS);\n+        if (!status.asText().equals(FINALIZED)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg4MjExOQ==", "bodyText": "thanks for catching that!", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434882119", "createdAt": "2020-06-03T22:00:36Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "diffHunk": "@@ -0,0 +1,120 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Represents a Segment in Changefeed.\n+ *\n+ * A segment is a blob that points to a manifest file.\n+ * The segment manifest file (meta.json) shows the path of the change feed files for that segment in the\n+ * chunkFilePaths property. (Note: these chunkFilePaths are really shardPaths in this implementation.)\n+ * The chunkFilePaths property looks something like this.\n+ * { ...\n+ * \"chunkFilePaths\": [\n+ *         \"$blobchangefeed/log/00/2019/02/22/1810/\",\n+ *         \"$blobchangefeed/log/01/2019/02/22/1810/\"\n+ *     ],\n+ * ...}\n+ */\n+class Segment {\n+\n+    private final ClientLogger logger = new ClientLogger(Segment.class);\n+\n+    private static final String CHUNK_FILE_PATHS = \"chunkFilePaths\";\n+    private static final String STATUS = \"status\";\n+    private static final String FINALIZED = \"Finalized\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String segmentPath; /* Segment manifest location. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with parent changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ShardFactory shardFactory;\n+\n+    /**\n+     * Creates a new Segment.\n+     */\n+    Segment(BlobContainerAsyncClient client, String segmentPath, ChangefeedCursor cfCursor,\n+        ChangefeedCursor userCursor, ShardFactory shardFactory) {\n+        this.client = client;\n+        this.segmentPath = segmentPath;\n+        this.cfCursor = cfCursor;\n+        this.userCursor = userCursor;\n+        this.shardFactory = shardFactory;\n+    }\n+\n+    /**\n+     * Get all the events for the Segment.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* Download JSON manifest file. */\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(client, segmentPath)\n+            .flatMap(this::parseJson)\n+            /* Parse the JSON for shards. */\n+            .flatMapMany(this::getShards)\n+            /* Get all events for each shard. */\n+            .concatMap(Shard::getEvents);\n+    }\n+\n+    private Mono<JsonNode> parseJson(String json) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+        try {\n+            JsonNode jsonNode = objectMapper.readTree(json);\n+            return Mono.just(jsonNode);\n+        } catch (IOException e) {\n+            return FluxUtil.monoError(logger, new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private Flux<Shard> getShards(JsonNode node) {\n+\n+        /* Determine if the segment is finalized.\n+           If the segment is not finalized, do not return events for this segment. */\n+        JsonNode status = node.get(STATUS);\n+        if (!status.asText().equals(FINALIZED)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA0NDk0OA=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDI3MzcyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxOTo1NToyMVrOGeBy7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNjozOToyMVrOGfOidA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzOTg4NA==", "bodyText": "Current pattern guildelines are putting PagedFlux and PagedIterable implementations into util.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434139884", "createdAt": "2020-06-02T19:55:21Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM0MDU4OA==", "bodyText": "@alzimmermsft That's quite stretching definition of what util is. https://www.vojtechruzicka.com/avoid-utility-classes/ .\nI'd expect util package to contain \"Utility Classes\". PagedFlux and PagedIterable and BlobChangefeedPagedFlux are quite well defined and thus packaging them could use better naming.\nIn general naming things util isn't bringing much value into understanding what they do http://ralin.io/blog/oop-anti-patterns-utility-or-helper-classes.html .", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435340588", "createdAt": "2020-06-04T15:18:56Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzOTg4NA=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM2Nzc5NA==", "bodyText": "Agreed that util may not be the best package location, just wanted to call out the guideline.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435367794", "createdAt": "2020-06-04T15:54:11Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzOTg4NA=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM5NzIzNg==", "bodyText": "In addition to that, moving these classes to util will force me to make some classes like Changefeed, Shard, Segment, etc public - which is something I'd rather not do", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435397236", "createdAt": "2020-06-04T16:39:21Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzOTg4NA=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDI5MDAzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMDowMDoyN1rOGeB9Nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzowNjozNVrOGewf_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0MjUxOQ==", "bodyText": "Do we need to download to string? We can pass the raw byte[] into ObjectMapper.readTree (API), which is actually Jackson's recommendation.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434142519", "createdAt": "2020-06-02T20:00:27Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNTA4Ng==", "bodyText": "changed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434905086", "createdAt": "2020-06-03T23:06:35Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0MjUxOQ=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDI5OTQ0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMDowMzo0NVrOGeCDZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxODo1Mzo0NlrOGfTqdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NDEwMw==", "bodyText": "We should create a static instance of ObjectMapper for this class and use .reader() to get a thread safe reader instance to deserialize the JSON into a tree. ObjectMapper has a non-trivial amount of overhead during creation and should be re-used as much as possible.\nhttps://github.com/FasterXML/jackson-docs/wiki/Presentation:-Jackson-Performance", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434144103", "createdAt": "2020-06-02T20:03:45Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4MTIwNQ==", "bodyText": "fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435481205", "createdAt": "2020-06-04T18:53:46Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NDEwMw=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDMwODA1OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMDowNjo1M1rOGeCJcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowMjoxNVrOGfT8gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NTY0OQ==", "bodyText": "Similar comments about reusing ObjectMapper and using byte[] instead of String as the value to pass into the deserializer.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434145649", "createdAt": "2020-06-02T20:06:53Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "diffHunk": "@@ -0,0 +1,120 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Represents a Segment in Changefeed.\n+ *\n+ * A segment is a blob that points to a manifest file.\n+ * The segment manifest file (meta.json) shows the path of the change feed files for that segment in the\n+ * chunkFilePaths property. (Note: these chunkFilePaths are really shardPaths in this implementation.)\n+ * The chunkFilePaths property looks something like this.\n+ * { ...\n+ * \"chunkFilePaths\": [\n+ *         \"$blobchangefeed/log/00/2019/02/22/1810/\",\n+ *         \"$blobchangefeed/log/01/2019/02/22/1810/\"\n+ *     ],\n+ * ...}\n+ */\n+class Segment {\n+\n+    private final ClientLogger logger = new ClientLogger(Segment.class);\n+\n+    private static final String CHUNK_FILE_PATHS = \"chunkFilePaths\";\n+    private static final String STATUS = \"status\";\n+    private static final String FINALIZED = \"Finalized\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String segmentPath; /* Segment manifest location. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with parent changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ShardFactory shardFactory;\n+\n+    /**\n+     * Creates a new Segment.\n+     */\n+    Segment(BlobContainerAsyncClient client, String segmentPath, ChangefeedCursor cfCursor,\n+        ChangefeedCursor userCursor, ShardFactory shardFactory) {\n+        this.client = client;\n+        this.segmentPath = segmentPath;\n+        this.cfCursor = cfCursor;\n+        this.userCursor = userCursor;\n+        this.shardFactory = shardFactory;\n+    }\n+\n+    /**\n+     * Get all the events for the Segment.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* Download JSON manifest file. */\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(client, segmentPath)\n+            .flatMap(this::parseJson)\n+            /* Parse the JSON for shards. */\n+            .flatMapMany(this::getShards)\n+            /* Get all events for each shard. */\n+            .concatMap(Shard::getEvents);\n+    }\n+\n+    private Mono<JsonNode> parseJson(String json) {\n+        ObjectMapper objectMapper = new ObjectMapper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NTgyNQ==", "bodyText": "fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435485825", "createdAt": "2020-06-04T19:02:15Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "diffHunk": "@@ -0,0 +1,120 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Represents a Segment in Changefeed.\n+ *\n+ * A segment is a blob that points to a manifest file.\n+ * The segment manifest file (meta.json) shows the path of the change feed files for that segment in the\n+ * chunkFilePaths property. (Note: these chunkFilePaths are really shardPaths in this implementation.)\n+ * The chunkFilePaths property looks something like this.\n+ * { ...\n+ * \"chunkFilePaths\": [\n+ *         \"$blobchangefeed/log/00/2019/02/22/1810/\",\n+ *         \"$blobchangefeed/log/01/2019/02/22/1810/\"\n+ *     ],\n+ * ...}\n+ */\n+class Segment {\n+\n+    private final ClientLogger logger = new ClientLogger(Segment.class);\n+\n+    private static final String CHUNK_FILE_PATHS = \"chunkFilePaths\";\n+    private static final String STATUS = \"status\";\n+    private static final String FINALIZED = \"Finalized\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String segmentPath; /* Segment manifest location. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with parent changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ShardFactory shardFactory;\n+\n+    /**\n+     * Creates a new Segment.\n+     */\n+    Segment(BlobContainerAsyncClient client, String segmentPath, ChangefeedCursor cfCursor,\n+        ChangefeedCursor userCursor, ShardFactory shardFactory) {\n+        this.client = client;\n+        this.segmentPath = segmentPath;\n+        this.cfCursor = cfCursor;\n+        this.userCursor = userCursor;\n+        this.shardFactory = shardFactory;\n+    }\n+\n+    /**\n+     * Get all the events for the Segment.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* Download JSON manifest file. */\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(client, segmentPath)\n+            .flatMap(this::parseJson)\n+            /* Parse the JSON for shards. */\n+            .flatMapMany(this::getShards)\n+            /* Get all events for each shard. */\n+            .concatMap(Shard::getEvents);\n+    }\n+\n+    private Mono<JsonNode> parseJson(String json) {\n+        ObjectMapper objectMapper = new ObjectMapper();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NTY0OQ=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDY2MDQ0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/ChangefeedCursor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMjowOTo0MVrOGeFpaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowMjoyNlrOGfT89Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwMjk4NQ==", "bodyText": "Should make this ObjectMapper instance static. For this method should use a writer(), for the deserialize case use a reader().\nDoes the mapper need any configuration to handle empty/null values?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434202985", "createdAt": "2020-06-02T22:09:41Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/ChangefeedCursor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+import java.util.Objects;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a cursor for BlobChangefeed.\n+ */\n+@Fluent\n+public class ChangefeedCursor {\n+\n+    private ClientLogger logger = new ClientLogger(ChangefeedCursor.class);\n+\n+    private String endTime;\n+    private String segmentTime;\n+    private String shardPath;\n+    private String chunkPath;\n+    private long blockOffset;\n+    private long objectBlockIndex;\n+\n+    /**\n+     * Default constructor (used to serialize and deserialize).\n+     */\n+    public ChangefeedCursor() {\n+    }\n+\n+    /**\n+     * Constructor for use by to*Cursor methods.\n+     */\n+    private ChangefeedCursor(String endTime, String segmentTime, String shardPath, String chunkPath, long blockOffset,\n+        long objectBlockIndex) {\n+        this.endTime = endTime;\n+        this.segmentTime = segmentTime;\n+        this.shardPath = shardPath;\n+        this.chunkPath = chunkPath;\n+        this.blockOffset = blockOffset;\n+        this.objectBlockIndex = objectBlockIndex;\n+    }\n+\n+    /**\n+     * Creates a new changefeed level cursor with the specified end time.\n+     *\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    public ChangefeedCursor(OffsetDateTime endTime) {\n+        this(endTime.toString(), null, null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new segment level cursor.\n+     *\n+     * @param segmentTime The {@link OffsetDateTime segment time}.\n+     * @return A new segment level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toSegmentCursor(OffsetDateTime segmentTime) {\n+        return new ChangefeedCursor(this.getEndTime(), segmentTime.toString(), null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new shard level cursor.\n+     *\n+     * @param shardPath The shard path.\n+     * @return A new shard level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toShardCursor(String shardPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), shardPath, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new chunk level cursor.\n+     *\n+     * @param chunkPath The chunk path.\n+     * @return A new chunk level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toChunkCursor(String chunkPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), chunkPath, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new event level cursor.\n+     *\n+     * @param blockOffset The block offset.\n+     * @param objectBlockIndex The object block index.\n+     * @return A new event level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toEventCursor(long blockOffset, long objectBlockIndex) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), this.getChunkPath(), blockOffset, objectBlockIndex);\n+\n+    }\n+\n+    /**\n+     * @return the end time.\n+     */\n+    public String getEndTime() {\n+        return endTime;\n+    }\n+\n+    /**\n+     * @return the segment time.\n+     */\n+    public String getSegmentTime() {\n+        return segmentTime;\n+    }\n+\n+    /**\n+     * @return the shard path.\n+     */\n+    public String getShardPath() {\n+        return shardPath;\n+    }\n+\n+    /**\n+     * @return the chunk path.\n+     */\n+    public String getChunkPath() {\n+        return chunkPath;\n+    }\n+\n+    /**\n+     * @return the block offset\n+     */\n+    public long getBlockOffset() {\n+        return blockOffset;\n+    }\n+\n+    /**\n+     * @return the object block index.\n+     */\n+    public long getObjectBlockIndex() {\n+        return objectBlockIndex;\n+    }\n+\n+    /**\n+     * @param endTime the end time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setEndTime(String endTime) {\n+        this.endTime = endTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param segmentTime the segment time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setSegmentTime(String segmentTime) {\n+        this.segmentTime = segmentTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param shardPath the shard path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setShardPath(String shardPath) {\n+        this.shardPath = shardPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param chunkPath the chunk path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setChunkPath(String chunkPath) {\n+        this.chunkPath = chunkPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param blockOffset the block offset.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setBlockOffset(long blockOffset) {\n+        this.blockOffset = blockOffset;\n+        return this;\n+    }\n+\n+    /**\n+     * @param objectBlockIndex the object block index.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setObjectBlockIndex(long objectBlockIndex) {\n+        this.objectBlockIndex = objectBlockIndex;\n+        return this;\n+    }\n+\n+    /**\n+     * Serializes a {@link ChangefeedCursor} into a String.\n+     *\n+     * @return The resulting serialized cursor.\n+     */\n+    public String serialize() {\n+        try {\n+            return new ObjectMapper().writeValueAsString(this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwODA3Mw==", "bodyText": "No we don't expect any of the values to be null ever when using ser/deser", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434908073", "createdAt": "2020-06-03T23:16:58Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/ChangefeedCursor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+import java.util.Objects;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a cursor for BlobChangefeed.\n+ */\n+@Fluent\n+public class ChangefeedCursor {\n+\n+    private ClientLogger logger = new ClientLogger(ChangefeedCursor.class);\n+\n+    private String endTime;\n+    private String segmentTime;\n+    private String shardPath;\n+    private String chunkPath;\n+    private long blockOffset;\n+    private long objectBlockIndex;\n+\n+    /**\n+     * Default constructor (used to serialize and deserialize).\n+     */\n+    public ChangefeedCursor() {\n+    }\n+\n+    /**\n+     * Constructor for use by to*Cursor methods.\n+     */\n+    private ChangefeedCursor(String endTime, String segmentTime, String shardPath, String chunkPath, long blockOffset,\n+        long objectBlockIndex) {\n+        this.endTime = endTime;\n+        this.segmentTime = segmentTime;\n+        this.shardPath = shardPath;\n+        this.chunkPath = chunkPath;\n+        this.blockOffset = blockOffset;\n+        this.objectBlockIndex = objectBlockIndex;\n+    }\n+\n+    /**\n+     * Creates a new changefeed level cursor with the specified end time.\n+     *\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    public ChangefeedCursor(OffsetDateTime endTime) {\n+        this(endTime.toString(), null, null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new segment level cursor.\n+     *\n+     * @param segmentTime The {@link OffsetDateTime segment time}.\n+     * @return A new segment level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toSegmentCursor(OffsetDateTime segmentTime) {\n+        return new ChangefeedCursor(this.getEndTime(), segmentTime.toString(), null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new shard level cursor.\n+     *\n+     * @param shardPath The shard path.\n+     * @return A new shard level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toShardCursor(String shardPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), shardPath, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new chunk level cursor.\n+     *\n+     * @param chunkPath The chunk path.\n+     * @return A new chunk level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toChunkCursor(String chunkPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), chunkPath, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new event level cursor.\n+     *\n+     * @param blockOffset The block offset.\n+     * @param objectBlockIndex The object block index.\n+     * @return A new event level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toEventCursor(long blockOffset, long objectBlockIndex) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), this.getChunkPath(), blockOffset, objectBlockIndex);\n+\n+    }\n+\n+    /**\n+     * @return the end time.\n+     */\n+    public String getEndTime() {\n+        return endTime;\n+    }\n+\n+    /**\n+     * @return the segment time.\n+     */\n+    public String getSegmentTime() {\n+        return segmentTime;\n+    }\n+\n+    /**\n+     * @return the shard path.\n+     */\n+    public String getShardPath() {\n+        return shardPath;\n+    }\n+\n+    /**\n+     * @return the chunk path.\n+     */\n+    public String getChunkPath() {\n+        return chunkPath;\n+    }\n+\n+    /**\n+     * @return the block offset\n+     */\n+    public long getBlockOffset() {\n+        return blockOffset;\n+    }\n+\n+    /**\n+     * @return the object block index.\n+     */\n+    public long getObjectBlockIndex() {\n+        return objectBlockIndex;\n+    }\n+\n+    /**\n+     * @param endTime the end time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setEndTime(String endTime) {\n+        this.endTime = endTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param segmentTime the segment time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setSegmentTime(String segmentTime) {\n+        this.segmentTime = segmentTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param shardPath the shard path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setShardPath(String shardPath) {\n+        this.shardPath = shardPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param chunkPath the chunk path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setChunkPath(String chunkPath) {\n+        this.chunkPath = chunkPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param blockOffset the block offset.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setBlockOffset(long blockOffset) {\n+        this.blockOffset = blockOffset;\n+        return this;\n+    }\n+\n+    /**\n+     * @param objectBlockIndex the object block index.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setObjectBlockIndex(long objectBlockIndex) {\n+        this.objectBlockIndex = objectBlockIndex;\n+        return this;\n+    }\n+\n+    /**\n+     * Serializes a {@link ChangefeedCursor} into a String.\n+     *\n+     * @return The resulting serialized cursor.\n+     */\n+    public String serialize() {\n+        try {\n+            return new ObjectMapper().writeValueAsString(this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwMjk4NQ=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NTk0MQ==", "bodyText": "fixed the static comment", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435485941", "createdAt": "2020-06-04T19:02:26Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/ChangefeedCursor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+import java.util.Objects;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a cursor for BlobChangefeed.\n+ */\n+@Fluent\n+public class ChangefeedCursor {\n+\n+    private ClientLogger logger = new ClientLogger(ChangefeedCursor.class);\n+\n+    private String endTime;\n+    private String segmentTime;\n+    private String shardPath;\n+    private String chunkPath;\n+    private long blockOffset;\n+    private long objectBlockIndex;\n+\n+    /**\n+     * Default constructor (used to serialize and deserialize).\n+     */\n+    public ChangefeedCursor() {\n+    }\n+\n+    /**\n+     * Constructor for use by to*Cursor methods.\n+     */\n+    private ChangefeedCursor(String endTime, String segmentTime, String shardPath, String chunkPath, long blockOffset,\n+        long objectBlockIndex) {\n+        this.endTime = endTime;\n+        this.segmentTime = segmentTime;\n+        this.shardPath = shardPath;\n+        this.chunkPath = chunkPath;\n+        this.blockOffset = blockOffset;\n+        this.objectBlockIndex = objectBlockIndex;\n+    }\n+\n+    /**\n+     * Creates a new changefeed level cursor with the specified end time.\n+     *\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    public ChangefeedCursor(OffsetDateTime endTime) {\n+        this(endTime.toString(), null, null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new segment level cursor.\n+     *\n+     * @param segmentTime The {@link OffsetDateTime segment time}.\n+     * @return A new segment level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toSegmentCursor(OffsetDateTime segmentTime) {\n+        return new ChangefeedCursor(this.getEndTime(), segmentTime.toString(), null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new shard level cursor.\n+     *\n+     * @param shardPath The shard path.\n+     * @return A new shard level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toShardCursor(String shardPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), shardPath, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new chunk level cursor.\n+     *\n+     * @param chunkPath The chunk path.\n+     * @return A new chunk level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toChunkCursor(String chunkPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), chunkPath, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new event level cursor.\n+     *\n+     * @param blockOffset The block offset.\n+     * @param objectBlockIndex The object block index.\n+     * @return A new event level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toEventCursor(long blockOffset, long objectBlockIndex) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), this.getChunkPath(), blockOffset, objectBlockIndex);\n+\n+    }\n+\n+    /**\n+     * @return the end time.\n+     */\n+    public String getEndTime() {\n+        return endTime;\n+    }\n+\n+    /**\n+     * @return the segment time.\n+     */\n+    public String getSegmentTime() {\n+        return segmentTime;\n+    }\n+\n+    /**\n+     * @return the shard path.\n+     */\n+    public String getShardPath() {\n+        return shardPath;\n+    }\n+\n+    /**\n+     * @return the chunk path.\n+     */\n+    public String getChunkPath() {\n+        return chunkPath;\n+    }\n+\n+    /**\n+     * @return the block offset\n+     */\n+    public long getBlockOffset() {\n+        return blockOffset;\n+    }\n+\n+    /**\n+     * @return the object block index.\n+     */\n+    public long getObjectBlockIndex() {\n+        return objectBlockIndex;\n+    }\n+\n+    /**\n+     * @param endTime the end time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setEndTime(String endTime) {\n+        this.endTime = endTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param segmentTime the segment time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setSegmentTime(String segmentTime) {\n+        this.segmentTime = segmentTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param shardPath the shard path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setShardPath(String shardPath) {\n+        this.shardPath = shardPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param chunkPath the chunk path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setChunkPath(String chunkPath) {\n+        this.chunkPath = chunkPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param blockOffset the block offset.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setBlockOffset(long blockOffset) {\n+        this.blockOffset = blockOffset;\n+        return this;\n+    }\n+\n+    /**\n+     * @param objectBlockIndex the object block index.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setObjectBlockIndex(long objectBlockIndex) {\n+        this.objectBlockIndex = objectBlockIndex;\n+        return this;\n+    }\n+\n+    /**\n+     * Serializes a {@link ChangefeedCursor} into a String.\n+     *\n+     * @return The resulting serialized cursor.\n+     */\n+    public String serialize() {\n+        try {\n+            return new ObjectMapper().writeValueAsString(this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwMjk4NQ=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDcwNjg5OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMjoyOTo1NFrOGeGF_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTowMjozNVrOGfT9Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxMDMwMw==", "bodyText": "Should we give a little more detail about how approximate this is?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434210303", "createdAt": "2020-06-02T22:29:54Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,111 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final BlobChangefeedPagedFluxFactory pagedFluxFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        ChangefeedFactory changefeedFactory = new ChangefeedFactory(segmentFactory, client);\n+        this.pagedFluxFactory = new BlobChangefeedPagedFluxFactory(changefeedFactory);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents}\n+     *\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents() {\n+        return getEvents(null, null);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#OffsetDateTime-OffsetDateTime}\n+     *\n+     * @param startTime Filters the results to return events approximately after the start time.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NjAxMQ==", "bodyText": "added detail", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435486011", "createdAt": "2020-06-04T19:02:35Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,111 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final BlobChangefeedPagedFluxFactory pagedFluxFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        ChangefeedFactory changefeedFactory = new ChangefeedFactory(segmentFactory, client);\n+        this.pagedFluxFactory = new BlobChangefeedPagedFluxFactory(changefeedFactory);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents}\n+     *\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents() {\n+        return getEvents(null, null);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#OffsetDateTime-OffsetDateTime}\n+     *\n+     * @param startTime Filters the results to return events approximately after the start time.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxMDMwMw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDc3MDI4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzowMDoxMVrOGeGtKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1NDowOFrOGexUQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIyMDMyOA==", "bodyText": "Formatting", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434220328", "createdAt": "2020-06-02T23:00:11Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundDownToNearestHour\"() {\n+        expect:\n+        TimeUtils.roundDownToNearestHour(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 17, 20, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundUpToNearestHour\"() {\n+        expect:\n+        TimeUtils.roundUpToNearestHour(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 1, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 17, 21, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 23, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 18, 0, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundDownToNearestYear\"() {\n+        expect:\n+        TimeUtils.roundDownToNearestYear(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"validTime\"() {\n+        expect:\n+        TimeUtils.validTimes(current, start, end) == valid\n+\n+        where:\n+        start | current | end  || valid\n+        /* Null checks. */\n+        null | null  | null || false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null | null  || false\n+        null | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null  || false\n+        null | null || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) || false\n+        /* All equal. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Increasing. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        OffsetDateTime.of(2019, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 6, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 8, 10, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Decreasing. */\n+        OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+    }\n+\n+    def \"validSegment\"() {\n+        expect:\n+        TimeUtils.validSegment(segment, start, end) == valid\n+\n+        where:\n+        start | segment | end  || valid\n+        /* Null checks. */\n+        null | null  | null || false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null | null  || false\n+        null | \"idx/segments/2019/11/02/1700/meta.json\" | null  || false\n+        null | null || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) || false\n+        /* All equal. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2019/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Increasing. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2019/01/01/0000/meta.json\" | OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        OffsetDateTime.of(2019, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC) | \"idx/segments/2019/06/01/0000/meta.json\" | OffsetDateTime.of(2019, 8, 10, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Decreasing. */\n+        OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2020/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2020/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+    }\n+\n+    def \"validYear\"() {\n+        expect:\n+        TimeUtils.validYear(year, start, end) == valid\n+\n+        where:\n+        start | year | end  || valid", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODQ2NA==", "bodyText": "fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434918464", "createdAt": "2020-06-03T23:54:08Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundDownToNearestHour\"() {\n+        expect:\n+        TimeUtils.roundDownToNearestHour(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 17, 20, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundUpToNearestHour\"() {\n+        expect:\n+        TimeUtils.roundUpToNearestHour(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 1, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 17, 21, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 23, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 18, 0, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundDownToNearestYear\"() {\n+        expect:\n+        TimeUtils.roundDownToNearestYear(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"validTime\"() {\n+        expect:\n+        TimeUtils.validTimes(current, start, end) == valid\n+\n+        where:\n+        start | current | end  || valid\n+        /* Null checks. */\n+        null | null  | null || false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null | null  || false\n+        null | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null  || false\n+        null | null || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) || false\n+        /* All equal. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Increasing. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        OffsetDateTime.of(2019, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 6, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 8, 10, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Decreasing. */\n+        OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+    }\n+\n+    def \"validSegment\"() {\n+        expect:\n+        TimeUtils.validSegment(segment, start, end) == valid\n+\n+        where:\n+        start | segment | end  || valid\n+        /* Null checks. */\n+        null | null  | null || false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null | null  || false\n+        null | \"idx/segments/2019/11/02/1700/meta.json\" | null  || false\n+        null | null || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) || false\n+        /* All equal. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2019/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Increasing. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2019/01/01/0000/meta.json\" | OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        OffsetDateTime.of(2019, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC) | \"idx/segments/2019/06/01/0000/meta.json\" | OffsetDateTime.of(2019, 8, 10, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Decreasing. */\n+        OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2020/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2020/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+    }\n+\n+    def \"validYear\"() {\n+        expect:\n+        TimeUtils.validYear(year, start, end) == valid\n+\n+        where:\n+        start | year | end  || valid", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIyMDMyOA=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDg0MzExOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/BlobChangefeedEventWrapper.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzozNzo1OVrOGeHaKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1NToyM1rOGexVqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzMTg1MA==", "bodyText": "Unfished", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434231850", "createdAt": "2020-06-02T23:37:59Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/BlobChangefeedEventWrapper.java", "diffHunk": "@@ -0,0 +1,41 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a wrapper to store a BlobChangefeedEvent along with the BlobChangefeedCursor associated with it.\n+ * This wrapper is required since the paging functionality does not have any information about where the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODgyNA==", "bodyText": "fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434918824", "createdAt": "2020-06-03T23:55:23Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/BlobChangefeedEventWrapper.java", "diffHunk": "@@ -0,0 +1,41 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a wrapper to store a BlobChangefeedEvent along with the BlobChangefeedCursor associated with it.\n+ * This wrapper is required since the paging functionality does not have any information about where the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzMTg1MA=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDg1ODEwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzo0NjozOFrOGeHjeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNjozMzozM1rOGfOUZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNDIzMw==", "bodyText": "Is there any need to test a path like idx/segments/2019/11/02/1730/meta.json at all?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434234233", "createdAt": "2020-06-02T23:46:38Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMDUxNQ==", "bodyText": "No real need but I was just including all possible paths in Changefeed Segments", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434910515", "createdAt": "2020-06-03T23:25:13Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNDIzMw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMjM3NA==", "bodyText": "Sorry I think the intention of my question was unclear. I was highlighting 1730 as opposed to 1700. Basically something that doesn't align along an hour.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434922374", "createdAt": "2020-06-04T00:08:50Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNDIzMw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM5MzM3Mg==", "bodyText": "Oh I missed that. Yeah I can add one like that and test that it evaluates to 1700", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435393372", "createdAt": "2020-06-04T16:33:10Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNDIzMw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM5MzYzNw==", "bodyText": "I don't think we ever expect it though just cause the segments are hourly", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435393637", "createdAt": "2020-06-04T16:33:33Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNDIzMw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDg4MDU4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderTest.groovy", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzo1OToxNVrOGeHw-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNjoyNDozOVrOGhSkng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNzY5MA==", "bodyText": "You might be able to use a Spy from Spock to help check that you call download the right number of times.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434237690", "createdAt": "2020-06-02T23:59:15Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderTest.groovy", "diffHunk": "@@ -0,0 +1,131 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.core.util.FluxUtil\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.models.BlobStorageException\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.publisher.Flux\n+import spock.lang.Unroll\n+\n+import java.nio.ByteBuffer\n+\n+class BlobLazyDownloaderTest extends APISpec {\n+\n+    BlobAsyncClient bc\n+    BlobLazyDownloaderFactory factory\n+\n+    def setup() {\n+        def cc = primaryBlobServiceAsyncClient.getBlobContainerAsyncClient(generateContainerName())\n+        cc.create().block()\n+        bc = cc.getBlobAsyncClient(generateBlobName())\n+        factory = new BlobLazyDownloaderFactory()\n+    }\n+\n+    byte[] downloadHelper(BlobLazyDownloader downloader) {\n+        OutputStream os = downloader.download()\n+            .reduce(new ByteArrayOutputStream(),  { outputStream, buffer ->\n+            outputStream.write(FluxUtil.byteBufferToArray(buffer))\n+            return outputStream;\n+        }).block()\n+        return os.toByteArray()\n+    }\n+\n+    byte[] uploadHelper(int size) {\n+        def input = getRandomByteArray(size)\n+        def data = Flux.just(ByteBuffer.wrap(input))\n+        bc.upload(data, null).block()\n+        return input\n+    }\n+\n+    @Unroll\n+    def \"download blockSize\"() {\n+        setup:\n+        byte[] input = uploadHelper(size)\n+\n+        when:\n+        byte[] output = downloadHelper(new BlobLazyDownloader(bc, blockSize, 0))\n+\n+        then:\n+        output == input", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MDQ3OA==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r437560478", "createdAt": "2020-06-09T16:24:39Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderTest.groovy", "diffHunk": "@@ -0,0 +1,131 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.core.util.FluxUtil\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.models.BlobStorageException\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.publisher.Flux\n+import spock.lang.Unroll\n+\n+import java.nio.ByteBuffer\n+\n+class BlobLazyDownloaderTest extends APISpec {\n+\n+    BlobAsyncClient bc\n+    BlobLazyDownloaderFactory factory\n+\n+    def setup() {\n+        def cc = primaryBlobServiceAsyncClient.getBlobContainerAsyncClient(generateContainerName())\n+        cc.create().block()\n+        bc = cc.getBlobAsyncClient(generateBlobName())\n+        factory = new BlobLazyDownloaderFactory()\n+    }\n+\n+    byte[] downloadHelper(BlobLazyDownloader downloader) {\n+        OutputStream os = downloader.download()\n+            .reduce(new ByteArrayOutputStream(),  { outputStream, buffer ->\n+            outputStream.write(FluxUtil.byteBufferToArray(buffer))\n+            return outputStream;\n+        }).block()\n+        return os.toByteArray()\n+    }\n+\n+    byte[] uploadHelper(int size) {\n+        def input = getRandomByteArray(size)\n+        def data = Flux.just(ByteBuffer.wrap(input))\n+        bc.upload(data, null).block()\n+        return input\n+    }\n+\n+    @Unroll\n+    def \"download blockSize\"() {\n+        setup:\n+        byte[] input = uploadHelper(size)\n+\n+        when:\n+        byte[] output = downloadHelper(new BlobLazyDownloader(bc, blockSize, 0))\n+\n+        then:\n+        output == input", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNzY5MA=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzcyNjYxOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjoyMTo0M1rOGejqFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNjo1NDozNFrOGfPHAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5NDY3Nw==", "bodyText": "Why are you using a method reference instead of just calling verifyWrapper directly?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434694677", "createdAt": "2020-06-03T16:21:43Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwMDg0Mw==", "bodyText": "Can't we get the blockOffset and blockIndex based off the flux index (tuple2.getT1())? Otherwise it seems like hard coding the blockOffset and blockIndex in the test like this is only really testing that we hard coded things the same way here and when generating the mock data", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434700843", "createdAt": "2020-06-03T16:30:57Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5NDY3Nw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNDY0Mw==", "bodyText": "Re: first comment, it's just invoking the function as a closure. https://stackoverflow.com/questions/1395166/invoke-method-as-closure", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434914643", "createdAt": "2020-06-03T23:40:22Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5NDY3Nw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMjA2Mg==", "bodyText": "I'm not sure I follow how invoking the method as a closure is different from just invoking the method in this case? Why not just use this.verifyWrapper?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434922062", "createdAt": "2020-06-04T00:07:40Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5NDY3Nw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQwNjU5Mw==", "bodyText": "just doing normal method call. I'm not sure why I used the closures. I think I googled a thing and it told me to do this.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435406593", "createdAt": "2020-06-04T16:54:34Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5NDY3Nw=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzc3MjY3OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjozMzoxM1rOGekH0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNjozMDozN1rOGfONcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwMjI5MQ==", "bodyText": "Why are you or'ing with true?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434702291", "createdAt": "2020-06-03T16:33:13Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMzc5NA==", "bodyText": "Groovy doesnt work so well with Mockito verify, but this is functionally equivalent https://stackoverflow.com/questions/16678126/simple-mockito-verify-works-in-junit-but-not-spock", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434913794", "createdAt": "2020-06-03T23:37:29Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwMjI5MQ=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMTgwNg==", "bodyText": "Shouldn't you and with true, though? It seems like these statements will always trivially evaluate to true", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434921806", "createdAt": "2020-06-04T00:06:37Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwMjI5MQ=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM5MTg1OA==", "bodyText": "it is a little weird. but since the || true comes after, if the verify step errors out indicating failure, it will still error out and not just \"work\".", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435391858", "createdAt": "2020-06-04T16:30:37Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwMjI5MQ=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzc4ODg3OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjozNzoyMlrOGekSag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxODo1MjoxNlrOGhX39A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwNTAwMg==", "bodyText": "Can you add some descriptions about why you're expecting this behavior?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434705002", "createdAt": "2020-06-03T16:37:22Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true\n+    }\n+\n+    /* Tests user cursor. */\n+    @Unroll\n+    def \"getEvents cursor\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class), any(Flux.class), anyLong(), anyLong()))\n+            .thenReturn(mockAvroReader)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, blockOffset, objectBlockIndex)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_HEADER_SIZE) || true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwNjUyMw==", "bodyText": "In particular I would expect that because you're using a cursor you would get different data out, but it looks like you're maybe using the same underlying data and just expecting different calls to the internal types (an extra call to get the header, resulting in an extra call to the downloader, and using a different overload of getAvroReader?)", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434706523", "createdAt": "2020-06-03T16:39:54Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true\n+    }\n+\n+    /* Tests user cursor. */\n+    @Unroll\n+    def \"getEvents cursor\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class), any(Flux.class), anyLong(), anyLong()))\n+            .thenReturn(mockAvroReader)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, blockOffset, objectBlockIndex)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_HEADER_SIZE) || true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwNTAwMg=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY0NzA1Mg==", "bodyText": "havent addressed this quite yet. I can add in an upcoming commit. Basically since we're mocking the tests we don't care about what the AvroReader returns. We are only testing the functionality of the Chunk which asks the AvroReader for some events and the Chunks takes them and combines them in some expected way", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r437647052", "createdAt": "2020-06-09T18:51:43Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true\n+    }\n+\n+    /* Tests user cursor. */\n+    @Unroll\n+    def \"getEvents cursor\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class), any(Flux.class), anyLong(), anyLong()))\n+            .thenReturn(mockAvroReader)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, blockOffset, objectBlockIndex)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_HEADER_SIZE) || true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwNTAwMg=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY0NzM0OA==", "bodyText": "So we just need to validate that the Chunk passes in the right params and then transforms the data correctly. This is true for all the other tests as well", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r437647348", "createdAt": "2020-06-09T18:52:16Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true\n+    }\n+\n+    /* Tests user cursor. */\n+    @Unroll\n+    def \"getEvents cursor\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class), any(Flux.class), anyLong(), anyLong()))\n+            .thenReturn(mockAvroReader)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, blockOffset, objectBlockIndex)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_HEADER_SIZE) || true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwNTAwMg=="}, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTA0OTk1OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzoyNTo0OVrOGew13Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMDo0NjowM1rOGfc37g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMDY4NQ==", "bodyText": "in case the eventWrappers completes without emitting any data then .last() operator will throw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434910685", "createdAt": "2020-06-03T23:25:49Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()\n+            /* Window the events to the page size. This takes the Flux<BlobChangefeedEventWrapper> and\n+               transforms it into a Flux<Flux<BlobChangefeedEventWrapper>>, where the internal Fluxes can have at most\n+               preferredPageSize elements. */\n+            .window(preferredPageSize)\n+            /* Convert the BlobChangefeedEventWrappers into BlobChangefeedEvents, and bundle them up with the last\n+               element's cursor. */\n+            .flatMap(eventWrappers -> {\n+                /* 1. cache the Flux to turn it into a HotFlux so we can subscribe to it multiple times. */\n+                Flux<BlobChangefeedEventWrapper> cachedEventWrappers = eventWrappers.cache();\n+                /* 2. Get the last element in the flux and grab it's cursor. This will be the continuationToken\n+                      returned to the user if they want to get the next page. */\n+                Mono<ChangefeedCursor> c = cachedEventWrappers.last()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4OTc1Ng==", "bodyText": "Is it possible for eventWrappers to not have any data? Won't window guarantee at least one event is in the flux?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435489756", "createdAt": "2020-06-04T19:09:43Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()\n+            /* Window the events to the page size. This takes the Flux<BlobChangefeedEventWrapper> and\n+               transforms it into a Flux<Flux<BlobChangefeedEventWrapper>>, where the internal Fluxes can have at most\n+               preferredPageSize elements. */\n+            .window(preferredPageSize)\n+            /* Convert the BlobChangefeedEventWrappers into BlobChangefeedEvents, and bundle them up with the last\n+               element's cursor. */\n+            .flatMap(eventWrappers -> {\n+                /* 1. cache the Flux to turn it into a HotFlux so we can subscribe to it multiple times. */\n+                Flux<BlobChangefeedEventWrapper> cachedEventWrappers = eventWrappers.cache();\n+                /* 2. Get the last element in the flux and grab it's cursor. This will be the continuationToken\n+                      returned to the user if they want to get the next page. */\n+                Mono<ChangefeedCursor> c = cachedEventWrappers.last()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMDY4NQ=="}, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU3NTQ4Ng==", "bodyText": "The window operator can also forward complete event if it's upstream completes without emitting any value. If it is guaranteed that the upstream emits min 1 event then this is not a concern.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435575486", "createdAt": "2020-06-04T21:57:26Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()\n+            /* Window the events to the page size. This takes the Flux<BlobChangefeedEventWrapper> and\n+               transforms it into a Flux<Flux<BlobChangefeedEventWrapper>>, where the internal Fluxes can have at most\n+               preferredPageSize elements. */\n+            .window(preferredPageSize)\n+            /* Convert the BlobChangefeedEventWrappers into BlobChangefeedEvents, and bundle them up with the last\n+               element's cursor. */\n+            .flatMap(eventWrappers -> {\n+                /* 1. cache the Flux to turn it into a HotFlux so we can subscribe to it multiple times. */\n+                Flux<BlobChangefeedEventWrapper> cachedEventWrappers = eventWrappers.cache();\n+                /* 2. Get the last element in the flux and grab it's cursor. This will be the continuationToken\n+                      returned to the user if they want to get the next page. */\n+                Mono<ChangefeedCursor> c = cachedEventWrappers.last()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMDY4NQ=="}, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMjExMA==", "bodyText": "I see, last() is applied not directly on window but inside a flatmap following it, yes, there is no chance to hit this validation error.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435632110", "createdAt": "2020-06-05T00:46:03Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()\n+            /* Window the events to the page size. This takes the Flux<BlobChangefeedEventWrapper> and\n+               transforms it into a Flux<Flux<BlobChangefeedEventWrapper>>, where the internal Fluxes can have at most\n+               preferredPageSize elements. */\n+            .window(preferredPageSize)\n+            /* Convert the BlobChangefeedEventWrappers into BlobChangefeedEvents, and bundle them up with the last\n+               element's cursor. */\n+            .flatMap(eventWrappers -> {\n+                /* 1. cache the Flux to turn it into a HotFlux so we can subscribe to it multiple times. */\n+                Flux<BlobChangefeedEventWrapper> cachedEventWrappers = eventWrappers.cache();\n+                /* 2. Get the last element in the flux and grab it's cursor. This will be the continuationToken\n+                      returned to the user if they want to get the next page. */\n+                Mono<ChangefeedCursor> c = cachedEventWrappers.last()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMDY4NQ=="}, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTA5NjY0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MjowNlrOGexSDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToxMDozOFrOGfUNxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzkwMw==", "bodyText": "If I understand correctly, when downstream subscription happens, the changefeed.getEvents() is the api that result in \"network calls\" to produce Flux<BlobChangefeedEventWrapper>.\nIf the code inside the below flatMap does not make any \"network calls\" and flatMap is there to only unwrap Flux<Flux> then I think concatMap should be sufficient. flatMap comes with some overhead to manage concurrency.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434917903", "createdAt": "2020-06-03T23:52:06Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5MDI0NQ==", "bodyText": "Good point. Changed to concatMap", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435490245", "createdAt": "2020-06-04T19:10:38Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzkwMw=="}, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTExNTMzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQwMDowMjowOFrOGexc2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMTo1NDo0MVrOGfZWpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMDY2Ng==", "bodyText": "just curious, is the reason for making the download call still is to get a ByteBuffer of size 0 or something else?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434920666", "createdAt": "2020-06-04T00:02:08Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc)\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5MjI4MA==", "bodyText": "That's perhaps a poor comment.\nWe already \"downloaded\" the empty blob in the downloadFirstChunk method above. We want Flux.range below to return a Flux with one element - \"0\" in it. If you look at the internals of downloadChunk below, when chunkNum = 0, we return whatever we downloaded initially (which would be nothing in this instance) instead of making another download call.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435492280", "createdAt": "2020-06-04T19:14:39Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc)\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMDY2Ng=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5MjM2Nw==", "bodyText": "Does that make any sense?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435492367", "createdAt": "2020-06-04T19:14:48Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc)\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMDY2Ng=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU3NDQzNg==", "bodyText": "sounds good, thanks for clarifying", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435574436", "createdAt": "2020-06-04T21:54:41Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc)\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMDY2Ng=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTEzMzgzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQwMDoxMjo1N1rOGexnvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToyMTozNVrOGfUjvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMzQ1Mg==", "bodyText": "is it the case that when change feed is enabled then storage account will have some special container to store them? so its absence means change-feed-not-enabled?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434923452", "createdAt": "2020-06-04T00:12:57Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+    private static final ObjectMapper mapper = new ObjectMapper();\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NTg3MA==", "bodyText": "Yes, so the changefeed feature relies on the existence of this special container that the service creates. All the blobs we read for this feature are in this container.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435495870", "createdAt": "2020-06-04T19:21:35Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+    private static final ObjectMapper mapper = new ObjectMapper();\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMzQ1Mg=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTE3NjEyOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQwMDozODo1MlrOGeyBEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMTo0OTowNVrOGfZNzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyOTkzNg==", "bodyText": "this pass var get shared across all the subscribers of the returned Flux<String> instance, if that is not intended then we can wrap filter in a defer.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434929936", "createdAt": "2020-06-04T00:38:52Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events (i.e. blockOffset = 0). */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU3MjE3Mg==", "bodyText": "fixed.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435572172", "createdAt": "2020-06-04T21:49:05Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events (i.e. blockOffset = 0). */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyOTkzNg=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTY5NDcwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxMToxMFrOGfKu6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOToyNDo1OVrOGfUqeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNDg5MA==", "bodyText": "nit: After giving it second thought I think this could go up to to the builder - the highest possible place. So that we have DI all the way hierarchy.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435334890", "createdAt": "2020-06-04T15:11:10Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,114 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        this.changefeedFactory = new ChangefeedFactory(segmentFactory, client);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NzU5NA==", "bodyText": "That sounds good. Updated it", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435497594", "createdAt": "2020-06-04T19:24:59Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,114 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        this.changefeedFactory = new ChangefeedFactory(segmentFactory, client);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNDg5MA=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTcwNzMzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxNDowMFrOGfK26g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxNjozOToyMVrOGhTImg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjkzOA==", "bodyText": "I'd elaborate more about how cursor works in this and similar javadoc. I.e. we'll get all events from where cursor is pointing to forward.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435336938", "createdAt": "2020-06-04T15:14:00Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,114 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        this.changefeedFactory = new ChangefeedFactory(segmentFactory, client);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents}\n+     *\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents() {\n+        return getEvents(null, null);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#OffsetDateTime-OffsetDateTime}\n+     *\n+     * @param startTime Filters the results to return events approximately after the start time. Note: A few events\n+     * belonging to the previous hour can also be returned. A few events belonging to this hour can be missing; to\n+     * ensure all events from the hour are returned, round the start time down by an hour.\n+     * @param endTime Filters the results to return events approximately before the end time. Note: A few events\n+     * belonging to the next hour can also be returned. A few events belonging to this hour can be missing; to ensure\n+     * all events from the hour are returned, round the end time up by an hour.\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents(OffsetDateTime startTime, OffsetDateTime endTime) {\n+        return new BlobChangefeedPagedFlux(changefeedFactory, startTime, endTime);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#String}\n+     *\n+     * @param cursor Identifies the portion of the events to be returned with the next get operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2OTY5MA==", "bodyText": "added more info", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r437569690", "createdAt": "2020-06-09T16:39:21Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,114 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        this.changefeedFactory = new ChangefeedFactory(segmentFactory, client);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents}\n+     *\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents() {\n+        return getEvents(null, null);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#OffsetDateTime-OffsetDateTime}\n+     *\n+     * @param startTime Filters the results to return events approximately after the start time. Note: A few events\n+     * belonging to the previous hour can also be returned. A few events belonging to this hour can be missing; to\n+     * ensure all events from the hour are returned, round the start time down by an hour.\n+     * @param endTime Filters the results to return events approximately before the end time. Note: A few events\n+     * belonging to the next hour can also be returned. A few events belonging to this hour can be missing; to ensure\n+     * all events from the hour are returned, round the end time up by an hour.\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents(OffsetDateTime startTime, OffsetDateTime endTime) {\n+        return new BlobChangefeedPagedFlux(changefeedFactory, startTime, endTime);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#String}\n+     *\n+     * @param cursor Identifies the portion of the events to be returned with the next get operation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjkzOA=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMTc5NTYxOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-internal-avro/src/main/java/com/azure/storage/internal/avro/implementation/AvroReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNTozMzowM1rOGfLt1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNjozNjoyNlrOGfObhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM1MDk5Ng==", "bodyText": "nit. I'd just call it read . One can deduct rest from return type.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435350996", "createdAt": "2020-06-04T15:33:03Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-internal-avro/src/main/java/com/azure/storage/internal/avro/implementation/AvroReader.java", "diffHunk": "@@ -0,0 +1,13 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.internal.avro.implementation;\n+\n+import reactor.core.publisher.Flux;\n+\n+/**\n+ * An interface that represents an AvroReader.\n+ */\n+public interface AvroReader {\n+    Flux<AvroObject> readAvroObjects();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM5NTQ2Mg==", "bodyText": "renamed. coming in future commit.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435395462", "createdAt": "2020-06-04T16:36:26Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-internal-avro/src/main/java/com/azure/storage/internal/avro/implementation/AvroReader.java", "diffHunk": "@@ -0,0 +1,13 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.internal.avro.implementation;\n+\n+import reactor.core.publisher.Flux;\n+\n+/**\n+ * An interface that represents an AvroReader.\n+ */\n+public interface AvroReader {\n+    Flux<AvroObject> readAvroObjects();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM1MDk5Ng=="}, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 12}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4374, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}