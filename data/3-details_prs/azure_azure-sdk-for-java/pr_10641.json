{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExNzMxNDU2", "number": 10641, "title": "Implemented query for blob and datalake", "bodyText": "Initial PR - #8688", "createdAt": "2020-04-30T17:27:02Z", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641", "merged": true, "mergeCommit": {"oid": "4ace35e96f457b05cf82e2099affcfe074ee92ff"}, "closed": true, "closedAt": "2020-05-27T18:08:17Z", "author": {"login": "gapra-msft"}, "timelineItems": {"totalCount": 84, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccKm7eAH2gAyNDExNzMxNDU2OjkzNjlhNjJkMjIwYzRiMWZhMDMzYzZjYzc1ZTg0OTA3YTJlNTk1YzE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcldOjCgFqTQxOTQ4MTYxOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "9369a62d220c4b1fa033c6cc75e84907a2e595c1", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/9369a62d220c4b1fa033c6cc75e84907a2e595c1", "committedDate": "2020-04-28T21:17:32Z", "message": "Implemented an Avro Parser"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eb955cc40a9fd16efff0fae26c022239ead0dbf", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1eb955cc40a9fd16efff0fae26c022239ead0dbf", "committedDate": "2020-04-28T21:37:49Z", "message": "Added changes to pass CI"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c5b444647eb0136581ecaea15a63bb3f03c8bad", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5c5b444647eb0136581ecaea15a63bb3f03c8bad", "committedDate": "2020-04-28T22:31:21Z", "message": "Added more excludes due to false positives"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a62fa7f567e97af090af03d1cb551f43a53c06b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/4a62fa7f567e97af090af03d1cb551f43a53c06b", "committedDate": "2020-04-28T22:49:43Z", "message": "Fixed excludes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "faab6f3c2a3382b4d7c770e33856e05d7fc712fc", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/faab6f3c2a3382b4d7c770e33856e05d7fc712fc", "committedDate": "2020-04-29T17:17:14Z", "message": "Regenerated for quick query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb2e98eeabef5329c1d629941783a63e8e6c4f8b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cb2e98eeabef5329c1d629941783a63e8e6c4f8b", "committedDate": "2020-04-29T20:38:21Z", "message": "Added tests and code for blob quick query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93ed5b61459373a6fbb418c09086d73e92aa433a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/93ed5b61459373a6fbb418c09086d73e92aa433a", "committedDate": "2020-04-29T22:48:54Z", "message": "Moved qq to blob base and included snapshot test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ccb0cdd004adc9ec9392bbe36072ced567fd93b1", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ccb0cdd004adc9ec9392bbe36072ced567fd93b1", "committedDate": "2020-04-29T22:49:32Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f0d3a9a8625295d5a4c2b4ca56864deb4d66f1a2", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f0d3a9a8625295d5a4c2b4ca56864deb4d66f1a2", "committedDate": "2020-04-30T00:00:56Z", "message": "Added test for OS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "594d62b19d5f3a966d9e1a9e1072a3ccf9b3dfd7", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/594d62b19d5f3a966d9e1a9e1072a3ccf9b3dfd7", "committedDate": "2020-04-30T17:26:14Z", "message": "Added samples"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f799610e72930a1d022c484dad280bd1534fed6", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/3f799610e72930a1d022c484dad280bd1534fed6", "committedDate": "2020-04-30T21:43:17Z", "message": "Added datalake and reocrdings"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ef18d8b56d8a0c7fb845bf24d97bc13801ebed4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2ef18d8b56d8a0c7fb845bf24d97bc13801ebed4", "committedDate": "2020-04-30T21:45:26Z", "message": "removed avro test file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc0b18494949405051686bd79dca2136f8582507", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/dc0b18494949405051686bd79dca2136f8582507", "committedDate": "2020-04-30T22:42:56Z", "message": "Added files for query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1cae0e10415c60c9fe72eb29873d2575de285a3c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1cae0e10415c60c9fe72eb29873d2575de285a3c", "committedDate": "2020-04-30T22:50:57Z", "message": "Fixed code snippets"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4eae9abd042cea677f0c8a5d1eb499fc273a0ab", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/b4eae9abd042cea677f0c8a5d1eb499fc273a0ab", "committedDate": "2020-04-30T23:11:25Z", "message": "Added more samples"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "725c00758f840e6052cbb743f36a8ae7e3d5e321", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/725c00758f840e6052cbb743f36a8ae7e3d5e321", "committedDate": "2020-04-30T23:45:42Z", "message": "More doc fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26bf158cd98fcc472d0da3e105f0c3a79d103faa", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/26bf158cd98fcc472d0da3e105f0c3a79d103faa", "committedDate": "2020-05-01T00:02:20Z", "message": "throw through logger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9271b5dbd56061592aa2afcf727d7e191cb670d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d9271b5dbd56061592aa2afcf727d7e191cb670d", "committedDate": "2020-05-01T00:16:54Z", "message": "Ovveride encrypted client to not support query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a767c0ee138aeec0121563ee915388962b122a0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0a767c0ee138aeec0121563ee915388962b122a0", "committedDate": "2020-05-01T00:24:31Z", "message": "more flux is exception logger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30d4862d284b8dc1facd7bd2f4abffbf6f80abcc", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/30d4862d284b8dc1facd7bd2f4abffbf6f80abcc", "committedDate": "2020-05-01T00:54:30Z", "message": "Added safe locking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1edeef9367b189a7e692be81dfed9add514f683c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1edeef9367b189a7e692be81dfed9add514f683c", "committedDate": "2020-05-01T01:07:22Z", "message": "Added extra lock"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c34708b142a76fbfa3e7914d1af0cb8e76d0f4e4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c34708b142a76fbfa3e7914d1af0cb8e76d0f4e4", "committedDate": "2020-05-01T14:27:33Z", "message": "Added stuff for CI"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "856c969baa43ae5910655dfd7280bcdcb94ea708", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/856c969baa43ae5910655dfd7280bcdcb94ea708", "committedDate": "2020-05-01T14:57:15Z", "message": "CI stuff"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0fc0a75a37861868e014ed5ecc52f71e8627238b", "committedDate": "2020-05-01T19:04:02Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzU3OTgy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404357982", "createdAt": "2020-05-01T19:36:33Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTozNjozM1rOGPTl9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTozNjozM1rOGPTl9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwMjgzNw==", "bodyText": "fix doc", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418702837", "createdAt": "2020-05-01T19:36:33Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQueryAsyncResponse.java", "diffHunk": "@@ -0,0 +1,30 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+import com.azure.core.http.HttpHeaders;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.rest.ResponseBase;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * This class contains the response information returned from the server when running a quick query a blob.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzU5Mzcx", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404359371", "createdAt": "2020-05-01T19:39:24Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTozOToyNVrOGPTqYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTozOToyNVrOGPTqYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwMzk3MA==", "bodyText": "todo: i think i generated off a slightly older version of swagger, so I need to undo this", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418703970", "createdAt": "2020-05-01T19:39:25Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1227,7 +1247,8 @@ private void downloadToFileCleanup(AsynchronousFileChannel channel, String fileP\n                     hd.isIncrementalCopy(), hd.getDestinationSnapshot(), AccessTier.fromString(hd.getAccessTier()),\n                     hd.isAccessTierInferred(), ArchiveStatus.fromString(hd.getArchiveStatus()),\n                     hd.getEncryptionKeySha256(), hd.getAccessTierChangeTime(), hd.getMetadata(),\n-                    hd.getBlobCommittedBlockCount(), hd.getVersionId(), hd.isCurrentVersion());\n+                    hd.getBlobCommittedBlockCount(), null, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzYwMTYz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404360163", "createdAt": "2020-05-01T19:40:59Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTo0MDo1OVrOGPTszw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTo0MDo1OVrOGPTszw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwNDU5MQ==", "bodyText": "todo: comment this part", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418704591", "createdAt": "2020-05-01T19:40:59Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1599,4 +1620,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 135}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzYxNTI3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404361527", "createdAt": "2020-05-01T19:43:37Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTo0MzozN1rOGPTw9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTo0MzozN1rOGPTw9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwNTY1NA==", "bodyText": "remove : this is redundant. hmm or should i? i need to return a mono error to convey an error to the FluxIS - I could remove the below check", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418705654", "createdAt": "2020-05-01T19:43:37Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1599,4 +1620,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 164}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzYzMTA5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404363109", "createdAt": "2020-05-01T19:47:01Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTo0NzowMVrOGPT2Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTo0NzowMVrOGPT2Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcwNzAzMA==", "bodyText": "This can be just a IllegalStateException or something?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418707030", "createdAt": "2020-05-01T19:47:01Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1599,4 +1620,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {\n+            return Mono.error(new IllegalArgumentException(\"Expected object to be of type Map\"));\n+        }\n+        AvroSchema.checkType(\"record\", quickQueryRecord, Map.class);\n+        Map<?, ?> record = (Map<?, ?>) quickQueryRecord;\n+        Object recordSchema = record.get(AvroConstants.RECORD);\n+\n+        switch (recordSchema.toString()) {\n+            case \"resultData\":\n+                return parseResultData(record);\n+            case \"end\":\n+                return parseEnd(record, progressReceiver);\n+            case \"progress\":\n+                return parseProgress(record, progressReceiver);\n+            case \"error\":\n+                return parseError(record, errorReceiver);\n+            default:\n+                return Mono.error(new UncheckedIOException(new IOException(String.format(\"Unknown record type %s \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 181}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0Mzc1ODI1", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404375825", "createdAt": "2020-05-01T20:12:26Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxMjoyNlrOGPUebA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxMjoyNlrOGPUebA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxNzI5Mg==", "bodyText": "uh oh - need to revert this", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418717292", "createdAt": "2020-05-01T20:12:26Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/BlobAPITest.groovy", "diffHunk": "@@ -3,30 +3,11 @@\n \n package com.azure.storage.blob\n \n-\n import com.azure.core.http.RequestConditions\n import com.azure.core.util.CoreUtils\n import com.azure.core.util.polling.LongRunningOperationStatus\n import com.azure.identity.DefaultAzureCredentialBuilder\n-import com.azure.storage.blob.models.AccessTier\n-import com.azure.storage.blob.models.ArchiveStatus\n-import com.azure.storage.blob.models.BlobErrorCode\n-import com.azure.storage.blob.models.BlobHttpHeaders\n-import com.azure.storage.blob.models.BlobRange\n-import com.azure.storage.blob.models.BlobRequestConditions\n-import com.azure.storage.blob.models.BlobStorageException\n-import com.azure.storage.blob.models.BlobType\n-import com.azure.storage.blob.models.BlockListType\n-import com.azure.storage.blob.models.CopyStatusType\n-import com.azure.storage.blob.models.CustomerProvidedKey\n-import com.azure.storage.blob.models.DeleteSnapshotsOptionType\n-import com.azure.storage.blob.models.DownloadRetryOptions\n-import com.azure.storage.blob.models.LeaseStateType\n-import com.azure.storage.blob.models.LeaseStatusType\n-import com.azure.storage.blob.models.ParallelTransferOptions\n-import com.azure.storage.blob.models.PublicAccessType\n-import com.azure.storage.blob.models.RehydratePriority\n-import com.azure.storage.blob.models.SyncCopyStatusType\n+import com.azure.storage.blob.models.*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0Mzc1OTYy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404375962", "createdAt": "2020-05-01T20:12:39Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxMjozOVrOGPUexw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxMjozOVrOGPUexw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxNzM4Mw==", "bodyText": "revert", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418717383", "createdAt": "2020-05-01T20:12:39Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlockBlobAPITest.groovy", "diffHunk": "@@ -11,22 +11,8 @@ import com.azure.core.http.HttpRequest\n import com.azure.core.util.Context\n import com.azure.core.util.FluxUtil\n import com.azure.identity.DefaultAzureCredentialBuilder\n-import com.azure.storage.blob.APISpec\n-import com.azure.storage.blob.BlobAsyncClient\n-import com.azure.storage.blob.BlobClient\n-import com.azure.storage.blob.BlobServiceClientBuilder\n-import com.azure.storage.blob.BlobUrlParts\n-import com.azure.storage.blob.ProgressReceiver\n-import com.azure.storage.blob.models.AccessTier\n-import com.azure.storage.blob.models.BlobErrorCode\n-import com.azure.storage.blob.models.BlobHttpHeaders\n-import com.azure.storage.blob.models.BlobRange\n-import com.azure.storage.blob.models.BlobRequestConditions\n-import com.azure.storage.blob.models.BlobStorageException\n-import com.azure.storage.blob.models.BlockListType\n-import com.azure.storage.blob.models.CustomerProvidedKey\n-import com.azure.storage.blob.models.ParallelTransferOptions\n-import com.azure.storage.blob.models.PublicAccessType\n+import com.azure.storage.blob.*\n+import com.azure.storage.blob.models.*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0Mzc2NTU3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-404376557", "createdAt": "2020-05-01T20:13:53Z", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxMzo1NFrOGPUglg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxMzo1NFrOGPUglg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxNzg0Ng==", "bodyText": "pointed at specific version to make generating easy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r418717846", "createdAt": "2020-05-01T20:13:54Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/swagger/README.md", "diffHunk": "@@ -15,7 +15,8 @@ autorest --use=@microsoft.azure/autorest.java@3.0.4 --use=jianghaolu/autorest.mo\n \n ### Code generation settings\n ``` yaml\n-input-file: https://raw.githubusercontent.com/Azure/azure-rest-api-specs/storage-dataplane-preview/specification/storage/data-plane/Microsoft.BlobStorage/preview/2019-12-12/blob.json\n+# input-file: https://raw.githubusercontent.com/Azure/azure-rest-api-specs/storage-dataplane-preview/specification/storage/data-plane/Microsoft.BlobStorage/preview/2019-12-12/blob.json\n+input-file: https://github.com/Azure/azure-rest-api-specs/blob/6cd6f02d824569dccb2427cf1611fe1cd8cc4350/specification/storage/data-plane/Microsoft.BlobStorage/preview/2019-12-12/blob.json", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7bf395fdfc02b2c002a9101cb092238a1272c4d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d7bf395fdfc02b2c002a9101cb092238a1272c4d", "committedDate": "2020-05-13T21:58:10Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1d3819f3cb3deea49d63d89591c3e411c4769319", "committedDate": "2020-05-13T22:07:08Z", "message": "regenerated"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc7fcef822f4c32bcd43b60cd0180e2b599cfbfa", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cc7fcef822f4c32bcd43b60cd0180e2b599cfbfa", "committedDate": "2020-05-13T22:23:01Z", "message": "Bumped avro reactor version"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExMzUyNzUy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-411352752", "createdAt": "2020-05-13T22:51:12Z", "commit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMjo1MToxMlrOGVGOiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QyMjo1MToxMlrOGVGOiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTMwNA==", "bodyText": "Can you add a description of what's happening here and what the data looks like? Like we get a stream of bytes that's avro. It's expected to have this schema. Once we get the data out of it, we need to apply these transformations, etc.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r424775304", "createdAt": "2020-05-13T22:51:12Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1623,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319"}, "originalPosition": 142}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "213a4173d7bf59ef03d053310c3ebec1b5401f31", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/213a4173d7bf59ef03d053310c3ebec1b5401f31", "committedDate": "2020-05-13T23:26:43Z", "message": "Added binary recordings for qq"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10d84f04e1b84d741b110a137cec51ef4aa3cde0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/10d84f04e1b84d741b110a137cec51ef4aa3cde0", "committedDate": "2020-05-14T00:01:22Z", "message": "Added binary recordings for datalake"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3dddb59cd9436c9673ab4eb48e1c03f42cb66cfe", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/3dddb59cd9436c9673ab4eb48e1c03f42cb66cfe", "committedDate": "2020-05-14T17:14:22Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31a43d592085371137dcd7c6e715918b648f4fad", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/31a43d592085371137dcd7c6e715918b648f4fad", "committedDate": "2020-05-14T17:20:39Z", "message": "Addressed my comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMDQxMDg0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412041084", "createdAt": "2020-05-14T17:41:20Z", "commit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo0MToyMFrOGVnVsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo0MToyMFrOGVnVsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMxNzgxMQ==", "bodyText": "Seems like the recordSeparator variable can just be a constant.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425317811", "createdAt": "2020-05-14T17:41:20Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,609 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.BlobQueryDelimitedSerialization\n+import com.azure.storage.blob.models.BlobQueryError\n+import com.azure.storage.blob.models.BlobQueryJsonSerialization\n+import com.azure.storage.blob.models.BlobQueryOptions\n+import com.azure.storage.blob.models.BlobQuerySerialization\n+import com.azure.storage.blob.models.BlobRequestConditions\n+import com.azure.storage.blob.models.BlobStorageException\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        where:\n+        numCopies | recordSeparator || _\n+        0         | '\\n'            || _", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319"}, "originalPosition": 180}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d36d397a83b0e08a894bff01d4531ceb5013fcd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0d36d397a83b0e08a894bff01d4531ceb5013fcd", "committedDate": "2020-05-14T17:43:56Z", "message": "Added some imports"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMDQ1ODIy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412045822", "createdAt": "2020-05-14T17:46:56Z", "commit": {"oid": "31a43d592085371137dcd7c6e715918b648f4fad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo0Njo1NlrOGVnm8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxNzo0Njo1NlrOGVnm8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTMyMjIyNA==", "bodyText": "I think in groovy you can just do a == between arrays and it'll compare them for you", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425322224", "createdAt": "2020-05-14T17:46:56Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,604 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31a43d592085371137dcd7c6e715918b648f4fad"}, "originalPosition": 157}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMDc2NjI2", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412076626", "createdAt": "2020-05-14T18:28:26Z", "commit": {"oid": "31a43d592085371137dcd7c6e715918b648f4fad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoyODoyNlrOGVpFrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODoyODoyNlrOGVpFrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0NjQ3Nw==", "bodyText": "Do you test all the options like escape character and line delimiter? I don't remember seeing tests for that. Maybe you can make your upload data methods more generic to accept these options.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425346477", "createdAt": "2020-05-14T18:28:26Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,604 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        where:\n+        numCopies | recordSeparator || _\n+        0         | '\\n'            || _\n+        10        | '\\n'            || _\n+        100       | '\\n'            || _\n+        1000      | '\\n'            || _\n+    }\n+\n+    def \"Query Input csv Output json\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization inSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(inSer, 1)\n+        BlobQueryJsonSerialization outSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"{\\\"_1\\\":\\\"100\\\",\\\"_2\\\":\\\"200\\\",\\\"_3\\\":\\\"300\\\",\\\"_4\\\":\\\"400\\\"}\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query Input json Output csv\"() {\n+        setup:\n+        BlobQueryJsonSerialization inSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        uploadSmallJson(2)\n+        BlobQueryDelimitedSerialization outSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"owner0,owner1\\n\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query non fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        MockErrorReceiver receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        def expression = \"SELECT _1 from BlobStorage WHERE _2 > 250\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        receiver.numErrors > 0\n+        notThrown(IOException)\n+\n+        /* Output Stream. */\n+        when:\n+        receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        notThrown(IOException)\n+        receiver.numErrors > 0\n+    }\n+\n+    def \"Query fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(true)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(new BlobQueryJsonSerialization())\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        thrown(IOException)\n+\n+        /* Output Stream. */\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(Exceptions.ReactiveException)\n+    }\n+\n+    def \"Query progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def sizeofBlobToRead = bc.getProperties().getBlobSize()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The QQ Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, Constants.MB)\n+\n+        then:\n+        // At least the size of blob to read will be in the progress list\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+    }\n+\n+    @Requires( { liveMode() } ) // Large amount of data.\n+    def \"Query multiple records with progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, 512000)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, 16 * Constants.MB)\n+\n+        then:\n+        long temp = 0\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        temp = 0\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+    }\n+\n+    class MockProgressReceiver implements ProgressReceiver {\n+\n+        List<Long> progressList\n+\n+        MockProgressReceiver() {\n+            this.progressList = new ArrayList<>()\n+        }\n+\n+        @Override\n+        void reportProgress(long bytesRead) {\n+            progressList.add(bytesRead)\n+        }\n+    }\n+\n+    class MockErrorReceiver implements ErrorReceiver<BlobQueryError> {\n+\n+        String expectedType\n+        int numErrors\n+\n+        MockErrorReceiver(String expectedType) {\n+            this.expectedType = expectedType\n+            this.numErrors = 0\n+        }\n+\n+        @Override\n+        void reportError(BlobQueryError nonFatalError) {\n+            assert !nonFatalError.isFatal()\n+            assert nonFatalError.getName() == expectedType\n+            numErrors++\n+        }\n+    }\n+\n+    def \"Query snapshot\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        /* Create snapshot of blob. */\n+        def snapshotClient = bc.createSnapshot()\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0, true) /* Make the blob empty. */\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        snapshotClient.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = snapshotClient.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        snapshotClient.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+    }\n+\n+    @Unroll\n+    def \"Query input output IA\"() {\n+        setup:\n+        /* Mock random impl of QQ Serialization*/\n+        BlobQuerySerialization ser = Spy() {\n+            return '\\n'\n+        }\n+        def inSer = input ? ser : null\n+        def outSer = output ? ser : null\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(inSer)\n+            .setOutputSerialization(outSer)\n+\n+        when:\n+        InputStream stream = bc.openQueryInputStream(expression, options)\n+        stream.read()\n+        stream.close()\n+\n+        then:\n+        def e = thrown(IOException)\n+        assert e.getCause() instanceof IllegalArgumentException\n+\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(IllegalArgumentException)\n+\n+        where:\n+        input   | output   || _\n+        true    | false    || _\n+        false   | true     || _\n+    }\n+\n+    @Unroll\n+    def \"Query AC\"() {\n+        setup:\n+        match = setupBlobMatchCondition(bc, match)\n+        leaseID = setupBlobLeaseCondition(bc, leaseID)\n+        def bac = new BlobRequestConditions()\n+            .setLeaseId(leaseID)\n+            .setIfMatch(match)\n+            .setIfNoneMatch(noneMatch)\n+            .setIfModifiedSince(modified)\n+            .setIfUnmodifiedSince(unmodified)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setRequestConditions(bac)\n+\n+        when:\n+        InputStream stream = bc.openQueryInputStream(expression, options)\n+        stream.read()\n+        stream.close()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options,null, null)\n+\n+        then:\n+        notThrown(BlobStorageException)\n+\n+        where:\n+        modified | unmodified | match        | noneMatch   | leaseID\n+        null     | null       | null         | null        | null\n+        oldDate  | null       | null         | null        | null\n+        null     | newDate    | null         | null        | null\n+        null     | null       | receivedEtag | null        | null\n+        null     | null       | null         | garbageEtag | null\n+        null     | null       | null         | null        | receivedLeaseID\n+    }\n+\n+    @Unroll\n+    def \"Query AC fail\"() {\n+        setup:\n+        setupBlobLeaseCondition(bc, leaseID)\n+        def bac = new BlobRequestConditions()\n+            .setLeaseId(leaseID)\n+            .setIfMatch(match)\n+            .setIfNoneMatch(setupBlobMatchCondition(bc, noneMatch))\n+            .setIfModifiedSince(modified)\n+            .setIfUnmodifiedSince(unmodified)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setRequestConditions(bac)\n+\n+        when:\n+        InputStream stream = bc.openQueryInputStream(expression, options)\n+        stream.read()\n+        stream.close()\n+\n+        then:\n+        def e = thrown(IOException)\n+        assert e.getCause() instanceof BlobStorageException\n+\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(BlobStorageException)\n+\n+        where:\n+        modified | unmodified | match       | noneMatch    | leaseID\n+        newDate  | null       | null        | null         | null\n+        null     | oldDate    | null        | null         | null\n+        null     | null       | garbageEtag | null         | null\n+        null     | null       | null        | receivedEtag | null\n+        null     | null       | null        | null         | garbageLeaseID\n+    }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31a43d592085371137dcd7c6e715918b648f4fad"}, "originalPosition": 604}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "committedDate": "2020-05-14T18:28:32Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ae13a7e91ca78b6311dfc9c4af01c50e127cbff", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5ae13a7e91ca78b6311dfc9c4af01c50e127cbff", "committedDate": "2020-05-14T18:39:04Z", "message": "Added semicolon"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c1e2a0b22a3f07e2cab87d1833a90fb8daecb04", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/8c1e2a0b22a3f07e2cab87d1833a90fb8daecb04", "committedDate": "2020-05-14T19:22:32Z", "message": "Added change to try and fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "894d77ab4d30475e1d238bb6aba2dccbf1c18536", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/894d77ab4d30475e1d238bb6aba2dccbf1c18536", "committedDate": "2020-05-14T19:40:33Z", "message": "imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a7b279cf1ae52e2717b16d83f0a6a8daaffe664", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1a7b279cf1ae52e2717b16d83f0a6a8daaffe664", "committedDate": "2020-05-14T19:43:15Z", "message": "potential fix for no error found"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMTY1NzI4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412165728", "createdAt": "2020-05-14T20:37:32Z", "commit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMDozNzozMlrOGVtTlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMDozNzozMlrOGVtTlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQxNTU3NA==", "bodyText": "This line feels spurious. Since the comment says it should only be null if there was an error or completion, and both tof those cases exit the method, we shouldn't need this line, right?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425415574", "createdAt": "2020-05-14T20:37:32Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "originalPosition": 95}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMTg3MDEz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412187013", "createdAt": "2020-05-14T21:10:09Z", "commit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMToxMDowOVrOGVuVaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMToxMDowOVrOGVuVaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMjQyNw==", "bodyText": "I think you may want to lock around checking if buffer is null and assigning to it here. You lock around that in your onNext below, so modifying it here without locking will give you a race condition, i think", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425432427", "createdAt": "2020-05-14T21:10:09Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMTg3ODI0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412187824", "createdAt": "2020-05-14T21:11:26Z", "commit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMToxMToyNlrOGVuYPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMToxMToyNlrOGVuYPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzE0OA==", "bodyText": "Is there any value in checking if the flux has completed before cancelling the subscription? Or is this a no-op in that case?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425433148", "createdAt": "2020-05-14T21:11:26Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMTkwODk2", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412190896", "createdAt": "2020-05-14T21:16:29Z", "commit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMToxNjoyOVrOGVuhiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMToxNjoyOVrOGVuhiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ==", "bodyText": "I don't think we need to wrap IllegalArgumentException in an IOException. And I don't think we need to wrap ioExceptions in another IOException.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425435529", "createdAt": "2020-05-14T21:16:29Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();\n+        if (this.buffer != null) {\n+            this.buffer.close();\n+        }\n+        super.close();\n+        if (this.lastError != null) {\n+            throw logger.logThrowableAsError(this.lastError);\n+        }\n+    }\n+\n+    /**\n+     * Request more data and wait on data to become available.\n+     */\n+    private void blockForData() {\n+        lock.lock();\n+        try {\n+            waitingForData = true;\n+            if (!subscribed) {\n+                subscribeToData();\n+            } else {\n+                subscription.request(1);\n+            }\n+            // Block current thread until data is available.\n+            while (waitingForData) {\n+                if (fluxComplete) {\n+                    break;\n+                } else {\n+                    try {\n+                        dataAvailable.await();\n+                    } catch (InterruptedException e) {\n+                        throw logger.logExceptionAsError(new RuntimeException(e));\n+                    }\n+                }\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Subscribes to the data with a special subscriber.\n+     */\n+    private void subscribeToData() {\n+        this.data\n+            .onBackpressureBuffer(Constants.GB)\n+            .subscribe(\n+                // ByteBuffer consumer\n+                byteBuffer -> {\n+                    lock.lock();\n+                    try {\n+                        this.buffer = new ByteArrayInputStream(FluxUtil.byteBufferToArray(byteBuffer));\n+                        this.waitingForData = false;\n+                        // Signal the consumer when data is available.\n+                        dataAvailable.signal();\n+                    } finally {\n+                        lock.unlock();\n+                    }\n+                },\n+                // Error consumer\n+                throwable -> {\n+                    // Signal the consumer in case an error occurs (indicates we completed without data).\n+                    signalOnCompleteOrError();\n+                    if (throwable instanceof HttpResponseException) {\n+                        this.lastError = new IOException(throwable);\n+                    } else if (throwable instanceof IllegalArgumentException) {\n+                        this.lastError = new IOException(throwable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "originalPosition": 190}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3d0dc63f1505d080b0875919195f5206a7f9f75", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d3d0dc63f1505d080b0875919195f5206a7f9f75", "committedDate": "2020-05-14T21:19:21Z", "message": "Fixed datalake test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6c831acb690de37a1da0b7e70f354f94efb3b87d", "committedDate": "2020-05-14T21:43:46Z", "message": "addressed some comments and mock IA tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "809e3d68b1fbd7b0ab7be6f38041e5e132b51e4e", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/809e3d68b1fbd7b0ab7be6f38041e5e132b51e4e", "committedDate": "2020-05-14T22:05:27Z", "message": "Mock not compatible with java 9+"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db30342799b9d64f13d2d8577a9a7d1bf05c8a4c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/db30342799b9d64f13d2d8577a9a7d1bf05c8a4c", "committedDate": "2020-05-14T22:24:34Z", "message": "Added docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMjEyNjE1", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412212615", "createdAt": "2020-05-14T21:54:02Z", "commit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMTo1NDowMlrOGVvktQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMjozODo0NVrOGVwmYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1MjcyNQ==", "bodyText": "I'd rephrase this into something like \"Cannot query data encrypted on client side\"", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425452725", "createdAt": "2020-05-14T21:54:02Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-cryptography/src/main/java/com/azure/storage/blob/specialized/cryptography/EncryptedBlobAsyncClient.java", "diffHunk": "@@ -447,4 +449,22 @@ This should be the only place we allocate memory in encryptBlob(). Although ther\n             throw logger.logExceptionAsError(Exceptions.propagate(e));\n         }\n     }\n+\n+    /**\n+     * Unsupported.\n+     */\n+    @Override\n+    public Flux<ByteBuffer> query(String expression) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query a client side encrypted client\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1MzAwNw==", "bodyText": "This may require more content explaining why.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425453007", "createdAt": "2020-05-14T21:54:41Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-cryptography/src/main/java/com/azure/storage/blob/specialized/cryptography/EncryptedBlobAsyncClient.java", "diffHunk": "@@ -447,4 +449,22 @@ This should be the only place we allocate memory in encryptBlob(). Although ther\n             throw logger.logExceptionAsError(Exceptions.propagate(e));\n         }\n     }\n+\n+    /**\n+     * Unsupported.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1NTI4MA==", "bodyText": "This should be annotated with @Fluent", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425455280", "createdAt": "2020-05-14T22:00:10Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQueryOptions.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+import com.azure.storage.common.ErrorReceiver;\n+import com.azure.storage.common.ProgressReceiver;\n+\n+/**\n+ * Optional parameters for Blob Query.\n+ */\n+public class BlobQueryOptions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ1NzM0MQ==", "bodyText": "@seanmcc-msft discovered that .NET XMLSerializer transform line endings. We should check if Java's XML engine does the same thing and if it does then we'd probably need to ask to extend generator.\nFor example \\n might be converted into \\r\\n. We should make sure whitespaces are sent to server side verbatim.\nOther thing to check is column separator in delimited serialization. For example adding test that would query a TSV file.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425457341", "createdAt": "2020-05-14T22:05:27Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQuerySerialization.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+/**\n+ * Defines the input and output serialization for a blob quick query request.\n+ * either {@link BlobQueryJsonSerialization} or {@link BlobQueryDelimitedSerialization}\n+ */\n+public abstract class BlobQuerySerialization {\n+\n+    protected char recordSeparator;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MDMzMg==", "bodyText": "This doesn't seem to throw anything.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425460332", "createdAt": "2020-05-14T22:12:53Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1625,292 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                /* Parse the avro reactive stream. */\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {\n+            return Mono.error(new IllegalArgumentException(\"Expected object to be of type Map\"));\n+        }\n+        Map<?, ?> record = (Map<?, ?>) quickQueryRecord;\n+        Object recordSchema = record.get(AvroConstants.RECORD);\n+\n+        switch (recordSchema.toString()) {\n+            case \"resultData\":\n+                return parseResultData(record);\n+            case \"end\":\n+                return parseEnd(record, progressReceiver);\n+            case \"progress\":\n+                return parseProgress(record, progressReceiver);\n+            case \"error\":\n+                return parseError(record, errorReceiver);\n+            default:\n+                return Mono.error(new IllegalStateException(String.format(\"Unknown record type %s \"\n+                    + \"while parsing query response. \", recordSchema.toString())));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query result data record.\n+     * @param dataRecord The quick query result data record.\n+     * @return The data in the record.\n+     */\n+    private Mono<ByteBuffer> parseResultData(Map<?, ?> dataRecord) {\n+        Object data = dataRecord.get(\"data\");\n+\n+        if (checkParametersNotNull(data)) {\n+            AvroSchema.checkType(\"data\", data, List.class);\n+            return Mono.just(ByteBuffer.wrap(AvroSchema.getBytes((List<?>) data)));\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse result data record from \"\n+                + \"query response stream.\"));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query end record.\n+     * @param endRecord The quick query end record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseEnd(Map<?, ?> endRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object total = endRecord.get(\"totalBytes\");\n+\n+            if (checkParametersNotNull(total)) {\n+                AvroSchema.checkType(\"total\", total, Long.class);\n+                progressReceiver.reportProgress((Long) total);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse end record from query \"\n+                    + \"response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query progress record.\n+     * @param progressRecord The quick query progress record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseProgress(Map<?, ?> progressRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object bytesScanned = progressRecord.get(\"bytesScanned\");\n+\n+            if (checkParametersNotNull(bytesScanned)) {\n+                AvroSchema.checkType(\"bytesScanned\", bytesScanned, Long.class);\n+                progressReceiver.reportProgress((Long) bytesScanned);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse progress record from \"\n+                    + \"query response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query error record.\n+     * @param errorRecord The quick query error record.\n+     * @param errorReceiver The error receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseError(Map<?, ?> errorRecord, ErrorReceiver<BlobQueryError> errorReceiver) {\n+        Object fatal = errorRecord.get(\"fatal\");\n+        Object name = errorRecord.get(\"name\");\n+        Object description = errorRecord.get(\"description\");\n+        Object position = errorRecord.get(\"position\");\n+\n+        if (checkParametersNotNull(fatal, name, description, position)) {\n+            AvroSchema.checkType(\"fatal\", fatal, Boolean.class);\n+            AvroSchema.checkType(\"name\", name, String.class);\n+            AvroSchema.checkType(\"description\", description, String.class);\n+            AvroSchema.checkType(\"position\", position, Long.class);\n+\n+            BlobQueryError error = new BlobQueryError((Boolean) fatal, (String) name,\n+                (String) description, (Long) position);\n+\n+            if (errorReceiver != null) {\n+                errorReceiver.reportError(error);\n+            } else {\n+                return Mono.error(new IOException(\"An error was reported during query response processing, \"\n+                        + System.lineSeparator() + error.toString()));\n+            }\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse error record from \"\n+                + \"query response stream.\"));\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Validates that all parameters are non-null. Throws IOException if any of them are.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 276}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MjI1NA==", "bodyText": "Why do we treat \\0 specially? Can customer choose \\0 as record separator (for whatever weird reason)?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425462254", "createdAt": "2020-05-14T22:18:03Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1625,292 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                /* Parse the avro reactive stream. */\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {\n+        AvroParser parser = new AvroParser();\n+        return avro\n+            .concatMap(parser::parse)\n+            .concatMap(objectHandler);\n+    }\n+\n+    /**\n+     * Parses a quick query record.\n+     *\n+     * @param quickQueryRecord The quick query record.\n+     * @param errorReceiver The error receiver.\n+     * @param progressReceiver The progress receiver.\n+     * @return The optional data in the record.\n+     */\n+    private Mono<ByteBuffer> parseRecord(Object quickQueryRecord, ErrorReceiver<BlobQueryError> errorReceiver,\n+        ProgressReceiver progressReceiver) {\n+        if (!(quickQueryRecord instanceof Map)) {\n+            return Mono.error(new IllegalArgumentException(\"Expected object to be of type Map\"));\n+        }\n+        Map<?, ?> record = (Map<?, ?>) quickQueryRecord;\n+        Object recordSchema = record.get(AvroConstants.RECORD);\n+\n+        switch (recordSchema.toString()) {\n+            case \"resultData\":\n+                return parseResultData(record);\n+            case \"end\":\n+                return parseEnd(record, progressReceiver);\n+            case \"progress\":\n+                return parseProgress(record, progressReceiver);\n+            case \"error\":\n+                return parseError(record, errorReceiver);\n+            default:\n+                return Mono.error(new IllegalStateException(String.format(\"Unknown record type %s \"\n+                    + \"while parsing query response. \", recordSchema.toString())));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query result data record.\n+     * @param dataRecord The quick query result data record.\n+     * @return The data in the record.\n+     */\n+    private Mono<ByteBuffer> parseResultData(Map<?, ?> dataRecord) {\n+        Object data = dataRecord.get(\"data\");\n+\n+        if (checkParametersNotNull(data)) {\n+            AvroSchema.checkType(\"data\", data, List.class);\n+            return Mono.just(ByteBuffer.wrap(AvroSchema.getBytes((List<?>) data)));\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse result data record from \"\n+                + \"query response stream.\"));\n+        }\n+    }\n+\n+    /**\n+     * Parses a quick query end record.\n+     * @param endRecord The quick query end record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseEnd(Map<?, ?> endRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object total = endRecord.get(\"totalBytes\");\n+\n+            if (checkParametersNotNull(total)) {\n+                AvroSchema.checkType(\"total\", total, Long.class);\n+                progressReceiver.reportProgress((Long) total);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse end record from query \"\n+                    + \"response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query progress record.\n+     * @param progressRecord The quick query progress record.\n+     * @param progressReceiver The progress receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseProgress(Map<?, ?> progressRecord, ProgressReceiver progressReceiver) {\n+        if (progressReceiver != null) {\n+            Object bytesScanned = progressRecord.get(\"bytesScanned\");\n+\n+            if (checkParametersNotNull(bytesScanned)) {\n+                AvroSchema.checkType(\"bytesScanned\", bytesScanned, Long.class);\n+                progressReceiver.reportProgress((Long) bytesScanned);\n+            } else {\n+                return Mono.error(new IllegalArgumentException(\"Failed to parse progress record from \"\n+                    + \"query response stream.\"));\n+            }\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Parses a quick query error record.\n+     * @param errorRecord The quick query error record.\n+     * @param errorReceiver The error receiver.\n+     * @return Mono.empty or Mono.error\n+     */\n+    private Mono<ByteBuffer> parseError(Map<?, ?> errorRecord, ErrorReceiver<BlobQueryError> errorReceiver) {\n+        Object fatal = errorRecord.get(\"fatal\");\n+        Object name = errorRecord.get(\"name\");\n+        Object description = errorRecord.get(\"description\");\n+        Object position = errorRecord.get(\"position\");\n+\n+        if (checkParametersNotNull(fatal, name, description, position)) {\n+            AvroSchema.checkType(\"fatal\", fatal, Boolean.class);\n+            AvroSchema.checkType(\"name\", name, String.class);\n+            AvroSchema.checkType(\"description\", description, String.class);\n+            AvroSchema.checkType(\"position\", position, Long.class);\n+\n+            BlobQueryError error = new BlobQueryError((Boolean) fatal, (String) name,\n+                (String) description, (Long) position);\n+\n+            if (errorReceiver != null) {\n+                errorReceiver.reportError(error);\n+            } else {\n+                return Mono.error(new IOException(\"An error was reported during query response processing, \"\n+                        + System.lineSeparator() + error.toString()));\n+            }\n+        } else {\n+            return Mono.error(new IllegalArgumentException(\"Failed to parse error record from \"\n+                + \"query response stream.\"));\n+        }\n+        return Mono.empty();\n+    }\n+\n+    /**\n+     * Validates that all parameters are non-null. Throws IOException if any of them are.\n+     */\n+    private boolean checkParametersNotNull(Object... data) {\n+        for (Object o : data) {\n+            if (o == null || o instanceof AvroNullSchema.Null) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Transforms a generic BlobQuickQuerySerialization into a QuickQuerySerialization.\n+     * @param userSerialization {@link BlobQuerySerialization}\n+     * @param logger {@link ClientLogger}\n+     * @return {@link QuickQuerySerialization}\n+     */\n+    private static QuickQuerySerialization transformSerialization(BlobQuerySerialization userSerialization,\n+        ClientLogger logger) {\n+        if (userSerialization == null) {\n+            return null;\n+        }\n+\n+        QuickQueryFormat generatedFormat = new QuickQueryFormat();\n+        if (userSerialization instanceof BlobQueryDelimitedSerialization) {\n+\n+            generatedFormat.setType(QuickQueryFormatType.DELIMITED);\n+            generatedFormat.setDelimitedTextConfiguration(transformDelimited(\n+                (BlobQueryDelimitedSerialization) userSerialization));\n+\n+        } else if (userSerialization instanceof BlobQueryJsonSerialization) {\n+\n+            generatedFormat.setType(QuickQueryFormatType.JSON);\n+            generatedFormat.setJsonTextConfiguration(transformJson(\n+                (BlobQueryJsonSerialization) userSerialization));\n+\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalArgumentException(\n+                String.format(\"'input' must be one of %s or %s\", BlobQueryJsonSerialization.class.getSimpleName(),\n+                    BlobQueryDelimitedSerialization.class.getSimpleName())));\n+        }\n+        return new QuickQuerySerialization().setFormat(generatedFormat);\n+    }\n+\n+    /**\n+     * Transforms a BlobQuickQueryDelimitedSerialization into a DelimitedTextConfiguration.\n+     *\n+     * @param delimitedSerialization {@link BlobQueryDelimitedSerialization}\n+     * @return {@link DelimitedTextConfiguration}\n+     */\n+    private static DelimitedTextConfiguration transformDelimited(\n+        BlobQueryDelimitedSerialization delimitedSerialization) {\n+        if (delimitedSerialization == null) {\n+            return null;\n+        }\n+        return new DelimitedTextConfiguration()\n+            .setColumnSeparator(charToString(delimitedSerialization.getColumnSeparator()))\n+            .setEscapeChar(charToString(delimitedSerialization.getEscapeChar()))\n+            .setFieldQuote(charToString(delimitedSerialization.getFieldQuote()))\n+            .setHeadersPresent(delimitedSerialization.isHeadersPresent())\n+            .setRecordSeparator(charToString(delimitedSerialization.getRecordSeparator()));\n+    }\n+\n+    /**\n+     * Transforms a BlobQuickQueryJsonSerialization into a JsonTextConfiguration.\n+     *\n+     * @param jsonSerialization {@link BlobQueryJsonSerialization}\n+     * @return {@link JsonTextConfiguration}\n+     */\n+    private static JsonTextConfiguration transformJson(BlobQueryJsonSerialization jsonSerialization) {\n+        if (jsonSerialization == null) {\n+            return null;\n+        }\n+        return new JsonTextConfiguration()\n+            .setRecordSeparator(charToString(jsonSerialization.getRecordSeparator()));\n+    }\n+\n+    private static String charToString(char c) {\n+        return c == '\\0' ? \"\" : Character.toString(c);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2MzU5Mg==", "bodyText": "This type doesn't seem to bring any value. Should we just use FluxInputStream or even just InputStream as return type from query method family ? I think if we should declare most abstract type as return type as we can.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425463592", "createdAt": "2020-05-14T22:21:41Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobQueryInputStream.java", "diffHunk": "@@ -0,0 +1,23 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.specialized;\n+\n+import com.azure.storage.common.implementation.FluxInputStream;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * An <code>InputStream</code> interface that represents the stream to use for reading the query response.\n+ */\n+public class BlobQueryInputStream extends FluxInputStream {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NDkzOQ==", "bodyText": "I'd move these classes to the bottom of the files so that we don't mix tests with test aids.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425464939", "createdAt": "2020-05-14T22:25:17Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,602 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.ErrorReceiver\n+import com.azure.storage.common.ProgressReceiver\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    /* Note: Input delimited tested everywhere else. */\n+    @Unroll\n+    def \"Query Input json\"() {\n+        setup:\n+        BlobQueryJsonSerialization ser = new BlobQueryJsonSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+        uploadSmallJson(numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert queryData[j] == downloadedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < downloadedData.length; j++) {\n+            assert osData[j] == downloadedData[j]\n+        }\n+\n+        where:\n+        numCopies | recordSeparator || _\n+        0         | '\\n'            || _\n+        10        | '\\n'            || _\n+        100       | '\\n'            || _\n+        1000      | '\\n'            || _\n+    }\n+\n+    def \"Query Input csv Output json\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization inSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(inSer, 1)\n+        BlobQueryJsonSerialization outSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"{\\\"_1\\\":\\\"100\\\",\\\"_2\\\":\\\"200\\\",\\\"_3\\\":\\\"300\\\",\\\"_4\\\":\\\"400\\\"}\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query Input json Output csv\"() {\n+        setup:\n+        BlobQueryJsonSerialization inSer = new BlobQueryJsonSerialization()\n+            .setRecordSeparator('\\n' as char)\n+        uploadSmallJson(2)\n+        BlobQueryDelimitedSerialization outSer = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        def expression = \"SELECT * from BlobStorage\"\n+        byte[] expectedData = \"owner0,owner1\\n\".getBytes()\n+        BlobQueryOptions options = new BlobQueryOptions().setInputSerialization(inSer).setOutputSerialization(outSer)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        byte[] queryData = readFromInputStream(qqStream, expectedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert queryData[j] == expectedData[j]\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, options, null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        for (int j = 0; j < expectedData.length; j++) {\n+            assert osData[j] == expectedData[j]\n+        }\n+    }\n+\n+    def \"Query non fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        MockErrorReceiver receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        def expression = \"SELECT _1 from BlobStorage WHERE _2 > 250\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        receiver.numErrors > 0\n+        notThrown(IOException)\n+\n+        /* Output Stream. */\n+        when:\n+        receiver = new MockErrorReceiver(\"InvalidColumnOrdinal\")\n+        options = new BlobQueryOptions()\n+            .setInputSerialization(base.setColumnSeparator(',' as char))\n+            .setOutputSerialization(base.setColumnSeparator(',' as char))\n+            .setErrorReceiver(receiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        notThrown(IOException)\n+        receiver.numErrors > 0\n+    }\n+\n+    def \"Query fatal error\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(true)\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setInputSerialization(new BlobQueryJsonSerialization())\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+        readFromInputStream(qqStream, Constants.KB)\n+\n+        then:\n+        thrown(Throwable)\n+\n+        /* Output Stream. */\n+        when:\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        thrown(Exceptions.ReactiveException)\n+    }\n+\n+    def \"Query progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization base = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+\n+        uploadCsv(base.setColumnSeparator('.' as char), 32)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def sizeofBlobToRead = bc.getProperties().getBlobSize()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The QQ Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, Constants.MB)\n+\n+        then:\n+        // At least the size of blob to read will be in the progress list\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        mockReceiver.progressList.contains(sizeofBlobToRead)\n+    }\n+\n+    @Requires( { liveMode() } ) // Large amount of data.\n+    def \"Query multiple records with progress receiver\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, 512000)\n+\n+        def mockReceiver = new MockProgressReceiver()\n+        def expression = \"SELECT * from BlobStorage\"\n+        BlobQueryOptions options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, options)\n+\n+        /* The Avro stream has the following pattern\n+           n * (data record -> progress record) -> end record */\n+        // 1KB of data will only come back as a single data record.\n+        /* Pretend to read more data because the input stream will not parse records following the data record if it\n+         doesn't need to. */\n+        readFromInputStream(qqStream, 16 * Constants.MB)\n+\n+        then:\n+        long temp = 0\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        mockReceiver = new MockProgressReceiver()\n+        temp = 0\n+        options = new BlobQueryOptions()\n+            .setProgressReceiver(mockReceiver as ProgressReceiver)\n+        bc.queryWithResponse(new ByteArrayOutputStream(), expression, options, null, null)\n+\n+        then:\n+        // Make sure theyre all increasingly bigger\n+        for (long progress : mockReceiver.progressList) {\n+            assert progress >= temp\n+            temp = progress\n+        }\n+    }\n+\n+    class MockProgressReceiver implements ProgressReceiver {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 418}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw==", "bodyText": "I think we could replace this with https://docs.oracle.com/javase/8/docs/api/java/util/function/Consumer.html in respective API and just call parameters errorConsumer.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425465503", "createdAt": "2020-05-14T22:26:52Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/ErrorReceiver.java", "diffHunk": "@@ -0,0 +1,20 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common;\n+\n+/**\n+ * An {@code ErrorReceiver} is a class that can be used to report errors on transfers. When specified on transfer\n+ * operations, the {@code reportError} method will be called whenever errors are encountered. The user may configure\n+ * this method to report errors in whatever format desired.\n+ */\n+public interface ErrorReceiver<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NjIwNA==", "bodyText": "What does it do? 1GB is quite high threshold .", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425466204", "createdAt": "2020-05-14T22:28:52Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,243 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();\n+        if (this.buffer != null) {\n+            this.buffer.close();\n+        }\n+        super.close();\n+        if (this.lastError != null) {\n+            throw logger.logThrowableAsError(this.lastError);\n+        }\n+    }\n+\n+    /**\n+     * Request more data and wait on data to become available.\n+     */\n+    private void blockForData() {\n+        lock.lock();\n+        try {\n+            waitingForData = true;\n+            if (!subscribed) {\n+                subscribeToData();\n+            } else {\n+                subscription.request(1);\n+            }\n+            // Block current thread until data is available.\n+            while (waitingForData) {\n+                if (fluxComplete) {\n+                    break;\n+                } else {\n+                    try {\n+                        dataAvailable.await();\n+                    } catch (InterruptedException e) {\n+                        throw logger.logExceptionAsError(new RuntimeException(e));\n+                    }\n+                }\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Subscribes to the data with a special subscriber.\n+     */\n+    private void subscribeToData() {\n+        this.data\n+            .onBackpressureBuffer(Constants.GB)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2Njc2Nw==", "bodyText": "IllegalArgumentException sounds like validation failed somewhere. That usually should bubble up. Isn't it runtimeexception?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425466767", "createdAt": "2020-05-14T22:30:30Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/FluxInputStream.java", "diffHunk": "@@ -0,0 +1,242 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.exception.HttpResponseException;\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import org.reactivestreams.Subscription;\n+import reactor.core.publisher.Flux;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+\n+/**\n+ * An InputStream interface that subscribes to a Flux allows data to be read.\n+ */\n+public class FluxInputStream extends InputStream {\n+\n+    private ClientLogger logger = new ClientLogger(FluxInputStream.class);\n+\n+    // The data to subscribe to.\n+    private Flux<ByteBuffer> data;\n+\n+    // Subscription to request more data from as needed\n+    private Subscription subscription;\n+\n+    private ByteArrayInputStream buffer;\n+\n+    private boolean subscribed;\n+    private boolean fluxComplete;\n+    private boolean waitingForData;\n+\n+    /* The following lock and condition variable is to synchronize access between the reader and the\n+        reactor thread asynchronously reading data from the Flux. If no data is available, the reader\n+        acquires the lock and waits on the dataAvailable condition variable. Once data is available\n+        (or an error or completion event occurs) the reactor thread acquires the lock and signals that\n+        data is available. */\n+    private final Lock lock;\n+    private final Condition dataAvailable;\n+\n+    private IOException lastError;\n+\n+    /**\n+     * Creates a new FluxInputStream\n+     *\n+     * @param data The data to subscribe to and read from.\n+     */\n+    public FluxInputStream(Flux<ByteBuffer> data) {\n+        this.subscribed = false;\n+        this.fluxComplete = false;\n+        this.waitingForData = false;\n+        this.data = data;\n+        this.lock = new ReentrantLock();\n+        this.dataAvailable = lock.newCondition();\n+    }\n+\n+    @Override\n+    public int read() throws IOException {\n+        byte[] ret = new byte[1];\n+        int count = read(ret, 0, 1);\n+        return count == -1 ? -1 : ret[0];\n+    }\n+\n+    @Override\n+    public int read(byte[] b, int off, int len) throws IOException {\n+        validateParameters(b, off, len);\n+\n+        /* If len is 0, then no bytes are read and 0 is returned. */\n+        if (len == 0) {\n+            return 0;\n+        }\n+        /* Attempt to read at least one byte. If no byte is available because the stream is at end of file,\n+           the value -1 is returned; otherwise, at least one byte is read and stored into b. */\n+\n+        /* Not subscribed? subscribe and block for data */\n+        if (!subscribed) {\n+            blockForData();\n+        }\n+        /* Now, we have subscribed. */\n+        /* At this point, buffer should not be null. If it is, that indicates either an error or completion event\n+           was emitted by the Flux. */\n+        if (this.buffer == null) { // Only executed on first subscription.\n+            if (this.lastError != null) {\n+                throw logger.logThrowableAsError(this.lastError);\n+            }\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            this.buffer = new ByteArrayInputStream(new byte[0]);\n+        }\n+\n+        /* Now we are guaranteed that buffer is SOMETHING. */\n+        /* No data is available in the buffer.  */\n+        if (this.buffer.available() == 0) {\n+            /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+            if (this.fluxComplete) {\n+                return -1;\n+            }\n+            /* Block current thread until data is available. */\n+            blockForData();\n+        }\n+\n+        /* Data available in buffer, read the buffer. */\n+        if (this.buffer.available() > 0) {\n+            return this.buffer.read(b, off, len);\n+        }\n+\n+        /* If the flux completed, there is no more data available to be read from the stream. Return -1. */\n+        if (this.fluxComplete) {\n+            return -1;\n+        } else {\n+            throw logger.logExceptionAsError(new IllegalStateException(\"An unexpected error occurred. No data was \"\n+                + \"read from the stream but the stream did not indicate completion.\"));\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        subscription.cancel();\n+        if (this.buffer != null) {\n+            this.buffer.close();\n+        }\n+        super.close();\n+        if (this.lastError != null) {\n+            throw logger.logThrowableAsError(this.lastError);\n+        }\n+    }\n+\n+    /**\n+     * Request more data and wait on data to become available.\n+     */\n+    private void blockForData() {\n+        lock.lock();\n+        try {\n+            waitingForData = true;\n+            if (!subscribed) {\n+                subscribeToData();\n+            } else {\n+                subscription.request(1);\n+            }\n+            // Block current thread until data is available.\n+            while (waitingForData) {\n+                if (fluxComplete) {\n+                    break;\n+                } else {\n+                    try {\n+                        dataAvailable.await();\n+                    } catch (InterruptedException e) {\n+                        throw logger.logExceptionAsError(new RuntimeException(e));\n+                    }\n+                }\n+            }\n+        } finally {\n+            lock.unlock();\n+        }\n+    }\n+\n+    /**\n+     * Subscribes to the data with a special subscriber.\n+     */\n+    private void subscribeToData() {\n+        this.data\n+            .onBackpressureBuffer(Constants.GB)\n+            .subscribe(\n+                // ByteBuffer consumer\n+                byteBuffer -> {\n+                    lock.lock();\n+                    try {\n+                        this.buffer = new ByteArrayInputStream(FluxUtil.byteBufferToArray(byteBuffer));\n+                        this.waitingForData = false;\n+                        // Signal the consumer when data is available.\n+                        dataAvailable.signal();\n+                    } finally {\n+                        lock.unlock();\n+                    }\n+                },\n+                // Error consumer\n+                throwable -> {\n+                    // Signal the consumer in case an error occurs (indicates we completed without data).\n+                    signalOnCompleteOrError();\n+                    if (throwable instanceof HttpResponseException) {\n+                        this.lastError = new IOException(throwable);\n+                    } else if (throwable instanceof IllegalArgumentException) {\n+                        this.lastError = new IOException(throwable);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzNTUyOQ=="}, "originalCommit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NzY4NQ==", "bodyText": "Shouldn't we just use ResponseBase<> ? this type doesn't seem to do anything new. If we need we can always introduce this in the future without breaking change.\nOr is this generated code?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425467685", "createdAt": "2020-05-14T22:33:11Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/models/FileQueryAsyncResponse.java", "diffHunk": "@@ -0,0 +1,30 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake.models;\n+\n+import com.azure.core.http.HttpHeaders;\n+import com.azure.core.http.HttpRequest;\n+import com.azure.core.http.rest.ResponseBase;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * This class contains the response information returned from the server when running a query on a file.\n+ */\n+public final class FileQueryAsyncResponse extends ResponseBase<FileQueryHeaders, Flux<ByteBuffer>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2Nzk0Mw==", "bodyText": "Same feedback here. I think APIs should just return InputStream. We don't need this type.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425467943", "createdAt": "2020-05-14T22:34:01Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/models/FileQueryInputStream.java", "diffHunk": "@@ -0,0 +1,23 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake.models;\n+\n+import com.azure.storage.common.implementation.FluxInputStream;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * An <code>InputStream</code> interface that represents the stream to use for reading the query response.\n+ */\n+public class FileQueryInputStream extends FluxInputStream {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2ODk4Ng==", "bodyText": "I think we should just have normal setter here. Derivatives shouldn't be concerned about this field - I assume we're using inheritance to share this property.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425468986", "createdAt": "2020-05-14T22:37:17Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/models/FileQuerySerialization.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.file.datalake.models;\n+\n+/**\n+ * Defines the input and output serialization for a file query request.\n+ * either {@link FileQueryJsonSerialization} or {@link FileQueryDelimitedSerialization}\n+ */\n+public abstract class FileQuerySerialization {\n+\n+    protected char recordSeparator;\n+\n+    /**\n+     * Gets the record separator.\n+     *\n+     * @return the record separator.\n+     */\n+    public char getRecordSeparator() {\n+        return recordSeparator;\n+    }\n+\n+    abstract FileQuerySerialization setRecordSeparator(char recordSeparator);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2OTUzNw==", "bodyText": "Same here. I think this should be just usual setter.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425469537", "createdAt": "2020-05-14T22:38:45Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQuerySerialization.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+/**\n+ * Defines the input and output serialization for a blob quick query request.\n+ * either {@link BlobQueryJsonSerialization} or {@link BlobQueryDelimitedSerialization}\n+ */\n+public abstract class BlobQuerySerialization {\n+\n+    protected char recordSeparator;\n+\n+    /**\n+     * Gets the record separator.\n+     *\n+     * @return the record separator.\n+     */\n+    public char getRecordSeparator() {\n+        return recordSeparator;\n+    }\n+\n+    abstract BlobQuerySerialization setRecordSeparator(char recordSeparator);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 23}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1400710d00c999789865097c4a751f7230429d35", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1400710d00c999789865097c4a751f7230429d35", "committedDate": "2020-05-14T23:32:51Z", "message": "changes to tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c27dada6785e357d40fc729e77a12c1cefe1ef4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1c27dada6785e357d40fc729e77a12c1cefe1ef4", "committedDate": "2020-05-15T17:04:31Z", "message": "Added weird char tests for rec sep"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyODM0NDE0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-412834414", "createdAt": "2020-05-15T17:26:14Z", "commit": {"oid": "1c27dada6785e357d40fc729e77a12c1cefe1ef4"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNzoyNjoxNVrOGWNnmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNzozMTo0N1rOGWNyZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0NDk4Ng==", "bodyText": "I put the Query API on BlockBlobClient, since it currently only works with block blobs.  If other blobs, we can move it to BaseBlobClient (assuming you have a BaseBlobClient in Java)", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425944986", "createdAt": "2020-05-15T17:26:15Z", "author": {"login": "seanmcc-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1625,292 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c27dada6785e357d40fc729e77a12c1cefe1ef4"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0NjYxNg==", "bodyText": "Also, if this is the main BlobClient, I think it might be best to put the Query private helper methods somewhere else, so we don't clutter it.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425946616", "createdAt": "2020-05-15T17:29:26Z", "author": {"login": "seanmcc-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/specialized/BlobAsyncClientBase.java", "diffHunk": "@@ -1605,4 +1623,293 @@ public String generateSas(BlobServiceSasSignatureValues blobServiceSasSignatureV\n             getSnapshotId(), getVersionId())\n             .generateSas(SasImplUtils.extractSharedKeyCredential(getHttpPipeline()));\n     }\n+\n+    /* TODO (gapra): Quick Query service docs. */\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.query#String}\n+     *\n+     * @param expression The query expression.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Flux<ByteBuffer> query(String expression) {\n+        return queryWithResponse(expression, null)\n+            .flatMapMany(BlobQueryAsyncResponse::getValue);\n+    }\n+\n+    /**\n+     * Queries the entire blob.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/rest/api/\">Azure Docs</a></p>\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.specialized.BlobAsyncClientBase.queryWithResponse#String-BlobQueryOptions}\n+     *\n+     * @param expression The query expression.\n+     * @param queryOptions {@link BlobQueryOptions The query options}.\n+     * @return A reactive response containing the queried data.\n+     */\n+    public Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions) {\n+        try {\n+            return withContext(context ->\n+                queryWithResponse(expression, queryOptions, context));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    Mono<BlobQueryAsyncResponse> queryWithResponse(String expression, BlobQueryOptions queryOptions, Context context) {\n+\n+        BlobQueryOptions finalQueryOptions = queryOptions == null ? new BlobQueryOptions() : queryOptions;\n+        BlobRequestConditions requestConditions = finalQueryOptions.getRequestConditions() == null\n+            ? new BlobRequestConditions() : finalQueryOptions.getRequestConditions();\n+\n+        QuickQuerySerialization in = transformSerialization(finalQueryOptions.getInputSerialization(), logger);\n+        QuickQuerySerialization out = transformSerialization(finalQueryOptions.getOutputSerialization(), logger);\n+\n+        QueryRequest qr = new QueryRequest()\n+            .setExpression(expression)\n+            .setInputSerialization(in)\n+            .setOutputSerialization(out);\n+\n+        return this.azureBlobStorage.blobs().quickQueryWithRestResponseAsync(null, null, qr,\n+            getSnapshotId(), null, requestConditions.getLeaseId(), requestConditions.getIfModifiedSince(),\n+            requestConditions.getIfUnmodifiedSince(), requestConditions.getIfMatch(),\n+            requestConditions.getIfNoneMatch(), null, getCustomerProvidedKey(), context)\n+            .map(response -> new BlobQueryAsyncResponse(response.getRequest(), response.getStatusCode(),\n+                response.getHeaders(),\n+                parse(response.getValue(), o -> this.parseRecord(o, finalQueryOptions.getErrorReceiver(),\n+                    finalQueryOptions.getProgressReceiver())),\n+                response.getDeserializedHeaders()));\n+    }\n+\n+    /**\n+     * Avro parses a quick query reactive stream.\n+     *\n+     * @param avro The reactive stream.\n+     * @param objectHandler The function to handle objects.\n+     * @return The parsed quick query reactive stream.\n+     */\n+    private Flux<ByteBuffer> parse(Flux<ByteBuffer> avro, Function<Object, Mono<ByteBuffer>> objectHandler) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc3NTMwNA=="}, "originalCommit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0Nzc1MQ==", "bodyText": "Ya in .NET we replaced this interface with a lamda.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r425947751", "createdAt": "2020-05-15T17:31:47Z", "author": {"login": "seanmcc-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/ErrorReceiver.java", "diffHunk": "@@ -0,0 +1,20 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common;\n+\n+/**\n+ * An {@code ErrorReceiver} is a class that can be used to report errors on transfers. When specified on transfer\n+ * operations, the {@code reportError} method will be called whenever errors are encountered. The user may configure\n+ * this method to report errors in whatever format desired.\n+ */\n+public interface ErrorReceiver<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2NTUwMw=="}, "originalCommit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c9da64f63e9e71e33e385549f717dd4634eaeea", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7c9da64f63e9e71e33e385549f717dd4634eaeea", "committedDate": "2020-05-15T18:13:44Z", "message": "Modified to accoutn for new line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "309ce91eb4191f5d7877275bc336695c4aa1c2d0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/309ce91eb4191f5d7877275bc336695c4aa1c2d0", "committedDate": "2020-05-15T20:09:52Z", "message": "Moved blob query logic to implementation class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc3dc842f19f768362ddd3b244a67338e5b51074", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/dc3dc842f19f768362ddd3b244a67338e5b51074", "committedDate": "2020-05-15T21:31:09Z", "message": "Addressed some comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f415fecefeab32f9bad2381f7607c81657ddd4e9", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f415fecefeab32f9bad2381f7607c81657ddd4e9", "committedDate": "2020-05-15T22:06:32Z", "message": "Converted receiver to Consumer interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "319b4e1ab9d90169ebee4c4ddd7313c0287ced79", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/319b4e1ab9d90169ebee4c4ddd7313c0287ced79", "committedDate": "2020-05-18T17:24:49Z", "message": "Addressed more comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f78b723531119286aa88cfd41cc5ef1a4e9c72c6", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f78b723531119286aa88cfd41cc5ef1a4e9c72c6", "committedDate": "2020-05-18T17:28:54Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/13fcf2ee1285db2ac87223d185b180cc9fee03dd", "committedDate": "2020-05-18T19:10:54Z", "message": "Added heades"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf0b18d033d37d7a009ca2dffeebcbb135e4c729", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/bf0b18d033d37d7a009ca2dffeebcbb135e4c729", "committedDate": "2020-05-18T19:16:37Z", "message": "Added headers and changelog"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzODc3NzAz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-413877703", "createdAt": "2020-05-18T19:25:59Z", "commit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOToyNTo1OVrOGXEnCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOToyNTo1OVrOGXEnCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0NTk2MQ==", "bodyText": "just noticed this was missed.  will fix", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426845961", "createdAt": "2020-05-18T19:25:59Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob-cryptography/src/main/java/com/azure/storage/blob/specialized/cryptography/EncryptedBlobClient.java", "diffHunk": "@@ -202,4 +207,41 @@ public PageBlobClient getPageBlobClient() {\n             + \" blob client\"));\n     }\n \n+    /**\n+     * Unsupported. Cannot query data encrypted on client side.\n+     */\n+    @Override\n+    public InputStream openQueryInputStream(String expression) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query data encrypted on client side.\"));\n+    }\n+\n+    /**\n+     * Unsupported. Cannot query data encrypted on client side.\n+     */\n+    @Override\n+    public InputStream openQueryInputStream(String expression, BlobQueryOptions queryOptions) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query data encrypted on client side.\"));\n+    }\n+\n+    /**\n+     * Unsupported. Cannot query data encrypted on client side.\n+     */\n+    @Override\n+    public void query(OutputStream stream, String expression) {\n+        throw logger.logExceptionAsError(new UnsupportedOperationException(\n+            \"Cannot query a client side encrypted client\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzODgxMjcy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-413881272", "createdAt": "2020-05-18T19:31:37Z", "commit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTozMTozN1rOGXExrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTozMTozN1rOGXExrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg0ODY4NQ==", "bodyText": "I think this will continue to be char from what I can tell from the discussions we've had with the QQ team", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426848685", "createdAt": "2020-05-18T19:31:37Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQueryDelimitedSerialization.java", "diffHunk": "@@ -0,0 +1,95 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.models;\n+\n+/**\n+ * Defines the input or output delimited (CSV) serialization for a blob quick query request.\n+ */\n+public class BlobQueryDelimitedSerialization extends\n+    BlobQuerySerialization {\n+\n+    /* TODO(gapra) : Consider diff datatype in case future break. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzODg2NjY3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-413886667", "createdAt": "2020-05-18T19:40:22Z", "commit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTo0MDoyM1rOGXFB6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTo0MDoyM1rOGXFB6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1Mjg0MA==", "bodyText": "TODO : rename to BlobQueryHeaders", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426852840", "createdAt": "2020-05-18T19:40:23Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/models/BlobQuickQueryHeaders.java", "diffHunk": "@@ -0,0 +1,1151 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+// Code generated by Microsoft (R) AutoRest Code Generator.\n+\n+package com.azure.storage.blob.models;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.core.annotation.HeaderCollection;\n+import com.azure.core.util.CoreUtils;\n+import com.azure.core.util.DateTimeRfc1123;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.dataformat.xml.annotation.JacksonXmlRootElement;\n+import java.time.OffsetDateTime;\n+import java.util.Map;\n+\n+/**\n+ * Defines headers for QuickQuery operation.\n+ */\n+@JacksonXmlRootElement(localName = \"Blob-QuickQuery-Headers\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzODkwOTMw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-413890930", "createdAt": "2020-05-18T19:46:47Z", "commit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTo0Njo0N1rOGXFN-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxOTo0Njo0N1rOGXFN-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg1NTkyOA==", "bodyText": "i'm not sure why or how some of my other cases got deleted. I'll add them back. Also forgot to add to file", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#discussion_r426855928", "createdAt": "2020-05-18T19:46:47Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/specialized/BlobBaseAPITest.groovy", "diffHunk": "@@ -0,0 +1,667 @@\n+package com.azure.storage.blob.specialized\n+\n+import com.azure.storage.blob.APISpec\n+import com.azure.storage.blob.BlobClient\n+import com.azure.storage.blob.models.*\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.Exceptions\n+import spock.lang.Requires\n+import spock.lang.Unroll\n+\n+import java.util.function.Consumer\n+\n+class BlobBaseAPITest extends APISpec {\n+\n+    BlobClient bc\n+    String blobName\n+\n+    def setup() {\n+        blobName = generateBlobName()\n+        bc = cc.getBlobClient(blobName)\n+        bc.upload(new ByteArrayInputStream(new byte[0]), 0)\n+    }\n+\n+    /* Quick Query Tests. */\n+\n+    // Generates and uploads a CSV file\n+    def uploadCsv(BlobQueryDelimitedSerialization s, int numCopies) {\n+        String header = String.join(new String(s.getColumnSeparator()), \"rn1\", \"rn2\", \"rn3\", \"rn4\")\n+            .concat(new String(s.getRecordSeparator()))\n+        byte[] headers = header.getBytes()\n+\n+        String csv = String.join(new String(s.getColumnSeparator()), \"100\", \"200\", \"300\", \"400\")\n+            .concat(new String(s.getRecordSeparator()))\n+            .concat(String.join(new String(s.getColumnSeparator()), \"300\", \"400\", \"500\", \"600\")\n+                .concat(new String(s.getRecordSeparator())))\n+\n+        byte[] csvData = csv.getBytes()\n+\n+        int headerLength = s.isHeadersPresent() ? headers.length : 0\n+        byte[] data = new byte[headerLength + csvData.length * numCopies]\n+        if (s.isHeadersPresent()) {\n+            System.arraycopy(headers, 0, data, 0, headers.length)\n+        }\n+\n+        for (int i = 0; i < numCopies; i++) {\n+            int o = i * csvData.length + headerLength;\n+            System.arraycopy(csvData, 0, data, o, csvData.length)\n+        }\n+\n+        InputStream inputStream = new ByteArrayInputStream(data)\n+\n+        bc.upload(inputStream, data.length, true)\n+    }\n+\n+    def uploadSmallJson(int numCopies) {\n+        StringBuilder b = new StringBuilder()\n+        b.append('{\\n')\n+        for(int i = 0; i < numCopies; i++) {\n+            b.append(String.format('\\t\"name%d\": \"owner%d\",\\n', i, i))\n+        }\n+        b.append('}')\n+\n+        InputStream inputStream = new ByteArrayInputStream(b.toString().getBytes())\n+\n+        bc.upload(inputStream, b.length(), true)\n+    }\n+\n+    byte[] readFromInputStream(InputStream stream, int numBytesToRead) {\n+        byte[] queryData = new byte[numBytesToRead]\n+\n+        def totalRead = 0\n+        def bytesRead = 0\n+        def length = numBytesToRead\n+\n+        while (bytesRead != -1 && totalRead < numBytesToRead) {\n+            bytesRead = stream.read(queryData, totalRead, length)\n+            if (bytesRead != -1) {\n+                totalRead += bytesRead\n+                length -= bytesRead\n+            }\n+        }\n+\n+        stream.close()\n+        return queryData\n+    }\n+\n+    @Unroll\n+    def \"Query min\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator('\\n' as char)\n+            .setColumnSeparator(',' as char)\n+            .setEscapeChar('\\0' as char)\n+            .setFieldQuote('\\0' as char)\n+            .setHeadersPresent(false)\n+        uploadCsv(ser, numCopies)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression)\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        queryData == downloadedData\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.query(os, expression)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        osData == downloadedData\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        numCopies | _\n+        1         | _ // 32 bytes\n+        32        | _ // 1 KB\n+        256       | _ // 8 KB\n+        400       | _ // 12 ish KB\n+        4000      | _ // 125 KB\n+    }\n+\n+    @Unroll\n+    def \"Query csv serialization\"() {\n+        setup:\n+        BlobQueryDelimitedSerialization ser = new BlobQueryDelimitedSerialization()\n+            .setRecordSeparator(recordSeparator as char)\n+            .setColumnSeparator(columnSeparator as char)\n+            .setEscapeChar(escapeChar as char)\n+            .setFieldQuote(fieldQuote as char)\n+            .setHeadersPresent(headersPresent)\n+        uploadCsv(ser, 32)\n+        def expression = \"SELECT * from BlobStorage\"\n+\n+        ByteArrayOutputStream downloadData = new ByteArrayOutputStream()\n+        bc.download(downloadData)\n+        byte[] downloadedData = downloadData.toByteArray()\n+\n+        /* Input Stream. */\n+        when:\n+        InputStream qqStream = bc.openQueryInputStream(expression, new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser))\n+        byte[] queryData = readFromInputStream(qqStream, downloadedData.length)\n+\n+        then:\n+        notThrown(IOException)\n+        if (headersPresent) {\n+            /* Account for 16 bytes of header. */\n+            for (int j = 16; j < downloadedData.length; j++) {\n+                assert queryData[j - 16] == downloadedData[j]\n+            }\n+            for (int k = downloadedData.length - 16; k < downloadedData.length; k++) {\n+                assert queryData[k] == 0\n+            }\n+        } else {\n+            queryData == downloadedData\n+        }\n+\n+        /* Output Stream. */\n+        when:\n+        OutputStream os = new ByteArrayOutputStream()\n+        bc.queryWithResponse(os, expression, new BlobQueryOptions().setInputSerialization(ser).setOutputSerialization(ser), null, null)\n+        byte[] osData = os.toByteArray()\n+\n+        then:\n+        notThrown(BlobStorageException)\n+        if (headersPresent) {\n+            assert osData.length == downloadedData.length - 16\n+            /* Account for 16 bytes of header. */\n+            for (int j = 16; j < downloadedData.length; j++) {\n+                assert osData[j - 16] == downloadedData[j]\n+            }\n+        } else {\n+            osData == downloadedData\n+        }\n+\n+        // To calculate the size of data being tested = numCopies * 32 bytes\n+        where:\n+        recordSeparator | columnSeparator | escapeChar | fieldQuote | headersPresent || _\n+        '\\n'            | ','             | '\\0'       | '\\0'       | false          || _ /* Default. */\n+        '\\n'            | ','             | '\\0'       | '\\0'       | true           || _ /* Headers. */\n+        '\\t'            | ','             | '\\0'       | '\\0'       | false          || _ /* Record Separators. */\n+        '\\r'            | ','             | '\\0'       | '\\0'       | false          || _\n+        '<'             | ','             | '\\0'       | '\\0'       | false          || _\n+        '>'             | ','             | '\\0'       | '\\0'       | false          || _\n+        '&'             | ','             | '\\0'       | '\\0'       | false          || _\n+        '\\\\'            | ','             | '\\0'       | '\\0'       | false          || _\n+        ','             | '.'             | '\\0'       | '\\0'       | false          || _ /* Column separator. */\n+//        | ','             | '\\0'       | '\\\\'       | false          || _ /* Field quote. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd"}, "originalPosition": 197}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5afd555b74e1f82ef6a791bc9856a45dd5bda32", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c5afd555b74e1f82ef6a791bc9856a45dd5bda32", "committedDate": "2020-05-18T21:02:34Z", "message": "Modified swagger to use the term Quick Query to Query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f06719b6cf2b5ee330b2f905e4a2da40e0781d66", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f06719b6cf2b5ee330b2f905e4a2da40e0781d66", "committedDate": "2020-05-18T22:21:41Z", "message": "updated abstract class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aabeb305831ffa3f2c95bc1326e2364b8a54d8e8", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/aabeb305831ffa3f2c95bc1326e2364b8a54d8e8", "committedDate": "2020-05-20T00:07:41Z", "message": "flux input stream lock changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e7a6a49990e488159a87768259640f60164ca46", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2e7a6a49990e488159a87768259640f60164ca46", "committedDate": "2020-05-26T17:43:55Z", "message": "Added separator tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84b49796666eef3534f7219dda90e003b214e3c0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/84b49796666eef3534f7219dda90e003b214e3c0", "committedDate": "2020-05-26T18:05:41Z", "message": "Added escape and field tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22041d34a2a3901b94a7de887700606dc01196b6", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/22041d34a2a3901b94a7de887700606dc01196b6", "committedDate": "2020-05-26T18:39:45Z", "message": "rerecorded all tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75d7c2f58172df2b957140c3b3026aee87e61084", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/75d7c2f58172df2b957140c3b3026aee87e61084", "committedDate": "2020-05-26T22:09:09Z", "message": "Modified inputstream logic to error on network and input validation early"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzE1MzU5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-418715359", "createdAt": "2020-05-26T22:12:55Z", "commit": {"oid": "75d7c2f58172df2b957140c3b3026aee87e61084"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94d46e6ce11aa4e1c60e5c263dab7bc8fa794da3", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/94d46e6ce11aa4e1c60e5c263dab7bc8fa794da3", "committedDate": "2020-05-26T22:40:04Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6968bca9722837f0b3369719dfa11a5f1fe4605", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a6968bca9722837f0b3369719dfa11a5f1fe4605", "committedDate": "2020-05-26T23:29:11Z", "message": "Throw via logger"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NDgxNjE4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10641#pullrequestreview-419481618", "createdAt": "2020-05-27T18:04:25Z", "commit": {"oid": "a6968bca9722837f0b3369719dfa11a5f1fe4605"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4477, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}