{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI2MTMxNjUz", "number": 17774, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwNzozNToxNVrOE8jYuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwNzo0MDowOVrOE8jfIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxOTI5Nzg1OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/test/scala/com/azure/cosmos/spark/TestReadE2EMain.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwNzozNToxNVrOH4vUjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOTozMTo1MVrOH5SHzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI1NzYxMg==", "bodyText": "I think we should discuss the format name - \"cosmos.items\" feels a little off to me.. can we go with the style we have in the unified Spark connector in Synapse", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529257612", "createdAt": "2020-11-24T07:35:15Z", "author": {"login": "tknandu"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/test/scala/com/azure/cosmos/spark/TestReadE2EMain.scala", "diffHunk": "@@ -0,0 +1,43 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.SparkSession\n+\n+/** sample test for query */\n+object TestReadE2EMain {\n+  def main(args: Array[String]) {\n+    val cosmosEndpoint = TestConfigurations.HOST\n+    val cosmosMasterKey = TestConfigurations.MASTER_KEY\n+    val cosmosDatabase = \"testDB\"\n+    val cosmosContainer = \"testContainer\"\n+\n+//    val client = new CosmosClientBuilder()\n+//      .endpoint(cosmosEndpoint)\n+//      .key(cosmosMasterKey)\n+//      .consistencyLevel(ConsistencyLevel.EVENTUAL)\n+//      .buildAsyncClient()\n+//\n+//    client.createDatabaseIfNotExists(cosmosDatabase).block()\n+//    client.getDatabase(cosmosDatabase).createContainerIfNotExists(cosmosContainer, \"/id\").block()\n+//    client.close()\n+\n+    val cfg = Map(\"spark.cosmos.accountEndpoint\" -> cosmosEndpoint,\n+      \"spark.cosmos.accountKey\" -> cosmosMasterKey,\n+      \"spark.cosmos.database\" -> cosmosDatabase,\n+      \"spark.cosmos.container\" -> cosmosContainer\n+    )\n+\n+    val spark = SparkSession.builder()\n+      .appName(\"spark connector sample\")\n+      .master(\"local\")\n+      .getOrCreate()\n+\n+    val df = spark.read.format(\"cosmos.items\").options(cfg).load()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d4ecfac5fc349c98201d82d4379a961401363b4"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNzc5MQ==", "bodyText": "let's discuss in the scrum.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529827791", "createdAt": "2020-11-24T19:31:51Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/test/scala/com/azure/cosmos/spark/TestReadE2EMain.scala", "diffHunk": "@@ -0,0 +1,43 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.SparkSession\n+\n+/** sample test for query */\n+object TestReadE2EMain {\n+  def main(args: Array[String]) {\n+    val cosmosEndpoint = TestConfigurations.HOST\n+    val cosmosMasterKey = TestConfigurations.MASTER_KEY\n+    val cosmosDatabase = \"testDB\"\n+    val cosmosContainer = \"testContainer\"\n+\n+//    val client = new CosmosClientBuilder()\n+//      .endpoint(cosmosEndpoint)\n+//      .key(cosmosMasterKey)\n+//      .consistencyLevel(ConsistencyLevel.EVENTUAL)\n+//      .buildAsyncClient()\n+//\n+//    client.createDatabaseIfNotExists(cosmosDatabase).block()\n+//    client.getDatabase(cosmosDatabase).createContainerIfNotExists(cosmosContainer, \"/id\").block()\n+//    client.close()\n+\n+    val cfg = Map(\"spark.cosmos.accountEndpoint\" -> cosmosEndpoint,\n+      \"spark.cosmos.accountKey\" -> cosmosMasterKey,\n+      \"spark.cosmos.database\" -> cosmosDatabase,\n+      \"spark.cosmos.container\" -> cosmosContainer\n+    )\n+\n+    val spark = SparkSession.builder()\n+      .appName(\"spark connector sample\")\n+      .master(\"local\")\n+      .getOrCreate()\n+\n+    val df = spark.read.format(\"cosmos.items\").options(cfg).load()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI1NzYxMg=="}, "originalCommit": {"oid": "5d4ecfac5fc349c98201d82d4379a961401363b4"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxOTMxNDI1OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosScan.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwNzo0MDowOVrOH4veOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOTozMTozMFrOH5SHEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI2MDA4OA==", "bodyText": "Critical for us to land support for FeedRange (single FeedRange covering multiple physical partition by eventual GA) mapping to 1 Spark task since that is core large customer requirement", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529260088", "createdAt": "2020-11-24T07:40:09Z", "author": {"login": "tknandu"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosScan.scala", "diffHunk": "@@ -0,0 +1,34 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.apache.spark.sql.connector.read.{Batch, InputPartition, PartitionReaderFactory, Scan}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+case class CosmosScan(config: Map[String, String], sqlQuerySpec: String)\n+  extends Scan\n+    with Batch\n+    with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def readSchema(): StructType = {\n+    // TODO: moderakh add support for schema inference\n+    // for now schema is hard coded to make TestE2EMain to work\n+    StructType(Seq(StructField(\"number\", IntegerType), StructField(\"word\", StringType)))\n+  }\n+\n+  override def planInputPartitions(): Array[InputPartition] = {\n+    // TODO: moderakh use get feed range?\n+    // for now we are returning one partition hence only one spark task will be created.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d4ecfac5fc349c98201d82d4379a961401363b4"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyNzYwMQ==", "bodyText": "of course. without this we are not prod ready.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17774#discussion_r529827601", "createdAt": "2020-11-24T19:31:30Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosScan.scala", "diffHunk": "@@ -0,0 +1,34 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.apache.spark.sql.connector.read.{Batch, InputPartition, PartitionReaderFactory, Scan}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+case class CosmosScan(config: Map[String, String], sqlQuerySpec: String)\n+  extends Scan\n+    with Batch\n+    with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def readSchema(): StructType = {\n+    // TODO: moderakh add support for schema inference\n+    // for now schema is hard coded to make TestE2EMain to work\n+    StructType(Seq(StructField(\"number\", IntegerType), StructField(\"word\", StringType)))\n+  }\n+\n+  override def planInputPartitions(): Array[InputPartition] = {\n+    // TODO: moderakh use get feed range?\n+    // for now we are returning one partition hence only one spark task will be created.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI2MDA4OA=="}, "originalCommit": {"oid": "5d4ecfac5fc349c98201d82d4379a961401363b4"}, "originalPosition": 22}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2817, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}