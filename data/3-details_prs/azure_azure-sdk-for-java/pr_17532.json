{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIwMDAyMzQz", "number": 17532, "title": "Cosmos spark3 write code path DataSourceV2 skeleton", "bodyText": "This PR\n\nadds the skeleton for write code path DataSourceV2\nadds skeleton for unit tests with basic unit tests for DataFrame to ObjectNode conversion see CosmosRowConverterSpec\nend to end write code path implementation see TestE2EMain this writes to cosmos db.\nDatasource is registered as \"cosmos.items\". name suggestion?\nmodule-info.java was removed (scala doesn't have support for java module system: scala/bug#11423)\n\nval df = Seq(\n  (8, \"bat\"),\n  (64, \"mouse\"),\n  (-27, \"horse\")\n).toDF(\"number\", \"word\")\n\ndf.printSchema()\n\ndf.write.format(\"cosmos.write\").mode(\"append\").options {\n  destCfg\n}.save()\nTODO (come later)\n\nschema inference (schema is hard coded to make TestE2EMain work)\npassing down user config, for now account endpoints etc are hard code to make TestE2EMain work\nmore discussion is required on Row <-> ObjectNode conversion. will come later.\nadd more tests for DataFrame to ObjectNode conversion", "createdAt": "2020-11-12T16:27:22Z", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532", "merged": true, "mergeCommit": {"oid": "744aa1c332dc26f7c2d5f230842a4ee0295aa9a3"}, "closed": true, "closedAt": "2020-11-14T01:29:56Z", "author": {"login": "moderakh"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdb1BC3AH2gAyNTIwMDAyMzQzOjAyNDZmYTYxMTJlN2NhNDNlM2ZhN2JlZWQ0YmRmYTEzZTEzZTVlY2I=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdcQMLjAH2gAyNTIwMDAyMzQzOmI5YWM3YWVjODRmMTIwZTMxMTFiZGQ0YWQ1ZTMwYTVlYmZjZWNkZGM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "0246fa6112e7ca43e3fa7beed4bdfa13e13e5ecb", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0246fa6112e7ca43e3fa7beed4bdfa13e13e5ecb", "committedDate": "2020-11-12T16:19:18Z", "message": "datasource v2 write initial version"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5Mjg4MDY5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529288069", "createdAt": "2020-11-12T16:53:01Z", "commit": {"oid": "0246fa6112e7ca43e3fa7beed4bdfa13e13e5ecb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNjo1MzowMlrOHyEI5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNjo1MzowMlrOHyEI5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjI1ODY2Mw==", "bodyText": "scala doesn't have support for module system:\nscala/bug#11423\nThis file causes compilation problem. hence removed.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522258663", "createdAt": "2020-11-12T16:53:02Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/java/module-info.java", "diffHunk": "@@ -1,6 +0,0 @@\n-// Copyright (c) Microsoft Corporation. All rights reserved.\n-// Licensed under the MIT License.\n-\n-module com.azure.cosmos.spark {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0246fa6112e7ca43e3fa7beed4bdfa13e13e5ecb"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41cef2f11478b411df4270eca2eacb1ad35ff7fb", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/41cef2f11478b411df4270eca2eacb1ad35ff7fb", "committedDate": "2020-11-12T16:54:48Z", "message": "cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6894401fa3bda63a71d126ccf1e4cce8796cd6a", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c6894401fa3bda63a71d126ccf1e4cce8796cd6a", "committedDate": "2020-11-12T17:24:26Z", "message": "handling null value with tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6747aaa384be5a74e7ec76338aaa730bd2bccbeb", "committedDate": "2020-11-12T17:37:28Z", "message": "array support"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDk0NDY3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529494467", "createdAt": "2020-11-12T21:02:07Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMjowN1rOHyOO2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMjowN1rOHyOO2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDAyNw==", "bodyText": "NIT -remove one LF", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522424027", "createdAt": "2020-11-12T21:02:07Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDk5Nzg5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529499789", "createdAt": "2020-11-12T21:10:25Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMDoyNVrOHyOfgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMDoyNVrOHyOfgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyODI4OA==", "bodyText": "Make log_ a lazy val instead?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522428288", "createdAt": "2020-11-12T21:10:25Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+\n+\n+trait CosmosLoggingTrait {\n+  // Make the log field transient so that objects with Logging can\n+  // be serialized and used on another machine\n+  @transient private var log_ : Logger = _ // scalastyle:ignore\n+\n+  // Method to get the logger name for this object\n+  protected def logName: String = {\n+    // Ignore trailing $'s in the class names for Scala objects\n+    this.getClass.getName.stripSuffix(\"$\")\n+  }\n+\n+  // Method to get or create the logger for this object\n+  protected def log: Logger = {\n+    if (log_ == null) {\n+      // scalastyle:ignore", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTAwOTk0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529500994", "createdAt": "2020-11-12T21:12:20Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMjoyMFrOHyOjMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMjoyMFrOHyOjMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTIzMw==", "bodyText": "In my mental model I though about something like\ncosmos.items (which would implement the inetrfaces for read and write(batch and point))\ncosmos.changefeed for changefeed - just read interfaces", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522429233", "createdAt": "2020-11-12T21:12:20Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def inferSchema(caseInsensitiveStringMap: CaseInsensitiveStringMap): StructType = {\n+    getTable(null,\n+      Array.empty[Transform],\n+      caseInsensitiveStringMap.asCaseSensitiveMap()).schema()\n+  }\n+\n+  override def shortName(): String = \"cosmos.write\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTAxMzA1", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529501305", "createdAt": "2020-11-12T21:12:49Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMjo0OVrOHyOkFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMjo0OVrOHyOkFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTQ2Mg==", "bodyText": "CosmosItemsDataSource?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522429462", "createdAt": "2020-11-12T21:12:49Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTAyMDMz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529502033", "createdAt": "2020-11-12T21:13:57Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMzo1N1rOHyOmMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMzo1N1rOHyOmMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMDAwMA==", "bodyText": "I assuem long term we would want to have a cache similar to what I added in the 3.* release for today's OLTP connector? If so I can take a stab at that early next week.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522430000", "createdAt": "2020-11-12T21:13:57Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util.UUID\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.connector.write.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+class CosmosDataWriteFactory extends DataWriterFactory with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def createWriter(i: Int, l: Long): DataWriter[InternalRow] = new CosmosWriter()\n+\n+  class CosmosWriter() extends DataWriter[InternalRow] {\n+    logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+    // TODO moderakh account config and databaseName, containerName need to passed down from the user\n+    val client = new CosmosClientBuilder()\n+      .key(TestConfigurations.MASTER_KEY)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTAzMTQ0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529503144", "createdAt": "2020-11-12T21:15:43Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxNTo0M1rOHyOpuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxNTo0M1rOHyOpuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMDkwNw==", "bodyText": "Looks liek the best approach - to generate it in the spark layer before calling the sdk", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522430907", "createdAt": "2020-11-12T21:15:43Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util.UUID\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.connector.write.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+class CosmosDataWriteFactory extends DataWriterFactory with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def createWriter(i: Int, l: Long): DataWriter[InternalRow] = new CosmosWriter()\n+\n+  class CosmosWriter() extends DataWriter[InternalRow] {\n+    logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+    // TODO moderakh account config and databaseName, containerName need to passed down from the user\n+    val client = new CosmosClientBuilder()\n+      .key(TestConfigurations.MASTER_KEY)\n+      .endpoint(TestConfigurations.HOST)\n+      .consistencyLevel(ConsistencyLevel.EVENTUAL)\n+      .buildAsyncClient();\n+    val databaseName = \"testDB\"\n+    val containerName = \"testContainer\"\n+\n+    override def write(internalRow: InternalRow): Unit = {\n+      // TODO moderakh: schema is hard coded for now to make end to end TestE2EMain work implement schema inference code\n+      val userProvidedSchema = StructType(Seq(StructField(\"number\", IntegerType), StructField(\"word\", StringType)))\n+\n+      val objectNode = CosmosRowConverter.internalRowToObjectNode(internalRow, userProvidedSchema)\n+      // TODO: moderakh how should we handle absence of id?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTA1OTQ4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529505948", "createdAt": "2020-11-12T21:20:14Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyMDoxNFrOHyOyZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyMDoxNFrOHyOyZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMzEyNg==", "bodyText": "Where is this implementation coming from OLAP, built-in Spark connectors like CSVDataDSource? I though Spark also added capability to transform DataFrame forma nd to json - my gut feeling is that it would be good to stick with taht one.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522433126", "createdAt": "2020-11-12T21:20:14Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/RowConverter.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.sql.{Date, Timestamp}\n+import java.util\n+\n+import com.azure.cosmos\n+import com.azure.cosmos.spark.CosmosLoggingTrait\n+import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\n+import com.fasterxml.jackson.databind.node.{ArrayNode, NullNode, ObjectNode}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+import org.apache.spark.sql.catalyst.expressions.UnsafeMapData\n+import org.apache.spark.sql.types.{BinaryType, BooleanType, DataType, DateType, DecimalType, DoubleType, FloatType, IntegerType, LongType, _}\n+//import org.json.{JSONArray, JSONObject}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable.ListBuffer\n+\n+/**\n+ * TODO add more unit tests for this class to CosmosRowConverterSpec.\n+ */\n+\n+object CosmosRowConverter\n+  extends Serializable\n+    with CosmosLoggingTrait {\n+\n+  // TODO moderakh make this configurable\n+  val objectMapper = new ObjectMapper();\n+\n+  def rowToObjectNode(row: Row): ObjectNode = {\n+\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    row.schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(row.get(i), field.dataType, isInternalRow = false)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  def internalRowToObjectNode(internalRow: InternalRow, schema: StructType): ObjectNode = {\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(internalRow.get(i, field.dataType), field.dataType, isInternalRow = true)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  private def addJsonPrimitive(jsonValue: Any, fieldName: String, objectNode : ObjectNode) : Unit = {\n+    jsonValue match {\n+      case element: Boolean => objectNode.put(fieldName, element.asInstanceOf[Boolean])\n+      case element: String => objectNode.put(fieldName, element.asInstanceOf[String])\n+      case element: Double => objectNode.put(fieldName, element.asInstanceOf[Double])\n+      case element: Float => objectNode.put(fieldName, element.asInstanceOf[Float])\n+      case element: Long => objectNode.put(fieldName, element.asInstanceOf[Long])\n+      case element: Int => objectNode.put(fieldName, element.asInstanceOf[Int])\n+      case element: JsonNode => objectNode.set(fieldName, element.asInstanceOf[JsonNode])\n+      case _ => objectNode.putNull(fieldName)\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTA2Njc1", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#pullrequestreview-529506675", "createdAt": "2020-11-12T21:21:25Z", "commit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60196807c2b035889a8e65847fa6fb70abba45d4", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/60196807c2b035889a8e65847fa6fb70abba45d4", "committedDate": "2020-11-12T23:51:13Z", "message": "code review comments, renaming, cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43bd36fe8b5b8a1f8b40bd763a4cf3ef0bdbb457", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/43bd36fe8b5b8a1f8b40bd763a4cf3ef0bdbb457", "committedDate": "2020-11-13T07:39:07Z", "message": "more tests, row conversion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "07720bb8333ce7ddb60434924c1ac8afa46bb543", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/07720bb8333ce7ddb60434924c1ac8afa46bb543", "committedDate": "2020-11-13T19:22:25Z", "message": "added tags to pom files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6058176ad70b63829ebba904dc4389392037ba5d", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6058176ad70b63829ebba904dc4389392037ba5d", "committedDate": "2020-11-13T22:40:11Z", "message": "code style fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa305016145c0f3eccabe7b27967e7d3216bfd24", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/fa305016145c0f3eccabe7b27967e7d3216bfd24", "committedDate": "2020-11-13T22:50:27Z", "message": "added cfg to sample"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f857af2caea63f00edeb14a457118a70e6b20bb0", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f857af2caea63f00edeb14a457118a70e6b20bb0", "committedDate": "2020-11-13T23:07:11Z", "message": "fixed pom depedency tag"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ef2250f1ce299e41bd40e2a763d2409d1a103e1", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2ef2250f1ce299e41bd40e2a763d2409d1a103e1", "committedDate": "2020-11-13T23:15:35Z", "message": "fixed pom depedency tag"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d17bb84d95206049fc69861517e6f81d1a5d6ca9", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d17bb84d95206049fc69861517e6f81d1a5d6ca9", "committedDate": "2020-11-13T23:56:46Z", "message": "fixed a typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9ac7aec84f120e3111bdd4ad5e30a5ebfcecddc", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/b9ac7aec84f120e3111bdd4ad5e30a5ebfcecddc", "committedDate": "2020-11-13T23:58:54Z", "message": "fixed a typo"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 115, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}