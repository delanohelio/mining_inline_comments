{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0MjQzMTAx", "number": 10839, "title": "Implemented Changefeed for Blobs", "bodyText": "Original PR : #9789\nEasiest order to review in AvroReader, LazyDownloader, Chunk, Shard, Segment, Changefeed, ChangefeedPagedFlux, then everything else.", "createdAt": "2020-05-06T17:50:55Z", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839", "merged": true, "mergeCommit": {"oid": "42bc4903bbcd5f54af696e2af6bba1e57ce53eb2"}, "closed": true, "closedAt": "2020-06-15T23:38:47Z", "author": {"login": "gapra-msft"}, "timelineItems": {"totalCount": 150, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABclypYcAH2gAyNDE0MjQzMTAxOjZiM2ZhNjE4ZDA2YjE4YzY3OGUyZWMzZWI2YmNiM2E1MzgzMWQ3Y2Y=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcro8aggH2gAyNDE0MjQzMTAxOjAzMDNhMmNkMWFmMGZlOTgxYjY4YjYxZTVmOTMyOGMyY2NiNDU4YTg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf", "committedDate": "2020-05-28T19:01:44Z", "message": "Changes to make it dependency injection correct"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwNDQyMzk4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-420442398", "createdAt": "2020-05-28T19:50:42Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MDo0MlrOGcENOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MDo0MlrOGcENOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MjIzMg==", "bodyText": "Why specify the scheduler here?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432082232", "createdAt": "2020-05-28T19:50:42Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwNDY3MDIy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-420467022", "createdAt": "2020-05-28T20:28:17Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDoyODoxN1rOGcFkSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDoyODoxN1rOGcFkSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNDUyMQ==", "bodyText": "You might be able to factor out the lambda into your ChunkedDownloadUtils class and then here you concatMap() and in downloadToFile we flatMap()?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432104521", "createdAt": "2020-05-28T20:28:17Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /* TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but\n+       wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc,\n+            Schedulers.immediate())\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;\n+\n+                BlobDownloadAsyncResponse initialResponse = setupTuple3.getT3();\n+                return Flux.range(0, numChunks)\n+                    .concatMap(chunkNum -> { /* TODO (gapra) : This was the biggest difference - downloadToFile does", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b9d82c495b05ca7d37e4ff7174012ef69713605", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/8b9d82c495b05ca7d37e4ff7174012ef69713605", "committedDate": "2020-05-28T21:27:14Z", "message": "Fixed paged flux behavior to not be stateful"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e198352656c6d494a717687fc9ef6280e9a4b16f", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/e198352656c6d494a717687fc9ef6280e9a4b16f", "committedDate": "2020-05-28T21:49:53Z", "message": "Addressed PR feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwNTM0NzAx", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-420534701", "createdAt": "2020-05-28T22:24:46Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoyNDo0NlrOGcI0GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoyNDo0NlrOGcI0GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NzcyMA==", "bodyText": "This type doesn't seem like it's actually lazy to me. While it is sequential, it will still continue to issue download requests as long as the subscription to the data is not cancelled. I haven't seen yet where this is being used, but it will probably need to apply back pressure in order to actually slow down requests and make it lazy.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432157720", "createdAt": "2020-05-28T22:24:46Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,91 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 23}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf0bbdd3ca3041771b8d6bb3cd4d70041763a28f", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cf0bbdd3ca3041771b8d6bb3cd4d70041763a28f", "committedDate": "2020-05-28T22:40:33Z", "message": "Added logic to not read from unfinalized segments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56395682dba5317285284372e78fc3287726e036", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/56395682dba5317285284372e78fc3287726e036", "committedDate": "2020-06-01T21:39:46Z", "message": "ChunkedDownload changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/25cf5e76a2acbf25fa76ecbed858c512467f8401", "committedDate": "2020-06-02T16:31:14Z", "message": "Fixed bug where end time was counted to check segment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIyODk1NDc3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-422895477", "createdAt": "2020-06-02T17:21:40Z", "commit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNzoyMTo0NlrOGd8AFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMjowOTo0MVrOGeFpaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA0NDk0OA==", "bodyText": "Would invert this check FINALIZED.equals(status.asText()). It'll be safer as its known to never NPE.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434044948", "createdAt": "2020-06-02T17:21:46Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "diffHunk": "@@ -0,0 +1,120 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Represents a Segment in Changefeed.\n+ *\n+ * A segment is a blob that points to a manifest file.\n+ * The segment manifest file (meta.json) shows the path of the change feed files for that segment in the\n+ * chunkFilePaths property. (Note: these chunkFilePaths are really shardPaths in this implementation.)\n+ * The chunkFilePaths property looks something like this.\n+ * { ...\n+ * \"chunkFilePaths\": [\n+ *         \"$blobchangefeed/log/00/2019/02/22/1810/\",\n+ *         \"$blobchangefeed/log/01/2019/02/22/1810/\"\n+ *     ],\n+ * ...}\n+ */\n+class Segment {\n+\n+    private final ClientLogger logger = new ClientLogger(Segment.class);\n+\n+    private static final String CHUNK_FILE_PATHS = \"chunkFilePaths\";\n+    private static final String STATUS = \"status\";\n+    private static final String FINALIZED = \"Finalized\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String segmentPath; /* Segment manifest location. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with parent changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ShardFactory shardFactory;\n+\n+    /**\n+     * Creates a new Segment.\n+     */\n+    Segment(BlobContainerAsyncClient client, String segmentPath, ChangefeedCursor cfCursor,\n+        ChangefeedCursor userCursor, ShardFactory shardFactory) {\n+        this.client = client;\n+        this.segmentPath = segmentPath;\n+        this.cfCursor = cfCursor;\n+        this.userCursor = userCursor;\n+        this.shardFactory = shardFactory;\n+    }\n+\n+    /**\n+     * Get all the events for the Segment.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* Download JSON manifest file. */\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(client, segmentPath)\n+            .flatMap(this::parseJson)\n+            /* Parse the JSON for shards. */\n+            .flatMapMany(this::getShards)\n+            /* Get all events for each shard. */\n+            .concatMap(Shard::getEvents);\n+    }\n+\n+    private Mono<JsonNode> parseJson(String json) {\n+        ObjectMapper objectMapper = new ObjectMapper();\n+        try {\n+            JsonNode jsonNode = objectMapper.readTree(json);\n+            return Mono.just(jsonNode);\n+        } catch (IOException e) {\n+            return FluxUtil.monoError(logger, new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private Flux<Shard> getShards(JsonNode node) {\n+\n+        /* Determine if the segment is finalized.\n+           If the segment is not finalized, do not return events for this segment. */\n+        JsonNode status = node.get(STATUS);\n+        if (!status.asText().equals(FINALIZED)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzOTg4NA==", "bodyText": "Current pattern guildelines are putting PagedFlux and PagedIterable implementations into util.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434139884", "createdAt": "2020-06-02T19:55:21Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0MjUxOQ==", "bodyText": "Do we need to download to string? We can pass the raw byte[] into ObjectMapper.readTree (API), which is actually Jackson's recommendation.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434142519", "createdAt": "2020-06-02T20:00:27Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NDEwMw==", "bodyText": "We should create a static instance of ObjectMapper for this class and use .reader() to get a thread safe reader instance to deserialize the JSON into a tree. ObjectMapper has a non-trivial amount of overhead during creation and should be re-used as much as possible.\nhttps://github.com/FasterXML/jackson-docs/wiki/Presentation:-Jackson-Performance", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434144103", "createdAt": "2020-06-02T20:03:45Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE0NTY0OQ==", "bodyText": "Similar comments about reusing ObjectMapper and using byte[] instead of String as the value to pass into the deserializer.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434145649", "createdAt": "2020-06-02T20:06:53Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Segment.java", "diffHunk": "@@ -0,0 +1,120 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Represents a Segment in Changefeed.\n+ *\n+ * A segment is a blob that points to a manifest file.\n+ * The segment manifest file (meta.json) shows the path of the change feed files for that segment in the\n+ * chunkFilePaths property. (Note: these chunkFilePaths are really shardPaths in this implementation.)\n+ * The chunkFilePaths property looks something like this.\n+ * { ...\n+ * \"chunkFilePaths\": [\n+ *         \"$blobchangefeed/log/00/2019/02/22/1810/\",\n+ *         \"$blobchangefeed/log/01/2019/02/22/1810/\"\n+ *     ],\n+ * ...}\n+ */\n+class Segment {\n+\n+    private final ClientLogger logger = new ClientLogger(Segment.class);\n+\n+    private static final String CHUNK_FILE_PATHS = \"chunkFilePaths\";\n+    private static final String STATUS = \"status\";\n+    private static final String FINALIZED = \"Finalized\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String segmentPath; /* Segment manifest location. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with parent changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ShardFactory shardFactory;\n+\n+    /**\n+     * Creates a new Segment.\n+     */\n+    Segment(BlobContainerAsyncClient client, String segmentPath, ChangefeedCursor cfCursor,\n+        ChangefeedCursor userCursor, ShardFactory shardFactory) {\n+        this.client = client;\n+        this.segmentPath = segmentPath;\n+        this.cfCursor = cfCursor;\n+        this.userCursor = userCursor;\n+        this.shardFactory = shardFactory;\n+    }\n+\n+    /**\n+     * Get all the events for the Segment.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* Download JSON manifest file. */\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(client, segmentPath)\n+            .flatMap(this::parseJson)\n+            /* Parse the JSON for shards. */\n+            .flatMapMany(this::getShards)\n+            /* Get all events for each shard. */\n+            .concatMap(Shard::getEvents);\n+    }\n+\n+    private Mono<JsonNode> parseJson(String json) {\n+        ObjectMapper objectMapper = new ObjectMapper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwMjk4NQ==", "bodyText": "Should make this ObjectMapper instance static. For this method should use a writer(), for the deserialize case use a reader().\nDoes the mapper need any configuration to handle empty/null values?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434202985", "createdAt": "2020-06-02T22:09:41Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/ChangefeedCursor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+import java.util.Objects;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a cursor for BlobChangefeed.\n+ */\n+@Fluent\n+public class ChangefeedCursor {\n+\n+    private ClientLogger logger = new ClientLogger(ChangefeedCursor.class);\n+\n+    private String endTime;\n+    private String segmentTime;\n+    private String shardPath;\n+    private String chunkPath;\n+    private long blockOffset;\n+    private long objectBlockIndex;\n+\n+    /**\n+     * Default constructor (used to serialize and deserialize).\n+     */\n+    public ChangefeedCursor() {\n+    }\n+\n+    /**\n+     * Constructor for use by to*Cursor methods.\n+     */\n+    private ChangefeedCursor(String endTime, String segmentTime, String shardPath, String chunkPath, long blockOffset,\n+        long objectBlockIndex) {\n+        this.endTime = endTime;\n+        this.segmentTime = segmentTime;\n+        this.shardPath = shardPath;\n+        this.chunkPath = chunkPath;\n+        this.blockOffset = blockOffset;\n+        this.objectBlockIndex = objectBlockIndex;\n+    }\n+\n+    /**\n+     * Creates a new changefeed level cursor with the specified end time.\n+     *\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    public ChangefeedCursor(OffsetDateTime endTime) {\n+        this(endTime.toString(), null, null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new segment level cursor.\n+     *\n+     * @param segmentTime The {@link OffsetDateTime segment time}.\n+     * @return A new segment level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toSegmentCursor(OffsetDateTime segmentTime) {\n+        return new ChangefeedCursor(this.getEndTime(), segmentTime.toString(), null, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new shard level cursor.\n+     *\n+     * @param shardPath The shard path.\n+     * @return A new shard level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toShardCursor(String shardPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), shardPath, null, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new chunk level cursor.\n+     *\n+     * @param chunkPath The chunk path.\n+     * @return A new chunk level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toChunkCursor(String chunkPath) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), chunkPath, 0, 0);\n+    }\n+\n+    /**\n+     * Creates a new event level cursor.\n+     *\n+     * @param blockOffset The block offset.\n+     * @param objectBlockIndex The object block index.\n+     * @return A new event level {@link ChangefeedCursor cursor}.\n+     */\n+    public ChangefeedCursor toEventCursor(long blockOffset, long objectBlockIndex) {\n+        return new ChangefeedCursor(this.getEndTime(), this.getSegmentTime(), this.getShardPath(), this.getChunkPath(), blockOffset, objectBlockIndex);\n+\n+    }\n+\n+    /**\n+     * @return the end time.\n+     */\n+    public String getEndTime() {\n+        return endTime;\n+    }\n+\n+    /**\n+     * @return the segment time.\n+     */\n+    public String getSegmentTime() {\n+        return segmentTime;\n+    }\n+\n+    /**\n+     * @return the shard path.\n+     */\n+    public String getShardPath() {\n+        return shardPath;\n+    }\n+\n+    /**\n+     * @return the chunk path.\n+     */\n+    public String getChunkPath() {\n+        return chunkPath;\n+    }\n+\n+    /**\n+     * @return the block offset\n+     */\n+    public long getBlockOffset() {\n+        return blockOffset;\n+    }\n+\n+    /**\n+     * @return the object block index.\n+     */\n+    public long getObjectBlockIndex() {\n+        return objectBlockIndex;\n+    }\n+\n+    /**\n+     * @param endTime the end time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setEndTime(String endTime) {\n+        this.endTime = endTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param segmentTime the segment time.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setSegmentTime(String segmentTime) {\n+        this.segmentTime = segmentTime;\n+        return this;\n+    }\n+\n+    /**\n+     * @param shardPath the shard path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setShardPath(String shardPath) {\n+        this.shardPath = shardPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param chunkPath the chunk path.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setChunkPath(String chunkPath) {\n+        this.chunkPath = chunkPath;\n+        return this;\n+    }\n+\n+    /**\n+     * @param blockOffset the block offset.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setBlockOffset(long blockOffset) {\n+        this.blockOffset = blockOffset;\n+        return this;\n+    }\n+\n+    /**\n+     * @param objectBlockIndex the object block index.\n+     * @return the updated BlobChangefeedCursor\n+     */\n+    public ChangefeedCursor setObjectBlockIndex(long objectBlockIndex) {\n+        this.objectBlockIndex = objectBlockIndex;\n+        return this;\n+    }\n+\n+    /**\n+     * Serializes a {@link ChangefeedCursor} into a String.\n+     *\n+     * @return The resulting serialized cursor.\n+     */\n+    public String serialize() {\n+        try {\n+            return new ObjectMapper().writeValueAsString(this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 205}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTAwNzQ1", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423100745", "createdAt": "2020-06-02T22:29:53Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMjoyOTo1NFrOGeGF_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMjoyOTo1NFrOGeGF_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxMDMwMw==", "bodyText": "Should we give a little more detail about how approximate this is?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434210303", "createdAt": "2020-06-02T22:29:54Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,111 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final BlobChangefeedPagedFluxFactory pagedFluxFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        ChangefeedFactory changefeedFactory = new ChangefeedFactory(segmentFactory, client);\n+        this.pagedFluxFactory = new BlobChangefeedPagedFluxFactory(changefeedFactory);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents}\n+     *\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents() {\n+        return getEvents(null, null);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#OffsetDateTime-OffsetDateTime}\n+     *\n+     * @param startTime Filters the results to return events approximately after the start time.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTEyNzAx", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423112701", "createdAt": "2020-06-02T23:00:11Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzowMDoxMVrOGeGtKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzowMDoxMVrOGeGtKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIyMDMyOA==", "bodyText": "Formatting", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434220328", "createdAt": "2020-06-02T23:00:11Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundDownToNearestHour\"() {\n+        expect:\n+        TimeUtils.roundDownToNearestHour(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 17, 20, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundUpToNearestHour\"() {\n+        expect:\n+        TimeUtils.roundUpToNearestHour(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 1, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 17, 21, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 23, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 3, 18, 0, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"roundDownToNearestYear\"() {\n+        expect:\n+        TimeUtils.roundDownToNearestYear(time) == roundedTime\n+\n+        where:\n+        time                                                            || roundedTime\n+        null                                                            || null\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)       || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        OffsetDateTime.of(2020, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC)   || OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+    }\n+\n+    def \"validTime\"() {\n+        expect:\n+        TimeUtils.validTimes(current, start, end) == valid\n+\n+        where:\n+        start | current | end  || valid\n+        /* Null checks. */\n+        null | null  | null || false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null | null  || false\n+        null | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null  || false\n+        null | null || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) || false\n+        /* All equal. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Increasing. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        OffsetDateTime.of(2019, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 6, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 8, 10, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Decreasing. */\n+        OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2020, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+    }\n+\n+    def \"validSegment\"() {\n+        expect:\n+        TimeUtils.validSegment(segment, start, end) == valid\n+\n+        where:\n+        start | segment | end  || valid\n+        /* Null checks. */\n+        null | null  | null || false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | null | null  || false\n+        null | \"idx/segments/2019/11/02/1700/meta.json\" | null  || false\n+        null | null || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) || false\n+        /* All equal. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2019/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Increasing. */\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2019/01/01/0000/meta.json\" | OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        OffsetDateTime.of(2019, 3, 17, 20, 25, 30, 0, ZoneOffset.UTC) | \"idx/segments/2019/06/01/0000/meta.json\" | OffsetDateTime.of(2019, 8, 10, 0, 0, 0, 0, ZoneOffset.UTC) | true\n+        /* Decreasing. */\n+        OffsetDateTime.of(2021, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2020/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+        OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | \"idx/segments/2020/01/01/0000/meta.json\" | OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC) | false\n+    }\n+\n+    def \"validYear\"() {\n+        expect:\n+        TimeUtils.validYear(year, start, end) == valid\n+\n+        where:\n+        start | year | end  || valid", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTI2NjA3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423126607", "createdAt": "2020-06-02T23:37:59Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzozNzo1OVrOGeHaKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzozNzo1OVrOGeHaKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzMTg1MA==", "bodyText": "Unfished", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434231850", "createdAt": "2020-06-02T23:37:59Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/models/BlobChangefeedEventWrapper.java", "diffHunk": "@@ -0,0 +1,41 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.models;\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Represents a wrapper to store a BlobChangefeedEvent along with the BlobChangefeedCursor associated with it.\n+ * This wrapper is required since the paging functionality does not have any information about where the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTI5NDgw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423129480", "createdAt": "2020-06-02T23:46:38Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzo0NjozOFrOGeHjeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzo0NjozOFrOGeHjeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNDIzMw==", "bodyText": "Is there any need to test a path like idx/segments/2019/11/02/1730/meta.json at all?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434234233", "createdAt": "2020-06-02T23:46:38Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/TimeUtilsTest.groovy", "diffHunk": "@@ -0,0 +1,126 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils\n+import spock.lang.Specification\n+\n+import java.time.OffsetDateTime\n+import java.time.ZoneOffset\n+\n+class TimeUtilsTest extends Specification {\n+\n+    def \"convertPathToTime\"() {\n+        expect:\n+        TimeUtils.convertPathToTime(path) == time\n+\n+        where:\n+        path                                     || time\n+        null                                     || null\n+        \"idx/segments/2019\"                      || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/\"                     || OffsetDateTime.of(2019, 1, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11\"                   || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/\"                  || OffsetDateTime.of(2019, 11, 1, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02\"                || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/\"               || OffsetDateTime.of(2019, 11, 2, 0, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700\"           || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/\"          || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)\n+        \"idx/segments/2019/11/02/1700/meta.json\" || OffsetDateTime.of(2019, 11, 2, 17, 0, 0, 0, ZoneOffset.UTC)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTMzNTM3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423133537", "createdAt": "2020-06-02T23:59:15Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzo1OToxNVrOGeHw-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMzo1OToxNVrOGeHw-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIzNzY5MA==", "bodyText": "You might be able to use a Spy from Spock to help check that you call download the right number of times.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434237690", "createdAt": "2020-06-02T23:59:15Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderTest.groovy", "diffHunk": "@@ -0,0 +1,131 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.core.util.FluxUtil\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.models.BlobStorageException\n+import com.azure.storage.common.implementation.Constants\n+import reactor.core.publisher.Flux\n+import spock.lang.Unroll\n+\n+import java.nio.ByteBuffer\n+\n+class BlobLazyDownloaderTest extends APISpec {\n+\n+    BlobAsyncClient bc\n+    BlobLazyDownloaderFactory factory\n+\n+    def setup() {\n+        def cc = primaryBlobServiceAsyncClient.getBlobContainerAsyncClient(generateContainerName())\n+        cc.create().block()\n+        bc = cc.getBlobAsyncClient(generateBlobName())\n+        factory = new BlobLazyDownloaderFactory()\n+    }\n+\n+    byte[] downloadHelper(BlobLazyDownloader downloader) {\n+        OutputStream os = downloader.download()\n+            .reduce(new ByteArrayOutputStream(),  { outputStream, buffer ->\n+            outputStream.write(FluxUtil.byteBufferToArray(buffer))\n+            return outputStream;\n+        }).block()\n+        return os.toByteArray()\n+    }\n+\n+    byte[] uploadHelper(int size) {\n+        def input = getRandomByteArray(size)\n+        def data = Flux.just(ByteBuffer.wrap(input))\n+        bc.upload(data, null).block()\n+        return input\n+    }\n+\n+    @Unroll\n+    def \"download blockSize\"() {\n+        setup:\n+        byte[] input = uploadHelper(size)\n+\n+        when:\n+        byte[] output = downloadHelper(new BlobLazyDownloader(bc, blockSize, 0))\n+\n+        then:\n+        output == input", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzI0MDUz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423724053", "createdAt": "2020-06-03T16:21:42Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjoyMTo0M1rOGejqFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjoyMTo0M1rOGejqFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5NDY3Nw==", "bodyText": "Why are you using a method reference instead of just calling verifyWrapper directly?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434694677", "createdAt": "2020-06-03T16:21:43Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzMzNzA2", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423733706", "createdAt": "2020-06-03T16:33:12Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjozMzoxM1rOGekH0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjozMzoxM1rOGekH0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwMjI5MQ==", "bodyText": "Why are you or'ing with true?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434702291", "createdAt": "2020-06-03T16:33:13Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzM3MTEy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-423737112", "createdAt": "2020-06-03T16:37:22Z", "commit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjozNzoyMlrOGekSag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjozNzoyMlrOGekSag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcwNTAwMg==", "bodyText": "Can you add some descriptions about why you're expecting this behavior?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434705002", "createdAt": "2020-06-03T16:37:22Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/ChunkTest.groovy", "diffHunk": "@@ -0,0 +1,209 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobAsyncClient\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.internal.avro.implementation.AvroObject\n+import com.azure.storage.internal.avro.implementation.AvroReader\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+import spock.lang.Specification\n+import spock.lang.Unroll\n+\n+import static org.mockito.ArgumentMatchers.*\n+import static org.mockito.Mockito.*\n+\n+class ChunkTest extends Specification {\n+\n+    BlobContainerAsyncClient mockContainer\n+    BlobAsyncClient mockBlob\n+    AvroReaderFactory mockAvroReaderFactory\n+    AvroReader mockAvroReader\n+    BlobLazyDownloaderFactory mockBlobLazyDownloaderFactory\n+    BlobLazyDownloader mockBlobLazyDownloader\n+\n+    String chunkPath = \"chunkPath\"\n+    ChangefeedCursor chunkCursor\n+\n+    List<BlobChangefeedEvent> mockEvents\n+    List<Map<String, Object>> mockRecords\n+    List<AvroObject> mockAvroObjects\n+\n+    def setup() {\n+        setupEvents()\n+        chunkCursor = new ChangefeedCursor(\"endTime\", \"segmentTime\", \"shardPath\", chunkPath, 0, 0)\n+\n+        mockContainer = mock(BlobContainerAsyncClient.class)\n+        mockBlob = mock(BlobAsyncClient.class)\n+        mockAvroReaderFactory = mock(AvroReaderFactory.class)\n+        mockAvroReader = mock(AvroReader.class)\n+        mockBlobLazyDownloaderFactory = mock(BlobLazyDownloaderFactory.class)\n+        mockBlobLazyDownloader = mock(BlobLazyDownloader.class)\n+\n+        when(mockContainer.getBlobAsyncClient(anyString()))\n+            .thenReturn(mockBlob)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong(), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+        /* The data returned by the lazy downloader does not matter since we're mocking avroObjects.  */\n+        when(mockBlobLazyDownloader.download())\n+            .thenReturn(Flux.empty())\n+        when(mockAvroReader.readAvroObjects())\n+            .thenReturn(Flux.fromIterable(mockAvroObjects))\n+    }\n+\n+    /* Tests no user cursor. */\n+    def \"getEvents min\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class)))\n+            .thenReturn(mockAvroReader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, 0, 0)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_BODY_SIZE, 0) || true\n+        verify(mockBlobLazyDownloader).download() || true\n+        verify(mockAvroReaderFactory).getAvroReader(Flux.empty()) || true\n+        verify(mockAvroReader).readAvroObjects() || true\n+    }\n+\n+    /* Tests user cursor. */\n+    @Unroll\n+    def \"getEvents cursor\"() {\n+        setup:\n+        when(mockAvroReaderFactory.getAvroReader(any(Flux.class), any(Flux.class), anyLong(), anyLong()))\n+            .thenReturn(mockAvroReader)\n+        when(mockBlobLazyDownloaderFactory.getBlobLazyDownloader(any(BlobAsyncClient.class), anyLong()))\n+            .thenReturn(mockBlobLazyDownloader)\n+\n+        when:\n+        ChunkFactory factory = new ChunkFactory(mockAvroReaderFactory, mockBlobLazyDownloaderFactory, mockContainer)\n+        Chunk chunk = factory.getChunk(chunkPath, chunkCursor, blockOffset, objectBlockIndex)\n+        def sv = StepVerifier.create(chunk.getEvents().index())\n+\n+        then:\n+        sv.expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 1234, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 1) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 2) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 5678, 3) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 0) })\n+            .expectNextMatches({ tuple2 -> this.&verifyWrapper(tuple2.getT2(), tuple2.getT1(), 9101, 1) })\n+            .verifyComplete()\n+\n+        verify(mockContainer).getBlobAsyncClient(chunkPath) || true\n+        verify(mockBlobLazyDownloaderFactory).getBlobLazyDownloader(mockBlob, ChunkFactory.DEFAULT_HEADER_SIZE) || true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b3fa618d06b18c678e2ec3eb6bcb3a53831d7cf"}, "originalPosition": 118}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5682f9b91a30d0a5f9c92abbe9d9666251ce632f", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5682f9b91a30d0a5f9c92abbe9d9666251ce632f", "committedDate": "2020-06-03T22:21:05Z", "message": "Changed String to byte array"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12ccd730e0feaa548d86dea9788216b258e5faf9", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/12ccd730e0feaa548d86dea9788216b258e5faf9", "committedDate": "2020-06-03T23:06:19Z", "message": "moved files to correct location"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/08e457fbad1cee680597a2d225fdb603aaf37e5e", "committedDate": "2020-06-03T23:21:25Z", "message": "Fixed all json parsing comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/544d129e1b1c5fae7f60245ed464ba9c1ed64b4e", "committedDate": "2020-06-03T23:58:44Z", "message": "more PR comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0MDAzMzA0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-424003304", "createdAt": "2020-06-03T23:25:49Z", "commit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzoyNTo0OVrOGew13Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQwMDozODo1MlrOGeyBEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxMDY4NQ==", "bodyText": "in case the eventWrappers completes without emitting any data then .last() operator will throw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434910685", "createdAt": "2020-06-03T23:25:49Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()\n+            /* Window the events to the page size. This takes the Flux<BlobChangefeedEventWrapper> and\n+               transforms it into a Flux<Flux<BlobChangefeedEventWrapper>>, where the internal Fluxes can have at most\n+               preferredPageSize elements. */\n+            .window(preferredPageSize)\n+            /* Convert the BlobChangefeedEventWrappers into BlobChangefeedEvents, and bundle them up with the last\n+               element's cursor. */\n+            .flatMap(eventWrappers -> {\n+                /* 1. cache the Flux to turn it into a HotFlux so we can subscribe to it multiple times. */\n+                Flux<BlobChangefeedEventWrapper> cachedEventWrappers = eventWrappers.cache();\n+                /* 2. Get the last element in the flux and grab it's cursor. This will be the continuationToken\n+                      returned to the user if they want to get the next page. */\n+                Mono<ChangefeedCursor> c = cachedEventWrappers.last()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzkwMw==", "bodyText": "If I understand correctly, when downstream subscription happens, the changefeed.getEvents() is the api that result in \"network calls\" to produce Flux<BlobChangefeedEventWrapper>.\nIf the code inside the below flatMap does not make any \"network calls\" and flatMap is there to only unwrap Flux<Flux> then I think concatMap should be sufficient. flatMap comes with some overhead to manage concurrency.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434917903", "createdAt": "2020-06-03T23:52:06Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.time.OffsetDateTime;\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final ClientLogger logger = new ClientLogger(BlobChangefeedPagedFlux.class);\n+\n+    private final ChangefeedFactory changefeedFactory;\n+    private final OffsetDateTime startTime;\n+    private final OffsetDateTime endTime;\n+    private final String cursor;\n+\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, OffsetDateTime startTime, OffsetDateTime endTime) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.cursor = null;\n+    }\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     */\n+    BlobChangefeedPagedFlux(ChangefeedFactory changefeedFactory, String cursor) {\n+        StorageImplUtils.assertNotNull(\"changefeedFactory\", changefeedFactory);\n+        this.changefeedFactory = changefeedFactory;\n+        this.startTime = null;\n+        this.endTime = null;\n+        this.cursor = cursor;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return FluxUtil.fluxError(logger, new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return FluxUtil.fluxError(logger, new IllegalArgumentException(\"preferredPageSize > 0 required but \"\n+                + \"provided: \" + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);\n+\n+        Changefeed changefeed;\n+        if (cursor != null) {\n+            changefeed = changefeedFactory.getChangefeed(cursor);\n+        } else {\n+            changefeed = changefeedFactory.getChangefeed(startTime, endTime);\n+        }\n+\n+        return changefeed.getEvents()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e457fbad1cee680597a2d225fdb603aaf37e5e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMDY2Ng==", "bodyText": "just curious, is the reason for making the download call still is to get a ByteBuffer of size 0 or something else?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434920666", "createdAt": "2020-06-04T00:02:08Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,72 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private final BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range\n+            -> client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.downloadFirstChunk(range, options, requestConditions, downloadFunc)\n+            .flatMapMany(setupTuple3 -> {\n+                long newCount = setupTuple3.getT1();\n+                BlobRequestConditions finalConditions = setupTuple3.getT2();\n+\n+                int numChunks = ChunkedDownloadUtils.calculateNumBlocks(newCount, options.getBlockSizeLong());\n+\n+                // In case it is an empty blob, this ensures we still actually perform a download operation.\n+                numChunks = numChunks == 0 ? 1 : numChunks;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyMzQ1Mg==", "bodyText": "is it the case that when change feed is enabled then storage account will have some special container to store them? so its absence means change-feed-not-enabled?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434923452", "createdAt": "2020-06-04T00:12:57Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,144 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.FluxUtil;\n+import com.azure.core.util.logging.ClientLogger;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private final ClientLogger logger = new ClientLogger(Changefeed.class);\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+    private static final ObjectMapper mapper = new ObjectMapper();\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .then(populateLastConsumable())\n+            .thenMany(listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return FluxUtil.monoError(logger, new RuntimeException(\"Changefeed has not been enabled for \"\n+                        + \"this account.\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkyOTkzNg==", "bodyText": "this pass var get shared across all the subscribers of the returned Flux<String> instance, if that is not intended then we can wrap filter in a defer.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r434929936", "createdAt": "2020-06-04T00:38:52Z", "author": {"login": "anuchandy"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events (i.e. blockOffset = 0). */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0NTUxMzgx", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-424551381", "createdAt": "2020-06-04T15:11:10Z", "commit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNToxMToxMFrOGfKu6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNTozMzowM1rOGfLt1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNDg5MA==", "bodyText": "nit: After giving it second thought I think this could go up to to the builder - the highest possible place. So that we have DI all the way hierarchy.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435334890", "createdAt": "2020-06-04T15:11:10Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,114 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        this.changefeedFactory = new ChangefeedFactory(segmentFactory, client);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjkzOA==", "bodyText": "I'd elaborate more about how cursor works in this and similar javadoc. I.e. we'll get all events from where cursor is pointing to forward.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435336938", "createdAt": "2020-06-04T15:14:00Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedAsyncClient.java", "diffHunk": "@@ -0,0 +1,114 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClient;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.BlobContainerClientBuilder;\n+import com.azure.storage.blob.BlobServiceVersion;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * This class provides a client that contains all operations that apply to Azure Storage Blob changefeed.\n+ *\n+ * @see BlobChangefeedClientBuilder\n+ */\n+@ServiceClient(builder = BlobChangefeedClientBuilder.class, isAsync = true)\n+public class BlobChangefeedAsyncClient {\n+\n+    static final String CHANGEFEED_CONTAINER_NAME = \"$blobchangefeed\";\n+\n+    private final BlobContainerAsyncClient client;\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedClientBuilder}.\n+     *\n+     * @param pipeline The pipeline used to send and receive service requests.\n+     * @param url The endpoint where to send service requests.\n+     * @param version The version of the service to receive requests.\n+     */\n+    BlobChangefeedAsyncClient(HttpPipeline pipeline, String url, BlobServiceVersion version) {\n+        this.client = new BlobContainerClientBuilder()\n+            .endpoint(url)\n+            .containerName(CHANGEFEED_CONTAINER_NAME)\n+            .pipeline(pipeline)\n+            .serviceVersion(version)\n+            .buildAsyncClient();\n+        AvroReaderFactory avroReaderFactory = new AvroReaderFactory();\n+        BlobLazyDownloaderFactory blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+        ChunkFactory chunkFactory = new ChunkFactory(avroReaderFactory, blobLazyDownloaderFactory, client);\n+        ShardFactory shardFactory = new ShardFactory(chunkFactory, client);\n+        SegmentFactory segmentFactory = new SegmentFactory(shardFactory, client);\n+        this.changefeedFactory = new ChangefeedFactory(segmentFactory, client);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents}\n+     *\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents() {\n+        return getEvents(null, null);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#OffsetDateTime-OffsetDateTime}\n+     *\n+     * @param startTime Filters the results to return events approximately after the start time. Note: A few events\n+     * belonging to the previous hour can also be returned. A few events belonging to this hour can be missing; to\n+     * ensure all events from the hour are returned, round the start time down by an hour.\n+     * @param endTime Filters the results to return events approximately before the end time. Note: A few events\n+     * belonging to the next hour can also be returned. A few events belonging to this hour can be missing; to ensure\n+     * all events from the hour are returned, round the end time up by an hour.\n+     * @return A reactive response emitting the changefeed events.\n+     */\n+    public BlobChangefeedPagedFlux getEvents(OffsetDateTime startTime, OffsetDateTime endTime) {\n+        return new BlobChangefeedPagedFlux(changefeedFactory, startTime, endTime);\n+    }\n+\n+    /**\n+     * Returns a reactive Publisher emitting all the changefeed events for this account lazily as needed.\n+     *\n+     * <p>\n+     * Changefeed events are returned in approximate temporal order.\n+     *\n+     * <p>For more information, see the\n+     * <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal\">Azure Docs</a>.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedAsyncClient.getEvents#String}\n+     *\n+     * @param cursor Identifies the portion of the events to be returned with the next get operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM0MDU4OA==", "bodyText": "@alzimmermsft That's quite stretching definition of what util is. https://www.vojtechruzicka.com/avoid-utility-classes/ .\nI'd expect util package to contain \"Utility Classes\". PagedFlux and PagedIterable and BlobChangefeedPagedFlux are quite well defined and thus packaging them could use better naming.\nIn general naming things util isn't bringing much value into understanding what they do http://ralin.io/blog/oop-anti-patterns-utility-or-helper-classes.html .", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435340588", "createdAt": "2020-06-04T15:18:56Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,125 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzOTg4NA=="}, "originalCommit": {"oid": "25cf5e76a2acbf25fa76ecbed858c512467f8401"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM0MzcwOQ==", "bodyText": "did you make any change? I can still see the old diff here.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435343709", "createdAt": "2020-06-04T15:23:06Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+\n+/**\n+ * Factory class for {@link BlobLazyDownloader}.\n+ */\n+class BlobLazyDownloaderFactory {\n+\n+    /**\n+     * Gets a new instance of a BlobLazyDownloader.\n+     *\n+     * @param client The blob client.\n+     * @param blockSize The block size to download.\n+     * @param offset The offset to start downloading from.\n+     * @return {@link BlobLazyDownloader}\n+     */\n+    BlobLazyDownloader getBlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Mjk5NQ=="}, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM1MDk5Ng==", "bodyText": "nit. I'd just call it read . One can deduct rest from return type.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r435350996", "createdAt": "2020-06-04T15:33:03Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-internal-avro/src/main/java/com/azure/storage/internal/avro/implementation/AvroReader.java", "diffHunk": "@@ -0,0 +1,13 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.internal.avro.implementation;\n+\n+import reactor.core.publisher.Flux;\n+\n+/**\n+ * An interface that represents an AvroReader.\n+ */\n+public interface AvroReader {\n+    Flux<AvroObject> readAvroObjects();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "544d129e1b1c5fae7f60245ed464ba9c1ed64b4e"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d0d24c45e1af16eb1029fe8922ed93addfc0350", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7d0d24c45e1af16eb1029fe8922ed93addfc0350", "committedDate": "2020-06-04T17:31:22Z", "message": "Added changes from PR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bc2adc3beeb0c7a040b6805667394b8f2baeb57", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2bc2adc3beeb0c7a040b6805667394b8f2baeb57", "committedDate": "2020-06-04T17:34:51Z", "message": "Fixed bug from .NET"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0NjgwNzI3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-424680727", "createdAt": "2020-06-04T17:39:39Z", "commit": {"oid": "2bc2adc3beeb0c7a040b6805667394b8f2baeb57"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "177eca5ba08c70332140d690835093b780c0d1ed", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/177eca5ba08c70332140d690835093b780c0d1ed", "committedDate": "2020-06-04T19:35:08Z", "message": "Fixed tests and addressed more comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "700e22dadd41753fd8b925eb910662daeadd7a86", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/700e22dadd41753fd8b925eb910662daeadd7a86", "committedDate": "2020-06-04T21:48:49Z", "message": "Wrapped filter in Flux.defer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ba62a21ceee324cc191c0053c923c3e872643d9", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/4ba62a21ceee324cc191c0053c923c3e872643d9", "committedDate": "2020-06-04T21:51:09Z", "message": "renamed mapper"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6bf75924fd758b0d113740da53044b63e0f731a5", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6bf75924fd758b0d113740da53044b63e0f731a5", "committedDate": "2020-06-04T22:25:57Z", "message": "Added logger to download util"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d00e5b63499a2b9243f92d3a3e5273d8b25db76", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7d00e5b63499a2b9243f92d3a3e5273d8b25db76", "committedDate": "2020-06-05T19:42:51Z", "message": "Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a252c0c964f2e2c3ed314def51a411e8b6a5f415", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a252c0c964f2e2c3ed314def51a411e8b6a5f415", "committedDate": "2020-06-09T16:59:30Z", "message": "Added last few feedback changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55adba601a69631a653317825ccdbd7e1dbf5ed5", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/55adba601a69631a653317825ccdbd7e1dbf5ed5", "committedDate": "2020-06-12T18:30:54Z", "message": "Merge branch 'feature/storage/stg73' into storage/changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a8fbb5f73114c62b1d5c2c7db0cfe29972b2aa4b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a8fbb5f73114c62b1d5c2c7db0cfe29972b2aa4b", "committedDate": "2020-06-12T18:32:41Z", "message": "removed TODo from readme"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f282d903c72d5f1835bfc075cb324088c8d4a2f7", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f282d903c72d5f1835bfc075cb324088c8d4a2f7", "committedDate": "2020-06-12T18:35:22Z", "message": "docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1888c4ce3571dbb51cc3e0ce5177e38ba47aaacd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1888c4ce3571dbb51cc3e0ce5177e38ba47aaacd", "committedDate": "2020-06-12T19:21:05Z", "message": "api docs fixed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "039bc5de6ad1bae946aa9a99d8784322ce6dcd3a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/039bc5de6ad1bae946aa9a99d8784322ce6dcd3a", "committedDate": "2020-06-12T19:57:23Z", "message": "unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53a4854807b8804d6b448d3445d96ad804efd39b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/53a4854807b8804d6b448d3445d96ad804efd39b", "committedDate": "2020-06-12T20:10:47Z", "message": "accidentally deleted wrong thing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea1035475a54fd684a56e0a38bbae5c2f1e58b6a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ea1035475a54fd684a56e0a38bbae5c2f1e58b6a", "committedDate": "2020-06-12T21:28:11Z", "message": "Fixed Readme sample diff"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5638fbf8c3edfd1f5fcd1df00cade01d9205ebcc", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5638fbf8c3edfd1f5fcd1df00cade01d9205ebcc", "committedDate": "2020-06-12T21:34:14Z", "message": "open blob to changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f9a961b375af8f8fd72414ac2cdfbd72c4f696c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6f9a961b375af8f8fd72414ac2cdfbd72c4f696c", "committedDate": "2020-06-12T21:59:18Z", "message": "Checkstyle fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7336002e50d8bbaece42284d671646753caaedbe", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7336002e50d8bbaece42284d671646753caaedbe", "committedDate": "2020-06-12T22:51:40Z", "message": "Aded opens in changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8eff9c9e61aa1e10379c892329594ae33244417a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/8eff9c9e61aa1e10379c892329594ae33244417a", "committedDate": "2020-06-15T21:02:47Z", "message": "Made getter boolean instead of Boolean"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a8ef9ed09e59c505f047d7cfc6e08b80f58d9ef", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2a8ef9ed09e59c505f047d7cfc6e08b80f58d9ef", "committedDate": "2020-06-15T21:03:16Z", "message": "Changed get to is"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "717d7c91a0be1ef3883ee716e399b65e71855f9c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/717d7c91a0be1ef3883ee716e399b65e71855f9c", "committedDate": "2020-06-15T21:30:35Z", "message": "Added opens to pom"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMDMyNDY5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-431032469", "createdAt": "2020-06-15T22:19:24Z", "commit": {"oid": "2a8ef9ed09e59c505f047d7cfc6e08b80f58d9ef"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0cab043e8c041a6e428ab5c819588b42ec875b83", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0cab043e8c041a6e428ab5c819588b42ec875b83", "committedDate": "2020-06-15T22:20:44Z", "message": "Renamed lazy to chunked"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59b7eed30c1463434edc2ee4f659c93dc66e9949", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/59b7eed30c1463434edc2ee4f659c93dc66e9949", "committedDate": "2020-06-15T22:47:42Z", "message": "Refactored code to allow for context passing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0303a2cd1af0fe981b68b61e5f9328c2ccb458a8", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0303a2cd1af0fe981b68b61e5f9328c2ccb458a8", "committedDate": "2020-06-15T23:07:01Z", "message": "renamed test recordings"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9369a62d220c4b1fa033c6cc75e84907a2e595c1", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/9369a62d220c4b1fa033c6cc75e84907a2e595c1", "committedDate": "2020-04-28T21:17:32Z", "message": "Implemented an Avro Parser"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eb955cc40a9fd16efff0fae26c022239ead0dbf", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1eb955cc40a9fd16efff0fae26c022239ead0dbf", "committedDate": "2020-04-28T21:37:49Z", "message": "Added changes to pass CI"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c5b444647eb0136581ecaea15a63bb3f03c8bad", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5c5b444647eb0136581ecaea15a63bb3f03c8bad", "committedDate": "2020-04-28T22:31:21Z", "message": "Added more excludes due to false positives"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a62fa7f567e97af090af03d1cb551f43a53c06b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/4a62fa7f567e97af090af03d1cb551f43a53c06b", "committedDate": "2020-04-28T22:49:43Z", "message": "Fixed excludes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "faab6f3c2a3382b4d7c770e33856e05d7fc712fc", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/faab6f3c2a3382b4d7c770e33856e05d7fc712fc", "committedDate": "2020-04-29T17:17:14Z", "message": "Regenerated for quick query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb2e98eeabef5329c1d629941783a63e8e6c4f8b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cb2e98eeabef5329c1d629941783a63e8e6c4f8b", "committedDate": "2020-04-29T20:38:21Z", "message": "Added tests and code for blob quick query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93ed5b61459373a6fbb418c09086d73e92aa433a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/93ed5b61459373a6fbb418c09086d73e92aa433a", "committedDate": "2020-04-29T22:48:54Z", "message": "Moved qq to blob base and included snapshot test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ccb0cdd004adc9ec9392bbe36072ced567fd93b1", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ccb0cdd004adc9ec9392bbe36072ced567fd93b1", "committedDate": "2020-04-29T22:49:32Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f0d3a9a8625295d5a4c2b4ca56864deb4d66f1a2", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f0d3a9a8625295d5a4c2b4ca56864deb4d66f1a2", "committedDate": "2020-04-30T00:00:56Z", "message": "Added test for OS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "594d62b19d5f3a966d9e1a9e1072a3ccf9b3dfd7", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/594d62b19d5f3a966d9e1a9e1072a3ccf9b3dfd7", "committedDate": "2020-04-30T17:26:14Z", "message": "Added samples"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f799610e72930a1d022c484dad280bd1534fed6", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/3f799610e72930a1d022c484dad280bd1534fed6", "committedDate": "2020-04-30T21:43:17Z", "message": "Added datalake and reocrdings"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ef18d8b56d8a0c7fb845bf24d97bc13801ebed4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2ef18d8b56d8a0c7fb845bf24d97bc13801ebed4", "committedDate": "2020-04-30T21:45:26Z", "message": "removed avro test file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc0b18494949405051686bd79dca2136f8582507", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/dc0b18494949405051686bd79dca2136f8582507", "committedDate": "2020-04-30T22:42:56Z", "message": "Added files for query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1cae0e10415c60c9fe72eb29873d2575de285a3c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1cae0e10415c60c9fe72eb29873d2575de285a3c", "committedDate": "2020-04-30T22:50:57Z", "message": "Fixed code snippets"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4eae9abd042cea677f0c8a5d1eb499fc273a0ab", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/b4eae9abd042cea677f0c8a5d1eb499fc273a0ab", "committedDate": "2020-04-30T23:11:25Z", "message": "Added more samples"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "725c00758f840e6052cbb743f36a8ae7e3d5e321", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/725c00758f840e6052cbb743f36a8ae7e3d5e321", "committedDate": "2020-04-30T23:45:42Z", "message": "More doc fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26bf158cd98fcc472d0da3e105f0c3a79d103faa", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/26bf158cd98fcc472d0da3e105f0c3a79d103faa", "committedDate": "2020-05-01T00:02:20Z", "message": "throw through logger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9271b5dbd56061592aa2afcf727d7e191cb670d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d9271b5dbd56061592aa2afcf727d7e191cb670d", "committedDate": "2020-05-01T00:16:54Z", "message": "Ovveride encrypted client to not support query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a767c0ee138aeec0121563ee915388962b122a0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0a767c0ee138aeec0121563ee915388962b122a0", "committedDate": "2020-05-01T00:24:31Z", "message": "more flux is exception logger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30d4862d284b8dc1facd7bd2f4abffbf6f80abcc", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/30d4862d284b8dc1facd7bd2f4abffbf6f80abcc", "committedDate": "2020-05-01T00:54:30Z", "message": "Added safe locking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1edeef9367b189a7e692be81dfed9add514f683c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1edeef9367b189a7e692be81dfed9add514f683c", "committedDate": "2020-05-01T01:07:22Z", "message": "Added extra lock"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c34708b142a76fbfa3e7914d1af0cb8e76d0f4e4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c34708b142a76fbfa3e7914d1af0cb8e76d0f4e4", "committedDate": "2020-05-01T14:27:33Z", "message": "Added stuff for CI"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "856c969baa43ae5910655dfd7280bcdcb94ea708", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/856c969baa43ae5910655dfd7280bcdcb94ea708", "committedDate": "2020-05-01T14:57:15Z", "message": "CI stuff"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9be84843281ca57a42d10255d5659a6c6fec66d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/e9be84843281ca57a42d10255d5659a6c6fec66d", "committedDate": "2020-05-01T19:01:26Z", "message": "Changefeed initial commit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fc0a75a37861868e014ed5ecc52f71e8627238b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0fc0a75a37861868e014ed5ecc52f71e8627238b", "committedDate": "2020-05-01T19:04:02Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9f5edd2e315d8ec9bb43086922a6df5df6d8444", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a9f5edd2e315d8ec9bb43086922a6df5df6d8444", "committedDate": "2020-05-02T00:08:54Z", "message": "Added tests for Chunk"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77d291a068b663ae83384af433fe718a907e56d4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/77d291a068b663ae83384af433fe718a907e56d4", "committedDate": "2020-05-04T20:07:44Z", "message": "Added Lazy download"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40737ca0b7e88e837e6806b96f0e00e6f71a2031", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/40737ca0b7e88e837e6806b96f0e00e6f71a2031", "committedDate": "2020-05-05T00:10:50Z", "message": "Added code for the parser to return the index and start of block"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e990651996437f4841887b67c312e5a3abd9e931", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/e990651996437f4841887b67c312e5a3abd9e931", "committedDate": "2020-05-05T16:26:59Z", "message": "Added a changefeed small test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c565b5b6d17e2f26dc0e60c19d066e5ecc303a79", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c565b5b6d17e2f26dc0e60c19d066e5ecc303a79", "committedDate": "2020-05-05T19:42:29Z", "message": "Added Avro Reader and AvroObject types for ease of use"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a83cbca3f83f893251d78a2ae76d446eb440761d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a83cbca3f83f893251d78a2ae76d446eb440761d", "committedDate": "2020-05-06T17:39:53Z", "message": "Added implementation of new cursor. Untested."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf3c54fb8e740f5a2b274594d9532bc427c3ffe2", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/bf3c54fb8e740f5a2b274594d9532bc427c3ffe2", "committedDate": "2020-05-06T17:49:16Z", "message": "Merge branch 'feature/storage/stg73' into storage/changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41943c9c2d69d54211c72389ecc308553821c01d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/41943c9c2d69d54211c72389ecc308553821c01d", "committedDate": "2020-05-07T21:27:38Z", "message": "Added tests for Chunk"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8998e283591dae8a997844c87ca4a0cb09dccd77", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/8998e283591dae8a997844c87ca4a0cb09dccd77", "committedDate": "2020-05-11T18:41:59Z", "message": "Added tests for Chunk and more documentation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ace649afba6cd69bccbb270f3d1076cb6a3d6cb", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5ace649afba6cd69bccbb270f3d1076cb6a3d6cb", "committedDate": "2020-05-12T00:09:00Z", "message": "Added tests for Cf"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa2a0eca3876381baf52eede759dd38fd5457935", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/fa2a0eca3876381baf52eede759dd38fd5457935", "committedDate": "2020-05-12T00:21:14Z", "message": "Added cursor docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8355ee4ea8f21b3757679a17f0343fb67db7433", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c8355ee4ea8f21b3757679a17f0343fb67db7433", "committedDate": "2020-05-12T18:21:49Z", "message": "Added more tests more docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db5fc0c2435d1ec237ff32e8063038c89085e211", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/db5fc0c2435d1ec237ff32e8063038c89085e211", "committedDate": "2020-05-12T18:47:45Z", "message": "Added docs to Changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e41771fd51b0ffb73884078c95c579c9d4fec63", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/2e41771fd51b0ffb73884078c95c579c9d4fec63", "committedDate": "2020-05-12T19:00:01Z", "message": "Added code snippets"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba1132189c5635c1bb88654d9fd659feae176d6b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ba1132189c5635c1bb88654d9fd659feae176d6b", "committedDate": "2020-05-12T22:25:44Z", "message": "Everything but segment test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7dfe7acd678a2323d9a6ad8631a1b6783813d05f", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7dfe7acd678a2323d9a6ad8631a1b6783813d05f", "committedDate": "2020-05-12T23:44:44Z", "message": "Added very basic Segment tests ---- THIS IS THE POINT AT WHICH THE COMPLEX CURSOR ECISTS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c", "committedDate": "2020-05-13T19:16:19Z", "message": "Added all tests + new cursor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7bf395fdfc02b2c002a9101cb092238a1272c4d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d7bf395fdfc02b2c002a9101cb092238a1272c4d", "committedDate": "2020-05-13T21:58:10Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d3819f3cb3deea49d63d89591c3e411c4769319", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1d3819f3cb3deea49d63d89591c3e411c4769319", "committedDate": "2020-05-13T22:07:08Z", "message": "regenerated"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc7fcef822f4c32bcd43b60cd0180e2b599cfbfa", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cc7fcef822f4c32bcd43b60cd0180e2b599cfbfa", "committedDate": "2020-05-13T22:23:01Z", "message": "Bumped avro reactor version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "213a4173d7bf59ef03d053310c3ebec1b5401f31", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/213a4173d7bf59ef03d053310c3ebec1b5401f31", "committedDate": "2020-05-13T23:26:43Z", "message": "Added binary recordings for qq"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10d84f04e1b84d741b110a137cec51ef4aa3cde0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/10d84f04e1b84d741b110a137cec51ef4aa3cde0", "committedDate": "2020-05-14T00:01:22Z", "message": "Added binary recordings for datalake"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3dddb59cd9436c9673ab4eb48e1c03f42cb66cfe", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/3dddb59cd9436c9673ab4eb48e1c03f42cb66cfe", "committedDate": "2020-05-14T17:14:22Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31a43d592085371137dcd7c6e715918b648f4fad", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/31a43d592085371137dcd7c6e715918b648f4fad", "committedDate": "2020-05-14T17:20:39Z", "message": "Addressed my comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d36d397a83b0e08a894bff01d4531ceb5013fcd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/0d36d397a83b0e08a894bff01d4531ceb5013fcd", "committedDate": "2020-05-14T17:43:56Z", "message": "Added some imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1df9ea99b26637cd38ac8e7a46ae7e297c223cdd", "committedDate": "2020-05-14T18:28:32Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ae13a7e91ca78b6311dfc9c4af01c50e127cbff", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5ae13a7e91ca78b6311dfc9c4af01c50e127cbff", "committedDate": "2020-05-14T18:39:04Z", "message": "Added semicolon"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c1e2a0b22a3f07e2cab87d1833a90fb8daecb04", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/8c1e2a0b22a3f07e2cab87d1833a90fb8daecb04", "committedDate": "2020-05-14T19:22:32Z", "message": "Added change to try and fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "894d77ab4d30475e1d238bb6aba2dccbf1c18536", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/894d77ab4d30475e1d238bb6aba2dccbf1c18536", "committedDate": "2020-05-14T19:40:33Z", "message": "imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a7b279cf1ae52e2717b16d83f0a6a8daaffe664", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1a7b279cf1ae52e2717b16d83f0a6a8daaffe664", "committedDate": "2020-05-14T19:43:15Z", "message": "potential fix for no error found"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3d0dc63f1505d080b0875919195f5206a7f9f75", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d3d0dc63f1505d080b0875919195f5206a7f9f75", "committedDate": "2020-05-14T21:19:21Z", "message": "Fixed datalake test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c831acb690de37a1da0b7e70f354f94efb3b87d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/6c831acb690de37a1da0b7e70f354f94efb3b87d", "committedDate": "2020-05-14T21:43:46Z", "message": "addressed some comments and mock IA tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "809e3d68b1fbd7b0ab7be6f38041e5e132b51e4e", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/809e3d68b1fbd7b0ab7be6f38041e5e132b51e4e", "committedDate": "2020-05-14T22:05:27Z", "message": "Mock not compatible with java 9+"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db30342799b9d64f13d2d8577a9a7d1bf05c8a4c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/db30342799b9d64f13d2d8577a9a7d1bf05c8a4c", "committedDate": "2020-05-14T22:24:34Z", "message": "Added docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1400710d00c999789865097c4a751f7230429d35", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1400710d00c999789865097c4a751f7230429d35", "committedDate": "2020-05-14T23:32:51Z", "message": "changes to tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNzUyMzk0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-412752394", "createdAt": "2020-05-15T15:31:39Z", "commit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "state": "COMMENTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTozNjoxNFrOGWJ52Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNjowNjo1NlrOGWK_dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NDEyMQ==", "bodyText": "this can be static constant.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425884121", "createdAt": "2020-05-15T15:36:14Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NTMzNA==", "bodyText": "Maybe we should interpret continuationToken as cursor  here ? both are strings and are effectively same thing -  opaque string that let's you resume iteration.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425885334", "createdAt": "2020-05-15T15:38:21Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjIxNg==", "bodyText": "I'd rather throw if preferredPageSize is too big. May spare surprise for someone trying to understand what's going on.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425886216", "createdAt": "2020-05-15T15:39:52Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private final Integer defaultPageSize = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, defaultPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, defaultPageSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MDA3MQ==", "bodyText": "I think client is more of dependency required to create these things rather than a parameter (I guess we use same client to create all of these things). If so then this should rather be passed in constructor to this factory.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425890071", "createdAt": "2020-05-15T15:46:19Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxFactory.java", "diffHunk": "@@ -0,0 +1,51 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * Factory class for {@link BlobChangefeedPagedFlux}.\n+ */\n+class BlobChangefeedPagedFluxFactory {\n+\n+    private final ChangefeedFactory changefeedFactory;\n+\n+    /**\n+     * Creates a default instance of the BlobChangefeedPagedFluxFactory.\n+     */\n+    BlobChangefeedPagedFluxFactory() {\n+        this.changefeedFactory = new ChangefeedFactory();\n+    }\n+\n+    /**\n+     * Creates a BlobChangefeedPagedFluxFactory with the designated factories.\n+     */\n+    BlobChangefeedPagedFluxFactory(ChangefeedFactory changefeedFactory) {\n+        this.changefeedFactory = changefeedFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param client The {@link BlobContainerAsyncClient changefeed client}.\n+     * @param startTime The {@link OffsetDateTime start time}.\n+     * @param endTime The {@link OffsetDateTime end time}.\n+     */\n+    BlobChangefeedPagedFlux getBlobChangefeedPagedFlux(BlobContainerAsyncClient client, OffsetDateTime startTime,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MDUwNQ==", "bodyText": "Do we need this type?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425890505", "createdAt": "2020-05-15T15:47:04Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedIterable.java", "diffHunk": "@@ -0,0 +1,26 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedIterable;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedIterable} for Changefeed where the continuation token type is\n+ * {@link String}, the element type is {@link BlobChangefeedEvent}, and the page type is\n+ * {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedIterable extends ContinuablePagedIterable<String,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTE4Mw==", "bodyText": "Oh, nice. So here we say that continuation token is cursor. Then we should honor it back as continuation token in the other place were we now throw.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425891183", "createdAt": "2020-05-15T15:48:17Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedResponse.java", "diffHunk": "@@ -0,0 +1,64 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.IterableStream;\n+import com.azure.core.util.paging.ContinuablePage;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+\n+import java.util.List;\n+\n+/**\n+ * Represents a page returned in BlobChangefeed.\n+ *\n+ * <p>A {@link BlobChangefeedPagedResponse} consists of {@link BlobChangefeedEvent} elements and {@link String}\n+ * continuation token (known as a cursor in Changefeed). </p>\n+ *\n+ * <p>A cursor can be used to re-initialize a BlobChangefeed to point to the next expected page. </p>\n+ *\n+ * @see BlobChangefeedPagedFlux\n+ * @see BlobChangefeedPagedIterable\n+ */\n+public class BlobChangefeedPagedResponse implements ContinuablePage<String, BlobChangefeedEvent> {\n+\n+    private final List<BlobChangefeedEvent> events;\n+    private final ChangefeedCursor cursor;\n+\n+    /**\n+     * Package-private constructor for use by {@link BlobChangefeedPagedFlux}\n+     * @param events A {@link List} of {@link BlobChangefeedEvent BlobChangefeedEvents}.\n+     * @param cursor A {@link ChangefeedCursor cursor}.\n+     */\n+    BlobChangefeedPagedResponse(List<BlobChangefeedEvent> events, ChangefeedCursor cursor) {\n+        this.events = events;\n+        this.cursor = cursor;\n+    }\n+\n+    /**\n+     * @inheritDoc\n+     */\n+    public IterableStream<BlobChangefeedEvent> getElements() {\n+        return new IterableStream<>(this.events);\n+    }\n+\n+    /**\n+     * Gets a {@link List} of elements in the page.\n+     *\n+     * @return A {@link List} containing the elements in the page.\n+     */\n+    public List<BlobChangefeedEvent> getValue() {\n+        return this.events;\n+    }\n+\n+    /**\n+     * Gets a reference to the next page, should you want to re-initialize the BlobChangefeed.\n+     *\n+     * @return The {@link String cursor} that references the next page.\n+     */\n+    public String getContinuationToken() {\n+        /* Serialize the cursor and return it to the user as a String. */\n+        return cursor.serialize();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5MTk3OQ==", "bodyText": "What does getSetupMono mean ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425891979", "createdAt": "2020-05-15T15:49:43Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloader.java", "diffHunk": "@@ -0,0 +1,88 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.http.rest.ResponseBase;\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.implementation.util.ChunkedDownloadUtils;\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Schedulers;\n+\n+import java.nio.ByteBuffer;\n+import java.util.function.Function;\n+\n+/**\n+ * FOR INTERNAL USE ONLY.\n+ * Class to lazily download a blob.\n+ */\n+class BlobLazyDownloader {\n+\n+    private final BlobAsyncClient client; /* Client to download from. */\n+    private final long blockSize; /* The block size. */\n+    private BlobRange range;\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download the rest of a blob at a certain offset.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {\n+        this.client = client;\n+        this.blockSize = blockSize;\n+        this.range = new BlobRange(offset);\n+    }\n+\n+    /**\n+     * Creates a new BlobLazyDownloader to download a partial blob.\n+     */\n+    BlobLazyDownloader(BlobAsyncClient client, long totalSize) {\n+        this.client = client;\n+        this.blockSize = totalSize;\n+        this.range = new BlobRange(0, totalSize);\n+    }\n+\n+    /*TODO (gapra) : It may be possible to unduplicate the code below as well to share between downloadToFile but wasnt immediately obvious to me */\n+    public Flux<ByteBuffer> download() {\n+        ParallelTransferOptions options = new ParallelTransferOptions()\n+            .setBlockSizeLong(blockSize);\n+        BlobRequestConditions requestConditions = new BlobRequestConditions();\n+\n+        Function<BlobRange, Mono<BlobDownloadAsyncResponse>> downloadFunc = range ->\n+            client.downloadWithResponse(range, null, new BlobRequestConditions(), false);\n+\n+        return ChunkedDownloadUtils.getSetupMono(range, options, requestConditions, downloadFunc,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Mjk5NQ==", "bodyText": "same here about client being dependency rather than parameter.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425892995", "createdAt": "2020-05-15T15:51:21Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobLazyDownloaderFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+\n+/**\n+ * Factory class for {@link BlobLazyDownloader}.\n+ */\n+class BlobLazyDownloaderFactory {\n+\n+    /**\n+     * Gets a new instance of a BlobLazyDownloader.\n+     *\n+     * @param client The blob client.\n+     * @param blockSize The block size to download.\n+     * @param offset The offset to start downloading from.\n+     * @return {@link BlobLazyDownloader}\n+     */\n+    BlobLazyDownloader getBlobLazyDownloader(BlobAsyncClient client, long blockSize, long offset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NDg5OQ==", "bodyText": "same feedback about client being parameter.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425894899", "createdAt": "2020-05-15T15:54:27Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChangefeedFactory.java", "diffHunk": "@@ -0,0 +1,55 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * Factory class for {@link ChangefeedFactory}.\n+ */\n+class ChangefeedFactory {\n+\n+    private final SegmentFactory segmentFactory;\n+\n+    /**\n+     * Creates a default instance of the ChangefeedFactory.\n+     */\n+    ChangefeedFactory() {\n+        this.segmentFactory = new SegmentFactory();\n+    }\n+\n+    /**\n+     * Creates a SegmentFactory with the designated factories.\n+     */\n+    ChangefeedFactory(SegmentFactory segmentFactory) {\n+        this.segmentFactory = segmentFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of a Changefeed.\n+     */\n+    Changefeed getChangefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTI0OQ==", "bodyText": "do we want to validate other parameters ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425895249", "createdAt": "2020-05-15T15:55:03Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Chunk.java", "diffHunk": "@@ -0,0 +1,56 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import reactor.core.publisher.Flux;\n+\n+/**\n+ * A class that represents a Chunk in Changefeed.\n+ *\n+ * A chunk is an append blob that contains avro encoded changefeed events.\n+ */\n+class Chunk {\n+\n+    private final String chunkPath; /* Chunk path. */\n+    private final ChangefeedCursor shardCursor; /* Cursor associated with parent shard. */\n+    private final AvroReader avroReader;\n+\n+    /**\n+     * Creates a new Chunk.\n+     */\n+    Chunk(String chunkPath, ChangefeedCursor shardCursor, AvroReader avroReader) {\n+        StorageImplUtils.assertNotNull(\"avroReader\", avroReader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTY1NQ==", "bodyText": "same feedback on client", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425895655", "createdAt": "2020-05-15T15:55:41Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {\n+        this.avroReaderFactory = new AvroReaderFactory();\n+        this.blobLazyDownloaderFactory = new BlobLazyDownloaderFactory();\n+    }\n+\n+    /**\n+     * Creates a ChunkFactory with the designated factories.\n+     */\n+    ChunkFactory(AvroReaderFactory avroReaderFactory, BlobLazyDownloaderFactory blobLazyDownloaderFactory) {\n+        this.avroReaderFactory = avroReaderFactory;\n+        this.blobLazyDownloaderFactory = blobLazyDownloaderFactory;\n+    }\n+\n+    /**\n+     * Gets a new instance of a Chunk.\n+     *\n+     * @param client The changefeed container client.\n+     * @param chunkPath The path to the chunk blob.\n+     * @param shardCursor The parent shard cursor.\n+     * @param blockOffset The offset of the block to start reading from. If 0, this indicates we should read the whole\n+     *                    avro file from the beginning.\n+     * @param objectBlockIndex The index of the last object in the block that was returned to the user.\n+     * @return {@link Chunk}\n+     */\n+    Chunk getChunk(BlobContainerAsyncClient client, String chunkPath, ChangefeedCursor shardCursor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5NTk0OQ==", "bodyText": "isn't changefeed format locked by us?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425895949", "createdAt": "2020-05-15T15:56:12Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5Njc5Mw==", "bodyText": "Do we need parameter-less constructors? I can imagine we could create all these factories in some top level class and use pure DI from there.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425896793", "createdAt": "2020-05-15T15:57:37Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/ChunkFactory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobAsyncClient;\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.common.implementation.Constants;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+import com.azure.storage.internal.avro.implementation.AvroReader;\n+import com.azure.storage.internal.avro.implementation.AvroReaderFactory;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Factory class for {@link Chunk}.\n+ */\n+class ChunkFactory {\n+\n+    /* TODO (gapra): This should probably be configurable by a user. */\n+    private static final long DEFAULT_HEADER_SIZE = 4 * Constants.KB;\n+    private static final long DEFAULT_BODY_SIZE = Constants.MB;\n+\n+    private final AvroReaderFactory avroReaderFactory;\n+    private final BlobLazyDownloaderFactory blobLazyDownloaderFactory;\n+\n+    /**\n+     * Creates a default instance of the ChunkFactory.\n+     */\n+    ChunkFactory() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg5OTYxNQ==", "bodyText": "What is HelperSpec ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425899615", "createdAt": "2020-05-15T16:02:33Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFluxTest.groovy", "diffHunk": "@@ -0,0 +1,244 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor\n+\n+import reactor.core.publisher.Flux\n+import reactor.test.StepVerifier\n+\n+import java.time.OffsetDateTime\n+\n+import static org.mockito.ArgumentMatchers.any\n+import static org.mockito.Mockito.*\n+\n+class BlobChangefeedPagedFluxTest extends HelperSpec {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTIwNw==", "bodyText": "Should this be MockedChangeFeedSpec or something more concrete?\nhttps://softwareengineering.stackexchange.com/questions/247267/what-is-a-helper-is-it-a-design-pattern-is-it-an-algorithm", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425901207", "createdAt": "2020-05-15T16:05:31Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/java/com/azure/storage/blob/changefeed/HelperSpec.groovy", "diffHunk": "@@ -0,0 +1,68 @@\n+package com.azure.storage.blob.changefeed\n+\n+import com.azure.core.util.FluxUtil\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventData\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEventType\n+import com.azure.storage.blob.models.BlobType\n+import reactor.core.publisher.Flux\n+import spock.lang.Specification\n+\n+import java.nio.ByteBuffer\n+import java.nio.channels.AsynchronousFileChannel\n+import java.nio.file.Path\n+import java.nio.file.Paths\n+import java.nio.file.StandardOpenOption\n+import java.time.OffsetDateTime\n+\n+class HelperSpec extends Specification {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTUzOA==", "bodyText": "what does it do?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425901538", "createdAt": "2020-05-15T16:06:07Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/test/resources/mockito-extensions/org.mockito.plugins.MockMaker", "diffHunk": "@@ -0,0 +1 @@\n+mock-maker-inline", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTkwMTk0Mg==", "bodyText": "Should this be called downloadFirstChunk?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r425901942", "createdAt": "2020-05-15T16:06:56Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-blob/src/main/java/com/azure/storage/blob/implementation/util/ChunkedDownloadUtils.java", "diffHunk": "@@ -0,0 +1,116 @@\n+package com.azure.storage.blob.implementation.util;\n+\n+import com.azure.storage.blob.models.BlobDownloadAsyncResponse;\n+import com.azure.storage.blob.models.BlobErrorCode;\n+import com.azure.storage.blob.models.BlobRange;\n+import com.azure.storage.blob.models.BlobRequestConditions;\n+import com.azure.storage.blob.models.BlobStorageException;\n+import com.azure.storage.blob.models.ParallelTransferOptions;\n+import reactor.core.publisher.Mono;\n+import reactor.core.scheduler.Scheduler;\n+import reactor.util.function.Tuple3;\n+\n+import java.util.function.Function;\n+\n+import static java.lang.StrictMath.toIntExact;\n+\n+/**\n+ * This class provides helper methods for lazy/chunked download.\n+ *\n+ * RESERVED FOR INTERNAL USE.\n+ */\n+public class ChunkedDownloadUtils {\n+\n+    /*\n+    Download the first chunk. Construct a Mono which will emit the total count for calculating the number of chunks,\n+    access conditions containing the etag to lock on, and the response from downloading the first chunk.\n+     */\n+    public static Mono<Tuple3<Long, BlobRequestConditions, BlobDownloadAsyncResponse>> getSetupMono(BlobRange range,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe4c9101d283e1628a05a1f75f921ecb1ca4ee8c"}, "originalPosition": 28}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c27dada6785e357d40fc729e77a12c1cefe1ef4", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1c27dada6785e357d40fc729e77a12c1cefe1ef4", "committedDate": "2020-05-15T17:04:31Z", "message": "Added weird char tests for rec sep"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c9da64f63e9e71e33e385549f717dd4634eaeea", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7c9da64f63e9e71e33e385549f717dd4634eaeea", "committedDate": "2020-05-15T18:13:44Z", "message": "Modified to accoutn for new line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "309ce91eb4191f5d7877275bc336695c4aa1c2d0", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/309ce91eb4191f5d7877275bc336695c4aa1c2d0", "committedDate": "2020-05-15T20:09:52Z", "message": "Moved blob query logic to implementation class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc3dc842f19f768362ddd3b244a67338e5b51074", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/dc3dc842f19f768362ddd3b244a67338e5b51074", "committedDate": "2020-05-15T21:31:09Z", "message": "Addressed some comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f415fecefeab32f9bad2381f7607c81657ddd4e9", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f415fecefeab32f9bad2381f7607c81657ddd4e9", "committedDate": "2020-05-15T22:06:32Z", "message": "Converted receiver to Consumer interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "319b4e1ab9d90169ebee4c4ddd7313c0287ced79", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/319b4e1ab9d90169ebee4c4ddd7313c0287ced79", "committedDate": "2020-05-18T17:24:49Z", "message": "Addressed more comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f78b723531119286aa88cfd41cc5ef1a4e9c72c6", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f78b723531119286aa88cfd41cc5ef1a4e9c72c6", "committedDate": "2020-05-18T17:28:54Z", "message": "Merge branch 'feature/storage/stg73' into storage/quickquery"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13fcf2ee1285db2ac87223d185b180cc9fee03dd", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/13fcf2ee1285db2ac87223d185b180cc9fee03dd", "committedDate": "2020-05-18T19:10:54Z", "message": "Added heades"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf0b18d033d37d7a009ca2dffeebcbb135e4c729", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/bf0b18d033d37d7a009ca2dffeebcbb135e4c729", "committedDate": "2020-05-18T19:16:37Z", "message": "Added headers and changelog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5afd555b74e1f82ef6a791bc9856a45dd5bda32", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/c5afd555b74e1f82ef6a791bc9856a45dd5bda32", "committedDate": "2020-05-18T21:02:34Z", "message": "Modified swagger to use the term Quick Query to Query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f06719b6cf2b5ee330b2f905e4a2da40e0781d66", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/f06719b6cf2b5ee330b2f905e4a2da40e0781d66", "committedDate": "2020-05-18T22:21:41Z", "message": "updated abstract class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99ef04504a10aaed079d59f19dd8511e8e10dc76", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/99ef04504a10aaed079d59f19dd8511e8e10dc76", "committedDate": "2020-05-18T22:59:23Z", "message": "Merge branch 'storage/quickquery' into storage/changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df578a75f72f07a35091f35491f59156b722694c", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/df578a75f72f07a35091f35491f59156b722694c", "committedDate": "2020-05-19T00:12:22Z", "message": "Fixed build"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "060c530f5b725a9280754de986f92176de59230b", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/060c530f5b725a9280754de986f92176de59230b", "committedDate": "2020-05-19T15:49:56Z", "message": "Updated versions in pom for changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7b8b240f65b55e2f383a2638474c3a582fa9dd9", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/e7b8b240f65b55e2f383a2638474c3a582fa9dd9", "committedDate": "2020-05-19T16:20:04Z", "message": "Added documentation to types in Avro"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "984bff0ede7d79fcaedb3ebd957dda9b5ff96d4e", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/984bff0ede7d79fcaedb3ebd957dda9b5ff96d4e", "committedDate": "2020-05-19T16:50:18Z", "message": "Fixed some blob analyze issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14457c145a783206f909230003ca39887afe3bf5", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/14457c145a783206f909230003ca39887afe3bf5", "committedDate": "2020-05-19T17:15:46Z", "message": "some changefeed analyze issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18edde3d194b9e990771af4620df97a239e6b049", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/18edde3d194b9e990771af4620df97a239e6b049", "committedDate": "2020-05-19T17:32:15Z", "message": "Added changes to fix build issue with IOException"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a63219ed4aade114949b9d74625fa6ee828bf983", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a63219ed4aade114949b9d74625fa6ee828bf983", "committedDate": "2020-05-19T17:51:35Z", "message": "changefeed analyze"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed9fc001c87322ae8845fe87b1373d395071a4ea", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ed9fc001c87322ae8845fe87b1373d395071a4ea", "committedDate": "2020-05-19T18:20:37Z", "message": "more analyze"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca0a2971cf2f232ff4d16355eefdcd6e2875b197", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ca0a2971cf2f232ff4d16355eefdcd6e2875b197", "committedDate": "2020-05-19T18:44:15Z", "message": "Added recordings for changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "709667e6e6ff192c749c4605990514928858f5a6", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/709667e6e6ff192c749c4605990514928858f5a6", "committedDate": "2020-05-19T21:36:43Z", "message": "Addressed comments from review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d101eb8c18f3e3270f24fe79f4dc802041c6df39", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d101eb8c18f3e3270f24fe79f4dc802041c6df39", "committedDate": "2020-05-19T21:37:45Z", "message": "line length"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d8b01a2760da45211c78e0ce3398f17b272fa19d", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/d8b01a2760da45211c78e0ce3398f17b272fa19d", "committedDate": "2020-05-19T21:46:20Z", "message": "Fixed static issue in PagedFlux"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a269964bec41bdd53fc418f8468dcbe86f8de3f3", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/a269964bec41bdd53fc418f8468dcbe86f8de3f3", "committedDate": "2020-05-19T22:06:15Z", "message": "cap constant"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f28a58ee241c6005a2ee41fac54e50e0f8a877a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7f28a58ee241c6005a2ee41fac54e50e0f8a877a", "committedDate": "2020-05-19T23:17:45Z", "message": "lazy downloader removed comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88bb03462d2b2b7fe36d6830a5929b6ae83a0da1", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/88bb03462d2b2b7fe36d6830a5929b6ae83a0da1", "committedDate": "2020-05-20T00:15:30Z", "message": "modified pom"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97f836e7344152b2575f6b43e2af770269506d15", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/97f836e7344152b2575f6b43e2af770269506d15", "committedDate": "2020-05-20T16:39:29Z", "message": "Added more opens"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5134371bebad6b686506742212fcf57ae34e5fc1", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5134371bebad6b686506742212fcf57ae34e5fc1", "committedDate": "2020-05-20T17:00:23Z", "message": "Opened some common and avro modules"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69f5ae06551b4b35291e8b182113f5458bcc1330", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/69f5ae06551b4b35291e8b182113f5458bcc1330", "committedDate": "2020-05-20T17:23:20Z", "message": "exports impl models changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcfc1099b7c5dfde6649164ff1c180228ae1693f", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/dcfc1099b7c5dfde6649164ff1c180228ae1693f", "committedDate": "2020-05-26T21:03:53Z", "message": "Fixed java 8 compile warning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be34424d4e03d41cb019c495c8651fb1cbaaa2e5", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/be34424d4e03d41cb019c495c8651fb1cbaaa2e5", "committedDate": "2020-05-26T21:28:44Z", "message": "removed List.of to be java 8 compatible"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad8682747d004153b57b6fed01e94392760d3187", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ad8682747d004153b57b6fed01e94392760d3187", "committedDate": "2020-05-27T17:40:32Z", "message": "Added a few README points"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fec5a73abf4b221eced1b51442f734b1f1a01bf9", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/fec5a73abf4b221eced1b51442f734b1f1a01bf9", "committedDate": "2020-05-27T17:48:01Z", "message": "more readme"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e9b6d0bb7a52ba69c26d5b11f8520800028552a", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/1e9b6d0bb7a52ba69c26d5b11f8520800028552a", "committedDate": "2020-05-27T18:34:46Z", "message": "Merge branch 'feature/storage/stg73' into storage/changefeed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15b8726a4bc90b2e20de8e89504a4d1db169ceda", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/15b8726a4bc90b2e20de8e89504a4d1db169ceda", "committedDate": "2020-05-27T19:03:04Z", "message": "Cleaned up imports, comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE5NTM4NjEy", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-419538612", "createdAt": "2020-05-27T19:20:53Z", "commit": {"oid": "1e9b6d0bb7a52ba69c26d5b11f8520800028552a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15f0631404c080526797ab12338026b0e4621f39", "author": {"user": null}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/15f0631404c080526797ab12338026b0e4621f39", "committedDate": "2020-05-27T20:29:23Z", "message": "Added more throwing"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwMjk2MzA0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#pullrequestreview-420296304", "createdAt": "2020-05-28T16:38:15Z", "commit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNjozODoxNVrOGb9mBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxNzo0ODo1MlrOGcAI2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3Mzg5NQ==", "bodyText": "If the package is changed from internal.avro to implementation.avro this won't be needed as implementation is always excluded from Javadocs.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431973895", "createdAt": "2020-05-28T16:38:15Z", "author": {"login": "alzimmermsft"}, "path": "sdk/parents/azure-client-sdk-parent/pom.xml", "diffHunk": "@@ -534,6 +534,7 @@\n               com.azure.core.test*:\n               com.azure.endtoend*:\n               com.azure.perf*\n+              com.azure.storage.internal.avro*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NTM3Ng==", "bodyText": "Would version ever be null? If it is ever null that is likely an issue where the default isn't being set elsewhere.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431975376", "createdAt": "2020-05-28T16:40:34Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedClientBuilder.java", "diffHunk": "@@ -0,0 +1,73 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.annotation.ServiceClientBuilder;\n+import com.azure.core.http.HttpPipeline;\n+import com.azure.storage.blob.BlobServiceAsyncClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceVersion;\n+\n+/**\n+ * This class provides a fluent builder API to help aid the configuration and instantiation of\n+ * {@link BlobChangefeedClient BlobChangefeedClients} and {@link BlobChangefeedAsyncClient BlobChangefeedAsyncClients}\n+ * when {@link #buildClient() buildClient} and {@link #buildAsyncClient() buildAsyncClient} are called respectively.\n+ */\n+@ServiceClientBuilder(serviceClients = {BlobChangefeedClient.class, BlobChangefeedAsyncClient.class})\n+public final class BlobChangefeedClientBuilder {\n+\n+    private final String accountUrl;\n+    private final HttpPipeline pipeline;\n+    private final BlobServiceVersion version;\n+\n+    /**\n+     * Constructs the {@link BlobChangefeedClientBuilder} from the URL and pipeline of the {@link BlobServiceClient}.\n+     *\n+     * @param client {@link BlobServiceClient} whose properties are used to configure the builder.\n+     */\n+    public BlobChangefeedClientBuilder(BlobServiceClient client) {\n+        this.accountUrl = client.getAccountUrl();\n+        this.pipeline = client.getHttpPipeline();\n+        this.version = client.getServiceVersion();\n+    }\n+\n+    /**\n+     * Constructs the {@link BlobChangefeedClientBuilder} from from the URL and pipeline of the\n+     * {@link BlobServiceAsyncClient}.\n+     *\n+     * @param client {@link BlobServiceClient} whose properties are used to configure the builder.\n+     */\n+    public BlobChangefeedClientBuilder(BlobServiceAsyncClient client) {\n+        this.accountUrl = client.getAccountUrl();\n+        this.pipeline = client.getHttpPipeline();\n+        this.version = client.getServiceVersion();\n+    }\n+\n+    /**\n+     * Creates a {@link BlobChangefeedClient}.\n+     *\n+     * <p><strong>Code sample</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedClientBuilder#buildClient}\n+     *\n+     * @return a {@link BlobChangefeedClient} created from the configurations in this builder.\n+     */\n+    public BlobChangefeedClient buildClient() {\n+        return new BlobChangefeedClient(buildAsyncClient());\n+    }\n+\n+    /**\n+     * Creates a {@link BlobChangefeedAsyncClient}.\n+     *\n+     * <p><strong>Code sample</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.blob.changefeed.BlobChangefeedClientBuilder#buildAsyncClient}\n+     *\n+     * @return a {@link BlobChangefeedAsyncClient} created from the configurations in this builder.\n+     */\n+    public BlobChangefeedAsyncClient buildAsyncClient() {\n+        BlobServiceVersion serviceVersion = version != null ? version : BlobServiceVersion.getLatest();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NjUwNA==", "bodyText": "Do you want to extend ContinuablePagedFlux or ContinuablePagedFluxCore? The latter contains some default implementations.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431976504", "createdAt": "2020-05-28T16:42:28Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3NzY0Mg==", "bodyText": "Could use FluxUtil.fluxError(ClientLogger, RuntimeException) to log this error.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431977642", "createdAt": "2020-05-28T16:44:27Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk3OTc1Mw==", "bodyText": "Do we want to do a min here? If preferredPageSize was actually passed shouldn't that take precedence over DEFAULT_PAGE_SIZE.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431979753", "createdAt": "2020-05-28T16:48:07Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/BlobChangefeedPagedFlux.java", "diffHunk": "@@ -0,0 +1,94 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.core.util.paging.ContinuablePagedFlux;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.models.BlobChangefeedEvent;\n+import reactor.core.CoreSubscriber;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.util.List;\n+\n+/**\n+ * Implementation of {@link ContinuablePagedFlux} for Changefeed where the continuation token type is {@link String},\n+ * the element type is {@link BlobChangefeedEvent}, and the page type is {@link BlobChangefeedPagedResponse}.\n+ */\n+public final class BlobChangefeedPagedFlux extends ContinuablePagedFlux<String, BlobChangefeedEvent,\n+    BlobChangefeedPagedResponse> {\n+\n+    private final Changefeed changefeed;\n+    private static final Integer DEFAULT_PAGE_SIZE = 5000;\n+\n+    /**\n+     * Creates an instance of {@link BlobChangefeedPagedFlux}.\n+     *\n+     * @param changefeed {@link Changefeed}\n+     */\n+    BlobChangefeedPagedFlux(Changefeed changefeed) {\n+        this.changefeed = changefeed;\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage() {\n+        return byPage(null, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken) {\n+        return byPage(continuationToken, DEFAULT_PAGE_SIZE);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(int preferredPageSize) {\n+        return byPage(null, preferredPageSize);\n+    }\n+\n+    @Override\n+    public Flux<BlobChangefeedPagedResponse> byPage(String continuationToken, int preferredPageSize) {\n+\n+        if (continuationToken != null) {\n+            return Flux.error(new UnsupportedOperationException(\"continuationToken not supported. Use \"\n+                + \"client.getEvents(String) to pass in a cursor.\"));\n+        }\n+        if (preferredPageSize <= 0) {\n+            return Flux.error(new IllegalArgumentException(\"preferredPageSize > 0 required but provided: \"\n+                + preferredPageSize));\n+        }\n+        preferredPageSize = Integer.min(preferredPageSize, DEFAULT_PAGE_SIZE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4NzUxNQ==", "bodyText": "Given these values are ignored, could we use then and thenMany?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431987515", "createdAt": "2020-05-28T17:01:04Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4ODc3NQ==", "bodyText": "Any chance this class could be used in multiple threads at once? Setting these class level variables could result in timing issues.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431988775", "createdAt": "2020-05-28T17:02:38Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *\n+ * The changefeed is a log of changes that are organized into hourly segments.\n+ * The listing of the $blobchangefeed/idx/segments/ virtual directory shows these segments ordered by time.\n+ * The path of the segment describes the start of the hourly time-range that the segment represents.\n+ * This list can be used to filter out the segments of logs that are interest.\n+ *\n+ * Note: The time represented by the segment is approximate with bounds of 15 minutes. So to ensure consumption of\n+ * all records within a specified time, consume the consecutive previous and next hour segment.\n+ */\n+class Changefeed {\n+\n+    private static final String SEGMENT_PREFIX = \"idx/segments/\";\n+    private static final String METADATA_SEGMENT_PATH = \"meta/segments.json\";\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final OffsetDateTime startTime; /* User provided start time. */\n+    private final OffsetDateTime endTime; /* User provided end time. */\n+    private OffsetDateTime lastConsumable; /* Last consumable time. The latest time the changefeed can safely be\n+                                              read from.*/\n+    private OffsetDateTime safeEndTime; /* Soonest time between lastConsumable and endTime. */\n+    private final ChangefeedCursor cfCursor; /* Cursor associated with changefeed. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final SegmentFactory segmentFactory; /* Segment factory. */\n+\n+    /**\n+     * Creates a new Changefeed.\n+     */\n+    Changefeed(BlobContainerAsyncClient client, OffsetDateTime startTime, OffsetDateTime endTime,\n+        ChangefeedCursor userCursor, SegmentFactory segmentFactory) {\n+        this.client = client;\n+        this.startTime = startTime;\n+        this.endTime = endTime;\n+        this.userCursor = userCursor;\n+        this.segmentFactory = segmentFactory;\n+\n+        this.cfCursor = new ChangefeedCursor(this.endTime);\n+        this.safeEndTime = endTime;\n+    }\n+\n+    /**\n+     * Get all the events for the Changefeed.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        return validateChangefeed()\n+            .flatMap(ignore -> populateLastConsumable())\n+            .flatMapMany(ignore -> listYears())\n+            .concatMap(this::listSegmentsForYear)\n+            .concatMap(this::getEventsForSegment);\n+    }\n+\n+    /**\n+     * Validates that changefeed has been enabled for the account.\n+     */\n+    private Mono<Boolean> validateChangefeed() {\n+        return this.client.exists()\n+            .flatMap(exists -> {\n+                if (exists == null || !exists) {\n+                    return Mono.error(new RuntimeException(\"Changefeed has not been enabled for this account.\"));\n+                }\n+                return Mono.just(true);\n+            });\n+    }\n+\n+    /**\n+     * Populates the last consumable property from changefeed metadata.\n+     * Log files in any segment that is dated after the date of the LastConsumable property in the\n+     * $blobchangefeed/meta/segments.json file, should not be consumed by your application.\n+     */\n+    private Mono<OffsetDateTime> populateLastConsumable() {\n+        /* We can keep the entire metadata file in memory since it is expected to only be a few hundred bytes. */\n+        return DownloadUtils.downloadToString(this.client, METADATA_SEGMENT_PATH)\n+            /* Parse JSON for last consumable. */\n+            .flatMap(json -> {\n+                try {\n+                    ObjectMapper objectMapper = new ObjectMapper();\n+                    JsonNode jsonNode = objectMapper.readTree(json);\n+                    this.lastConsumable = OffsetDateTime.parse(jsonNode.get(\"lastConsumable\").asText());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTk4OTY0MQ==", "bodyText": "* <p>", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r431989641", "createdAt": "2020-05-28T17:03:38Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Changefeed.java", "diffHunk": "@@ -0,0 +1,138 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.changefeed.implementation.util.DownloadUtils;\n+import com.azure.storage.blob.changefeed.implementation.util.TimeUtils;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import reactor.core.publisher.Flux;\n+import reactor.core.publisher.Mono;\n+\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+\n+/**\n+ * A class that represents a Changefeed.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAwMTEzNQ==", "bodyText": "Let me know if I understand this incorrectly.\nSo the structure of these paths would be similar to the following\n\"$blobchangefeed/log/00/2019/02/22/1810/\",\n\"$blobchangefeed/log/01/2019/02/22/1810/\"\n\nThis is checking if we hit a chunk with a matching cursor to what a customer passed such as 2019/02.\nCould this logic be simplified, with a small overhead in perf, to just check chunkPath.startsWith(userCursor.getChunkPath()) and that be the only logic needed to be checked.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432001135", "createdAt": "2020-05-28T17:23:23Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/Shard.java", "diffHunk": "@@ -0,0 +1,97 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import com.azure.storage.blob.changefeed.implementation.models.BlobChangefeedEventWrapper;\n+import com.azure.storage.blob.changefeed.implementation.models.ChangefeedCursor;\n+import com.azure.storage.blob.models.BlobItem;\n+import com.azure.storage.blob.models.ListBlobsOptions;\n+import reactor.core.publisher.Flux;\n+\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * A class that represents a Shard in Changefeed.\n+ *\n+ * A shard is a virtual directory that contains a number of chunks.\n+ *\n+ * The log files in each shardPath are guaranteed to contain mutually exclusive blobs, and can be consumed and\n+ * processed in parallel without violating the ordering of modifications per blob during the iteration.\n+ */\n+class Shard  {\n+\n+    private final BlobContainerAsyncClient client; /* Changefeed container */\n+    private final String shardPath; /* Shard virtual directory path/prefix. */\n+    private final ChangefeedCursor segmentCursor; /* Cursor associated with parent segment. */\n+    private final ChangefeedCursor userCursor; /* User provided cursor. */\n+    private final ChunkFactory chunkFactory;\n+\n+    /**\n+     * Creates a new Shard.\n+     */\n+    Shard(BlobContainerAsyncClient client, String shardPath, ChangefeedCursor segmentCursor,\n+        ChangefeedCursor userCursor, ChunkFactory chunkFactory) {\n+        this.client = client;\n+        this.shardPath = shardPath;\n+        this.segmentCursor = segmentCursor;\n+        this.userCursor = userCursor;\n+        this.chunkFactory = chunkFactory;\n+    }\n+\n+    /**\n+     * Get events for the Shard.\n+     * @return A reactive stream of {@link BlobChangefeedEventWrapper}\n+     */\n+    Flux<BlobChangefeedEventWrapper> getEvents() {\n+        /* List relevant chunks. */\n+        return listChunks()\n+            .concatMap(chunkPath -> {\n+                /* Defaults for blockOffset and objectBlockIndex. */\n+                long blockOffset = 0;\n+                long objectBlockIndex = 0;\n+                /* If a user cursor was provided and it points to this chunk path, the chunk should get events based\n+                   off the blockOffset and objectBlockIndex.\n+                   This just makes sure only the targeted chunkPath uses the blockOffset and objectBlockIndex to\n+                   read events. Any subsequent chunk will read all of its events. */\n+                if (userCursor != null && userCursor.getChunkPath().equals(chunkPath)) {\n+                    blockOffset = userCursor.getBlockOffset();\n+                    objectBlockIndex = userCursor.getObjectBlockIndex();\n+                }\n+                return chunkFactory.getChunk(client, chunkPath, segmentCursor.toChunkCursor(chunkPath),\n+                    blockOffset, objectBlockIndex)\n+                    .getEvents();\n+            });\n+    }\n+\n+    /**\n+     * Lists relevant chunks in a shard.\n+     * @return A reactive stream of chunks.\n+     */\n+    private Flux<String> listChunks() {\n+        Flux<String> chunks = client.listBlobs(new ListBlobsOptions().setPrefix(shardPath))\n+            .map(BlobItem::getName);\n+        /* If no user cursor was provided, just return all chunks without filtering. */\n+        if (userCursor == null) {\n+            return chunks;\n+        /* If a user cursor was provided, filter out chunks that come before the chunk specified in the cursor. */\n+        } else {\n+            AtomicBoolean pass = new AtomicBoolean(); /* Whether or not to pass the event through. */\n+            return chunks.filter(chunkPath -> {\n+                if (pass.get()) {\n+                    return true;\n+                } else {\n+                    /* If we hit the chunk specified in the user cursor, set pass to true and pass this chunk\n+                       and any subsequent chunks through. */\n+                    if (userCursor.getChunkPath().equals(chunkPath)) {\n+                        pass.set(true); /* This allows us to pass subsequent chunks through.*/\n+                        return true; /* This allows us to pass this chunk through. */\n+                    } else {\n+                        return false;\n+                    }\n+                }\n+            });", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAxMzE0MQ==", "bodyText": "Would watch out on the usage of ByteBuffer.array as this will throw if the ByteBuffer is a DirectByteBuffer. May want to add in using FluxUtil.byteBufferToArray.\nsb.append(new String(FluxUtil.byteBufferToArray(buffer), StandardCharsets.UTF_8));", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432013141", "createdAt": "2020-05-28T17:44:29Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/DownloadUtils.java", "diffHunk": "@@ -0,0 +1,24 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.util;\n+\n+import com.azure.storage.blob.BlobContainerAsyncClient;\n+import reactor.core.publisher.Mono;\n+\n+import java.nio.charset.StandardCharsets;\n+\n+public class DownloadUtils {\n+\n+    /**\n+     * Reduces a Flux of ByteBuffer into a Mono of String\n+     */\n+    public static Mono<String> downloadToString(BlobContainerAsyncClient client, String blobPath) {\n+        return client.getBlobAsyncClient(blobPath)\n+            .download()\n+            .reduce(new StringBuilder(), (sb, buffer) -> {\n+                sb.append(new String(buffer.array(), StandardCharsets.UTF_8));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjAxNTU3OA==", "bodyText": "Given a segment path idx/segments/2020/05/28/1047/meta.json why do we always set minute to 0? Instead should we set it to the following\nsplitPath.length < 6 ? 0 : Integer.parseInt(splitPath[5]) % 100 /* minute */", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10839#discussion_r432015578", "createdAt": "2020-05-28T17:48:52Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob-changefeed/src/main/java/com/azure/storage/blob/changefeed/implementation/util/TimeUtils.java", "diffHunk": "@@ -0,0 +1,122 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.blob.changefeed.implementation.util;\n+\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+\n+public class TimeUtils {\n+\n+    /**\n+     * Converts a path to an OffsetDateTime.\n+     * <p>For example,\n+     * <p>Segment path : idx/segments/1601/01/01/0000/meta.json\n+     * <p>OffsetDateTime : year - 1601, month - 01, day - 01, hour - 00, minute - 00\n+     * <p>OR\n+     * <p>Year path : idx/segments/1601/\n+     * <p>OffsetDateTime : year - 1601, month - 00, day - 00, hour - 00, minute - 00\n+     *\n+     * @param path The path to convert.\n+     * @return The time associated with the path.\n+     */\n+    public static OffsetDateTime convertPathToTime(String path) {\n+        if (path == null) {\n+            return null;\n+        }\n+        String[] splitPath = path.split(\"/\");\n+\n+        return OffsetDateTime.of(\n+            Integer.parseInt(splitPath[2]), /* year */\n+            splitPath.length < 4 ? 1 : Integer.parseInt(splitPath[3]), /* month */\n+            splitPath.length < 5 ? 1 : Integer.parseInt(splitPath[4]), /* day */\n+            splitPath.length < 6 ? 0 : Integer.parseInt(splitPath[5]) / 100, /* hour */\n+            0, /* minute */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15f0631404c080526797ab12338026b0e4621f39"}, "originalPosition": 35}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4438, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}