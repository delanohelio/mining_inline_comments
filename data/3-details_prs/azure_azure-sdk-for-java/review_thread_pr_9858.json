{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk4NDAzMjYx", "number": 9858, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNjo0MDo1M1rODvNqyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwNDozNDoyM1rODyVXOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwODMzNjA5OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNjo0MDo1M1rOGBgfmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwNzo1OToxNFrOGFCq4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNDEzNw==", "bodyText": "when a class is annotated with jackson annotation, jackson uses reflection for serialization/deserialization to json.\nreflection is expensive and that being said, If there are pre existing classes in the code which needs to be serialized/deserialized to json we should ideally use a jackson serializer rather than annotating the class with jackson annotation.\n(not necessarily in the scope of this PR, but something to be addressed outside of this PR).", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404234137", "createdAt": "2020-04-06T16:40:53Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -63,7 +63,8 @@ public ServiceItemLease withId(String id) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n+    @JsonProperty(\"_etag\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI1ODU1Mg==", "bodyText": "n this case the \"_etag\" field was not passed through the serialization and that resulted in \"IF MATCH\" condition to fail when updating the lease document.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404258552", "createdAt": "2020-04-06T17:18:44Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -63,7 +63,8 @@ public ServiceItemLease withId(String id) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n+    @JsonProperty(\"_etag\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNDEzNw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI2MzEzOQ==", "bodyText": "The ask is to add something similar to this:\nhttps://github.com/Azure/azure-sdk-for-java/blob/master/sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/routing/PartitionKeyInternal.java#L35-L36\nThe internal types should not rely on reflection for serialization/deserialization.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404263139", "createdAt": "2020-04-06T17:26:12Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -63,7 +63,8 @@ public ServiceItemLease withId(String id) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n+    @JsonProperty(\"_etag\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNDEzNw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzAzMDI5NQ==", "bodyText": "Fixed. We only need a serializer here, the de-serialization is done at the Document/CosmosItemProperties level and that instance it is later used to create a Lease instance.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r407030295", "createdAt": "2020-04-11T07:28:15Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -63,7 +63,8 @@ public ServiceItemLease withId(String id) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n+    @JsonProperty(\"_etag\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNDEzNw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzkzOTgxMQ==", "bodyText": "If you have a serializer you need a deserializer too.\nDocument/CosmosItemProperties should call to the custom deserializer for deserialization. no?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r407939811", "createdAt": "2020-04-14T07:59:14Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -63,7 +63,8 @@ public ServiceItemLease withId(String id) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n+    @JsonProperty(\"_etag\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNDEzNw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwODM1ODI0OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNjo0NjoxMlrOGBgtZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNzoyMDo1OVrOGBiENA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNzY3MQ==", "bodyText": "partition split normally takes 10-20 min to complete. how are we ensuring that the test sees partition split completing within its life time?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404237671", "createdAt": "2020-04-06T16:46:12Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI1OTg5Mg==", "bodyText": "There's a check later to ensure that we see at least 2 partitions. In regular case it takes couple minutes (not 10 minutes) to detect the split.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404259892", "createdAt": "2020-04-06T17:20:59Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzNzY3MQ=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwODM2MTQ1OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNjo0NzowMVrOGBgvgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNzo0NzoxMlrOGBjDwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODIwOQ==", "bodyText": "don't catch InterruptedException if that's thrown, we can fail the test.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404238209", "createdAt": "2020-04-06T16:47:01Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI2MzAyOQ==", "bodyText": "We need to in this case, in order to avoid compilation errors...", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404263029", "createdAt": "2020-04-06T17:26:02Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODIwOQ=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI2NTE3OQ==", "bodyText": "why can't we add exception to the test method signature?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404265179", "createdAt": "2020-04-06T17:29:18Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODIwOQ=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI3NjE2MQ==", "bodyText": "replied on a similar comment below...", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404276161", "createdAt": "2020-04-06T17:47:12Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODIwOQ=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwODM2MjAyOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxNjo0NzowOVrOGBgv3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQxOTowMjo0NFrOGBlx-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw==", "bodyText": "don't catch InterruptedException if that's thrown, we can fail the test.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404238303", "createdAt": "2020-04-06T16:47:09Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODU1NQ==", "bodyText": "here and elsewhere please. we don't need to catch this, let it get thrown", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404238555", "createdAt": "2020-04-06T16:47:32Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI2MjE1NA==", "bodyText": "The try/catch is needed to avoid compilation errors such as: \"Error:(440, 29) java: unreported exception java.lang.InterruptedException; must be caught or declared to be thrown\"", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404262154", "createdAt": "2020-04-06T17:24:36Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI2NTcyNA==", "bodyText": "makes sense here.\nso please use this instead to translate checked to unchecked exception.\nhttps://projectreactor.io/docs/core/release/api/reactor/core/Exceptions.html#propagate-java.lang.Throwable-\nif InterruptedException we should error out.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404265724", "createdAt": "2020-04-06T17:30:07Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI3NTkwMw==", "bodyText": "Good suggestion, but it goes way beyond what we need to address at this time which is to release a fix for the CFP ASAP.\nFeel free to open an issue and assign it to me to refactor these tests and remove the handling of the exception.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404275903", "createdAt": "2020-04-06T17:46:43Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDI4ODg4Mw==", "bodyText": "@milismsft I have already signed off on v3, and I agree with you on v3 as this is hotfix for v3 it should go out as soon as possible.\nIMO, on v4 we should do the right thing (this is not in prod yet) rather than introducing technical debt.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404288883", "createdAt": "2020-04-06T18:08:25Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDMxOTI3NQ==", "bodyText": "V4 latest release is currently used by couple customers a preview.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404319275", "createdAt": "2020-04-06T19:00:00Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDMyMDc2MQ==", "bodyText": "I'll deal with the debt later; the changes you're requesting go beyond what this PR/work item should address. IMHO et's keep things simple enough in case we need to review this later and use a different PR to address  other things.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r404320761", "createdAt": "2020-04-06T19:02:44Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +315,136 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDIzODMwMw=="}, "originalCommit": {"oid": "cfed2526a1a897ae11832382960b93f6130d8185"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjU5MDQyOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwODowMDowMlrOGFCsmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo0ODo0MFrOGFVOFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0MDI1MQ==", "bodyText": "Please add unit test scoped to serialization.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r407940251", "createdAt": "2020-04-14T08:00:02Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -20,6 +22,7 @@\n /**\n  * Document service lease.\n  */\n+@JsonSerialize(using = ServiceItemLease.ServiceItemLeaseJsonSerializer.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0MzczNQ==", "bodyText": "Test added.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408243735", "createdAt": "2020-04-14T15:48:40Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -20,6 +22,7 @@\n /**\n  * Document service lease.\n  */\n+@JsonSerialize(using = ServiceItemLease.ServiceItemLeaseJsonSerializer.class)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0MDI1MQ=="}, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjYzMTE1OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwODoxMToyMlrOGFDGaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo0ODo1NlrOGFVO4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0Njg1OQ==", "bodyText": "stale code. remove", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r407946859", "createdAt": "2020-04-14T08:11:22Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -181,34 +182,17 @@ public ServiceItemLease withTimestamp(ZonedDateTime timestamp) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0MzkzNg==", "bodyText": "removed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408243936", "createdAt": "2020-04-14T15:48:56Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -181,34 +182,17 @@ public ServiceItemLease withTimestamp(ZonedDateTime timestamp) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0Njg1OQ=="}, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjYzMTYzOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwODoxMToyOFrOGFDGsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo0OTowNFrOGFVPWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0NjkyOA==", "bodyText": "ditto", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r407946928", "createdAt": "2020-04-14T08:11:28Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -181,34 +182,17 @@ public ServiceItemLease withTimestamp(ZonedDateTime timestamp) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n     public String getExplicitTimestamp() {\n         return this.timestamp;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0NDA1Nw==", "bodyText": "removed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408244057", "createdAt": "2020-04-14T15:49:04Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -181,34 +182,17 @@ public ServiceItemLease withTimestamp(ZonedDateTime timestamp) {\n         return this;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore\n     public String getExplicitTimestamp() {\n         return this.timestamp;\n     }\n \n-    @JsonIgnore\n+//    @JsonIgnore", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0NjkyOA=="}, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjY0MzQ5OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwODoxNDoyNVrOGFDN0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNDo1ODo0M1rOGFS4Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0ODc1Mw==", "bodyText": "LeaseToken, ContinuationToken, Owner are Camel case, but timestamp, id, etag are not is that expected?\nalso for Owner, timestamp, ContinuationToken, LeaseToken we probably should define constants.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r407948753", "createdAt": "2020-04-14T08:14:25Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +235,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {\n+        // this value should be incremented if changes are made to the ServiceItemLease class members\n+        private static final long serialVersionUID = 1L;\n+\n+        protected ServiceItemLeaseJsonSerializer() { this(null); }\n+\n+        protected ServiceItemLeaseJsonSerializer(Class<ServiceItemLease> t) {\n+            super(t);\n+        }\n+\n+        @Override\n+        public void serialize(ServiceItemLease lease, JsonGenerator writer, SerializerProvider serializerProvider) {\n+            try {\n+                writer.writeStartObject();\n+                writer.writeStringField(Constants.Properties.ID, lease.getId());\n+                writer.writeStringField(Constants.Properties.E_TAG, lease.getETag());\n+                writer.writeStringField(\"LeaseToken\", lease.getLeaseToken());\n+                writer.writeStringField(\"ContinuationToken\", lease.getContinuationToken());\n+                writer.writeStringField(\"timestamp\", lease.getTimestamp());\n+                writer.writeStringField(\"Owner\", lease.getOwner());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODIwNTM0Mw==", "bodyText": "The current naming convention follows .Net and the content of a lease document as written from that code. The idea behind this design is that you can have CFP instances resuming work from both platforms, Java and .NET.\nThe ServiceItemLease is internal only (not exposed to the user); for the easy of debugging keeping the same names for members and the content of the lease document as written in Cosmos is actually beneficial.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408205343", "createdAt": "2020-04-14T14:58:43Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +235,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {\n+        // this value should be incremented if changes are made to the ServiceItemLease class members\n+        private static final long serialVersionUID = 1L;\n+\n+        protected ServiceItemLeaseJsonSerializer() { this(null); }\n+\n+        protected ServiceItemLeaseJsonSerializer(Class<ServiceItemLease> t) {\n+            super(t);\n+        }\n+\n+        @Override\n+        public void serialize(ServiceItemLease lease, JsonGenerator writer, SerializerProvider serializerProvider) {\n+            try {\n+                writer.writeStartObject();\n+                writer.writeStringField(Constants.Properties.ID, lease.getId());\n+                writer.writeStringField(Constants.Properties.E_TAG, lease.getETag());\n+                writer.writeStringField(\"LeaseToken\", lease.getLeaseToken());\n+                writer.writeStringField(\"ContinuationToken\", lease.getContinuationToken());\n+                writer.writeStringField(\"timestamp\", lease.getTimestamp());\n+                writer.writeStringField(\"Owner\", lease.getOwner());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0ODc1Mw=="}, "originalCommit": {"oid": "29eb83a7735a3a5d54f58d6b815eafe80dcf6a48"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNDQ5OTAzOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo0ODowM1rOGFVMNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo1MTozNlrOGFVWsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0MzI1Mw==", "bodyText": "thank you for the test. as a code style rule we don't add \"test\" as prefix.\nPlease drop the \"test\" prefix to be consistent with other tests.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408243253", "createdAt": "2020-04-14T15:48:03Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -424,6 +427,46 @@ public void readFeedDocumentsAfterSplit() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void testServiceItemLeaseSerialization() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ce2d3d238e26673d430d940d7e311b6d3789e6c"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0NTkzOA==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408245938", "createdAt": "2020-04-14T15:51:36Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -424,6 +427,46 @@ public void readFeedDocumentsAfterSplit() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void testServiceItemLeaseSerialization() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0MzI1Mw=="}, "originalCommit": {"oid": "7ce2d3d238e26673d430d940d7e311b6d3789e6c"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNDUwMzI4OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo0ODo1NFrOGFVO0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNTo1NToxNFrOGFVhXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0MzkyMA==", "bodyText": "please drop the try/catch block here. if there is a failure the failure should get thrown and the exception needs to be added to the test method signature.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408243920", "createdAt": "2020-04-14T15:48:54Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -424,6 +427,46 @@ public void readFeedDocumentsAfterSplit() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void testServiceItemLeaseSerialization() {\n+        ZonedDateTime timeNow = ZonedDateTime.now();\n+        String timeNowValue = timeNow.toString();\n+\n+        Lease lease1 = new ServiceItemLease()\n+            .withId(\"id1\")\n+            .withLeaseToken(\"1\")\n+            .withETag(\"etag1\")\n+            .withOwner(\"Owner1\")\n+            .withContinuationToken(\"12\")\n+            .withTimestamp(timeNow)\n+            .withTs(\"122311231\");\n+\n+        Lease lease2 = new ServiceItemLease()\n+            .withId(\"id2\")\n+            .withLeaseToken(\"2\")\n+            .withETag(\"etag2\")\n+            .withContinuationToken(\"22\")\n+            .withTimestamp(timeNow)\n+            .withTs(\"122311232\");\n+\n+        ObjectMapper mapper = new ObjectMapper();\n+\n+        try {\n+            assertThat(mapper.writeValueAsString(lease1)).isEqualTo(\n+                String.format(\"%s%s%s\",\n+                    \"{\\\"id\\\":\\\"id1\\\",\\\"_etag\\\":\\\"etag1\\\",\\\"LeaseToken\\\":\\\"1\\\",\\\"ContinuationToken\\\":\\\"12\\\",\\\"timestamp\\\":\\\"\",\n+                    timeNowValue,\n+                    \"\\\",\\\"Owner\\\":\\\"Owner1\\\"}\"));\n+            assertThat(mapper.writeValueAsString(lease2)).isEqualTo(\n+                String.format(\"%s%s%s\",\n+                    \"{\\\"id\\\":\\\"id2\\\",\\\"_etag\\\":\\\"etag2\\\",\\\"LeaseToken\\\":\\\"2\\\",\\\"ContinuationToken\\\":\\\"22\\\",\\\"timestamp\\\":\\\"\",\n+                    timeNowValue,\n+                    \"\\\",\\\"Owner\\\":null}\"));\n+        } catch(Exception ex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7ce2d3d238e26673d430d940d7e311b6d3789e6c"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0ODY2OQ==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408248669", "createdAt": "2020-04-14T15:55:14Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -424,6 +427,46 @@ public void readFeedDocumentsAfterSplit() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void testServiceItemLeaseSerialization() {\n+        ZonedDateTime timeNow = ZonedDateTime.now();\n+        String timeNowValue = timeNow.toString();\n+\n+        Lease lease1 = new ServiceItemLease()\n+            .withId(\"id1\")\n+            .withLeaseToken(\"1\")\n+            .withETag(\"etag1\")\n+            .withOwner(\"Owner1\")\n+            .withContinuationToken(\"12\")\n+            .withTimestamp(timeNow)\n+            .withTs(\"122311231\");\n+\n+        Lease lease2 = new ServiceItemLease()\n+            .withId(\"id2\")\n+            .withLeaseToken(\"2\")\n+            .withETag(\"etag2\")\n+            .withContinuationToken(\"22\")\n+            .withTimestamp(timeNow)\n+            .withTs(\"122311232\");\n+\n+        ObjectMapper mapper = new ObjectMapper();\n+\n+        try {\n+            assertThat(mapper.writeValueAsString(lease1)).isEqualTo(\n+                String.format(\"%s%s%s\",\n+                    \"{\\\"id\\\":\\\"id1\\\",\\\"_etag\\\":\\\"etag1\\\",\\\"LeaseToken\\\":\\\"1\\\",\\\"ContinuationToken\\\":\\\"12\\\",\\\"timestamp\\\":\\\"\",\n+                    timeNowValue,\n+                    \"\\\",\\\"Owner\\\":\\\"Owner1\\\"}\"));\n+            assertThat(mapper.writeValueAsString(lease2)).isEqualTo(\n+                String.format(\"%s%s%s\",\n+                    \"{\\\"id\\\":\\\"id2\\\",\\\"_etag\\\":\\\"etag2\\\",\\\"LeaseToken\\\":\\\"2\\\",\\\"ContinuationToken\\\":\\\"22\\\",\\\"timestamp\\\":\\\"\",\n+                    timeNowValue,\n+                    \"\\\",\\\"Owner\\\":null}\"));\n+        } catch(Exception ex) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI0MzkyMA=="}, "originalCommit": {"oid": "7ce2d3d238e26673d430d940d7e311b6d3789e6c"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNDU1NjczOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNjowMDoyNVrOGFVwNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNjoyNTo1NVrOGFW2QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1MjQ2OQ==", "bodyText": "we should add a deserializer as well. Document/CosmosItemProperties can use the custom deserializer.\nYou can take a look at PartitionKeyInternal.java as example.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408252469", "createdAt": "2020-04-14T16:00:25Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +234,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1OTkwNg==", "bodyText": "Won't fix.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408259906", "createdAt": "2020-04-14T16:10:51Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +234,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1MjQ2OQ=="}, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI2MTQ0MA==", "bodyText": "Won't fix.\n\nwhy?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408261440", "createdAt": "2020-04-14T16:13:02Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +234,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1MjQ2OQ=="}, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI2Nzc0NA==", "bodyText": "already explained \"why\"; you're asking to add dead code because there's no direct Json deserialization that is needed for ServiceItemLease at this time. The \"deserialization\" is done via fromDocument() method call.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408267744", "createdAt": "2020-04-14T16:22:13Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +234,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1MjQ2OQ=="}, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI3MDQwMQ==", "bodyText": "the explanation on fromDocument() makes sense. so that acts as the deserializer, that's what needed. thanks.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408270401", "createdAt": "2020-04-14T16:25:55Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/changefeed/ServiceItemLease.java", "diffHunk": "@@ -236,4 +234,32 @@ public String toString() {\n             this.getTimestamp(),\n             UNIX_START_TIME.plusSeconds(Long.parseLong(this.getTs())));\n     }\n+\n+    @SuppressWarnings(\"serial\")\n+    static final class ServiceItemLeaseJsonSerializer extends StdSerializer<ServiceItemLease> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1MjQ2OQ=="}, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNDU3NjA2OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNjowNDozMlrOGFV8AQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNjoyNjo0NFrOGFW4OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1NTQ4OQ==", "bodyText": "\"simple\" test group is only invoked as integration test in the presence of a cosmosdb endpoint.\nas this is unit test, not integration test, the group needs to be \"unit\".  (groups = { \"unit\" })\nalso the unit tests should go in a separate class, e.g.,  please move the test to  the new class ServiceItemLeaseTest.java", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408255489", "createdAt": "2020-04-14T16:04:32Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +325,172 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {\n+                    log.error(e.getMessage());\n+                }\n+                return true;\n+            })\n+            .last().block();\n+\n+        assertThat(changeFeedProcessor.isStarted()).as(\"Change Feed Processor instance is running\").isTrue();\n+\n+        // generate the second batch of documents\n+        createReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        // Wait for the feed processor to receive and process the second batch of documents.\n+        waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT * 2);\n+\n+        changeFeedProcessor.stop().subscribeOn(Schedulers.elastic()).timeout(Duration.ofMillis(CHANGE_FEED_PROCESSOR_TIMEOUT)).subscribe();\n+\n+        for (CosmosItemProperties item : createdDocuments) {\n+            assertThat(receivedDocuments.containsKey(item.getId())).as(\"Document with getId: \" + item.getId()).isTrue();\n+        }\n+\n+        // Wait for the feed processor to shutdown.\n+        try {\n+            Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+        receivedDocuments.clear();\n+    }\n+\n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI2OTE0NA==", "bodyText": "Won't fix.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408269144", "createdAt": "2020-04-14T16:24:07Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +325,172 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {\n+                    log.error(e.getMessage());\n+                }\n+                return true;\n+            })\n+            .last().block();\n+\n+        assertThat(changeFeedProcessor.isStarted()).as(\"Change Feed Processor instance is running\").isTrue();\n+\n+        // generate the second batch of documents\n+        createReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        // Wait for the feed processor to receive and process the second batch of documents.\n+        waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT * 2);\n+\n+        changeFeedProcessor.stop().subscribeOn(Schedulers.elastic()).timeout(Duration.ofMillis(CHANGE_FEED_PROCESSOR_TIMEOUT)).subscribe();\n+\n+        for (CosmosItemProperties item : createdDocuments) {\n+            assertThat(receivedDocuments.containsKey(item.getId())).as(\"Document with getId: \" + item.getId()).isTrue();\n+        }\n+\n+        // Wait for the feed processor to shutdown.\n+        try {\n+            Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+        receivedDocuments.clear();\n+    }\n+\n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1NTQ4OQ=="}, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI3MDkwNQ==", "bodyText": "Won't fix.\n\nwhy?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r408270905", "createdAt": "2020-04-14T16:26:44Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -332,10 +325,172 @@ public void staledLeaseAcquiring() {\n         receivedDocuments.clear();\n     }\n \n+    @Test(groups = { \"simple\" }, timeOut = 50 * CHANGE_FEED_PROCESSOR_TIMEOUT)\n+    public void readFeedDocumentsAfterSplit() {\n+        createdFeedCollectionForSplit = createFeedCollection(FEED_COLLECTION_THROUGHPUT_FOR_SPLIT);\n+\n+        // generate a first batch of documents\n+        setupReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        changeFeedProcessor = ChangeFeedProcessor.changeFeedProcessorBuilder()\n+            .hostName(hostName)\n+            .handleChanges(changeFeedProcessorHandler())\n+            .feedContainer(createdFeedCollectionForSplit)\n+            .leaseContainer(createdLeaseCollection)\n+            .options(new ChangeFeedProcessorOptions()\n+                .setLeasePrefix(\"TEST\")\n+                .setStartFromBeginning(true)\n+                .setMaxItemCount(10)\n+            )\n+            .build();\n+\n+        changeFeedProcessor.start().subscribeOn(Schedulers.elastic())\n+            .timeout(Duration.ofMillis(2 * CHANGE_FEED_PROCESSOR_TIMEOUT))\n+            .onErrorResume(throwable -> {\n+                log.error(\"Change feed processor did not start in the expected time\", throwable);\n+                return Mono.error(throwable);\n+            })\n+            .doOnSuccess(aVoid -> {\n+                // Wait for the feed processor to receive and process the first batch of documents.\n+                waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT);\n+            })\n+            .then(\n+                // increase throughput to force a single partition collection to go through a split\n+                createdFeedCollectionForSplit.readProvisionedThroughput().subscribeOn(Schedulers.elastic())\n+                    .flatMap(currentThroughput ->\n+                        createdFeedCollectionForSplit.replaceProvisionedThroughput(FEED_COLLECTION_THROUGHPUT).subscribeOn(Schedulers.elastic())\n+                    )\n+                .then()\n+            )\n+            .subscribe();\n+\n+        // Wait for the feed processor to receive and process the first batch of documents and apply throughput change.\n+        try {\n+            Thread.sleep(4 * CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+\n+        // Loop through reading the current partition count until we get a split\n+        //   This can take up to two minute or more.\n+        String partitionKeyRangesPath = extractContainerSelfLink(createdFeedCollectionForSplit);\n+        FeedOptions feedOptions = new FeedOptions();\n+        feedOptions.setRequestContinuation(null);\n+\n+        AsyncDocumentClient contextClient = getContextClient(createdDatabase);\n+        Flux.just(1).subscribeOn(Schedulers.elastic())\n+            .flatMap( value -> {\n+                log.warn(\"Reading current hroughput change.\");\n+                return contextClient.readPartitionKeyRanges(partitionKeyRangesPath, feedOptions);\n+            })\n+            .map(partitionKeyRangeFeedResponse -> {\n+                int count = partitionKeyRangeFeedResponse.getResults().size();\n+\n+                if ( count < 2) {\n+                    log.warn(\"Throughput change is pending.\");\n+                    throw new RuntimeException(\"Throughput change is not done.\");\n+                }\n+                return count;\n+            })\n+            // this will timeout approximately after 3 minutes\n+            .retry(40, throwable -> {\n+                try {\n+                    log.warn(\"Retrying...\");\n+                    Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+                } catch (InterruptedException e) {\n+                    log.error(e.getMessage());\n+                }\n+                return true;\n+            })\n+            .last().block();\n+\n+        assertThat(changeFeedProcessor.isStarted()).as(\"Change Feed Processor instance is running\").isTrue();\n+\n+        // generate the second batch of documents\n+        createReadFeedDocuments(createdFeedCollectionForSplit, FEED_COUNT);\n+\n+        // Wait for the feed processor to receive and process the second batch of documents.\n+        waitToReceiveDocuments(2 * CHANGE_FEED_PROCESSOR_TIMEOUT, FEED_COUNT * 2);\n+\n+        changeFeedProcessor.stop().subscribeOn(Schedulers.elastic()).timeout(Duration.ofMillis(CHANGE_FEED_PROCESSOR_TIMEOUT)).subscribe();\n+\n+        for (CosmosItemProperties item : createdDocuments) {\n+            assertThat(receivedDocuments.containsKey(item.getId())).as(\"Document with getId: \" + item.getId()).isTrue();\n+        }\n+\n+        // Wait for the feed processor to shutdown.\n+        try {\n+            Thread.sleep(CHANGE_FEED_PROCESSOR_TIMEOUT);\n+        } catch (InterruptedException e) {\n+            log.error(e.getMessage());\n+        }\n+        receivedDocuments.clear();\n+    }\n+\n+    @Test(groups = { \"simple\" }, timeOut = CHANGE_FEED_PROCESSOR_TIMEOUT)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI1NTQ4OQ=="}, "originalCommit": {"oid": "cb9a8f6dab209235b8b2641bccc086d1b7effd57"}, "originalPosition": 296}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTA1NDAxOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwNDozNDoyM1rOGGUOQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxOTowMzoyOVrOGGzOIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI3NTk3MA==", "bodyText": "I suspect you have client leak in your test class. this might cause CI test issues later.\nthe afterClass is annotated as part of emulator test group. But your partition split test is in a different test group, i.e., simple test group. Due to that the afterClass may not get invoked for the partition split test.\nOur test pattern is to not mix different test groups in the same class for simplicity.\nDifferent test group go to different test classes, otherwise you need to be very cautious in the cleaning logic.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r409275970", "createdAt": "2020-04-16T04:34:23Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -360,27 +493,18 @@ public void before_ChangeFeedProcessorTest() {\n //            log.warn(\"Database delete\", e);\n //        }\n //        createdDatabase = createDatabase(client, databaseId);\n-        createdDatabase = getSharedCosmosDatabase(client);\n     }\n \n     @AfterMethod(groups = { \"emulator\" }, timeOut = 3 * SHUTDOWN_TIMEOUT, alwaysRun = true)\n     public void afterMethod() {\n-        safeDeleteCollection(createdFeedCollection);\n-        safeDeleteCollection(createdLeaseCollection);\n-\n-        // Allow some time for the collections and the database to be deleted before exiting.\n-        try {\n-            Thread.sleep(500);\n-        } catch (Exception e){ }\n     }\n \n     @AfterClass(groups = { \"emulator\" }, timeOut = 2 * SHUTDOWN_TIMEOUT, alwaysRun = true)\n     public void afterClass() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "935a1c3958070d375115593d391bf7f0c0fd6d36"}, "originalPosition": 690}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI3ODUyNw==", "bodyText": "@kushagraThapar FYI", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r409278527", "createdAt": "2020-04-16T04:44:44Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -360,27 +493,18 @@ public void before_ChangeFeedProcessorTest() {\n //            log.warn(\"Database delete\", e);\n //        }\n //        createdDatabase = createDatabase(client, databaseId);\n-        createdDatabase = getSharedCosmosDatabase(client);\n     }\n \n     @AfterMethod(groups = { \"emulator\" }, timeOut = 3 * SHUTDOWN_TIMEOUT, alwaysRun = true)\n     public void afterMethod() {\n-        safeDeleteCollection(createdFeedCollection);\n-        safeDeleteCollection(createdLeaseCollection);\n-\n-        // Allow some time for the collections and the database to be deleted before exiting.\n-        try {\n-            Thread.sleep(500);\n-        } catch (Exception e){ }\n     }\n \n     @AfterClass(groups = { \"emulator\" }, timeOut = 2 * SHUTDOWN_TIMEOUT, alwaysRun = true)\n     public void afterClass() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI3NTk3MA=="}, "originalCommit": {"oid": "935a1c3958070d375115593d391bf7f0c0fd6d36"}, "originalPosition": 690}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc4Mzg0Mw==", "bodyText": "Fixed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/9858#discussion_r409783843", "createdAt": "2020-04-16T19:03:29Z", "author": {"login": "milismsft"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/rx/ChangeFeedProcessorTest.java", "diffHunk": "@@ -360,27 +493,18 @@ public void before_ChangeFeedProcessorTest() {\n //            log.warn(\"Database delete\", e);\n //        }\n //        createdDatabase = createDatabase(client, databaseId);\n-        createdDatabase = getSharedCosmosDatabase(client);\n     }\n \n     @AfterMethod(groups = { \"emulator\" }, timeOut = 3 * SHUTDOWN_TIMEOUT, alwaysRun = true)\n     public void afterMethod() {\n-        safeDeleteCollection(createdFeedCollection);\n-        safeDeleteCollection(createdLeaseCollection);\n-\n-        // Allow some time for the collections and the database to be deleted before exiting.\n-        try {\n-            Thread.sleep(500);\n-        } catch (Exception e){ }\n     }\n \n     @AfterClass(groups = { \"emulator\" }, timeOut = 2 * SHUTDOWN_TIMEOUT, alwaysRun = true)\n     public void afterClass() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTI3NTk3MA=="}, "originalCommit": {"oid": "935a1c3958070d375115593d391bf7f0c0fd6d36"}, "originalPosition": 690}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1972, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}