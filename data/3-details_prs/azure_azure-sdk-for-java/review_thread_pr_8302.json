{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc3NDIxMTY2", "number": 8302, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxOTo0MzowOFrODhp-Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMTo0Njo1MlrODiBYBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjE3MjcwOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/ParallelTransferOptions.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxOTo0MzowOFrOFsgv4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMTo1MjoxMFrOFskZ4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIxODIwOQ==", "bodyText": "Is this also the value we use for chunks in downloadToFile? Perhaps we should update these docs", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382218209", "createdAt": "2020-02-20T19:43:08Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/ParallelTransferOptions.java", "diffHunk": "@@ -0,0 +1,84 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+\n+/**\n+ * This class contains configuration used to parallelize data transfer operations. Note that not all values are used\n+ * by every method which accepts this type. Please refer to the javadoc on specific methods for these cases.\n+ */\n+@Fluent\n+public final class ParallelTransferOptions {\n+\n+    private final Integer blockSize;\n+    private final Integer numBuffers;\n+    private final ProgressReceiver progressReceiver;\n+    private final Integer maxSingleUploadSize;\n+\n+    /**\n+     * Creates a new {@link ParallelTransferOptions} with default parameters applied.\n+     *\n+     * @param blockSize The block size.\n+     * For upload, The block size is the size of each block that will be staged. This value also determines the number", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI3ODExNA==", "bodyText": "I added a sentence for downloadToFile", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382278114", "createdAt": "2020-02-20T21:52:10Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/ParallelTransferOptions.java", "diffHunk": "@@ -0,0 +1,84 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common;\n+\n+import com.azure.core.annotation.Fluent;\n+import com.azure.storage.common.implementation.StorageImplUtils;\n+\n+/**\n+ * This class contains configuration used to parallelize data transfer operations. Note that not all values are used\n+ * by every method which accepts this type. Please refer to the javadoc on specific methods for these cases.\n+ */\n+@Fluent\n+public final class ParallelTransferOptions {\n+\n+    private final Integer blockSize;\n+    private final Integer numBuffers;\n+    private final ProgressReceiver progressReceiver;\n+    private final Integer maxSingleUploadSize;\n+\n+    /**\n+     * Creates a new {@link ParallelTransferOptions} with default parameters applied.\n+     *\n+     * @param blockSize The block size.\n+     * For upload, The block size is the size of each block that will be staged. This value also determines the number", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIxODIwOQ=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjIwNzk0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxOTo1NDoyMlrOFshGEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxNzozOTozNFrOFs_EKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyMzg4OQ==", "bodyText": "I wonder if it'd be helpful to have an abstract type we define like Uploader that has abstract methods for validateOptions, initialize, determineUploadFullOrChunked, uploadInChunks, uploadFull, and finalize and then it's like execute/upload/whatever method strings these together and each package has a subtype that overrides each of these methods? Does that make sense and/or do you see any value in that? On the one hand, it seems cleaner to me to have this logic centralized and to deal with well defined overload. On the other hand, I can see why that would feel stiff or overly structured.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382223889", "createdAt": "2020-02-20T19:54:22Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNDkyMw==", "bodyText": "#8383", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382714923", "createdAt": "2020-02-21T17:39:34Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyMzg4OQ=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjIxODQ2OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxOTo1Nzo1MlrOFshMjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDo1ODoxOVrOFsi8wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyNTU0OA==", "bodyText": "Can you give a more thorough description of what this scan method is doing/how it works? Yes, there's the reactor docs, but perhaps walking through a few iterations in our own example will help improve clarity. In particular, some of the arithmetic is not exactly self evident as to its effect.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382225548", "createdAt": "2020-02-20T19:57:52Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI1NDI3Mw==", "bodyText": "made a Tuple 3, and added a block comment.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382254273", "createdAt": "2020-02-20T20:58:19Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyNTU0OA=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjIzMzQ2OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDowMjozOFrOFshV-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMTo0OTozN1rOFskVVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyNzk2Mg==", "bodyText": "I think then() is a bit of a cleaner way of doing this", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382227962", "createdAt": "2020-02-20T20:02:38Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {\n+                ByteBuffer buffer = source.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long lastBytesWritten = result.getT2();\n+\n+                return Tuples.of(buffer, currentBufferLength + lastBytesWritten);\n+            })\n+            .flatMapSequential(tuple2 -> {\n+                ByteBuffer buffer = tuple2.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long currentOffset = tuple2.getT2() - currentBufferLength + fileOffset;\n+                // Report progress as necessary.\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    Flux.just(buffer), parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+                return appendWithResponse(progressData, currentOffset, currentBufferLength, null,\n+                    requestConditions.getLeaseId())\n+                    .doFinally(x -> pool.returnBuffer(buffer))\n+                    .flux();\n+            })\n+            .last()\n+            .flatMap(resp -> flushWithResponse(length, false, false, httpHeaders, requestConditions));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI3Njk1MQ==", "bodyText": "need to use flatmap now to pass length", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382276951", "createdAt": "2020-02-20T21:49:37Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {\n+                ByteBuffer buffer = source.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long lastBytesWritten = result.getT2();\n+\n+                return Tuples.of(buffer, currentBufferLength + lastBytesWritten);\n+            })\n+            .flatMapSequential(tuple2 -> {\n+                ByteBuffer buffer = tuple2.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long currentOffset = tuple2.getT2() - currentBufferLength + fileOffset;\n+                // Report progress as necessary.\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    Flux.just(buffer), parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+                return appendWithResponse(progressData, currentOffset, currentBufferLength, null,\n+                    requestConditions.getLeaseId())\n+                    .doFinally(x -> pool.returnBuffer(buffer))\n+                    .flux();\n+            })\n+            .last()\n+            .flatMap(resp -> flushWithResponse(length, false, false, httpHeaders, requestConditions));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyNzk2Mg=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 231}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjI5MTYzOnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDoyMzowOFrOFsh6yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDoyMzowOFrOFsh6yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIzNzM4Ng==", "bodyText": "Same comment about possibly abstracting uploadFromFile stuff", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382237386", "createdAt": "2020-02-20T20:23:08Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {\n+                ByteBuffer buffer = source.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long lastBytesWritten = result.getT2();\n+\n+                return Tuples.of(buffer, currentBufferLength + lastBytesWritten);\n+            })\n+            .flatMapSequential(tuple2 -> {\n+                ByteBuffer buffer = tuple2.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long currentOffset = tuple2.getT2() - currentBufferLength + fileOffset;\n+                // Report progress as necessary.\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    Flux.just(buffer), parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+                return appendWithResponse(progressData, currentOffset, currentBufferLength, null,\n+                    requestConditions.getLeaseId())\n+                    .doFinally(x -> pool.returnBuffer(buffer))\n+                    .flux();\n+            })\n+            .last()\n+            .flatMap(resp -> flushWithResponse(length, false, false, httpHeaders, requestConditions));\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long fileOffset, long length,\n+        PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions) {\n+        return appendWithResponse(data, fileOffset, length, null, requestConditions.getLeaseId())\n+            .flatMap(resp -> flushWithResponse(fileOffset + length, false, false, httpHeaders,\n+                requestConditions));\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file. By default this method will not overwrite an\n+     * existing file.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String}\n+     *\n+     * @param filePath Path to the upload file\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath) {\n+        try {\n+            return uploadFromFile(filePath, false);\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String-boolean}\n+     *\n+     * @param filePath Path to the upload file\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath, boolean overwrite) {\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 274}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjMwNTU4OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDoyNzoxOFrOFsiC-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMTowMjowNlrOFsjFTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIzOTQ4MA==", "bodyText": "These I think can also move into the common utility, right?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382239480", "createdAt": "2020-02-20T20:27:18Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {\n+                ByteBuffer buffer = source.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long lastBytesWritten = result.getT2();\n+\n+                return Tuples.of(buffer, currentBufferLength + lastBytesWritten);\n+            })\n+            .flatMapSequential(tuple2 -> {\n+                ByteBuffer buffer = tuple2.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long currentOffset = tuple2.getT2() - currentBufferLength + fileOffset;\n+                // Report progress as necessary.\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    Flux.just(buffer), parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+                return appendWithResponse(progressData, currentOffset, currentBufferLength, null,\n+                    requestConditions.getLeaseId())\n+                    .doFinally(x -> pool.returnBuffer(buffer))\n+                    .flux();\n+            })\n+            .last()\n+            .flatMap(resp -> flushWithResponse(length, false, false, httpHeaders, requestConditions));\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long fileOffset, long length,\n+        PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions) {\n+        return appendWithResponse(data, fileOffset, length, null, requestConditions.getLeaseId())\n+            .flatMap(resp -> flushWithResponse(fileOffset + length, false, false, httpHeaders,\n+                requestConditions));\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file. By default this method will not overwrite an\n+     * existing file.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String}\n+     *\n+     * @param filePath Path to the upload file\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath) {\n+        try {\n+            return uploadFromFile(filePath, false);\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String-boolean}\n+     *\n+     * @param filePath Path to the upload file\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath, boolean overwrite) {\n+        try {\n+            Mono<Void> overwriteCheck = Mono.empty();\n+            DataLakeRequestConditions requestConditions = null;\n+\n+            // Note that if the file will be uploaded using a putBlob, we also can skip the exists check.\n+            if (!overwrite) {\n+                if (uploadInBlocks(filePath, DataLakeFileAsyncClient.MAX_APPEND_FILE_BYTES)) {\n+                    overwriteCheck = exists().flatMap(exists -> exists\n+                        ? monoError(logger, new IllegalArgumentException(Constants.FILE_ALREADY_EXISTS))\n+                        : Mono.empty());\n+                }\n+\n+                requestConditions = new DataLakeRequestConditions()\n+                    .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+            }\n+\n+            return overwriteCheck.then(uploadFromFile(filePath, null, null, null, requestConditions));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * @param filePath Path to the upload file\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} to use to upload from file. Number of parallel\n+     * transfers parameter is ignored.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions,\n+        PathHttpHeaders headers, Map<String, String> metadata, DataLakeRequestConditions requestConditions) {\n+        Integer originalBlockSize = (parallelTransferOptions == null)\n+            ? null\n+            : parallelTransferOptions.getBlockSize();\n+\n+        DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+            ? new DataLakeRequestConditions() : requestConditions;\n+        /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+           after creation, so e remove them for the append/flush calls. */\n+        DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+            .setLeaseId(validatedRequestConditions.getLeaseId());\n+\n+        final ParallelTransferOptions finalParallelTransferOptions =\n+            ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+        long fileOffset = 0;\n+\n+        try {\n+            return Mono.using(() -> uploadFileResourceSupplier(filePath),\n+                channel -> {\n+                    try {\n+                        long fileSize = channel.size();\n+\n+                        if (fileSize == 0) {\n+                            throw logger.logExceptionAsError(new IllegalArgumentException(\"Size of the file must be \"\n+                                + \"greater than 0.\"));\n+                        }\n+                        if (uploadInBlocks(filePath, finalParallelTransferOptions.getMaxSingleUploadSize())) {\n+                            return createWithResponse(null, null, headers, metadata, validatedRequestConditions)\n+                                .then(uploadBlocks(fileOffset, fileSize, finalParallelTransferOptions,\n+                                originalBlockSize, headers, validatedUploadRequestConditions, channel));\n+                        } else {\n+                            // Otherwise we know it can be sent in a single request reducing network overhead.\n+                            return createWithResponse(null, null, headers, metadata, validatedRequestConditions)\n+                                .then(uploadWithResponse(FluxUtil.readFile(channel), fileOffset, fileSize, headers,\n+                                validatedUploadRequestConditions))\n+                                .then();\n+                        }\n+\n+                    } catch (IOException ex) {\n+                        return Mono.error(ex);\n+                    }\n+                }, this::uploadFileCleanup);\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    boolean uploadInBlocks(String filePath, Integer maxSingleUploadSize) {\n+        AsynchronousFileChannel channel = uploadFileResourceSupplier(filePath);\n+        boolean retVal;\n+        try {\n+            retVal = channel.size() > maxSingleUploadSize;\n+        } catch (IOException e) {\n+            throw logger.logExceptionAsError(new UncheckedIOException(e));\n+        } finally {\n+            uploadFileCleanup(channel);\n+        }\n+\n+        return retVal;\n+    }\n+\n+    private Mono<Void> uploadBlocks(long fileOffset, long fileSize, ParallelTransferOptions parallelTransferOptions,\n+        Integer originalBlockSize, PathHttpHeaders headers, DataLakeRequestConditions requestConditions,\n+        AsynchronousFileChannel channel) {\n+        // parallelTransferOptions are finalized in the calling method.\n+\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        return Flux.fromIterable(sliceFile(fileSize, originalBlockSize, parallelTransferOptions.getBlockSize()))\n+            .flatMap(chunk -> {\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    FluxUtil.readFile(channel, chunk.getOffset(), chunk.getCount()),\n+                    parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+\n+                return appendWithResponse(progressData, fileOffset + chunk.getOffset(), chunk.getCount(), null,\n+                    requestConditions.getLeaseId());\n+            })\n+            .then(Mono.defer(() ->\n+                flushWithResponse(fileSize, false, false, headers, requestConditions)))\n+            .then();\n+    }\n+\n+    /**\n+     * RESERVED FOR INTERNAL USE.\n+     *\n+     * Resource Supplier for UploadFile.\n+     *\n+     * @param filePath The path for the file\n+     * @return {@code AsynchronousFileChannel}\n+     * @throws UncheckedIOException an input output exception.\n+     */\n+    protected AsynchronousFileChannel uploadFileResourceSupplier(String filePath) {\n+        try {\n+            return AsynchronousFileChannel.open(Paths.get(filePath), StandardOpenOption.READ);\n+        } catch (IOException e) {\n+            throw logger.logExceptionAsError(new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private void uploadFileCleanup(AsynchronousFileChannel channel) {\n+        try {\n+            channel.close();\n+        } catch (IOException e) {\n+            throw logger.logExceptionAsError(new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private List<FileRange> sliceFile(long fileSize, Integer originalBlockSize, int blockSize) {\n+        List<FileRange> ranges = new ArrayList<>();\n+        if (fileSize > 100 * Constants.MB && originalBlockSize == null) {\n+            blockSize = BlobAsyncClient.BLOB_DEFAULT_HTBB_UPLOAD_BLOCK_SIZE;\n+        }\n+        for (long pos = 0; pos < fileSize; pos += blockSize) {\n+            long count = blockSize;\n+            if (pos + count > fileSize) {\n+                count = fileSize - pos;\n+            }\n+            ranges.add(new FileRange(pos, count));\n+        }\n+        return ranges;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 436}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI1NjQ2MQ==", "bodyText": "yep, there was some difficulty with some of the logging stuff but it should go in common - I'll work on that", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382256461", "createdAt": "2020-02-20T21:02:06Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders headers, Map<String, String> metadata,\n+        DataLakeRequestConditions requestConditions) {\n+        try {\n+            Objects.requireNonNull(data, \"'data' must not be null\");\n+            DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+                ? new DataLakeRequestConditions() : requestConditions;\n+            /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+             after creation, so remove them for the append/flush calls. */\n+            DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+                .setLeaseId(validatedRequestConditions.getLeaseId());\n+            final ParallelTransferOptions validatedParallelTransferOptions =\n+                ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+            long fileOffset = 0;\n+\n+            Function<Flux<ByteBuffer>, Mono<Response<PathInfo>>> uploadInChunksFunction = (stream) ->\n+                uploadInChunks(stream, fileOffset, length, validatedParallelTransferOptions, headers,\n+                    validatedUploadRequestConditions);\n+\n+            BiFunction<Flux<ByteBuffer>, Long, Mono<Response<PathInfo>>> uploadFullMethod =\n+                (stream, lengthUploaded) -> uploadWithResponse(ProgressReporter\n+                        .addProgressReporting(stream, validatedParallelTransferOptions.getProgressReceiver()),\n+                    fileOffset, length, headers, validatedUploadRequestConditions);\n+\n+            return createWithResponse(null, null, headers, metadata, requestConditions)\n+                .then(UploadUtils.determineUploadFullOrChunked(data, validatedParallelTransferOptions,\n+                uploadInChunksFunction, uploadFullMethod));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadInChunks(Flux<ByteBuffer> data, long fileOffset, long length,\n+        ParallelTransferOptions parallelTransferOptions, PathHttpHeaders httpHeaders,\n+        DataLakeRequestConditions requestConditions) {\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        // Validation done in the constructor.\n+        UploadBufferPool pool = new UploadBufferPool(parallelTransferOptions.getNumBuffers(),\n+            parallelTransferOptions.getBlockSize(), MAX_APPEND_FILE_BYTES);\n+\n+        Flux<ByteBuffer> chunkedSource = UploadUtils.chunkSource(data, parallelTransferOptions);\n+\n+        /*\n+         Write to the pool and upload the output.\n+         */\n+        return chunkedSource.concatMap(pool::write)\n+            .concatWith(Flux.defer(pool::flush))\n+            /* Map the data to a tuple, writing in the buffer.remaining temporarily */\n+            .map(buffer -> Tuples.of(buffer, (long) buffer.remaining()))\n+            /* The tuple keeps track of the next buffer to write and the fileOffset for the next buffer */\n+            .scan((result, source) -> {\n+                ByteBuffer buffer = source.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long lastBytesWritten = result.getT2();\n+\n+                return Tuples.of(buffer, currentBufferLength + lastBytesWritten);\n+            })\n+            .flatMapSequential(tuple2 -> {\n+                ByteBuffer buffer = tuple2.getT1();\n+                long currentBufferLength = buffer.remaining();\n+                long currentOffset = tuple2.getT2() - currentBufferLength + fileOffset;\n+                // Report progress as necessary.\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    Flux.just(buffer), parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+                return appendWithResponse(progressData, currentOffset, currentBufferLength, null,\n+                    requestConditions.getLeaseId())\n+                    .doFinally(x -> pool.returnBuffer(buffer))\n+                    .flux();\n+            })\n+            .last()\n+            .flatMap(resp -> flushWithResponse(length, false, false, httpHeaders, requestConditions));\n+    }\n+\n+    private Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long fileOffset, long length,\n+        PathHttpHeaders httpHeaders, DataLakeRequestConditions requestConditions) {\n+        return appendWithResponse(data, fileOffset, length, null, requestConditions.getLeaseId())\n+            .flatMap(resp -> flushWithResponse(fileOffset + length, false, false, httpHeaders,\n+                requestConditions));\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file. By default this method will not overwrite an\n+     * existing file.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String}\n+     *\n+     * @param filePath Path to the upload file\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath) {\n+        try {\n+            return uploadFromFile(filePath, false);\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String-boolean}\n+     *\n+     * @param filePath Path to the upload file\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath, boolean overwrite) {\n+        try {\n+            Mono<Void> overwriteCheck = Mono.empty();\n+            DataLakeRequestConditions requestConditions = null;\n+\n+            // Note that if the file will be uploaded using a putBlob, we also can skip the exists check.\n+            if (!overwrite) {\n+                if (uploadInBlocks(filePath, DataLakeFileAsyncClient.MAX_APPEND_FILE_BYTES)) {\n+                    overwriteCheck = exists().flatMap(exists -> exists\n+                        ? monoError(logger, new IllegalArgumentException(Constants.FILE_ALREADY_EXISTS))\n+                        : Mono.empty());\n+                }\n+\n+                requestConditions = new DataLakeRequestConditions()\n+                    .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+            }\n+\n+            return overwriteCheck.then(uploadFromFile(filePath, null, null, null, requestConditions));\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    /**\n+     * Creates a new file, with the content of the specified file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadFromFile#String-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * @param filePath Path to the upload file\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} to use to upload from file. Number of parallel\n+     * transfers parameter is ignored.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return An empty response\n+     * @throws UncheckedIOException If an I/O error occurs\n+     */\n+    public Mono<Void> uploadFromFile(String filePath, ParallelTransferOptions parallelTransferOptions,\n+        PathHttpHeaders headers, Map<String, String> metadata, DataLakeRequestConditions requestConditions) {\n+        Integer originalBlockSize = (parallelTransferOptions == null)\n+            ? null\n+            : parallelTransferOptions.getBlockSize();\n+\n+        DataLakeRequestConditions validatedRequestConditions = requestConditions == null\n+            ? new DataLakeRequestConditions() : requestConditions;\n+        /* Since we are creating a file with the request conditions, everything but lease id becomes invalid\n+           after creation, so e remove them for the append/flush calls. */\n+        DataLakeRequestConditions validatedUploadRequestConditions = new DataLakeRequestConditions()\n+            .setLeaseId(validatedRequestConditions.getLeaseId());\n+\n+        final ParallelTransferOptions finalParallelTransferOptions =\n+            ModelHelper.populateAndApplyDefaults(parallelTransferOptions);\n+        long fileOffset = 0;\n+\n+        try {\n+            return Mono.using(() -> uploadFileResourceSupplier(filePath),\n+                channel -> {\n+                    try {\n+                        long fileSize = channel.size();\n+\n+                        if (fileSize == 0) {\n+                            throw logger.logExceptionAsError(new IllegalArgumentException(\"Size of the file must be \"\n+                                + \"greater than 0.\"));\n+                        }\n+                        if (uploadInBlocks(filePath, finalParallelTransferOptions.getMaxSingleUploadSize())) {\n+                            return createWithResponse(null, null, headers, metadata, validatedRequestConditions)\n+                                .then(uploadBlocks(fileOffset, fileSize, finalParallelTransferOptions,\n+                                originalBlockSize, headers, validatedUploadRequestConditions, channel));\n+                        } else {\n+                            // Otherwise we know it can be sent in a single request reducing network overhead.\n+                            return createWithResponse(null, null, headers, metadata, validatedRequestConditions)\n+                                .then(uploadWithResponse(FluxUtil.readFile(channel), fileOffset, fileSize, headers,\n+                                validatedUploadRequestConditions))\n+                                .then();\n+                        }\n+\n+                    } catch (IOException ex) {\n+                        return Mono.error(ex);\n+                    }\n+                }, this::uploadFileCleanup);\n+        } catch (RuntimeException ex) {\n+            return monoError(logger, ex);\n+        }\n+    }\n+\n+    boolean uploadInBlocks(String filePath, Integer maxSingleUploadSize) {\n+        AsynchronousFileChannel channel = uploadFileResourceSupplier(filePath);\n+        boolean retVal;\n+        try {\n+            retVal = channel.size() > maxSingleUploadSize;\n+        } catch (IOException e) {\n+            throw logger.logExceptionAsError(new UncheckedIOException(e));\n+        } finally {\n+            uploadFileCleanup(channel);\n+        }\n+\n+        return retVal;\n+    }\n+\n+    private Mono<Void> uploadBlocks(long fileOffset, long fileSize, ParallelTransferOptions parallelTransferOptions,\n+        Integer originalBlockSize, PathHttpHeaders headers, DataLakeRequestConditions requestConditions,\n+        AsynchronousFileChannel channel) {\n+        // parallelTransferOptions are finalized in the calling method.\n+\n+        // See ProgressReporter for an explanation on why this lock is necessary and why we use AtomicLong.\n+        AtomicLong totalProgress = new AtomicLong();\n+        Lock progressLock = new ReentrantLock();\n+\n+        return Flux.fromIterable(sliceFile(fileSize, originalBlockSize, parallelTransferOptions.getBlockSize()))\n+            .flatMap(chunk -> {\n+                Flux<ByteBuffer> progressData = ProgressReporter.addParallelProgressReporting(\n+                    FluxUtil.readFile(channel, chunk.getOffset(), chunk.getCount()),\n+                    parallelTransferOptions.getProgressReceiver(), progressLock, totalProgress);\n+\n+                return appendWithResponse(progressData, fileOffset + chunk.getOffset(), chunk.getCount(), null,\n+                    requestConditions.getLeaseId());\n+            })\n+            .then(Mono.defer(() ->\n+                flushWithResponse(fileSize, false, false, headers, requestConditions)))\n+            .then();\n+    }\n+\n+    /**\n+     * RESERVED FOR INTERNAL USE.\n+     *\n+     * Resource Supplier for UploadFile.\n+     *\n+     * @param filePath The path for the file\n+     * @return {@code AsynchronousFileChannel}\n+     * @throws UncheckedIOException an input output exception.\n+     */\n+    protected AsynchronousFileChannel uploadFileResourceSupplier(String filePath) {\n+        try {\n+            return AsynchronousFileChannel.open(Paths.get(filePath), StandardOpenOption.READ);\n+        } catch (IOException e) {\n+            throw logger.logExceptionAsError(new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private void uploadFileCleanup(AsynchronousFileChannel channel) {\n+        try {\n+            channel.close();\n+        } catch (IOException e) {\n+            throw logger.logExceptionAsError(new UncheckedIOException(e));\n+        }\n+    }\n+\n+    private List<FileRange> sliceFile(long fileSize, Integer originalBlockSize, int blockSize) {\n+        List<FileRange> ranges = new ArrayList<>();\n+        if (fileSize > 100 * Constants.MB && originalBlockSize == null) {\n+            blockSize = BlobAsyncClient.BLOB_DEFAULT_HTBB_UPLOAD_BLOCK_SIZE;\n+        }\n+        for (long pos = 0; pos < fileSize; pos += blockSize) {\n+            long count = blockSize;\n+            if (pos + count > fileSize) {\n+                count = fileSize - pos;\n+            }\n+            ranges.add(new FileRange(pos, count));\n+        }\n+        return ranges;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIzOTQ4MA=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 436}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjMyODc2OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/FileAPITest.groovy", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDozNToyNVrOFsiRHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMTo0OToxOFrOFskUyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI0MzEwMQ==", "bodyText": "grammar", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382243101", "createdAt": "2020-02-20T20:35:25Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/FileAPITest.groovy", "diffHunk": "@@ -1476,4 +1493,691 @@ class FileAPITest extends APISpec {\n         thrown(IllegalArgumentException)\n     }\n \n+    // no overwrite interrupted tests not ported over for datalake since the access conditions check is done on the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI3NjgwOQ==", "bodyText": "changed", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382276809", "createdAt": "2020-02-20T21:49:18Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/FileAPITest.groovy", "diffHunk": "@@ -1476,4 +1493,691 @@ class FileAPITest extends APISpec {\n         thrown(IllegalArgumentException)\n     }\n \n+    // no overwrite interrupted tests not ported over for datalake since the access conditions check is done on the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI0MzEwMQ=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NjM1NjE5OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDo0NToyOFrOFsiilw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMTo0OToxM1rOFskUoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI0NzU3NQ==", "bodyText": "Remove the length parameter as buffered upload should be able to calculate it.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382247575", "createdAt": "2020-02-20T20:45:28Z", "author": {"login": "rickle-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI3Njc2OQ==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382276769", "createdAt": "2020-02-20T21:49:13Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -148,6 +183,368 @@ public String getFileName() {\n \n     }\n \n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions) {\n+        return upload(data, length, parallelTransferOptions, false);\n+    }\n+\n+    /**\n+     * Creates a new file and uploads content.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.upload#Flux-long-ParallelTransferOptions-boolean}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param overwrite Whether or not to overwrite, should the file already exist.\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<PathInfo> upload(Flux<ByteBuffer> data, long length, ParallelTransferOptions parallelTransferOptions,\n+        boolean overwrite) {\n+\n+        Mono<Void> overwriteCheck;\n+        DataLakeRequestConditions requestConditions;\n+\n+        if (overwrite) {\n+            overwriteCheck = Mono.empty();\n+            requestConditions = null;\n+        } else {\n+            overwriteCheck = exists().flatMap(exists -> exists\n+                ? monoError(logger, new IllegalArgumentException(Constants.BLOB_ALREADY_EXISTS))\n+                : Mono.empty());\n+            requestConditions = new DataLakeRequestConditions()\n+                .setIfNoneMatch(Constants.HeaderConstants.ETAG_WILDCARD);\n+        }\n+\n+        return overwriteCheck\n+            .then(uploadWithResponse(data, length, parallelTransferOptions, null, null, requestConditions))\n+            .flatMap(FluxUtil::toMono);\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     * <p>\n+     * To avoid overwriting, pass \"*\" to {@link DataLakeRequestConditions#setIfNoneMatch(String)}.\n+     *\n+     * <p><strong>Code Samples</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions}\n+     *\n+     * <p><strong>Using Progress Reporting</strong></p>\n+     *\n+     * {@codesnippet com.azure.storage.file.datalake.DataLakeFileAsyncClient.uploadWithResponse#Flux-long-ParallelTransferOptions-PathHttpHeaders-Map-DataLakeRequestConditions.ProgressReporter}\n+     *\n+     * @param data The data to write to the file. Unlike other upload methods, this method does not require that the\n+     * {@code Flux} be replayable. In other words, it does not have to support multiple subscribers and is not expected\n+     * to produce the same values across subscriptions.\n+     * @param length The exact length of the data. It is important that this value match precisely the length of the\n+     * data emitted by the {@code Flux}.\n+     * @param parallelTransferOptions {@link ParallelTransferOptions} used to configure buffered uploading.\n+     * @param headers {@link PathHttpHeaders}\n+     * @param metadata Metadata to associate with the resource.\n+     * @param requestConditions {@link DataLakeRequestConditions}\n+     * @return A reactive response containing the information of the uploaded file.\n+     */\n+    public Mono<Response<PathInfo>> uploadWithResponse(Flux<ByteBuffer> data, long length,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI0NzU3NQ=="}, "originalCommit": {"oid": "94a87aabf71bf99546069295bc174b76736e2bce"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2Njk4OTk0OnYy", "diffSide": "RIGHT", "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/UploadBufferPool.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQwMToxMToxN1rOFsopcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxNzozODowM1rOFs_BOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM0NzYzNA==", "bodyText": "Because this class is blocking, and because it will block for significant amounts of time given we're handling buffers with it, we should wrap it in a reactive data structure using the tips described here for wrapping synchronous, blocking calls.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382347634", "createdAt": "2020-02-21T01:11:17Z", "author": {"login": "jaschrep-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/UploadBufferPool.java", "diffHunk": "@@ -0,0 +1,197 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.util.logging.ClientLogger;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+\n+/**\n+ * This type is to support the implementation of buffered upload only. It is mandatory that the caller has broken the\n+ * source into ByteBuffers that are no greater than the size of a chunk and therefore a buffer in the pool. This is\n+ * necessary because it upper bounds the number of buffers we need for a given call to write() to 2. If the size of\n+ * ByteBuffer passed into write() were unbounded, the pool could stall as it would run out of buffers before it is able\n+ * to return a result, and if it is unable to return, no data can be uploaded and therefore no pools returned.\n+ *\n+ * It is incumbent upon the caller to return the buffers after an upload is completed. It is also the caller's\n+ * responsibility to signal to the pool when the stream is empty and call flush to return any data still sitting in the\n+ * pool.\n+ *\n+ * Broadly, the workflow of this operation is to chunk the source into reasonable sized pieces. On each piece, one\n+ * thread will call write on the pool. The pool will grab a buffer from the queue to write to, possibly waiting for one\n+ * to be available, and either store the incomplete buffer to be filled on the next write or return the filled buffer to\n+ * be sent. Filled buffers can be uploaded in parallel and should return buffers to the pool after the upload completes.\n+ * Once the source terminates, it should call flush.\n+ *\n+ * RESERVED FOR INTERNAL USE ONLY\n+ */\n+public final class UploadBufferPool {\n+    private final ClientLogger logger = new ClientLogger(UploadBufferPool.class);\n+\n+    /*\n+    Note that a blocking on a synchronized object is not the same as blocking on a reactive operation; blocking on this\n+    queue will not compromise the async nature of this workflow. Fluxes themselves are internally synchronized to ensure\n+    only one call to onNext happens at a time.\n+     */\n+    private final BlockingQueue<ByteBuffer> buffers;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40e3b37e29b446f1f4132bc3177d06fa6bef518d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcwMDM4Nw==", "bodyText": "Just cause this class was a straight up copy paste from what exists in blobs, and it shouldnt affect any public APIs, I'll create an issue for this", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382700387", "createdAt": "2020-02-21T17:08:11Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/UploadBufferPool.java", "diffHunk": "@@ -0,0 +1,197 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.util.logging.ClientLogger;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+\n+/**\n+ * This type is to support the implementation of buffered upload only. It is mandatory that the caller has broken the\n+ * source into ByteBuffers that are no greater than the size of a chunk and therefore a buffer in the pool. This is\n+ * necessary because it upper bounds the number of buffers we need for a given call to write() to 2. If the size of\n+ * ByteBuffer passed into write() were unbounded, the pool could stall as it would run out of buffers before it is able\n+ * to return a result, and if it is unable to return, no data can be uploaded and therefore no pools returned.\n+ *\n+ * It is incumbent upon the caller to return the buffers after an upload is completed. It is also the caller's\n+ * responsibility to signal to the pool when the stream is empty and call flush to return any data still sitting in the\n+ * pool.\n+ *\n+ * Broadly, the workflow of this operation is to chunk the source into reasonable sized pieces. On each piece, one\n+ * thread will call write on the pool. The pool will grab a buffer from the queue to write to, possibly waiting for one\n+ * to be available, and either store the incomplete buffer to be filled on the next write or return the filled buffer to\n+ * be sent. Filled buffers can be uploaded in parallel and should return buffers to the pool after the upload completes.\n+ * Once the source terminates, it should call flush.\n+ *\n+ * RESERVED FOR INTERNAL USE ONLY\n+ */\n+public final class UploadBufferPool {\n+    private final ClientLogger logger = new ClientLogger(UploadBufferPool.class);\n+\n+    /*\n+    Note that a blocking on a synchronized object is not the same as blocking on a reactive operation; blocking on this\n+    queue will not compromise the async nature of this workflow. Fluxes themselves are internally synchronized to ensure\n+    only one call to onNext happens at a time.\n+     */\n+    private final BlockingQueue<ByteBuffer> buffers;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM0NzYzNA=="}, "originalCommit": {"oid": "40e3b37e29b446f1f4132bc3177d06fa6bef518d"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjcxNDE3MA==", "bodyText": "#8382", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382714170", "createdAt": "2020-02-21T17:38:03Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-common/src/main/java/com/azure/storage/common/implementation/UploadBufferPool.java", "diffHunk": "@@ -0,0 +1,197 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.storage.common.implementation;\n+\n+import com.azure.core.util.logging.ClientLogger;\n+import reactor.core.publisher.Flux;\n+\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+\n+/**\n+ * This type is to support the implementation of buffered upload only. It is mandatory that the caller has broken the\n+ * source into ByteBuffers that are no greater than the size of a chunk and therefore a buffer in the pool. This is\n+ * necessary because it upper bounds the number of buffers we need for a given call to write() to 2. If the size of\n+ * ByteBuffer passed into write() were unbounded, the pool could stall as it would run out of buffers before it is able\n+ * to return a result, and if it is unable to return, no data can be uploaded and therefore no pools returned.\n+ *\n+ * It is incumbent upon the caller to return the buffers after an upload is completed. It is also the caller's\n+ * responsibility to signal to the pool when the stream is empty and call flush to return any data still sitting in the\n+ * pool.\n+ *\n+ * Broadly, the workflow of this operation is to chunk the source into reasonable sized pieces. On each piece, one\n+ * thread will call write on the pool. The pool will grab a buffer from the queue to write to, possibly waiting for one\n+ * to be available, and either store the incomplete buffer to be filled on the next write or return the filled buffer to\n+ * be sent. Filled buffers can be uploaded in parallel and should return buffers to the pool after the upload completes.\n+ * Once the source terminates, it should call flush.\n+ *\n+ * RESERVED FOR INTERNAL USE ONLY\n+ */\n+public final class UploadBufferPool {\n+    private final ClientLogger logger = new ClientLogger(UploadBufferPool.class);\n+\n+    /*\n+    Note that a blocking on a synchronized object is not the same as blocking on a reactive operation; blocking on this\n+    queue will not compromise the async nature of this workflow. Fluxes themselves are internally synchronized to ensure\n+    only one call to onNext happens at a time.\n+     */\n+    private final BlockingQueue<ByteBuffer> buffers;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjM0NzYzNA=="}, "originalCommit": {"oid": "40e3b37e29b446f1f4132bc3177d06fa6bef518d"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDAwNzA4OnYy", "diffSide": "LEFT", "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/FileAPITest.groovy", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMTo0Njo1MlrOFtFiRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMTo0Njo1MlrOFtFiRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjgyMDkzMg==", "bodyText": "Nit - Can we get a TODO on things like this for easy discovery?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/8302#discussion_r382820932", "createdAt": "2020-02-21T21:46:52Z", "author": {"login": "jaschrep-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/FileAPITest.groovy", "diffHunk": "@@ -219,7 +230,7 @@ class FileAPITest extends APISpec {\n         def e = thrown(DataLakeStorageException)\n         e.getResponse().getStatusCode() == 404\n         e.getErrorCode() == BlobErrorCode.BLOB_NOT_FOUND.toString()\n-//        e.getServiceMessage().contains(\"The specified blob does not exist.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "392203656238aa364da8089c59da66fda52dd245"}, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3140, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}