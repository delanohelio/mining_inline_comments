{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMxNjAxODkz", "number": 17952, "title": "Cosmos Spark End to End Integration Test against Cosmos Emulator runs in CI", "bodyText": "Now we have gated CI for end to end test spark <-> cosmos db emulator.\nCosmos Spark End to End Test against Cosmos Emulator runs in CI.\n\nany test tagged by newly introduced tag, RequiresCosmosEndpoint will get included in sparkE2E test group and executed by the newly added CI, Spark_Integration_Tests_Java8 against cosmos db emulator.\nsee SparkE2EWriteSpec.scala for end to end spark sample test. It writes data from spark to cosmos db emulator. Emulator CI\n\nWe have two spark test groups running in the CI:\n\nunit: (only unit tests) this is the default test group.\nhow to run locally in your dev machine: mvn -e   -Dgpg.skip -Dmaven.javadoc.skip=true -Dspotbugs.skip=true -Dcheckstyle.skip=true -Drevapi.skip=true -pl ,azure-cosmos-spark_3-0_2-12 -am  clean test\nsparkE2E: requires cosmos db endpoint (integration tests runs against cosmos emulator)\nhow to run locally in your dev machine: mvn -e   -Dgpg.skip -Dmaven.javadoc.skip=true -Dspotbugs.skip=true -Dcheckstyle.skip=true -Drevapi.skip=true -pl ,azure-cosmos-spark_3-0_2-12 -am  -PsparkE2E clean test\n\n\nTODO:\n\nwe need a CI for java11.\nCosmos Emulator requires windows, hence the current CI is running on windows, we should add a CI on Linux (targeting prod account)\nsome patterns in the integration tests need to be figured out.\n-- proper resource (Database, Container) cleaning\n-- proper shutdown of CosmosClient and Spark session\n-- possible sharing of the CosmosClient and spark session between tests\n-- which scala test style should be used?", "createdAt": "2020-12-03T08:31:32Z", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17952", "merged": true, "mergeCommit": {"oid": "d7e9797a5b2cfe078998b10dbe738ed887ca0511"}, "closed": true, "closedAt": "2020-12-08T02:28:23Z", "author": {"login": "moderakh"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdh-y5zgH2gAyNTMxNjAxODkzOjc0YzhjZjNmNTM2NThjYTZjMTc4ZjI5NDY1YmViNzkyYTdhMDcyMDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdkACOEAFqTU0NjY3OTYxNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "74c8cf3f53658ca6c178f29465beb792a7a07208", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/74c8cf3f53658ca6c178f29465beb792a7a07208", "committedDate": "2020-12-01T19:06:27Z", "message": "more code comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23abec55612ba943b5851f1d11de1f217dc2a2ca", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/23abec55612ba943b5851f1d11de1f217dc2a2ca", "committedDate": "2020-12-02T21:43:43Z", "message": "Merge branch 'feature/cosmos/spark30' into users/moderakh/20201101-spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad763b6db5dd6da738065cf24b1b5bde306cbe42", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ad763b6db5dd6da738065cf24b1b5bde306cbe42", "committedDate": "2020-12-03T08:29:26Z", "message": "spark end to end test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "382619dfdb551be055fb6e49c0f0deae95476d21", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/382619dfdb551be055fb6e49c0f0deae95476d21", "committedDate": "2020-12-03T08:34:28Z", "message": "added missing files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffbcd34d97de5d2354ecb0a85e782ec32f702b37", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ffbcd34d97de5d2354ecb0a85e782ec32f702b37", "committedDate": "2020-12-03T08:37:05Z", "message": "undid an unrelated change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce5f0dd9f419fab01e054dbd1b0f6ad85644b95e", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ce5f0dd9f419fab01e054dbd1b0f6ad85644b95e", "committedDate": "2020-12-03T08:40:51Z", "message": "cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ea96116f06973cc4508a220d075e52706926f4f", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/5ea96116f06973cc4508a220d075e52706926f4f", "committedDate": "2020-12-03T16:29:13Z", "message": "removed intentionally failing test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48830b5a58c04347b13ef12f36bf965aeb26583b", "author": {"user": {"login": "moderakh", "name": "Mohammad Derakhshani"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/48830b5a58c04347b13ef12f36bf965aeb26583b", "committedDate": "2020-12-03T16:35:08Z", "message": "updated comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ2Njc4ODM0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17952#pullrequestreview-546678834", "createdAt": "2020-12-08T01:38:52Z", "commit": {"oid": "48830b5a58c04347b13ef12f36bf965aeb26583b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMTozODo1M1rOIBC0vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwMTozODo1M1rOIBC0vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzk2NTc1Nw==", "bodyText": "Good point - I think it might be useful to see whether we can make that decision based on \"avg.\" document size? Like < 1 KB don't push down pruning - but for larger documents do it?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17952#discussion_r537965757", "createdAt": "2020-12-08T01:38:53Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosScanBuilder.scala", "diffHunk": "@@ -51,7 +51,21 @@ case class CosmosScanBuilder(config: CaseInsensitiveStringMap)\n     CosmosScan(config.asScala.toMap, this.processedPredicates.get.cosmosParametrizedQuery)\n   }\n \n+  /**\n+    * Applies column pruning w.r.t. the given requiredSchema.\n+    *\n+    * Implementation should try its best to prune the unnecessary columns or nested fields, but it's\n+    * also OK to do the pruning partially, e.g., a data source may not be able to prune nested\n+    * fields, and only prune top-level columns.\n+    *\n+    * Note that, {@link Scan# readSchema ( )} implementation should take care of the column\n+    * pruning applied here.\n+    */\n   override def pruneColumns(requiredSchema: StructType): Unit = {\n-    // TODO moderakh add projection to the query\n+    // TODO moderakh: we need to decide whether do a push down or not on the projection", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48830b5a58c04347b13ef12f36bf965aeb26583b"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ2Njc5NjE0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17952#pullrequestreview-546679614", "createdAt": "2020-12-08T01:40:56Z", "commit": {"oid": "48830b5a58c04347b13ef12f36bf965aeb26583b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1336, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}