{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2MzM5MzY0", "number": 10383, "title": "[Storage][Jumbo blobs] Bump single upload max threshold. Preserve defaults.", "bodyText": "Max single upload threshold goes to 5000MB for blobs (and 4000MB for datalake).\nKeeping previous defaults 256MB for blob and 100MB for datalake if not specified, so that customers leveraging parallel uploads for payloads larger than former defaults won't see performance degradation.", "createdAt": "2020-04-20T22:40:11Z", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383", "merged": true, "mergeCommit": {"oid": "eef31265e1bc92729a616409ff8219a3b891aad5"}, "closed": true, "closedAt": "2020-04-21T20:13:59Z", "author": {"login": "kasobol-msft"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcXRm5qgH2gAyNDA2MzM5MzY0OjU5Yzc4YjIxN2E1ZjNiY2QxODhiMDUxZTBkOGEzYjU0ZjVlNmJhOWI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZ5funAFqTM5NzY0MDU0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "59c78b217a5f3bcd188b051e0d8a3b54f5e6ba9b", "author": {"user": {"login": "kasobol-msft", "name": "Kamil Sobol"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/59c78b217a5f3bcd188b051e0d8a3b54f5e6ba9b", "committedDate": "2020-04-13T16:37:13Z", "message": "change versioned account for tests variable names"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cbfa86af23cf366fc9904fa86040a473c042ef75", "author": {"user": {"login": "kasobol-msft", "name": "Kamil Sobol"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cbfa86af23cf366fc9904fa86040a473c042ef75", "committedDate": "2020-04-20T15:00:42Z", "message": "Merge remote-tracking branch 'upstream/feature/storage/stg73' into feature/storage/stg73"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ded73b52c0c044bbe0500fe587b84b68c4d06a25", "author": {"user": {"login": "kasobol-msft", "name": "Kamil Sobol"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/ded73b52c0c044bbe0500fe587b84b68c4d06a25", "committedDate": "2020-04-20T22:36:44Z", "message": "[Storage][Jumbo blobs] Bump single upload threshold. Preserve current default single upload."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2ODcxNzk1", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#pullrequestreview-396871795", "createdAt": "2020-04-20T23:04:13Z", "commit": {"oid": "ded73b52c0c044bbe0500fe587b84b68c4d06a25"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMzowNDoxM1rOGIrMZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMzowNDoxM1rOGIrMZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTQ3OQ==", "bodyText": "Is this TODO meant to be part of this PR?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411749479", "createdAt": "2020-04-20T23:04:13Z", "author": {"login": "gapra-msft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -70,6 +77,84 @@ class LargeBlobTest extends APISpec {\n         blockList.committedBlocks.get(0).getSizeLong() == maxBlockSize\n     }\n \n+    @Requires({ liveMode() })\n+    @Ignore(\"Takes really long time\")\n+    // This test sends payload over the wire\n+    def \"Upload Real Large Blob in Single Upload\"() {\n+        given:\n+        // TODO (kasobol-msft) Bump this to 5000MB.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ded73b52c0c044bbe0500fe587b84b68c4d06a25"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2ODcyNTc4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#pullrequestreview-396872578", "createdAt": "2020-04-20T23:06:19Z", "commit": {"oid": "ded73b52c0c044bbe0500fe587b84b68c4d06a25"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf3b2ee230f6876929ab843905b148b3934b777b", "author": {"user": {"login": "kasobol-msft", "name": "Kamil Sobol"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/cf3b2ee230f6876929ab843905b148b3934b777b", "committedDate": "2020-04-21T07:06:58Z", "message": "cover default data lake single upload threshod. and fix bug in datalake parallel upload."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3MDQ3Mjkw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#pullrequestreview-397047290", "createdAt": "2020-04-21T07:12:57Z", "commit": {"oid": "cf3b2ee230f6876929ab843905b148b3934b777b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNzoxMjo1OFrOGI2M3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwNzoxMjo1OFrOGI2M3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTkyOTgyMw==", "bodyText": "Should have used .lenght() everywhere in this  method after transition from ByteBuffer to BufferAggregator.\nConfusing method has been hidden and renamed...", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r411929823", "createdAt": "2020-04-21T07:12:58Z", "author": {"login": "kasobol-msft"}, "path": "sdk/storage/azure-storage-file-datalake/src/main/java/com/azure/storage/file/datalake/DataLakeFileAsyncClient.java", "diffHunk": "@@ -307,7 +307,7 @@ public String getFileName() {\n         return chunkedSource.concatMap(pool::write)\n             .concatWith(Flux.defer(pool::flush))\n             /* Map the data to a tuple 3, of buffer, buffer length, buffer offset */\n-            .map(buffer -> Tuples.of(buffer, buffer.remaining(), 0L))\n+            .map(bufferAggregator -> Tuples.of(bufferAggregator, bufferAggregator.length(), 0L))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf3b2ee230f6876929ab843905b148b3934b777b"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1", "author": {"user": {"login": "kasobol-msft", "name": "Kamil Sobol"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/165658247ecc4023ffca708a676deb8ac181d1b1", "committedDate": "2020-04-21T16:13:57Z", "message": "fix find bugs."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3NTU0NTc4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#pullrequestreview-397554578", "createdAt": "2020-04-21T18:11:49Z", "commit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxODoxMTo0OVrOGJRxOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQxODoyOTo1NFrOGJShgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4MTQ5Nw==", "bodyText": "Should these be test class level properties or be migrated into the pipeline policy being used to test this functionality? I don't know when in the test loop, if ever, these would be reset to the default value again.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412381497", "createdAt": "2020-04-21T18:11:49Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -28,8 +32,11 @@ class LargeBlobTest extends APISpec {\n     BlobClient blobClient\n     BlobAsyncClient blobAsyncClient\n     String blobName\n-    List<Mono<Long>> putBlockPayloadSizes = Collections.synchronizedList(new ArrayList<>())\n-    AtomicLong count = new AtomicLong()\n+    List<Long> putBlockPayloadSizes = Collections.synchronizedList(new ArrayList<>())\n+    AtomicLong blocksCount = new AtomicLong()\n+    List<Long> putBlobPayloadSizes = Collections.synchronizedList(new ArrayList<>())\n+    AtomicLong singleUploadCount = new AtomicLong()\n+    ConcurrentHashMap<String, Boolean> retryTracker = new ConcurrentHashMap<>()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM4NzYyMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    int reminder = (int) (size - numberOfSubBuffers * bufferSize)\n          \n          \n            \n                    int remainder = (int) (size - numberOfSubBuffers * bufferSize)", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412387622", "createdAt": "2020-04-21T18:20:44Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-file-datalake/src/test/java/com/azure/storage/file/datalake/LargeFileTest.groovy", "diffHunk": "@@ -148,16 +152,44 @@ class LargeFileTest extends APISpec{\n         notThrown(DataLakeStorageException)\n     }\n \n+    @Unroll\n+    @Requires({ liveMode() })\n+    // This test does not send large payload over the wire\n+    def \"Should honor default single upload threshold\"() {\n+        given:\n+        def data = createLargeBuffer(dataSize)\n+        def transferOptions = new ParallelTransferOptions()\n+            .setBlockSizeLong(10L * Constants.MB) // set this much lower than default single upload size to make it tempting.\n+\n+        when:\n+        fcAsyncPayloadDropping.upload(data, transferOptions, true).block()\n+\n+        then:\n+        notThrown(DataLakeStorageException)\n+        count.get() == expectedAppendRequests\n+\n+        where:\n+        dataSize                         | expectedAppendRequests\n+        defaultSingleUploadThreshold     | 1\n+        defaultSingleUploadThreshold + 1 | 11\n+    }\n+\n     private Flux<ByteBuffer> createLargeBuffer(long size) {\n         return createLargeBuffer(size, Constants.MB)\n     }\n \n     private Flux<ByteBuffer> createLargeBuffer(long size, int bufferSize) {\n         def bytes = getRandomByteArray(bufferSize)\n         long numberOfSubBuffers = (long) (size / bufferSize)\n-        return Flux.just(ByteBuffer.wrap(bytes))\n+        int reminder = (int) (size - numberOfSubBuffers * bufferSize)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MTk5OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    int reminder = (int) (size - numberOfSubBuffers * bufferSize)\n          \n          \n            \n                    int remainder = (int) (size - numberOfSubBuffers * bufferSize)", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412391999", "createdAt": "2020-04-21T18:27:10Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -204,9 +286,15 @@ class LargeBlobTest extends APISpec {\n     private Flux<ByteBuffer> createLargeBuffer(long size, int bufferSize) {\n         def bytes = getRandomByteArray(bufferSize)\n         long numberOfSubBuffers = (long) (size / bufferSize)\n-        return Flux.just(ByteBuffer.wrap(bytes))\n+        int reminder = (int) (size - numberOfSubBuffers * bufferSize)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzE5NA==", "bodyText": "Would it be better to check for certain parts of the query string being null in case this gets used by a client authorizing with a SAS token.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412393194", "createdAt": "2020-04-21T18:28:56Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -232,20 +320,57 @@ class LargeBlobTest extends APISpec {\n         Mono<HttpResponse> process(HttpPipelineCallContext httpPipelineCallContext, HttpPipelineNextPolicy httpPipelineNextPolicy) {\n             def request = httpPipelineCallContext.httpRequest\n             // Substitute large body for put block requests and collect size of original body\n-            if (request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\")) {\n-                if (collectSize) {\n-                    def bytesReceived = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n-                        @Override\n-                        Long apply(Long a, ByteBuffer byteBuffer) {\n-                            return a + byteBuffer.remaining()\n-                        }\n-                    })\n-                    putBlockPayloadSizes.add(bytesReceived)\n+            def urlString = request.getUrl().toString()\n+            if (isPutBlockRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    blocksCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlockPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n+                }\n+            } else if (isSinglePutBlobRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    singleUploadCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlobPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n                 }\n-                count.incrementAndGet()\n-                request.setBody(\"dummyBody\")\n             }\n             return httpPipelineNextPolicy.process()\n         }\n+\n+        private Mono<Long> interceptBody(HttpRequest request) {\n+            Mono<Long> result = null\n+            if (collectSize) {\n+                result = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n+                    @Override\n+                    Long apply(Long a, ByteBuffer byteBuffer) {\n+                        return a + byteBuffer.remaining()\n+                    }\n+                })\n+            }\n+            request.setBody(\"dummyBody\")\n+            return result\n+        }\n+\n+        private boolean isPutBlockRequest(HttpRequest request) {\n+            return request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\");\n+        }\n+\n+        private boolean isSinglePutBlobRequest(HttpRequest request) {\n+            return request.getHttpMethod().equals(HttpMethod.PUT) &&\n+                request.getUrl().toString().endsWith(blobName) &&\n+                request.getUrl().getQuery() == null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5MzUwMA==", "bodyText": "I think this should check that the path ends with the blob name.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412393500", "createdAt": "2020-04-21T18:29:23Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -232,20 +320,57 @@ class LargeBlobTest extends APISpec {\n         Mono<HttpResponse> process(HttpPipelineCallContext httpPipelineCallContext, HttpPipelineNextPolicy httpPipelineNextPolicy) {\n             def request = httpPipelineCallContext.httpRequest\n             // Substitute large body for put block requests and collect size of original body\n-            if (request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\")) {\n-                if (collectSize) {\n-                    def bytesReceived = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n-                        @Override\n-                        Long apply(Long a, ByteBuffer byteBuffer) {\n-                            return a + byteBuffer.remaining()\n-                        }\n-                    })\n-                    putBlockPayloadSizes.add(bytesReceived)\n+            def urlString = request.getUrl().toString()\n+            if (isPutBlockRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    blocksCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlockPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n+                }\n+            } else if (isSinglePutBlobRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    singleUploadCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlobPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n                 }\n-                count.incrementAndGet()\n-                request.setBody(\"dummyBody\")\n             }\n             return httpPipelineNextPolicy.process()\n         }\n+\n+        private Mono<Long> interceptBody(HttpRequest request) {\n+            Mono<Long> result = null\n+            if (collectSize) {\n+                result = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n+                    @Override\n+                    Long apply(Long a, ByteBuffer byteBuffer) {\n+                        return a + byteBuffer.remaining()\n+                    }\n+                })\n+            }\n+            request.setBody(\"dummyBody\")\n+            return result\n+        }\n+\n+        private boolean isPutBlockRequest(HttpRequest request) {\n+            return request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\");\n+        }\n+\n+        private boolean isSinglePutBlobRequest(HttpRequest request) {\n+            return request.getHttpMethod().equals(HttpMethod.PUT) &&\n+                request.getUrl().toString().endsWith(blobName) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5Mzg1OA==", "bodyText": "Should this check that the query contains comp=block.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#discussion_r412393858", "createdAt": "2020-04-21T18:29:54Z", "author": {"login": "alzimmermsft"}, "path": "sdk/storage/azure-storage-blob/src/test/java/com/azure/storage/blob/LargeBlobTest.groovy", "diffHunk": "@@ -232,20 +320,57 @@ class LargeBlobTest extends APISpec {\n         Mono<HttpResponse> process(HttpPipelineCallContext httpPipelineCallContext, HttpPipelineNextPolicy httpPipelineNextPolicy) {\n             def request = httpPipelineCallContext.httpRequest\n             // Substitute large body for put block requests and collect size of original body\n-            if (request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\")) {\n-                if (collectSize) {\n-                    def bytesReceived = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n-                        @Override\n-                        Long apply(Long a, ByteBuffer byteBuffer) {\n-                            return a + byteBuffer.remaining()\n-                        }\n-                    })\n-                    putBlockPayloadSizes.add(bytesReceived)\n+            def urlString = request.getUrl().toString()\n+            if (isPutBlockRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    blocksCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlockPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n+                }\n+            } else if (isSinglePutBlobRequest(request)) {\n+                if (!retryTracker.get(urlString, false)) {\n+                    singleUploadCount.incrementAndGet()\n+                    retryTracker.put(urlString, true)\n+                }\n+                Mono<Long> count = interceptBody(request)\n+                if (count != null) {\n+                    return count.flatMap { bytes ->\n+                        putBlobPayloadSizes.add(bytes)\n+                        return httpPipelineNextPolicy.process()\n+                    }\n                 }\n-                count.incrementAndGet()\n-                request.setBody(\"dummyBody\")\n             }\n             return httpPipelineNextPolicy.process()\n         }\n+\n+        private Mono<Long> interceptBody(HttpRequest request) {\n+            Mono<Long> result = null\n+            if (collectSize) {\n+                result = request.getBody().reduce(0L, new BiFunction<Long, ByteBuffer, Long>() {\n+                    @Override\n+                    Long apply(Long a, ByteBuffer byteBuffer) {\n+                        return a + byteBuffer.remaining()\n+                    }\n+                })\n+            }\n+            request.setBody(\"dummyBody\")\n+            return result\n+        }\n+\n+        private boolean isPutBlockRequest(HttpRequest request) {\n+            return request.url.getQuery() != null && request.url.getQuery().endsWith(\"comp=block\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "165658247ecc4023ffca708a676deb8ac181d1b1"}, "originalPosition": 258}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8591eeeac05e14f0c7debbb63d3214b6a72f984", "author": {"user": {"login": "kasobol-msft", "name": "Kamil Sobol"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/e8591eeeac05e14f0c7debbb63d3214b6a72f984", "committedDate": "2020-04-21T19:09:28Z", "message": "pr feedback."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3NjAwMzgw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#pullrequestreview-397600380", "createdAt": "2020-04-21T19:14:57Z", "commit": {"oid": "e8591eeeac05e14f0c7debbb63d3214b6a72f984"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3NjQwNTQ0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/10383#pullrequestreview-397640544", "createdAt": "2020-04-21T20:13:26Z", "commit": {"oid": "e8591eeeac05e14f0c7debbb63d3214b6a72f984"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4721, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}