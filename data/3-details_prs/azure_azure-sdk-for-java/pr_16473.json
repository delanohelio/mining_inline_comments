{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA1Mjc0NTM0", "number": 16473, "title": "Changing the 449 retry policy to force back-off even on initial retry and start with shorter back-offs", "bodyText": "", "createdAt": "2020-10-17T12:33:09Z", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473", "merged": true, "mergeCommit": {"oid": "efc3d3065128874fe32b6d13be91ac3216c71078"}, "closed": true, "closedAt": "2020-10-20T21:49:25Z", "author": {"login": "FabianMeiswinkel"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdTPuAAgH2gAyNTA1Mjc0NTM0OjRiZWQzNjExNjU4OWE0MTdhOWQ0NTNjZmM3ZjRiOTEyYmY5YTI4OGI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdUdA8gAFqTUxMzAxNDQxNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "4bed36116589a417a9d453cfc7f4b912bf9a288b", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/4bed36116589a417a9d453cfc7f4b912bf9a288b", "committedDate": "2020-10-17T00:20:37Z", "message": "Reafactoring GoneAndRetryWithRetryPolicy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "887c4a164e4344cbe6a4c9b01ceb254eade26197", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/887c4a164e4344cbe6a4c9b01ceb254eade26197", "committedDate": "2020-10-17T00:22:20Z", "message": "Removing separate classes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ebe234a95be2ed0dc556eee1423ead820025b38", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/3ebe234a95be2ed0dc556eee1423ead820025b38", "committedDate": "2020-10-17T00:42:00Z", "message": "Fixing compile time issue"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8602b3508564089708de85936dadb3e5fb618222", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/8602b3508564089708de85936dadb3e5fb618222", "committedDate": "2020-10-17T10:09:11Z", "message": "Iterating on 449 change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "597a1007ee27903becd16dfff1aab4d65caeb417", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/597a1007ee27903becd16dfff1aab4d65caeb417", "committedDate": "2020-10-17T11:03:28Z", "message": "Merge branch 'master' of https://github.com/Azure/azure-sdk-for-java into users/fabianm/449RetryBackoff"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/7a4e2dee6c5898cedb7eae728d7749f83a160adb", "committedDate": "2020-10-17T12:22:21Z", "message": "Fixing test regressions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwOTkyNTUw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-510992550", "createdAt": "2020-10-17T15:13:40Z", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNToxMzo0MFrOHjd7-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNToxMzo0MFrOHjd7-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjk1MjY5Nw==", "bodyText": "Nice refractoring of leveraging the composition.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r506952697", "createdAt": "2020-10-17T15:13:40Z", "author": {"login": "kirankumarkolli"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "originalPosition": 107}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwOTkyNzM4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-510992738", "createdAt": "2020-10-17T15:16:55Z", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNToxNjo1NVrOHjd9GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNToxNjo1NVrOHjd9GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjk1Mjk4NA==", "bodyText": "NON-BLOCKER: Is exponential retry used in many retry policies, if so would it help if its aslo refractored out?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r506952984", "createdAt": "2020-10-17T15:16:55Z", "author": {"login": "kirankumarkolli"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "originalPosition": 107}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwOTkyOTE0", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-510992914", "createdAt": "2020-10-17T15:19:57Z", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNToxOTo1N1rOHjd-AA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNToxOTo1N1rOHjd-AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjk1MzIxNg==", "bodyText": "Do the level needs to be higher like at-least WARN?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r506953216", "createdAt": "2020-10-17T15:19:57Z", "author": {"login": "kirankumarkolli"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final StopWatch durationTimer;\n+        private final int waitTimeInSeconds;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            StopWatch durationTimer) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.durationTimer = durationTimer;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            RetryWithException lastRetryWithException = this.request.getLastRetryWithException();\n+\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);\n+\n+                return (CosmosException)exception;\n+            } else {\n+                throw new IllegalStateException(\"Invalid exception type\", exception);\n+            }\n+\n+            if (lastRetryWithException != null) {\n+                logger.warn(\n+                    \"Received {} after backoff/retry including at least one RetryWithException. \"\n+                        + \"Will fail the request with RetryWithException. {}: {}. RetryWithException: {}\",\n+                    exceptionType,\n+                    exceptionType,\n+                    exception,\n+                    lastRetryWithException);\n+\n+                return lastRetryWithException;\n+            }\n \n             logger.warn(\n-                \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n-                    \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n-                this.attemptCount,\n+                \"Received {} after backoff/retry. Will fail the request. {}\",\n+                exceptionType,\n                 exception);\n-            stopStopWatch(this.durationTimer);\n-\n-            return Mono.just(ShouldRetryResult.noRetry(\n-                Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n-        } else if (exception instanceof RetryWithException) {\n-            this.lastRetryWithException = (RetryWithException) exception;\n+            return BridgeInternal.createServiceUnavailableException(exception);\n         }\n-        long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n-        int currentRetryAttemptCount = this.attemptCount;\n-        if (this.attemptCount++ > 1) {\n-            if (remainingSeconds <= 0) {\n-                if (exception instanceof GoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received gone exception after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\"Received gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof PartitionKeyRangeGoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry including at least one RetryWithException.\"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof InvalidPartitionException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received InvalidPartitionException after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. InvalidPartitionException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                    } else {\n-                        logger.warn(\n-                                \"Received invalid collection partition exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else {\n-                    logger.warn(\"Received retrywith exception after backoff/retry. Will fail the request. {}\",\n-                            exception.toString());\n+\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            CosmosException exceptionToThrow;\n+            Duration backoffTime = Duration.ofSeconds(0);\n+            Duration timeout;\n+            boolean forceRefreshAddressCache;\n+            if (!isRetryableException(exception)) {\n+\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            } else if (exception instanceof GoneException &&\n+                !request.isReadOnly() &&\n+                BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+                logger.warn(\n+                    \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n+                        \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n+                    this.attemptCount,\n+                    exception);\n+\n+                return Mono.just(ShouldRetryResult.noRetry(\n+                    Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n+            }\n+\n+            long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n+            int currentRetryAttemptCount = this.attemptCount;\n+            if (this.attemptCount++ > 1) {\n+                if (remainingSeconds <= 0) {\n+                    exceptionToThrow = logAndWrapExceptionWithLastRetryWithException(exception);\n+                    return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n                 }\n-                stopStopWatch(this.durationTimer);\n-                return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n+\n+                backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n+                    GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n+                this.currentBackoffSeconds *= GoneRetryPolicy.BACK_OFF_MULTIPLIER;\n+                logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n             }\n-            backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n-                    GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n-            this.currentBackoffSeconds *= GoneAndRetryWithRetryPolicy.BACK_OFF_MULTIPLIER;\n-            logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n+\n+            // Calculate the remaining time based after accounting for the backoff that we\n+            // will perform\n+            long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n+            timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n+                : Duration.ofSeconds(GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n+\n+            Pair<Mono<ShouldRetryResult>, Boolean> exceptionHandlingResult = handleException(exception);\n+            Mono<ShouldRetryResult> result = exceptionHandlingResult.getLeft();\n+            if (result != null) {\n+                return result;\n+            }\n+\n+            forceRefreshAddressCache = exceptionHandlingResult.getRight();\n+\n+            return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n+                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleException(Exception exception) {\n+            if (exception instanceof GoneException) {\n+                return handleGoneException((GoneException)exception);\n+            } else if (exception instanceof PartitionIsMigratingException) {\n+                return handlePartitionIsMigratingException((PartitionIsMigratingException)exception);\n+            } else if (exception instanceof InvalidPartitionException) {\n+                return handleInvalidPartitionException((InvalidPartitionException)exception);\n+            } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n+                return handlePartitionKeyIsSplittingException((PartitionKeyRangeIsSplittingException) exception);\n+            }\n+\n+            throw new IllegalStateException(\"Invalid exception type\", exception);\n         }\n \n-        // Calculate the remaining time based after accounting for the backoff that we\n-        // will perform\n-        long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n-        timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n-                : Duration.ofSeconds(GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n-        if (exception instanceof GoneException) {\n-            logger.debug(\"Received gone exception, will retry, {}\", exception.toString());\n-            forceRefreshAddressCache = true; // indicate we are in retry.\n-        } else if (exception instanceof PartitionIsMigratingException) {\n-            logger.warn(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleGoneException(GoneException exception) {\n+            logger.info(\"Received gone exception, will retry, {}\", exception.toString());\n+            return Pair.of(null, true); // indicate we are in retry.\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionIsMigratingException(PartitionIsMigratingException exception) {\n+            logger.info(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n             this.request.forceCollectionRoutingMapRefresh = true;\n-            forceRefreshAddressCache = true;\n-        } else if (exception instanceof InvalidPartitionException) {\n+            return Pair.of(null, true);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionKeyIsSplittingException(PartitionKeyRangeIsSplittingException exception) {\n+            this.request.requestContext.resolvedPartitionKeyRange = null;\n+            this.request.requestContext.quorumSelectedLSN = -1;\n+            this.request.requestContext.quorumSelectedStoreResponse = null;\n+            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n+            this.request.forcePartitionKeyRangeRefresh = true;\n+            return Pair.of(null, false);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleInvalidPartitionException(InvalidPartitionException exception) {\n             this.request.requestContext.quorumSelectedLSN = -1;\n             this.request.requestContext.resolvedPartitionKeyRange = null;\n             this.request.requestContext.quorumSelectedStoreResponse = null;\n             this.request.requestContext.globalCommittedSelectedLSN = -1;\n             if (this.attemptCountInvalidPartition++ > 2) {\n                 // for second InvalidPartitionException, stop retrying.\n                 logger.warn(\"Received second InvalidPartitionException after backoff/retry. Will fail the request. {}\",\n-                        exception.toString());\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createServiceUnavailableException(exception)));\n+                    exception.toString());\n+                return Pair.of(\n+                    Mono.just(ShouldRetryResult.error(BridgeInternal.createServiceUnavailableException(exception))),\n+                    false);\n             }\n \n-            if (this.request != null) {\n-                logger.warn(\"Received invalid collection exception, will retry, {}\", exception.toString());\n-                this.request.forceNameCacheRefresh = true;\n-            } else {\n-                logger.error(\"Received unexpected invalid collection exception, request should be non-null.\",\n-                        exception);\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createCosmosException(HttpConstants.StatusCodes.INTERNAL_SERVER_ERROR, exception)));\n-            }\n-            forceRefreshAddressCache = false;\n-        } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n-            this.request.requestContext.resolvedPartitionKeyRange = null;\n-            this.request.requestContext.quorumSelectedLSN = -1;\n-            this.request.requestContext.quorumSelectedStoreResponse = null;\n-            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n-            this.request.forcePartitionKeyRangeRefresh = true;\n-            forceRefreshAddressCache = false;\n-        } else {\n-            logger.warn(\"Received retrywith exception, will retry, {}\", exception);\n-            // For RetryWithException, prevent the caller\n-            // from refreshing any caches.\n-            forceRefreshAddressCache = false;\n+            logger.info(\"Received invalid collection exception, will retry, {}\", exception.toString());\n+            this.request.forceNameCacheRefresh = true;\n+\n+            return Pair.of(null, false);\n         }\n-        return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n-                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n     }\n \n-    private void stopStopWatch(StopWatch stopwatch) {\n-        synchronized (stopwatch) {\n-            stopwatch.stop();\n+    static class RetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_MS = 15000;\n+        private final static int INITIAL_BACKOFF_TIME_MS = 10;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int currentBackoffMilliseconds = RetryWithRetryPolicy.INITIAL_BACKOFF_TIME_MS;\n+\n+        private final int waitTimeInSeconds;\n+        private final StopWatch durationTimer;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public RetryWithRetryPolicy(RxDocumentServiceRequest request,\n+                                    Integer waitTimeInSeconds,\n+                                    StopWatch durationTimer) {\n+            this.request = request;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+            this.durationTimer = durationTimer;\n         }\n-    }\n \n-    private void startStopWatch(StopWatch stopwatch) {\n-        synchronized (stopwatch) {\n-            stopwatch.start();\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            Duration backoffTime;\n+            Duration timeout;\n+\n+            if (!(exception instanceof RetryWithException)) {\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            }\n+\n+            RetryWithException lastRetryWithException = (RetryWithException)exception;\n+            this.request.setLastRetryWithException(lastRetryWithException);\n+\n+            long remainingMilliseconds = (this.waitTimeInSeconds * 1000) - this.durationTimer.getTime();\n+            int currentRetryAttemptCount = this.attemptCount++;\n+\n+            if (remainingMilliseconds <= 0) {\n+                logger.debug(\"Received retrywith exception after backoff/retry. Will fail the request.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "originalPosition": 450}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwOTk0MTU4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-510994158", "createdAt": "2020-10-17T15:38:38Z", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNTozODozOFrOHjeE0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNTozODozOFrOHjeE0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjk1NDk2MQ==", "bodyText": "If the values needs experimentation, we can make them configurable through JVM args", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r506954961", "createdAt": "2020-10-17T15:38:38Z", "author": {"login": "kirankumarkolli"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final StopWatch durationTimer;\n+        private final int waitTimeInSeconds;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            StopWatch durationTimer) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.durationTimer = durationTimer;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            RetryWithException lastRetryWithException = this.request.getLastRetryWithException();\n+\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);\n+\n+                return (CosmosException)exception;\n+            } else {\n+                throw new IllegalStateException(\"Invalid exception type\", exception);\n+            }\n+\n+            if (lastRetryWithException != null) {\n+                logger.warn(\n+                    \"Received {} after backoff/retry including at least one RetryWithException. \"\n+                        + \"Will fail the request with RetryWithException. {}: {}. RetryWithException: {}\",\n+                    exceptionType,\n+                    exceptionType,\n+                    exception,\n+                    lastRetryWithException);\n+\n+                return lastRetryWithException;\n+            }\n \n             logger.warn(\n-                \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n-                    \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n-                this.attemptCount,\n+                \"Received {} after backoff/retry. Will fail the request. {}\",\n+                exceptionType,\n                 exception);\n-            stopStopWatch(this.durationTimer);\n-\n-            return Mono.just(ShouldRetryResult.noRetry(\n-                Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n-        } else if (exception instanceof RetryWithException) {\n-            this.lastRetryWithException = (RetryWithException) exception;\n+            return BridgeInternal.createServiceUnavailableException(exception);\n         }\n-        long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n-        int currentRetryAttemptCount = this.attemptCount;\n-        if (this.attemptCount++ > 1) {\n-            if (remainingSeconds <= 0) {\n-                if (exception instanceof GoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received gone exception after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\"Received gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof PartitionKeyRangeGoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry including at least one RetryWithException.\"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof InvalidPartitionException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received InvalidPartitionException after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. InvalidPartitionException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                    } else {\n-                        logger.warn(\n-                                \"Received invalid collection partition exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else {\n-                    logger.warn(\"Received retrywith exception after backoff/retry. Will fail the request. {}\",\n-                            exception.toString());\n+\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            CosmosException exceptionToThrow;\n+            Duration backoffTime = Duration.ofSeconds(0);\n+            Duration timeout;\n+            boolean forceRefreshAddressCache;\n+            if (!isRetryableException(exception)) {\n+\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            } else if (exception instanceof GoneException &&\n+                !request.isReadOnly() &&\n+                BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+                logger.warn(\n+                    \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n+                        \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n+                    this.attemptCount,\n+                    exception);\n+\n+                return Mono.just(ShouldRetryResult.noRetry(\n+                    Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n+            }\n+\n+            long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n+            int currentRetryAttemptCount = this.attemptCount;\n+            if (this.attemptCount++ > 1) {\n+                if (remainingSeconds <= 0) {\n+                    exceptionToThrow = logAndWrapExceptionWithLastRetryWithException(exception);\n+                    return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n                 }\n-                stopStopWatch(this.durationTimer);\n-                return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n+\n+                backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n+                    GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n+                this.currentBackoffSeconds *= GoneRetryPolicy.BACK_OFF_MULTIPLIER;\n+                logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n             }\n-            backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n-                    GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n-            this.currentBackoffSeconds *= GoneAndRetryWithRetryPolicy.BACK_OFF_MULTIPLIER;\n-            logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n+\n+            // Calculate the remaining time based after accounting for the backoff that we\n+            // will perform\n+            long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n+            timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n+                : Duration.ofSeconds(GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n+\n+            Pair<Mono<ShouldRetryResult>, Boolean> exceptionHandlingResult = handleException(exception);\n+            Mono<ShouldRetryResult> result = exceptionHandlingResult.getLeft();\n+            if (result != null) {\n+                return result;\n+            }\n+\n+            forceRefreshAddressCache = exceptionHandlingResult.getRight();\n+\n+            return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n+                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleException(Exception exception) {\n+            if (exception instanceof GoneException) {\n+                return handleGoneException((GoneException)exception);\n+            } else if (exception instanceof PartitionIsMigratingException) {\n+                return handlePartitionIsMigratingException((PartitionIsMigratingException)exception);\n+            } else if (exception instanceof InvalidPartitionException) {\n+                return handleInvalidPartitionException((InvalidPartitionException)exception);\n+            } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n+                return handlePartitionKeyIsSplittingException((PartitionKeyRangeIsSplittingException) exception);\n+            }\n+\n+            throw new IllegalStateException(\"Invalid exception type\", exception);\n         }\n \n-        // Calculate the remaining time based after accounting for the backoff that we\n-        // will perform\n-        long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n-        timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n-                : Duration.ofSeconds(GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n-        if (exception instanceof GoneException) {\n-            logger.debug(\"Received gone exception, will retry, {}\", exception.toString());\n-            forceRefreshAddressCache = true; // indicate we are in retry.\n-        } else if (exception instanceof PartitionIsMigratingException) {\n-            logger.warn(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleGoneException(GoneException exception) {\n+            logger.info(\"Received gone exception, will retry, {}\", exception.toString());\n+            return Pair.of(null, true); // indicate we are in retry.\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionIsMigratingException(PartitionIsMigratingException exception) {\n+            logger.info(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n             this.request.forceCollectionRoutingMapRefresh = true;\n-            forceRefreshAddressCache = true;\n-        } else if (exception instanceof InvalidPartitionException) {\n+            return Pair.of(null, true);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionKeyIsSplittingException(PartitionKeyRangeIsSplittingException exception) {\n+            this.request.requestContext.resolvedPartitionKeyRange = null;\n+            this.request.requestContext.quorumSelectedLSN = -1;\n+            this.request.requestContext.quorumSelectedStoreResponse = null;\n+            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n+            this.request.forcePartitionKeyRangeRefresh = true;\n+            return Pair.of(null, false);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleInvalidPartitionException(InvalidPartitionException exception) {\n             this.request.requestContext.quorumSelectedLSN = -1;\n             this.request.requestContext.resolvedPartitionKeyRange = null;\n             this.request.requestContext.quorumSelectedStoreResponse = null;\n             this.request.requestContext.globalCommittedSelectedLSN = -1;\n             if (this.attemptCountInvalidPartition++ > 2) {\n                 // for second InvalidPartitionException, stop retrying.\n                 logger.warn(\"Received second InvalidPartitionException after backoff/retry. Will fail the request. {}\",\n-                        exception.toString());\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createServiceUnavailableException(exception)));\n+                    exception.toString());\n+                return Pair.of(\n+                    Mono.just(ShouldRetryResult.error(BridgeInternal.createServiceUnavailableException(exception))),\n+                    false);\n             }\n \n-            if (this.request != null) {\n-                logger.warn(\"Received invalid collection exception, will retry, {}\", exception.toString());\n-                this.request.forceNameCacheRefresh = true;\n-            } else {\n-                logger.error(\"Received unexpected invalid collection exception, request should be non-null.\",\n-                        exception);\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createCosmosException(HttpConstants.StatusCodes.INTERNAL_SERVER_ERROR, exception)));\n-            }\n-            forceRefreshAddressCache = false;\n-        } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n-            this.request.requestContext.resolvedPartitionKeyRange = null;\n-            this.request.requestContext.quorumSelectedLSN = -1;\n-            this.request.requestContext.quorumSelectedStoreResponse = null;\n-            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n-            this.request.forcePartitionKeyRangeRefresh = true;\n-            forceRefreshAddressCache = false;\n-        } else {\n-            logger.warn(\"Received retrywith exception, will retry, {}\", exception);\n-            // For RetryWithException, prevent the caller\n-            // from refreshing any caches.\n-            forceRefreshAddressCache = false;\n+            logger.info(\"Received invalid collection exception, will retry, {}\", exception.toString());\n+            this.request.forceNameCacheRefresh = true;\n+\n+            return Pair.of(null, false);\n         }\n-        return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n-                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n     }\n \n-    private void stopStopWatch(StopWatch stopwatch) {\n-        synchronized (stopwatch) {\n-            stopwatch.stop();\n+    static class RetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_MS = 15000;\n+        private final static int INITIAL_BACKOFF_TIME_MS = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "originalPosition": 407}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwOTk0MjI5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-510994229", "createdAt": "2020-10-17T15:39:44Z", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExOTI3NzEz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-511927713", "createdAt": "2020-10-19T16:08:16Z", "commit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxNjowODoxNlrOHkWRrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxNjowODoxNlrOHkWRrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg3NTc1OA==", "bodyText": "what happen if the code throws unrelated exception to this retry policy? do we log it here?\nif so an exception will get logged here and elsewhere where it is truely handled. (e.g., 429). We shouldn't log unrelated exceptions to this policy and let related other policy to handle or log the exception.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507875758", "createdAt": "2020-10-19T16:08:16Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final StopWatch durationTimer;\n+        private final int waitTimeInSeconds;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            StopWatch durationTimer) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.durationTimer = durationTimer;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            RetryWithException lastRetryWithException = this.request.getLastRetryWithException();\n+\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "originalPosition": 165}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76e2b068efdbb5a02d235a71ffc0e65873c9a6b8", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/76e2b068efdbb5a02d235a71ffc0e65873c9a6b8", "committedDate": "2020-10-19T16:10:19Z", "message": "Adding unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/e756ce41b456a1321dbbb851e493e9ec1ae3c6bb", "committedDate": "2020-10-19T16:10:24Z", "message": "Merge branch 'master' of https://github.com/Azure/azure-sdk-for-java into users/fabianm/449RetryBackoff"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExOTMzMTY5", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-511933169", "createdAt": "2020-10-19T16:14:30Z", "commit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "state": "APPROVED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxNjoxNDozMFrOHkWhtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxNjozODo0M1rOHkXgoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg3OTg2Mg==", "bodyText": "this logs stacktrace with warning level.\nwe are logging stacktrace with warning level inside the inner policies too. this way we will get stacktrace dumped twice.\nwe should log stacktrace with warn only at one place. I wonder if we can remove this one and fully rely on the warn from the inner retryPolicy log?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507879862", "createdAt": "2020-10-19T16:14:30Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg4MjYyNw==", "bodyText": "why do we still need to handle retryWithException here? don't we have a standalone retryPolicy for that?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507882627", "createdAt": "2020-10-19T16:18:35Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final StopWatch durationTimer;\n+        private final int waitTimeInSeconds;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            StopWatch durationTimer) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.durationTimer = durationTimer;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            RetryWithException lastRetryWithException = this.request.getLastRetryWithException();\n+\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);\n+\n+                return (CosmosException)exception;\n+            } else {\n+                throw new IllegalStateException(\"Invalid exception type\", exception);\n+            }\n+\n+            if (lastRetryWithException != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg4NDM1Mg==", "bodyText": "GoneAndRetryWithRetryPolicy is created per operation. Why can't we remember the lastRetryException in the RetryPolicy itself?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507884352", "createdAt": "2020-10-19T16:21:06Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/RxDocumentServiceRequest.java", "diffHunk": "@@ -276,6 +280,15 @@ public void setByteBuffer(ByteBuffer byteBuffer) {\n         this.contentAsByteArray = toByteArray(byteBuffer);\n     }\n \n+    public RetryWithException getLastRetryWithException() {\n+        return this.lastRetryWithException;\n+    }\n+\n+    public void setLastRetryWithException(RetryWithException exception) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg4ODA2Mg==", "bodyText": "stopwatch durationTimer  is not thread-safe. previously we had accessed it with synchronized block which that also was not ideal.\nMy suggestion is to change durationTimer type as long and make it volatile.\nprivate volatile long durationTimerMs\nThat way you can safely access it everywhere and we won't need synchronized block", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507888062", "createdAt": "2020-10-19T16:26:27Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final StopWatch durationTimer;\n+        private final int waitTimeInSeconds;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            StopWatch durationTimer) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.durationTimer = durationTimer;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            RetryWithException lastRetryWithException = this.request.getLastRetryWithException();\n+\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);\n+\n+                return (CosmosException)exception;\n+            } else {\n+                throw new IllegalStateException(\"Invalid exception type\", exception);\n+            }\n+\n+            if (lastRetryWithException != null) {\n+                logger.warn(\n+                    \"Received {} after backoff/retry including at least one RetryWithException. \"\n+                        + \"Will fail the request with RetryWithException. {}: {}. RetryWithException: {}\",\n+                    exceptionType,\n+                    exceptionType,\n+                    exception,\n+                    lastRetryWithException);\n+\n+                return lastRetryWithException;\n+            }\n \n             logger.warn(\n-                \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n-                    \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n-                this.attemptCount,\n+                \"Received {} after backoff/retry. Will fail the request. {}\",\n+                exceptionType,\n                 exception);\n-            stopStopWatch(this.durationTimer);\n-\n-            return Mono.just(ShouldRetryResult.noRetry(\n-                Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n-        } else if (exception instanceof RetryWithException) {\n-            this.lastRetryWithException = (RetryWithException) exception;\n+            return BridgeInternal.createServiceUnavailableException(exception);\n         }\n-        long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n-        int currentRetryAttemptCount = this.attemptCount;\n-        if (this.attemptCount++ > 1) {\n-            if (remainingSeconds <= 0) {\n-                if (exception instanceof GoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received gone exception after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\"Received gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof PartitionKeyRangeGoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry including at least one RetryWithException.\"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof InvalidPartitionException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received InvalidPartitionException after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. InvalidPartitionException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                    } else {\n-                        logger.warn(\n-                                \"Received invalid collection partition exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else {\n-                    logger.warn(\"Received retrywith exception after backoff/retry. Will fail the request. {}\",\n-                            exception.toString());\n+\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            CosmosException exceptionToThrow;\n+            Duration backoffTime = Duration.ofSeconds(0);\n+            Duration timeout;\n+            boolean forceRefreshAddressCache;\n+            if (!isRetryableException(exception)) {\n+\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            } else if (exception instanceof GoneException &&\n+                !request.isReadOnly() &&\n+                BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+                logger.warn(\n+                    \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n+                        \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n+                    this.attemptCount,\n+                    exception);\n+\n+                return Mono.just(ShouldRetryResult.noRetry(\n+                    Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n+            }\n+\n+            long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n+            int currentRetryAttemptCount = this.attemptCount;\n+            if (this.attemptCount++ > 1) {\n+                if (remainingSeconds <= 0) {\n+                    exceptionToThrow = logAndWrapExceptionWithLastRetryWithException(exception);\n+                    return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n                 }\n-                stopStopWatch(this.durationTimer);\n-                return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n+\n+                backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n+                    GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n+                this.currentBackoffSeconds *= GoneRetryPolicy.BACK_OFF_MULTIPLIER;\n+                logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n             }\n-            backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n-                    GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n-            this.currentBackoffSeconds *= GoneAndRetryWithRetryPolicy.BACK_OFF_MULTIPLIER;\n-            logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n+\n+            // Calculate the remaining time based after accounting for the backoff that we\n+            // will perform\n+            long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n+            timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n+                : Duration.ofSeconds(GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n+\n+            Pair<Mono<ShouldRetryResult>, Boolean> exceptionHandlingResult = handleException(exception);\n+            Mono<ShouldRetryResult> result = exceptionHandlingResult.getLeft();\n+            if (result != null) {\n+                return result;\n+            }\n+\n+            forceRefreshAddressCache = exceptionHandlingResult.getRight();\n+\n+            return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n+                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleException(Exception exception) {\n+            if (exception instanceof GoneException) {\n+                return handleGoneException((GoneException)exception);\n+            } else if (exception instanceof PartitionIsMigratingException) {\n+                return handlePartitionIsMigratingException((PartitionIsMigratingException)exception);\n+            } else if (exception instanceof InvalidPartitionException) {\n+                return handleInvalidPartitionException((InvalidPartitionException)exception);\n+            } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n+                return handlePartitionKeyIsSplittingException((PartitionKeyRangeIsSplittingException) exception);\n+            }\n+\n+            throw new IllegalStateException(\"Invalid exception type\", exception);\n         }\n \n-        // Calculate the remaining time based after accounting for the backoff that we\n-        // will perform\n-        long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n-        timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n-                : Duration.ofSeconds(GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n-        if (exception instanceof GoneException) {\n-            logger.debug(\"Received gone exception, will retry, {}\", exception.toString());\n-            forceRefreshAddressCache = true; // indicate we are in retry.\n-        } else if (exception instanceof PartitionIsMigratingException) {\n-            logger.warn(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleGoneException(GoneException exception) {\n+            logger.info(\"Received gone exception, will retry, {}\", exception.toString());\n+            return Pair.of(null, true); // indicate we are in retry.\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionIsMigratingException(PartitionIsMigratingException exception) {\n+            logger.info(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n             this.request.forceCollectionRoutingMapRefresh = true;\n-            forceRefreshAddressCache = true;\n-        } else if (exception instanceof InvalidPartitionException) {\n+            return Pair.of(null, true);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionKeyIsSplittingException(PartitionKeyRangeIsSplittingException exception) {\n+            this.request.requestContext.resolvedPartitionKeyRange = null;\n+            this.request.requestContext.quorumSelectedLSN = -1;\n+            this.request.requestContext.quorumSelectedStoreResponse = null;\n+            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n+            this.request.forcePartitionKeyRangeRefresh = true;\n+            return Pair.of(null, false);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleInvalidPartitionException(InvalidPartitionException exception) {\n             this.request.requestContext.quorumSelectedLSN = -1;\n             this.request.requestContext.resolvedPartitionKeyRange = null;\n             this.request.requestContext.quorumSelectedStoreResponse = null;\n             this.request.requestContext.globalCommittedSelectedLSN = -1;\n             if (this.attemptCountInvalidPartition++ > 2) {\n                 // for second InvalidPartitionException, stop retrying.\n                 logger.warn(\"Received second InvalidPartitionException after backoff/retry. Will fail the request. {}\",\n-                        exception.toString());\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createServiceUnavailableException(exception)));\n+                    exception.toString());\n+                return Pair.of(\n+                    Mono.just(ShouldRetryResult.error(BridgeInternal.createServiceUnavailableException(exception))),\n+                    false);\n             }\n \n-            if (this.request != null) {\n-                logger.warn(\"Received invalid collection exception, will retry, {}\", exception.toString());\n-                this.request.forceNameCacheRefresh = true;\n-            } else {\n-                logger.error(\"Received unexpected invalid collection exception, request should be non-null.\",\n-                        exception);\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createCosmosException(HttpConstants.StatusCodes.INTERNAL_SERVER_ERROR, exception)));\n-            }\n-            forceRefreshAddressCache = false;\n-        } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n-            this.request.requestContext.resolvedPartitionKeyRange = null;\n-            this.request.requestContext.quorumSelectedLSN = -1;\n-            this.request.requestContext.quorumSelectedStoreResponse = null;\n-            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n-            this.request.forcePartitionKeyRangeRefresh = true;\n-            forceRefreshAddressCache = false;\n-        } else {\n-            logger.warn(\"Received retrywith exception, will retry, {}\", exception);\n-            // For RetryWithException, prevent the caller\n-            // from refreshing any caches.\n-            forceRefreshAddressCache = false;\n+            logger.info(\"Received invalid collection exception, will retry, {}\", exception.toString());\n+            this.request.forceNameCacheRefresh = true;\n+\n+            return Pair.of(null, false);\n         }\n-        return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n-                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n     }\n \n-    private void stopStopWatch(StopWatch stopwatch) {\n-        synchronized (stopwatch) {\n-            stopwatch.stop();\n+    static class RetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_MS = 15000;\n+        private final static int INITIAL_BACKOFF_TIME_MS = 10;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int currentBackoffMilliseconds = RetryWithRetryPolicy.INITIAL_BACKOFF_TIME_MS;\n+\n+        private final int waitTimeInSeconds;\n+        private final StopWatch durationTimer;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public RetryWithRetryPolicy(RxDocumentServiceRequest request,\n+                                    Integer waitTimeInSeconds,\n+                                    StopWatch durationTimer) {\n+            this.request = request;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+            this.durationTimer = durationTimer;\n         }\n-    }\n \n-    private void startStopWatch(StopWatch stopwatch) {\n-        synchronized (stopwatch) {\n-            stopwatch.start();\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            Duration backoffTime;\n+            Duration timeout;\n+\n+            if (!(exception instanceof RetryWithException)) {\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            }\n+\n+            RetryWithException lastRetryWithException = (RetryWithException)exception;\n+            this.request.setLastRetryWithException(lastRetryWithException);\n+\n+            long remainingMilliseconds = (this.waitTimeInSeconds * 1_000L) - this.durationTimer.getTime();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 446}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg5MDAxMg==", "bodyText": "in the new code in some places this is accessed without synchronized block that may cause potential thread-safety issue.\ntwo options:\n\nideally this should become private volatile durationMs to access safely from different threads. that way we wouldn't need any synchronization block around it. (this is more preferred) as volatile is more preferred than synchronized in this case.\naccess this StopWatch everywhere with synchrnozied block.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507890012", "createdAt": "2020-10-19T16:29:22Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg5MjE5Nw==", "bodyText": "thanks for clarification. makes sense.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507892197", "createdAt": "2020-10-19T16:32:46Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -16,189 +15,324 @@\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n \n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n     private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n+        this.goneRetryPolicy = new GoneRetryPolicy(request, waitTimeInSeconds, durationTimer);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(request, waitTimeInSeconds, this.durationTimer);\n         startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.warn(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        stopStopWatch(this.durationTimer);\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private void stopStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.stop();\n+        }\n+    }\n+\n+    private void startStopWatch(StopWatch stopwatch) {\n+        synchronized (stopwatch) {\n+            stopwatch.start();\n+        }\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final StopWatch durationTimer;\n+        private final int waitTimeInSeconds;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            StopWatch durationTimer) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.durationTimer = durationTimer;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            RetryWithException lastRetryWithException = this.request.getLastRetryWithException();\n+\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg3NTc1OA=="}, "originalCommit": {"oid": "7a4e2dee6c5898cedb7eae728d7749f83a160adb"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg5Mzc4Mg==", "bodyText": "great test thank you.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507893782", "createdAt": "2020-10-19T16:35:15Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/implementation/directconnectivity/ReplicatedResourceClientRetryWithTest.java", "diffHunk": "@@ -0,0 +1,190 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+\n+package com.azure.cosmos.implementation.directconnectivity;\n+\n+import com.azure.cosmos.ConsistencyLevel;\n+import com.azure.cosmos.implementation.Configs;\n+import com.azure.cosmos.implementation.DocumentServiceRequestContext;\n+import com.azure.cosmos.implementation.HttpConstants;\n+import com.azure.cosmos.implementation.IAuthorizationTokenProvider;\n+import com.azure.cosmos.implementation.OperationType;\n+import com.azure.cosmos.implementation.PartitionKeyRange;\n+import com.azure.cosmos.implementation.ResourceType;\n+import com.azure.cosmos.implementation.RetryWithException;\n+import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n+import com.azure.cosmos.implementation.SessionContainer;\n+import com.azure.cosmos.implementation.StoreResponseBuilder;\n+import com.azure.cosmos.implementation.http.HttpHeaders;\n+import io.reactivex.subscribers.TestSubscriber;\n+import org.mockito.Mockito;\n+import org.testng.annotations.Test;\n+import reactor.core.publisher.Mono;\n+\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Function;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import static com.azure.cosmos.implementation.TestUtils.mockDiagnosticsClientContext;\n+\n+public class ReplicatedResourceClientRetryWithTest {\n+    protected static final int TIMEOUT = 120000;\n+\n+    @Test(groups = { \"unit\" }, timeOut = TIMEOUT)\n+    public void retryWith_RetrySucceeds() throws URISyntaxException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg5NTk2OQ==", "bodyText": "if it is possible I would move the result exception from request to the retryPolicy.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507895969", "createdAt": "2020-10-19T16:38:43Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/RxDocumentServiceRequest.java", "diffHunk": "@@ -276,6 +280,15 @@ public void setByteBuffer(ByteBuffer byteBuffer) {\n         this.contentAsByteArray = toByteArray(byteBuffer);\n     }\n \n+    public RetryWithException getLastRetryWithException() {\n+        return this.lastRetryWithException;\n+    }\n+\n+    public void setLastRetryWithException(RetryWithException exception) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzg4NDM1Mg=="}, "originalCommit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExOTU3MzMw", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-511957330", "createdAt": "2020-10-19T16:43:10Z", "commit": {"oid": "e756ce41b456a1321dbbb851e493e9ec1ae3c6bb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/b5c79d47a2ecef726755f4b3f57195be71d1f592", "committedDate": "2020-10-19T18:00:35Z", "message": "Reacting to CR feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMDQyMzY4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-512042368", "createdAt": "2020-10-19T18:23:41Z", "commit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxODoyMzo0MVrOHkcEwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxODozNjo0NlrOHkckDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzk3MDc1Mw==", "bodyText": "GoneException can be retried with certain conditions, and with certain conditions it can't be retried.\nI see that you coded those conditions outside and the caller of isRetryableException(exception) do the additional check.\nso there for some GoneException isRetryableException(GoneEx) may return true but the code may decide to not retry. that may make the naming a bit counter-intuitive.\nI wonder if we should negate the logic and call this isNotRetryableException. as for \"not\" case we can be certain. thougth?\nnot a big deal either way.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507970753", "createdAt": "2020-10-19T18:23:41Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -15,190 +14,347 @@\n import com.azure.cosmos.implementation.RetryPolicyWithDiagnostics;\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n-import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n+import java.time.Instant;\n+import java.util.function.Supplier;\n+\n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n+public class GoneAndRetryWithRetryPolicy\n+    extends RetryPolicyWithDiagnostics\n+    implements LastRetryWithExceptionHolder, LastRetryWithExceptionProvider {\n \n-public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n-    private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n+    private final Instant start;\n+    private volatile Instant end;\n+\n+    private RetryWithException lastRetryWithException;\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n-        startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n+        this.goneRetryPolicy = new GoneRetryPolicy(\n+            request,\n+            waitTimeInSeconds,\n+            this::getElapsedTime,\n+            this);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(\n+            waitTimeInSeconds,\n+            this::getElapsedTime,\n+            this);\n+        this.start = Instant.now();\n+    }\n+\n+    @Override\n+    public void setLastRetryWithException(RetryWithException lastRetryWithException) {\n+        this.lastRetryWithException = lastRetryWithException;\n+    }\n+\n+    @Override\n+    public RetryWithException getLastRetryWithException() {\n+        return this.lastRetryWithException;\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.debug(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        this.end = Instant.now();\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private Duration getElapsedTime() {\n+        Instant endSnapshot = this.end != null ? this.end : Instant.now();\n+\n+        return Duration.between(this.start, endSnapshot);\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final Supplier<Duration> getElapsedTimeSupplier;\n+        private final int waitTimeInSeconds;\n+        private final LastRetryWithExceptionProvider lastRetryWithExceptionProvider;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            Supplier<Duration> getElapsedTimeSupplier,\n+            LastRetryWithExceptionProvider lastRetryWithExceptionProvider) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.getElapsedTimeSupplier = getElapsedTimeSupplier;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+            this.lastRetryWithExceptionProvider = lastRetryWithExceptionProvider;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzk3MTc5Mg==", "bodyText": "Duration.toSeconds() is a java9 + api. doesn't exist on java8\nhttps://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/time/Duration.html#toSeconds()\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        long remainingSeconds = this.waitTimeInSeconds - this.getElapsedTimeSupplier.get().toSeconds();\n          \n          \n            \n                        long remainingSeconds = this.waitTimeInSeconds - (this.getElapsedTimeSupplier.get().toMillis() / 1000);", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507971792", "createdAt": "2020-10-19T18:25:28Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -15,190 +14,347 @@\n import com.azure.cosmos.implementation.RetryPolicyWithDiagnostics;\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n-import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n+import java.time.Instant;\n+import java.util.function.Supplier;\n+\n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n+public class GoneAndRetryWithRetryPolicy\n+    extends RetryPolicyWithDiagnostics\n+    implements LastRetryWithExceptionHolder, LastRetryWithExceptionProvider {\n \n-public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n-    private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n+    private final Instant start;\n+    private volatile Instant end;\n+\n+    private RetryWithException lastRetryWithException;\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n-        startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n+        this.goneRetryPolicy = new GoneRetryPolicy(\n+            request,\n+            waitTimeInSeconds,\n+            this::getElapsedTime,\n+            this);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(\n+            waitTimeInSeconds,\n+            this::getElapsedTime,\n+            this);\n+        this.start = Instant.now();\n+    }\n+\n+    @Override\n+    public void setLastRetryWithException(RetryWithException lastRetryWithException) {\n+        this.lastRetryWithException = lastRetryWithException;\n+    }\n+\n+    @Override\n+    public RetryWithException getLastRetryWithException() {\n+        return this.lastRetryWithException;\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.debug(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        this.end = Instant.now();\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private Duration getElapsedTime() {\n+        Instant endSnapshot = this.end != null ? this.end : Instant.now();\n+\n+        return Duration.between(this.start, endSnapshot);\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final Supplier<Duration> getElapsedTimeSupplier;\n+        private final int waitTimeInSeconds;\n+        private final LastRetryWithExceptionProvider lastRetryWithExceptionProvider;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            Supplier<Duration> getElapsedTimeSupplier,\n+            LastRetryWithExceptionProvider lastRetryWithExceptionProvider) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.getElapsedTimeSupplier = getElapsedTimeSupplier;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+            this.lastRetryWithExceptionProvider = lastRetryWithExceptionProvider;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);\n+\n+                return (CosmosException)exception;\n+            } else {\n+                throw new IllegalStateException(\"Invalid exception type\", exception);\n+            }\n+\n+            RetryWithException lastRetryWithExceptionSnapshot =\n+                lastRetryWithExceptionProvider.getLastRetryWithException();\n+            if (lastRetryWithExceptionSnapshot != null) {\n+                logger.warn(\n+                    \"Received {} after backoff/retry including at least one RetryWithException. \"\n+                        + \"Will fail the request with RetryWithException. {}: {}. RetryWithException: {}\",\n+                    exceptionType,\n+                    exceptionType,\n+                    exception,\n+                    lastRetryWithExceptionSnapshot);\n+\n+                return lastRetryWithExceptionSnapshot;\n+            }\n \n             logger.warn(\n-                \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n-                    \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n-                this.attemptCount,\n+                \"Received {} after backoff/retry. Will fail the request. {}\",\n+                exceptionType,\n                 exception);\n-            stopStopWatch(this.durationTimer);\n-\n-            return Mono.just(ShouldRetryResult.noRetry(\n-                Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n-        } else if (exception instanceof RetryWithException) {\n-            this.lastRetryWithException = (RetryWithException) exception;\n+            return BridgeInternal.createServiceUnavailableException(exception);\n         }\n-        long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n-        int currentRetryAttemptCount = this.attemptCount;\n-        if (this.attemptCount++ > 1) {\n-            if (remainingSeconds <= 0) {\n-                if (exception instanceof GoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received gone exception after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\"Received gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof PartitionKeyRangeGoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry including at least one RetryWithException.\"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof InvalidPartitionException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received InvalidPartitionException after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. InvalidPartitionException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                    } else {\n-                        logger.warn(\n-                                \"Received invalid collection partition exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else {\n-                    logger.warn(\"Received retrywith exception after backoff/retry. Will fail the request. {}\",\n-                            exception.toString());\n+\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            CosmosException exceptionToThrow;\n+            Duration backoffTime = Duration.ofSeconds(0);\n+            Duration timeout;\n+            boolean forceRefreshAddressCache;\n+            if (!isRetryableException(exception)) {\n+\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            } else if (exception instanceof GoneException &&\n+                !request.isReadOnly() &&\n+                BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+                logger.warn(\n+                    \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n+                        \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n+                    this.attemptCount,\n+                    exception);\n+\n+                return Mono.just(ShouldRetryResult.noRetry(\n+                    Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n+            }\n+\n+            long remainingSeconds = this.waitTimeInSeconds - this.getElapsedTimeSupplier.get().toSeconds();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzk3ODc2Nw==", "bodyText": "why do we need these two new interfaces LastRetryWithExceptionHolder, LastRetryWithExceptionProvider?\nIs it to provide the linkage between the inner policies and the outer policy?\nThe inner class should be able to access the outer class field. RetryWithPolicy should be able to access  the GoneAndRetryWithRetryPolicy.retryWithException with GoneAndRetryWithRetryPolicy.this.retryWithException\nhttps://www.programiz.com/java-programming/nested-inner-class\nI think we should be able to get rid of the two new interfaces with above appraoch.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507978767", "createdAt": "2020-10-19T18:36:46Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -15,190 +14,347 @@\n import com.azure.cosmos.implementation.RetryPolicyWithDiagnostics;\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n-import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n+import java.time.Instant;\n+import java.util.function.Supplier;\n+\n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n+public class GoneAndRetryWithRetryPolicy\n+    extends RetryPolicyWithDiagnostics\n+    implements LastRetryWithExceptionHolder, LastRetryWithExceptionProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMDI4MTEz", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-512028113", "createdAt": "2020-10-19T18:04:38Z", "commit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxODo0MDoxNlrOHkcr6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxODo0MjoyNVrOHkcwlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzk4MDc3OA==", "bodyText": "Nit: extra imports , if you send another iteration", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507980778", "createdAt": "2020-10-19T18:40:16Z", "author": {"login": "simplynaveen20"}, "path": "sdk/cosmos/azure-cosmos/src/test/java/com/azure/cosmos/implementation/directconnectivity/ReplicatedResourceClientTest.java", "diffHunk": "@@ -20,6 +23,8 @@\n import reactor.core.Exceptions;\n import reactor.core.publisher.Mono;\n \n+import java.net.URI;\n+import java.net.URISyntaxException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzk4MTk3Mw==", "bodyText": "Does this mean we will have more retries within 30 sec as it start with 10 ms ?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#discussion_r507981973", "createdAt": "2020-10-19T18:42:25Z", "author": {"login": "simplynaveen20"}, "path": "sdk/cosmos/azure-cosmos/src/main/java/com/azure/cosmos/implementation/directconnectivity/GoneAndRetryWithRetryPolicy.java", "diffHunk": "@@ -15,190 +14,347 @@\n import com.azure.cosmos.implementation.RetryPolicyWithDiagnostics;\n import com.azure.cosmos.implementation.RetryWithException;\n import com.azure.cosmos.implementation.RxDocumentServiceRequest;\n-import com.azure.cosmos.implementation.apachecommons.lang.time.StopWatch;\n+import com.azure.cosmos.implementation.apachecommons.lang.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import reactor.core.publisher.Mono;\n \n import java.time.Duration;\n+import java.time.Instant;\n+import java.util.function.Supplier;\n+\n+import static com.azure.cosmos.implementation.guava25.base.Preconditions.checkNotNull;\n+\n+public class GoneAndRetryWithRetryPolicy\n+    extends RetryPolicyWithDiagnostics\n+    implements LastRetryWithExceptionHolder, LastRetryWithExceptionProvider {\n \n-public class GoneAndRetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n     private final static Logger logger = LoggerFactory.getLogger(GoneAndRetryWithRetryPolicy.class);\n-    private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n-    private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n-    private final static int INITIAL_BACKOFF_TIME = 1;\n-    private final static int BACK_OFF_MULTIPLIER = 2;\n-\n-    private final RxDocumentServiceRequest request;\n-    private volatile int attemptCount = 1;\n-    private volatile int attemptCountInvalidPartition = 1;\n-    private volatile int currentBackoffSeconds = GoneAndRetryWithRetryPolicy.INITIAL_BACKOFF_TIME;\n-    private volatile RetryWithException lastRetryWithException;\n-    private final StopWatch durationTimer = new StopWatch();\n-    private final int waitTimeInSeconds;\n-    //TODO once this is moved to IRetryPolicy, remove from here.\n-    public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n-            Duration.ofSeconds(60), 0);\n+    private final GoneRetryPolicy goneRetryPolicy;\n+    private final RetryWithRetryPolicy retryWithRetryPolicy;\n+    private final Instant start;\n+    private volatile Instant end;\n+\n+    private RetryWithException lastRetryWithException;\n \n     public GoneAndRetryWithRetryPolicy(RxDocumentServiceRequest request, Integer waitTimeInSeconds) {\n-        this.request = request;\n-        startStopWatch(this.durationTimer);\n-        if (waitTimeInSeconds != null) {\n-            this.waitTimeInSeconds = waitTimeInSeconds;\n-        } else {\n-            this.waitTimeInSeconds = DEFAULT_WAIT_TIME_IN_SECONDS;\n-        }\n+        this.goneRetryPolicy = new GoneRetryPolicy(\n+            request,\n+            waitTimeInSeconds,\n+            this::getElapsedTime,\n+            this);\n+        this.retryWithRetryPolicy = new RetryWithRetryPolicy(\n+            waitTimeInSeconds,\n+            this::getElapsedTime,\n+            this);\n+        this.start = Instant.now();\n+    }\n+\n+    @Override\n+    public void setLastRetryWithException(RetryWithException lastRetryWithException) {\n+        this.lastRetryWithException = lastRetryWithException;\n+    }\n+\n+    @Override\n+    public RetryWithException getLastRetryWithException() {\n+        return this.lastRetryWithException;\n     }\n \n     @Override\n     public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n-        CosmosException exceptionToThrow = null;\n-        Duration backoffTime = Duration.ofSeconds(0);\n-        Duration timeout = Duration.ofSeconds(0);\n-        boolean forceRefreshAddressCache = false;\n-        if (!(exception instanceof GoneException) &&\n-            !(exception instanceof RetryWithException) &&\n-            !(exception instanceof PartitionIsMigratingException) &&\n-            !(exception instanceof InvalidPartitionException &&\n-                (this.request.getPartitionKeyRangeIdentity() == null ||\n-                this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null)) &&\n-            !(exception instanceof PartitionKeyRangeIsSplittingException)) {\n-\n-            logger.warn(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n-                exception);\n-            stopStopWatch(this.durationTimer);\n-            return Mono.just(ShouldRetryResult.noRetry());\n-        } else if (exception instanceof GoneException &&\n-            !request.isReadOnly() &&\n-            BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+        return this.retryWithRetryPolicy.shouldRetry(exception)\n+                                        .flatMap((retryWithResult) -> {\n+\n+            if (retryWithResult.shouldRetry) {\n+                return Mono.just(retryWithResult);\n+            }\n+\n+            return this.goneRetryPolicy.shouldRetry(exception)\n+                .flatMap((goneRetryResult) -> {\n+                    if (!goneRetryResult.shouldRetry) {\n+                        logger.debug(\"Operation will NOT be retried. Exception:\",\n+                            exception);\n+                        this.end = Instant.now();\n+                    }\n+\n+                    return Mono.just(goneRetryResult);\n+                });\n+        });\n+    }\n+\n+    private Duration getElapsedTime() {\n+        Instant endSnapshot = this.end != null ? this.end : Instant.now();\n+\n+        return Duration.between(this.start, endSnapshot);\n+    }\n+\n+    static class GoneRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_SECONDS = 15;\n+        private final static int INITIAL_BACKOFF_TIME = 1;\n+        private final static int BACK_OFF_MULTIPLIER = 2;\n+\n+        private final RxDocumentServiceRequest request;\n+        private volatile int attemptCount = 1;\n+        private volatile int attemptCountInvalidPartition = 1;\n+        private volatile int currentBackoffSeconds = GoneRetryPolicy.INITIAL_BACKOFF_TIME;\n+        private final Supplier<Duration> getElapsedTimeSupplier;\n+        private final int waitTimeInSeconds;\n+        private final LastRetryWithExceptionProvider lastRetryWithExceptionProvider;\n+        //TODO once this is moved to IRetryPolicy, remove from here.\n+        public final static Quadruple<Boolean, Boolean, Duration, Integer> INITIAL_ARGUMENT_VALUE_POLICY_ARG = Quadruple.with(false, false,\n+            Duration.ofSeconds(60), 0);\n+\n+        public GoneRetryPolicy(\n+            RxDocumentServiceRequest request,\n+            Integer waitTimeInSeconds,\n+            Supplier<Duration> getElapsedTimeSupplier,\n+            LastRetryWithExceptionProvider lastRetryWithExceptionProvider) {\n+\n+            checkNotNull(request, \"request must not be null.\");\n+            this.request = request;\n+            this.getElapsedTimeSupplier = getElapsedTimeSupplier;\n+            this.waitTimeInSeconds = waitTimeInSeconds != null ? waitTimeInSeconds : DEFAULT_WAIT_TIME_IN_SECONDS;\n+            this.lastRetryWithExceptionProvider = lastRetryWithExceptionProvider;\n+        }\n+\n+        private boolean isRetryableException(Exception exception) {\n+            if (exception instanceof GoneException ||\n+                exception instanceof RetryWithException ||\n+                exception instanceof PartitionIsMigratingException ||\n+                exception instanceof PartitionKeyRangeIsSplittingException) {\n+\n+                return true;\n+            }\n+\n+            if (exception instanceof InvalidPartitionException) {\n+                return this.request.getPartitionKeyRangeIdentity() == null ||\n+                    this.request.getPartitionKeyRangeIdentity().getCollectionRid() == null;\n+            }\n+\n+            return false;\n+        }\n+\n+        private CosmosException logAndWrapExceptionWithLastRetryWithException(Exception exception) {\n+            String exceptionType;\n+            if (exception instanceof GoneException) {\n+                exceptionType = \"GoneException\";\n+            } else if (exception instanceof PartitionKeyRangeGoneException) {\n+                exceptionType = \"PartitionKeyRangeGoneException\";\n+            } else if (exception instanceof  InvalidPartitionException) {\n+                exceptionType = \"InvalidPartitionException\";\n+            } else if (exception instanceof  PartitionKeyRangeIsSplittingException) {\n+                exceptionType = \"PartitionKeyRangeIsSplittingException\";\n+            } else if (exception instanceof CosmosException) {\n+                logger.warn(\"Received CosmosException after backoff/retry. Will fail the request.\",\n+                    exception);\n+\n+                return (CosmosException)exception;\n+            } else {\n+                throw new IllegalStateException(\"Invalid exception type\", exception);\n+            }\n+\n+            RetryWithException lastRetryWithExceptionSnapshot =\n+                lastRetryWithExceptionProvider.getLastRetryWithException();\n+            if (lastRetryWithExceptionSnapshot != null) {\n+                logger.warn(\n+                    \"Received {} after backoff/retry including at least one RetryWithException. \"\n+                        + \"Will fail the request with RetryWithException. {}: {}. RetryWithException: {}\",\n+                    exceptionType,\n+                    exceptionType,\n+                    exception,\n+                    lastRetryWithExceptionSnapshot);\n+\n+                return lastRetryWithExceptionSnapshot;\n+            }\n \n             logger.warn(\n-                \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n-                    \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n-                this.attemptCount,\n+                \"Received {} after backoff/retry. Will fail the request. {}\",\n+                exceptionType,\n                 exception);\n-            stopStopWatch(this.durationTimer);\n-\n-            return Mono.just(ShouldRetryResult.noRetry(\n-                Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n-        } else if (exception instanceof RetryWithException) {\n-            this.lastRetryWithException = (RetryWithException) exception;\n+            return BridgeInternal.createServiceUnavailableException(exception);\n         }\n-        long remainingSeconds = this.waitTimeInSeconds - this.durationTimer.getTime() / 1000;\n-        int currentRetryAttemptCount = this.attemptCount;\n-        if (this.attemptCount++ > 1) {\n-            if (remainingSeconds <= 0) {\n-                if (exception instanceof GoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received gone exception after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\"Received gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof PartitionKeyRangeGoneException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry including at least one RetryWithException.\"\n-                                        + \"Will fail the request with RetryWithException. GoneException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                        exceptionToThrow = this.lastRetryWithException;\n-                    } else {\n-                        logger.warn(\n-                                \"Received partition key range gone exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else if (exception instanceof InvalidPartitionException) {\n-                    if (this.lastRetryWithException != null) {\n-                        logger.warn(\n-                                \"Received InvalidPartitionException after backoff/retry including at least one RetryWithException. \"\n-                                        + \"Will fail the request with RetryWithException. InvalidPartitionException: {}. RetryWithException: {}\",\n-                                exception, this.lastRetryWithException);\n-                    } else {\n-                        logger.warn(\n-                                \"Received invalid collection partition exception after backoff/retry. Will fail the request. {}\",\n-                                exception.toString());\n-                        exceptionToThrow = BridgeInternal.createServiceUnavailableException(exception);\n-                    }\n-                } else {\n-                    logger.warn(\"Received retrywith exception after backoff/retry. Will fail the request. {}\",\n-                            exception.toString());\n+\n+        @Override\n+        public Mono<ShouldRetryResult> shouldRetry(Exception exception) {\n+            CosmosException exceptionToThrow;\n+            Duration backoffTime = Duration.ofSeconds(0);\n+            Duration timeout;\n+            boolean forceRefreshAddressCache;\n+            if (!isRetryableException(exception)) {\n+\n+                logger.debug(\"Operation will NOT be retried. Current attempt {}, Exception: \", this.attemptCount,\n+                    exception);\n+                return Mono.just(ShouldRetryResult.noRetry());\n+            } else if (exception instanceof GoneException &&\n+                !request.isReadOnly() &&\n+                BridgeInternal.hasSendingRequestStarted((CosmosException)exception)) {\n+\n+                logger.warn(\n+                    \"Operation will NOT be retried. Write operations can not be retried safely when sending the request \" +\n+                        \"to the service because they aren't idempotent. Current attempt {}, Exception: \",\n+                    this.attemptCount,\n+                    exception);\n+\n+                return Mono.just(ShouldRetryResult.noRetry(\n+                    Quadruple.with(true, true, Duration.ofMillis(0), this.attemptCount)));\n+            }\n+\n+            long remainingSeconds = this.waitTimeInSeconds - this.getElapsedTimeSupplier.get().toSeconds();\n+            int currentRetryAttemptCount = this.attemptCount;\n+            if (this.attemptCount++ > 1) {\n+                if (remainingSeconds <= 0) {\n+                    exceptionToThrow = logAndWrapExceptionWithLastRetryWithException(exception);\n+                    return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n                 }\n-                stopStopWatch(this.durationTimer);\n-                return Mono.just(ShouldRetryResult.error(exceptionToThrow));\n+\n+                backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n+                    GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n+                this.currentBackoffSeconds *= GoneRetryPolicy.BACK_OFF_MULTIPLIER;\n+                logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n             }\n-            backoffTime = Duration.ofSeconds(Math.min(Math.min(this.currentBackoffSeconds, remainingSeconds),\n-                    GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS));\n-            this.currentBackoffSeconds *= GoneAndRetryWithRetryPolicy.BACK_OFF_MULTIPLIER;\n-            logger.debug(\"BackoffTime: {} seconds.\", backoffTime.getSeconds());\n+\n+            // Calculate the remaining time based after accounting for the backoff that we\n+            // will perform\n+            long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n+            timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n+                : Duration.ofSeconds(GoneRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n+\n+            Pair<Mono<ShouldRetryResult>, Boolean> exceptionHandlingResult = handleException(exception);\n+            Mono<ShouldRetryResult> result = exceptionHandlingResult.getLeft();\n+            if (result != null) {\n+                return result;\n+            }\n+\n+            forceRefreshAddressCache = exceptionHandlingResult.getRight();\n+\n+            return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n+                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleException(Exception exception) {\n+            if (exception instanceof GoneException) {\n+                return handleGoneException((GoneException)exception);\n+            } else if (exception instanceof PartitionIsMigratingException) {\n+                return handlePartitionIsMigratingException((PartitionIsMigratingException)exception);\n+            } else if (exception instanceof InvalidPartitionException) {\n+                return handleInvalidPartitionException((InvalidPartitionException)exception);\n+            } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n+                return handlePartitionKeyIsSplittingException((PartitionKeyRangeIsSplittingException) exception);\n+            }\n+\n+            throw new IllegalStateException(\"Invalid exception type\", exception);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleGoneException(GoneException exception) {\n+            logger.info(\"Received gone exception, will retry, {}\", exception.toString());\n+            return Pair.of(null, true); // indicate we are in retry.\n         }\n \n-        // Calculate the remaining time based after accounting for the backoff that we\n-        // will perform\n-        long timeoutInMillSec = remainingSeconds*1000 - backoffTime.toMillis();\n-        timeout = timeoutInMillSec > 0 ? Duration.ofMillis(timeoutInMillSec)\n-                : Duration.ofSeconds(GoneAndRetryWithRetryPolicy.MAXIMUM_BACKOFF_TIME_IN_SECONDS);\n-        if (exception instanceof GoneException) {\n-            logger.debug(\"Received gone exception, will retry, {}\", exception.toString());\n-            forceRefreshAddressCache = true; // indicate we are in retry.\n-        } else if (exception instanceof PartitionIsMigratingException) {\n-            logger.warn(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionIsMigratingException(PartitionIsMigratingException exception) {\n+            logger.info(\"Received PartitionIsMigratingException, will retry, {}\", exception.toString());\n             this.request.forceCollectionRoutingMapRefresh = true;\n-            forceRefreshAddressCache = true;\n-        } else if (exception instanceof InvalidPartitionException) {\n+            return Pair.of(null, true);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handlePartitionKeyIsSplittingException(PartitionKeyRangeIsSplittingException exception) {\n+            this.request.requestContext.resolvedPartitionKeyRange = null;\n+            this.request.requestContext.quorumSelectedLSN = -1;\n+            this.request.requestContext.quorumSelectedStoreResponse = null;\n+            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n+            this.request.forcePartitionKeyRangeRefresh = true;\n+            return Pair.of(null, false);\n+        }\n+\n+        private Pair<Mono<ShouldRetryResult>, Boolean> handleInvalidPartitionException(InvalidPartitionException exception) {\n             this.request.requestContext.quorumSelectedLSN = -1;\n             this.request.requestContext.resolvedPartitionKeyRange = null;\n             this.request.requestContext.quorumSelectedStoreResponse = null;\n             this.request.requestContext.globalCommittedSelectedLSN = -1;\n             if (this.attemptCountInvalidPartition++ > 2) {\n                 // for second InvalidPartitionException, stop retrying.\n                 logger.warn(\"Received second InvalidPartitionException after backoff/retry. Will fail the request. {}\",\n-                        exception.toString());\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createServiceUnavailableException(exception)));\n+                    exception.toString());\n+                return Pair.of(\n+                    Mono.just(ShouldRetryResult.error(BridgeInternal.createServiceUnavailableException(exception))),\n+                    false);\n             }\n \n-            if (this.request != null) {\n-                logger.warn(\"Received invalid collection exception, will retry, {}\", exception.toString());\n-                this.request.forceNameCacheRefresh = true;\n-            } else {\n-                logger.error(\"Received unexpected invalid collection exception, request should be non-null.\",\n-                        exception);\n-                return Mono.just(ShouldRetryResult\n-                        .error(BridgeInternal.createCosmosException(HttpConstants.StatusCodes.INTERNAL_SERVER_ERROR, exception)));\n-            }\n-            forceRefreshAddressCache = false;\n-        } else if (exception instanceof PartitionKeyRangeIsSplittingException) {\n-            this.request.requestContext.resolvedPartitionKeyRange = null;\n-            this.request.requestContext.quorumSelectedLSN = -1;\n-            this.request.requestContext.quorumSelectedStoreResponse = null;\n-            logger.info(\"Received partition key range splitting exception, will retry, {}\", exception.toString());\n-            this.request.forcePartitionKeyRangeRefresh = true;\n-            forceRefreshAddressCache = false;\n-        } else {\n-            logger.warn(\"Received retrywith exception, will retry, {}\", exception);\n-            // For RetryWithException, prevent the caller\n-            // from refreshing any caches.\n-            forceRefreshAddressCache = false;\n+            logger.info(\"Received invalid collection exception, will retry, {}\", exception.toString());\n+            this.request.forceNameCacheRefresh = true;\n+\n+            return Pair.of(null, false);\n         }\n-        return Mono.just(ShouldRetryResult.retryAfter(backoffTime,\n-                Quadruple.with(forceRefreshAddressCache, true, timeout, currentRetryAttemptCount)));\n     }\n \n-    private void stopStopWatch(StopWatch stopwatch) {\n-        synchronized (stopwatch) {\n-            stopwatch.stop();\n+    static class RetryWithRetryPolicy extends RetryPolicyWithDiagnostics {\n+        private final static int DEFAULT_WAIT_TIME_IN_SECONDS = 30;\n+        private final static int MAXIMUM_BACKOFF_TIME_IN_MS = 15000;\n+        private final static int INITIAL_BACKOFF_TIME_MS = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5c79d47a2ecef726755f4b3f57195be71d1f592"}, "originalPosition": 433}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4405c1c88c6b97042c043a9a27082b3f2ec017e9", "author": {"user": {"login": "FabianMeiswinkel", "name": "Fabian Meiswinkel"}}, "url": "https://github.com/Azure/azure-sdk-for-java/commit/4405c1c88c6b97042c043a9a27082b3f2ec017e9", "committedDate": "2020-10-19T20:09:28Z", "message": "Reacting to CR feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMTM2ODg4", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-512136888", "createdAt": "2020-10-19T20:32:50Z", "commit": {"oid": "4405c1c88c6b97042c043a9a27082b3f2ec017e9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzMDE0NDE3", "url": "https://github.com/Azure/azure-sdk-for-java/pull/16473#pullrequestreview-513014417", "createdAt": "2020-10-20T18:24:01Z", "commit": {"oid": "4405c1c88c6b97042c043a9a27082b3f2ec017e9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1925, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}