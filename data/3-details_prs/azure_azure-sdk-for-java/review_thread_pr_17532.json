{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIwMDAyMzQz", "number": 17532, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNjo1MzowMlrOE4QNrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyMDoxNFrOE4W8PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NDIxMzU5OnYy", "diffSide": "LEFT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/java/module-info.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNjo1MzowMlrOHyEI5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNjo1MzowMlrOHyEI5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjI1ODY2Mw==", "bodyText": "scala doesn't have support for module system:\nscala/bug#11423\nThis file causes compilation problem. hence removed.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522258663", "createdAt": "2020-11-12T16:53:02Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/java/module-info.java", "diffHunk": "@@ -1,6 +0,0 @@\n-// Copyright (c) Microsoft Corporation. All rights reserved.\n-// Licensed under the MIT License.\n-\n-module com.azure.cosmos.spark {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0246fa6112e7ca43e3fa7beed4bdfa13e13e5ecb"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTI1Njg4OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMjowN1rOHyOO2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMzo1MTozN1rOHyTwAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDAyNw==", "bodyText": "NIT -remove one LF", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522424027", "createdAt": "2020-11-12T21:02:07Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUwNzI1MA==", "bodyText": "next commit.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522507250", "createdAt": "2020-11-12T23:39:20Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDAyNw=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNDQzNQ==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522514435", "createdAt": "2020-11-12T23:51:37Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDAyNw=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTI4NDkyOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMDoyNVrOHyOfgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMzo1MTo1NVrOHyTwYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyODI4OA==", "bodyText": "Make log_ a lazy val instead?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522428288", "createdAt": "2020-11-12T21:10:25Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+\n+\n+trait CosmosLoggingTrait {\n+  // Make the log field transient so that objects with Logging can\n+  // be serialized and used on another machine\n+  @transient private var log_ : Logger = _ // scalastyle:ignore\n+\n+  // Method to get the logger name for this object\n+  protected def logName: String = {\n+    // Ignore trailing $'s in the class names for Scala objects\n+    this.getClass.getName.stripSuffix(\"$\")\n+  }\n+\n+  // Method to get or create the logger for this object\n+  protected def log: Logger = {\n+    if (log_ == null) {\n+      // scalastyle:ignore", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNDUyOA==", "bodyText": "done.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522514528", "createdAt": "2020-11-12T23:51:55Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosLoggingTrait.scala", "diffHunk": "@@ -0,0 +1,69 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import org.slf4j.{Logger, LoggerFactory}\n+\n+\n+trait CosmosLoggingTrait {\n+  // Make the log field transient so that objects with Logging can\n+  // be serialized and used on another machine\n+  @transient private var log_ : Logger = _ // scalastyle:ignore\n+\n+  // Method to get the logger name for this object\n+  protected def logName: String = {\n+    // Ignore trailing $'s in the class names for Scala objects\n+    this.getClass.getName.stripSuffix(\"$\")\n+  }\n+\n+  // Method to get or create the logger for this object\n+  protected def log: Logger = {\n+    if (log_ == null) {\n+      // scalastyle:ignore", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyODI4OA=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTI5MDYwOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMjoyMFrOHyOjMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNjowMzoxMlrOHy0Lzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTIzMw==", "bodyText": "In my mental model I though about something like\ncosmos.items (which would implement the inetrfaces for read and write(batch and point))\ncosmos.changefeed for changefeed - just read interfaces", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522429233", "createdAt": "2020-11-12T21:12:20Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def inferSchema(caseInsensitiveStringMap: CaseInsensitiveStringMap): StructType = {\n+    getTable(null,\n+      Array.empty[Transform],\n+      caseInsensitiveStringMap.asCaseSensitiveMap()).schema()\n+  }\n+\n+  override def shortName(): String = \"cosmos.write\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNDIyNg==", "bodyText": "I had picked \"cosmos.write\" to get started. \"cosmos.items\" for batch read/write looks better to me.\nregarding \"cosmos.changefeed\", that will be used mainly for streaming scenario right? should we add streaming suffix?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522434226", "createdAt": "2020-11-12T21:22:30Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def inferSchema(caseInsensitiveStringMap: CaseInsensitiveStringMap): StructType = {\n+    getTable(null,\n+      Array.empty[Transform],\n+      caseInsensitiveStringMap.asCaseSensitiveMap()).schema()\n+  }\n+\n+  override def shortName(): String = \"cosmos.write\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTIzMw=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNDU5NA==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522514594", "createdAt": "2020-11-12T23:52:03Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def inferSchema(caseInsensitiveStringMap: CaseInsensitiveStringMap): StructType = {\n+    getTable(null,\n+      Array.empty[Transform],\n+      caseInsensitiveStringMap.asCaseSensitiveMap()).schema()\n+  }\n+\n+  override def shortName(): String = \"cosmos.write\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTIzMw=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA0NTgzOQ==", "bodyText": "Changefeed would be used for both streaming and batch (mostly streaming i guess but we also have plenty of customers using batch in regular intervals)\nStreaming and batch are just different notions on the read write capabilities so I would not add any prefixes there\nBut between Items and Changefeed there are significant differences - write on CF isn't possible and even read is very different because for example predicate push down isn't possible for CF etc.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r523045839", "createdAt": "2020-11-13T16:03:12Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def inferSchema(caseInsensitiveStringMap: CaseInsensitiveStringMap): StructType = {\n+    getTable(null,\n+      Array.empty[Transform],\n+      caseInsensitiveStringMap.asCaseSensitiveMap()).schema()\n+  }\n+\n+  override def shortName(): String = \"cosmos.write\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTIzMw=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTI5MjAxOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMjo0OVrOHyOkFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMzo1MjowN1rOHyTwxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTQ2Mg==", "bodyText": "CosmosItemsDataSource?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522429462", "createdAt": "2020-11-12T21:12:49Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUwNzEwNw==", "bodyText": "sure. next commit will be renamed.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522507107", "createdAt": "2020-11-12T23:38:58Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTQ2Mg=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjUxNDYzMA==", "bodyText": "done", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522514630", "createdAt": "2020-11-12T23:52:07Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataSource.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util\n+\n+import org.apache.spark.sql.connector.catalog.{Table, TableProvider}\n+import org.apache.spark.sql.connector.expressions.Transform\n+import org.apache.spark.sql.sources.DataSourceRegister\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+class CosmosDataSource extends DataSourceRegister with TableProvider with CosmosLoggingTrait {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyOTQ2Mg=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTI5NTE5OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxMzo1N1rOHyOmMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxNDo0OVrOHyOn8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMDAwMA==", "bodyText": "I assuem long term we would want to have a cache similar to what I added in the 3.* release for today's OLTP connector? If so I can take a stab at that early next week.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522430000", "createdAt": "2020-11-12T21:13:57Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util.UUID\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.connector.write.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+class CosmosDataWriteFactory extends DataWriterFactory with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def createWriter(i: Int, l: Long): DataWriter[InternalRow] = new CosmosWriter()\n+\n+  class CosmosWriter() extends DataWriter[InternalRow] {\n+    logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+    // TODO moderakh account config and databaseName, containerName need to passed down from the user\n+    val client = new CosmosClientBuilder()\n+      .key(TestConfigurations.MASTER_KEY)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMDQ0OA==", "bodyText": "Similar to CosmosDBConnectionCache.scala", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522430448", "createdAt": "2020-11-12T21:14:49Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util.UUID\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.connector.write.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+class CosmosDataWriteFactory extends DataWriterFactory with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def createWriter(i: Int, l: Long): DataWriter[InternalRow] = new CosmosWriter()\n+\n+  class CosmosWriter() extends DataWriter[InternalRow] {\n+    logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+    // TODO moderakh account config and databaseName, containerName need to passed down from the user\n+    val client = new CosmosClientBuilder()\n+      .key(TestConfigurations.MASTER_KEY)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMDAwMA=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTMwMTE3OnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxNTo0M1rOHyOpuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToxNTo0M1rOHyOpuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMDkwNw==", "bodyText": "Looks liek the best approach - to generate it in the spark layer before calling the sdk", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522430907", "createdAt": "2020-11-12T21:15:43Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/CosmosDataWriteFactory.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.util.UUID\n+\n+import com.azure.cosmos.implementation.TestConfigurations\n+import com.azure.cosmos.{ConsistencyLevel, CosmosClientBuilder}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.connector.write.{DataWriter, DataWriterFactory, WriterCommitMessage}\n+import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n+\n+class CosmosDataWriteFactory extends DataWriterFactory with CosmosLoggingTrait {\n+  logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+  override def createWriter(i: Int, l: Long): DataWriter[InternalRow] = new CosmosWriter()\n+\n+  class CosmosWriter() extends DataWriter[InternalRow] {\n+    logInfo(s\"Instantiated ${this.getClass.getSimpleName}\")\n+\n+    // TODO moderakh account config and databaseName, containerName need to passed down from the user\n+    val client = new CosmosClientBuilder()\n+      .key(TestConfigurations.MASTER_KEY)\n+      .endpoint(TestConfigurations.HOST)\n+      .consistencyLevel(ConsistencyLevel.EVENTUAL)\n+      .buildAsyncClient();\n+    val databaseName = \"testDB\"\n+    val containerName = \"testContainer\"\n+\n+    override def write(internalRow: InternalRow): Unit = {\n+      // TODO moderakh: schema is hard coded for now to make end to end TestE2EMain work implement schema inference code\n+      val userProvidedSchema = StructType(Seq(StructField(\"number\", IntegerType), StructField(\"word\", StringType)))\n+\n+      val objectNode = CosmosRowConverter.internalRowToObjectNode(internalRow, userProvidedSchema)\n+      // TODO: moderakh how should we handle absence of id?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NTMxNTgxOnYy", "diffSide": "RIGHT", "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/RowConverter.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyMDoxNFrOHyOyZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQwMToyOToxNVrOHzFoxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMzEyNg==", "bodyText": "Where is this implementation coming from OLAP, built-in Spark connectors like CSVDataDSource? I though Spark also added capability to transform DataFrame forma nd to json - my gut feeling is that it would be good to stick with taht one.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522433126", "createdAt": "2020-11-12T21:20:14Z", "author": {"login": "FabianMeiswinkel"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/RowConverter.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.sql.{Date, Timestamp}\n+import java.util\n+\n+import com.azure.cosmos\n+import com.azure.cosmos.spark.CosmosLoggingTrait\n+import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\n+import com.fasterxml.jackson.databind.node.{ArrayNode, NullNode, ObjectNode}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+import org.apache.spark.sql.catalyst.expressions.UnsafeMapData\n+import org.apache.spark.sql.types.{BinaryType, BooleanType, DataType, DateType, DecimalType, DoubleType, FloatType, IntegerType, LongType, _}\n+//import org.json.{JSONArray, JSONObject}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable.ListBuffer\n+\n+/**\n+ * TODO add more unit tests for this class to CosmosRowConverterSpec.\n+ */\n+\n+object CosmosRowConverter\n+  extends Serializable\n+    with CosmosLoggingTrait {\n+\n+  // TODO moderakh make this configurable\n+  val objectMapper = new ObjectMapper();\n+\n+  def rowToObjectNode(row: Row): ObjectNode = {\n+\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    row.schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(row.get(i), field.dataType, isInternalRow = false)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  def internalRowToObjectNode(internalRow: InternalRow, schema: StructType): ObjectNode = {\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(internalRow.get(i, field.dataType), field.dataType, isInternalRow = true)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  private def addJsonPrimitive(jsonValue: Any, fieldName: String, objectNode : ObjectNode) : Unit = {\n+    jsonValue match {\n+      case element: Boolean => objectNode.put(fieldName, element.asInstanceOf[Boolean])\n+      case element: String => objectNode.put(fieldName, element.asInstanceOf[String])\n+      case element: Double => objectNode.put(fieldName, element.asInstanceOf[Double])\n+      case element: Float => objectNode.put(fieldName, element.asInstanceOf[Float])\n+      case element: Long => objectNode.put(fieldName, element.asInstanceOf[Long])\n+      case element: Int => objectNode.put(fieldName, element.asInstanceOf[Int])\n+      case element: JsonNode => objectNode.set(fieldName, element.asInstanceOf[JsonNode])\n+      case _ => objectNode.putNull(fieldName)\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNTg4MA==", "bodyText": "oh please don't review RowConverter yet. this class is evolving ...", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522435880", "createdAt": "2020-11-12T21:25:48Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/RowConverter.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.sql.{Date, Timestamp}\n+import java.util\n+\n+import com.azure.cosmos\n+import com.azure.cosmos.spark.CosmosLoggingTrait\n+import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\n+import com.fasterxml.jackson.databind.node.{ArrayNode, NullNode, ObjectNode}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+import org.apache.spark.sql.catalyst.expressions.UnsafeMapData\n+import org.apache.spark.sql.types.{BinaryType, BooleanType, DataType, DateType, DecimalType, DoubleType, FloatType, IntegerType, LongType, _}\n+//import org.json.{JSONArray, JSONObject}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable.ListBuffer\n+\n+/**\n+ * TODO add more unit tests for this class to CosmosRowConverterSpec.\n+ */\n+\n+object CosmosRowConverter\n+  extends Serializable\n+    with CosmosLoggingTrait {\n+\n+  // TODO moderakh make this configurable\n+  val objectMapper = new ObjectMapper();\n+\n+  def rowToObjectNode(row: Row): ObjectNode = {\n+\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    row.schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(row.get(i), field.dataType, isInternalRow = false)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  def internalRowToObjectNode(internalRow: InternalRow, schema: StructType): ObjectNode = {\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(internalRow.get(i, field.dataType), field.dataType, isInternalRow = true)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  private def addJsonPrimitive(jsonValue: Any, fieldName: String, objectNode : ObjectNode) : Unit = {\n+    jsonValue match {\n+      case element: Boolean => objectNode.put(fieldName, element.asInstanceOf[Boolean])\n+      case element: String => objectNode.put(fieldName, element.asInstanceOf[String])\n+      case element: Double => objectNode.put(fieldName, element.asInstanceOf[Double])\n+      case element: Float => objectNode.put(fieldName, element.asInstanceOf[Float])\n+      case element: Long => objectNode.put(fieldName, element.asInstanceOf[Long])\n+      case element: Int => objectNode.put(fieldName, element.asInstanceOf[Int])\n+      case element: JsonNode => objectNode.set(fieldName, element.asInstanceOf[JsonNode])\n+      case _ => objectNode.putNull(fieldName)\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMzEyNg=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc0OTYxMg==", "bodyText": "Row -> ObjectNode, I didn't find any out of the box suitable Row -> ObjectNode conversion.\nbased on my reading, a few interesting things which I found:\n\n\nThere is a \"private internal\" class converter in the spark code\u00a0\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JacksonGenerator.scala\nAs this is a private class not suitable for us.\n\n\nThe other option is serializing Row to a json String and passing Json string to CosmosClient. Not a good options as it requires parsing the string to extract PK (perf overhead)\n\n\nthe other option is dealing with\nhttps://spark.apache.org/docs/3.0.1/api/java/org/apache/spark/sql/Row.html#jsonValue--\nIt should be possible to convert from org.json4s to jackson\n\n\nbut anyway this option also doesn't seem to be a good one. The Row.jsonValue in the Scala code seem to be a private method not a public one.\nsee here: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L548\n\none other thing I would like to read about is Row Encoders and Decoders:\nhttps://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Encoder.html\n\n########\nThe RowConverter code in this PR is a rewritten version of what exists in OLTP spark connector today.\nI rewrote it to work with jackson with some fixes and added some unit tests.\nAre you referring to some other workaround?", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r522749612", "createdAt": "2020-11-13T07:52:08Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/RowConverter.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.sql.{Date, Timestamp}\n+import java.util\n+\n+import com.azure.cosmos\n+import com.azure.cosmos.spark.CosmosLoggingTrait\n+import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\n+import com.fasterxml.jackson.databind.node.{ArrayNode, NullNode, ObjectNode}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+import org.apache.spark.sql.catalyst.expressions.UnsafeMapData\n+import org.apache.spark.sql.types.{BinaryType, BooleanType, DataType, DateType, DecimalType, DoubleType, FloatType, IntegerType, LongType, _}\n+//import org.json.{JSONArray, JSONObject}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable.ListBuffer\n+\n+/**\n+ * TODO add more unit tests for this class to CosmosRowConverterSpec.\n+ */\n+\n+object CosmosRowConverter\n+  extends Serializable\n+    with CosmosLoggingTrait {\n+\n+  // TODO moderakh make this configurable\n+  val objectMapper = new ObjectMapper();\n+\n+  def rowToObjectNode(row: Row): ObjectNode = {\n+\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    row.schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(row.get(i), field.dataType, isInternalRow = false)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  def internalRowToObjectNode(internalRow: InternalRow, schema: StructType): ObjectNode = {\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(internalRow.get(i, field.dataType), field.dataType, isInternalRow = true)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  private def addJsonPrimitive(jsonValue: Any, fieldName: String, objectNode : ObjectNode) : Unit = {\n+    jsonValue match {\n+      case element: Boolean => objectNode.put(fieldName, element.asInstanceOf[Boolean])\n+      case element: String => objectNode.put(fieldName, element.asInstanceOf[String])\n+      case element: Double => objectNode.put(fieldName, element.asInstanceOf[Double])\n+      case element: Float => objectNode.put(fieldName, element.asInstanceOf[Float])\n+      case element: Long => objectNode.put(fieldName, element.asInstanceOf[Long])\n+      case element: Int => objectNode.put(fieldName, element.asInstanceOf[Int])\n+      case element: JsonNode => objectNode.set(fieldName, element.asInstanceOf[JsonNode])\n+      case _ => objectNode.putNull(fieldName)\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMzEyNg=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzMzMTc4MQ==", "bodyText": "I added this to TODO section. This requires more discussion/investigation. Will be done after this PR.", "url": "https://github.com/Azure/azure-sdk-for-java/pull/17532#discussion_r523331781", "createdAt": "2020-11-14T01:29:15Z", "author": {"login": "moderakh"}, "path": "sdk/cosmos/azure-cosmos-spark_3-0_2-12/src/main/scala/com/azure/cosmos/spark/RowConverter.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+// Copyright (c) Microsoft Corporation. All rights reserved.\n+// Licensed under the MIT License.\n+package com.azure.cosmos.spark\n+\n+import java.sql.{Date, Timestamp}\n+import java.util\n+\n+import com.azure.cosmos\n+import com.azure.cosmos.spark.CosmosLoggingTrait\n+import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\n+import com.fasterxml.jackson.databind.node.{ArrayNode, NullNode, ObjectNode}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n+import org.apache.spark.sql.catalyst.expressions.UnsafeMapData\n+import org.apache.spark.sql.types.{BinaryType, BooleanType, DataType, DateType, DecimalType, DoubleType, FloatType, IntegerType, LongType, _}\n+//import org.json.{JSONArray, JSONObject}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.util.ArrayData\n+import org.apache.spark.unsafe.types.UTF8String\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.immutable.HashMap\n+import scala.collection.mutable.ListBuffer\n+\n+/**\n+ * TODO add more unit tests for this class to CosmosRowConverterSpec.\n+ */\n+\n+object CosmosRowConverter\n+  extends Serializable\n+    with CosmosLoggingTrait {\n+\n+  // TODO moderakh make this configurable\n+  val objectMapper = new ObjectMapper();\n+\n+  def rowToObjectNode(row: Row): ObjectNode = {\n+\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    row.schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(row.get(i), field.dataType, isInternalRow = false)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  def internalRowToObjectNode(internalRow: InternalRow, schema: StructType): ObjectNode = {\n+    val jsonObject: ObjectNode = objectMapper.createObjectNode();\n+    schema.fields.zipWithIndex.foreach({\n+      case (field, i) => {\n+        val jsonValue = convertToJson(internalRow.get(i, field.dataType), field.dataType, isInternalRow = true)\n+        addJsonPrimitive(jsonValue, field.name, jsonObject)\n+      }\n+    })\n+    jsonObject\n+  }\n+\n+  private def addJsonPrimitive(jsonValue: Any, fieldName: String, objectNode : ObjectNode) : Unit = {\n+    jsonValue match {\n+      case element: Boolean => objectNode.put(fieldName, element.asInstanceOf[Boolean])\n+      case element: String => objectNode.put(fieldName, element.asInstanceOf[String])\n+      case element: Double => objectNode.put(fieldName, element.asInstanceOf[Double])\n+      case element: Float => objectNode.put(fieldName, element.asInstanceOf[Float])\n+      case element: Long => objectNode.put(fieldName, element.asInstanceOf[Long])\n+      case element: Int => objectNode.put(fieldName, element.asInstanceOf[Int])\n+      case element: JsonNode => objectNode.set(fieldName, element.asInstanceOf[JsonNode])\n+      case _ => objectNode.putNull(fieldName)\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzMzEyNg=="}, "originalCommit": {"oid": "6747aaa384be5a74e7ec76338aaa730bd2bccbeb"}, "originalPosition": 72}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2935, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}