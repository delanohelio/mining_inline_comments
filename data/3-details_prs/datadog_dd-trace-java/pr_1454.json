{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3MjY2MDU2", "number": 1454, "title": "move batching to trace processor to ease memory buffer reuse", "bodyText": "Notes on the changes\n\nThe trace processor and trace writer's responsibilities have been changed\n\nthe trace processor serialises the data into a buffer, and publishes the buffer whenever\n\nthe buffer is full\nthere is a heartbeat OR a time threshold is reached\nthe DDAgentWriter performs a flush (mostly for testing and shutdown)\n\n\nthe trace writer takes these buffers and just sends them to the agent\nthe trace writer (or what was BatchWritingDisruptor)'s ring buffer is now small, with preallocated buffers. The trace processor claims sequences from this buffer, and serialises into the buffer. Once one of the aforementioned publication conditions is met, the transaction is completed and the buffer becomes available for the trace writer to consume.\n\n\nThe memory reuse is best effort, because of message pack limitations. Whenever the buffer is breached by completing the serialisation of a trace, an allocation occurs. Worse, as far as I can tell, message pack doesn't allow us to keep hold of the older buffer, so this will cause old gen activity. There is a heuristic based on a moving average to prevent this, but it's susceptible to highly variable trace sizes. This can be mitigated by over-sizing the buffer, reducing the sizes of traces, and reducing the variance of trace sizes.\nIn light of the divergent responsibilities of the two disruptors, the abstract base class has gone as it is no longer a large enough common abstraction.", "createdAt": "2020-05-13T10:23:13Z", "url": "https://github.com/DataDog/dd-trace-java/pull/1454", "merged": true, "mergeCommit": {"oid": "7bf630539953d195b2d4a4cdf86a760f5f4123ce"}, "closed": true, "closedAt": "2020-05-18T17:58:17Z", "author": {"login": "richardstartin"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcg2l26gBqjMzMzE0NzM3NDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcijq3GAFqTQxMzgxNDMzMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4f87f1008e74d073c17a17fa216430cb3d33168c", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/4f87f1008e74d073c17a17fa216430cb3d33168c", "committedDate": "2020-05-13T10:21:13Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}, "afterCommit": {"oid": "5fb0c4e837d8b2e3d48fe9095370ee32a6e79f56", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/5fb0c4e837d8b2e3d48fe9095370ee32a6e79f56", "committedDate": "2020-05-13T10:47:32Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "06ff799bf4f278a7c2d20c7ae42e799f1d06a576", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/06ff799bf4f278a7c2d20c7ae42e799f1d06a576", "committedDate": "2020-05-13T11:53:14Z", "message": "accept api breaks"}, "afterCommit": {"oid": "21cf41bfa85f5e807c696972dd41e702ff0a865b", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/21cf41bfa85f5e807c696972dd41e702ff0a865b", "committedDate": "2020-05-13T13:26:58Z", "message": "accept api breaks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwOTMxNTM1", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-410931535", "createdAt": "2020-05-13T13:37:08Z", "commit": {"oid": "21cf41bfa85f5e807c696972dd41e702ff0a865b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzozNzowOFrOGUx95g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzozNzowOFrOGUx95g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ0MzM2Ng==", "bodyText": "Yes, I'm in agreement with removing this.  Even prior to this change, I was little concerned that it was creating some unnecessary complexity.", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r424443366", "createdAt": "2020-05-13T13:37:08Z", "author": {"login": "dougqh"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/AbstractDisruptor.java", "diffHunk": "@@ -1,96 +0,0 @@\n-package datadog.trace.common.writer.ddagent;\n-\n-import com.lmax.disruptor.EventHandler;\n-import com.lmax.disruptor.SleepingWaitStrategy;\n-import com.lmax.disruptor.dsl.Disruptor;\n-import com.lmax.disruptor.dsl.ProducerType;\n-import datadog.common.exec.DaemonThreadFactory;\n-import java.io.Closeable;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n-import lombok.extern.slf4j.Slf4j;\n-\n-@Slf4j\n-abstract class AbstractDisruptor<T> implements Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21cf41bfa85f5e807c696972dd41e702ff0a865b"}, "originalPosition": 14}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "21cf41bfa85f5e807c696972dd41e702ff0a865b", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/21cf41bfa85f5e807c696972dd41e702ff0a865b", "committedDate": "2020-05-13T13:26:58Z", "message": "accept api breaks"}, "afterCommit": {"oid": "1051e41a075afdce72325392773d697fd95a44de", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/1051e41a075afdce72325392773d697fd95a44de", "committedDate": "2020-05-13T20:38:45Z", "message": "run revapi"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExNzgzMTk1", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-411783195", "createdAt": "2020-05-14T13:06:23Z", "commit": {"oid": "1051e41a075afdce72325392773d697fd95a44de"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxMzowNjoyM1rOGVbUQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxMzowNjoyM1rOGVbUQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTEyMDgzMw==", "bodyText": "This spams the logs quite a bit \ud83d\ude09", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425120833", "createdAt": "2020-05-14T13:06:23Z", "author": {"login": "bantonsson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/DDAgentApi.java", "diffHunk": "@@ -75,83 +73,81 @@ public void addResponseListener(final DDAgentResponseListener listener) {\n     }\n   }\n \n-  Response sendSerializedTraces(\n-      final int representativeCount,\n-      final int traceCount,\n-      final int sizeInBytes,\n-      final List<TraceBuffer> traces) {\n+  Response sendSerializedTraces(final TraceBuffer traces) {\n     if (httpClient == null) {\n       detectEndpointAndBuildClient();\n     }\n \n     try {\n       final Request request =\n           prepareRequest(tracesUrl)\n-              .addHeader(X_DATADOG_TRACE_COUNT, String.valueOf(representativeCount))\n-              .put(new MsgPackRequestBody(traceCount, sizeInBytes, traces))\n+              .addHeader(X_DATADOG_TRACE_COUNT, traces.representativeCount() + \"\")\n+              .put(new MsgPackRequestBody(traces))\n               .build();\n-\n       try (final okhttp3.Response response = httpClient.newCall(request).execute()) {\n         if (response.code() != 200) {\n           if (log.isDebugEnabled()) {\n             log.debug(\n                 \"Error while sending {} of {} traces to the DD agent. Status: {}, Response: {}, Body: {}\",\n-                traces.size(),\n-                representativeCount,\n+                traces.traceCount(),\n+                traces.representativeCount(),\n                 response.code(),\n                 response.message(),\n                 response.body().string());\n           } else if (nextAllowedLogTime < System.currentTimeMillis()) {\n             nextAllowedLogTime = System.currentTimeMillis() + MILLISECONDS_BETWEEN_ERROR_LOG;\n             log.warn(\n                 \"Error while sending {} of {} traces to the DD agent. Status: {} {} (going silent for {} minutes)\",\n-                traces.size(),\n-                representativeCount,\n+                traces.traceCount(),\n+                traces.representativeCount(),\n                 response.code(),\n                 response.message(),\n                 TimeUnit.MILLISECONDS.toMinutes(MILLISECONDS_BETWEEN_ERROR_LOG));\n           }\n           return Response.failed(response.code());\n         }\n-\n-        log.debug(\n-            \"Successfully sent {} of {} traces to the DD agent.\",\n-            traces.size(),\n-            representativeCount);\n-\n+        if (log.isDebugEnabled()) {\n+          log.debug(\n+              \"Successfully sent {} of {} traces to the DD agent.\",\n+              traces.traceCount(),\n+              traces.representativeCount());\n+        }\n         final String responseString = response.body().string().trim();\n         try {\n           if (!\"\".equals(responseString) && !\"OK\".equalsIgnoreCase(responseString)) {\n             final Map<String, Map<String, Number>> parsedResponse =\n                 RESPONSE_ADAPTER.fromJson(responseString);\n             final String endpoint = tracesUrl.toString();\n-\n             for (final DDAgentResponseListener listener : responseListeners) {\n               listener.onResponse(endpoint, parsedResponse);\n             }\n           }\n           return Response.success(response.code());\n         } catch (final IOException e) {\n           log.debug(\"Failed to parse DD agent response: \" + responseString, e);\n-\n           return Response.success(response.code(), e);\n         }\n       }\n     } catch (final IOException e) {\n       if (log.isDebugEnabled()) {\n         log.debug(\n             \"Error while sending \"\n-                + traces.size()\n+                + traces.traceCount()\n                 + \" of \"\n-                + representativeCount\n+                + traces.representativeCount()\n+                + \" (size=\"\n+                + (traces.sizeInBytes() / 1024)\n+                + \"KB)\"\n                 + \" traces to the DD agent.\",\n             e);\n-      } else if (nextAllowedLogTime < System.currentTimeMillis()) {\n+      } else { // if (nextAllowedLogTime < System.currentTimeMillis()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1051e41a075afdce72325392773d697fd95a44de"}, "originalPosition": 102}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5ef920c8f8e9c40ff00007eeb7fd389f603f5105", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/5ef920c8f8e9c40ff00007eeb7fd389f603f5105", "committedDate": "2020-05-14T13:32:11Z", "message": "revert change to string encoding: always check tags in string table"}, "afterCommit": {"oid": "3ab72b1480169a72df1395802086b2a2b9af2fa6", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3ab72b1480169a72df1395802086b2a2b9af2fa6", "committedDate": "2020-05-14T21:18:43Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3ab72b1480169a72df1395802086b2a2b9af2fa6", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3ab72b1480169a72df1395802086b2a2b9af2fa6", "committedDate": "2020-05-14T21:18:43Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}, "afterCommit": {"oid": "1c3a5935879f7a8c2ec047504b6136e3842dae6e", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/1c3a5935879f7a8c2ec047504b6136e3842dae6e", "committedDate": "2020-05-14T21:23:35Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1c3a5935879f7a8c2ec047504b6136e3842dae6e", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/1c3a5935879f7a8c2ec047504b6136e3842dae6e", "committedDate": "2020-05-14T21:23:35Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}, "afterCommit": {"oid": "dc357372e2c43d2580e297c2ddab0294420e79d8", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/dc357372e2c43d2580e297c2ddab0294420e79d8", "committedDate": "2020-05-14T21:27:54Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNjg5ODgy", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-412689882", "createdAt": "2020-05-15T14:17:48Z", "commit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNDoxNzo0OFrOGWGy8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNDo1OTowNVrOGWIduw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTgzMzIwMw==", "bodyText": "I have never used Palantir, so I'll ignore this part for now. Would be good with a quick walkthrough.", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425833203", "createdAt": "2020-05-15T14:17:48Z", "author": {"login": "bantonsson"}, "path": ".palantir/revapi.yml", "diffHunk": "@@ -134,8 +133,35 @@ acceptedBreaks:\n     - code: \"java.class.removed\"\n       old: \"interface datadog.opentracing.scopemanager.ScopeContext\"\n       justification: \"Non-api class moved to core\"\n-    # End: Classes moved to core\n-    # Begin: LogHandler method changes\n+    - code: \"java.field.removed\"\n+      old: \"field datadog.opentracing.DDTracer.TRACE_ID_MAX\"\n+      justification: \"Internal field\"\n+    - code: \"java.field.removed\"\n+      old: \"field datadog.opentracing.DDTracer.TRACE_ID_MIN\"\n+      justification: \"Internal field\"\n+    - code: \"java.method.exception.checkedAdded\"\n+      old: \"method void datadog.opentracing.DDTracer::finalize()\"\n+      new: \"method void java.lang.Object::finalize() throws java.lang.Throwable @\\\n+        \\ datadog.opentracing.DDTracer\"\n+      justification: \"finalize only needed on CoreTracer\"\n+    - code: \"java.method.movedToSuperClass\"\n+      old: \"method void datadog.opentracing.DDTracer::finalize()\"\n+      new: \"method void java.lang.Object::finalize() throws java.lang.Throwable @\\\n+        \\ datadog.opentracing.DDTracer\"\n+      justification: \"finalize only needed on CoreTracer\"\n+    - code: \"java.method.numberOfParametersChanged\"\n+      old: \"method void datadog.opentracing.DDTracer.DDSpanBuilder::<init>(java.lang.String,\\\n+        \\ io.opentracing.ScopeManager)\"\n+      new: \"method void datadog.opentracing.DDTracer.DDSpanBuilder::<init>(java.lang.String)\"\n+      justification: \"ScopeManager should always be the DDTracer's scopemanager\"\n+    - code: \"java.method.parameterTypeChanged\"\n+      old: \"parameter datadog.opentracing.DDTracer.DDTracerBuilder datadog.opentracing.DDTracer.DDTracerBuilder::extractor(===datadog.opentracing.propagation.HttpCodec.Extractor===)\"\n+      new: \"parameter datadog.opentracing.DDTracer.DDTracerBuilder datadog.opentracing.DDTracer.DDTracerBuilder::extractor(===datadog.trace.core.propagation.HttpCodec.Extractor===)\"\n+      justification: \"HttpCodec package and interface changed\"\n+    - code: \"java.method.parameterTypeChanged\"\n+      old: \"parameter datadog.opentracing.DDTracer.DDTracerBuilder datadog.opentracing.DDTracer.DDTracerBuilder::injector(===datadog.opentracing.propagation.HttpCodec.Injector===)\"\n+      new: \"parameter datadog.opentracing.DDTracer.DDTracerBuilder datadog.opentracing.DDTracer.DDTracerBuilder::injector(===datadog.trace.core.propagation.HttpCodec.Injector===)\"\n+      justification: \"HttpCodec package and interface changed\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg2MDUzOQ==", "bodyText": "From looking at the code it seems that we are running this on every HeartBeat which is every 100 ms. I'm not sure what this does or accomplishes. I would need to get a walk-through of the code.", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425860539", "createdAt": "2020-05-15T14:59:05Z", "author": {"login": "bantonsson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,227 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush(millisecondTime());\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }\n       try {\n-        if (event.data != null) {\n-          // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n-          try {\n-            event.data = processor.onTraceComplete(event.data);\n-            // the intention is that batching ends up being handled here,\n-            // rather than on the BatchWritingDisruptor, which would\n-            // be responsible for sending trace buffers to the agent\n-            // synchronously, before returning the trace buffer for\n-            // reuse.\n-            serializer.serialize(event.data);\n-            TraceBuffer serializedTrace = serializer.getBuffer();\n-            int sizeInBytes = serializedTrace.sizeInBytes();\n-            batchWritingDisruptor.publish(serializedTrace, event.representativeCount);\n-            monitor.onSerialize(writer, event.data, sizeInBytes);\n-            event.representativeCount = 0; // reset in case flush is invoked below.\n-          } catch (final Throwable e) {\n-            log.debug(\"Error while serializing trace\", e);\n-            monitor.onFailedSerialize(writer, event.data, e);\n+        try {\n+          if (event.force\n+              || (representativeCount > 0 && serializer.shouldFlush())\n+              || event.flushLatch != null) {\n+            commitTransaction(event.flushLatch);\n+            beginTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 187}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNzA5MTQ0", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-412709144", "createdAt": "2020-05-15T14:41:01Z", "commit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNDo0MTowMVrOGWHuYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNTo0NTowN1rOGWKOLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg0ODQxNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          .addHeader(X_DATADOG_TRACE_COUNT, traces.representativeCount() + \"\")\n          \n          \n            \n                          .addHeader(X_DATADOG_TRACE_COUNT, Integer.toString(traces.representativeCount()))", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425848417", "createdAt": "2020-05-15T14:41:01Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/DDAgentApi.java", "diffHunk": "@@ -75,83 +73,81 @@ public void addResponseListener(final DDAgentResponseListener listener) {\n     }\n   }\n \n-  Response sendSerializedTraces(\n-      final int representativeCount,\n-      final int traceCount,\n-      final int sizeInBytes,\n-      final List<TraceBuffer> traces) {\n+  Response sendSerializedTraces(final TraceBuffer traces) {\n     if (httpClient == null) {\n       detectEndpointAndBuildClient();\n     }\n \n     try {\n       final Request request =\n           prepareRequest(tracesUrl)\n-              .addHeader(X_DATADOG_TRACE_COUNT, String.valueOf(representativeCount))\n-              .put(new MsgPackRequestBody(traceCount, sizeInBytes, traces))\n+              .addHeader(X_DATADOG_TRACE_COUNT, traces.representativeCount() + \"\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg1NzgyOA==", "bodyText": "These could perhaps be package private?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425857828", "createdAt": "2020-05-15T14:55:08Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/DispatchingDisruptor.java", "diffHunk": "@@ -0,0 +1,126 @@\n+package datadog.trace.common.writer.ddagent;\n+\n+import com.lmax.disruptor.BlockingWaitStrategy;\n+import com.lmax.disruptor.EventFactory;\n+import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.DaemonThreadFactory;\n+import datadog.trace.common.writer.DDAgentWriter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Disruptor that takes serialized traces and dispatches them to the DD agent\n+ *\n+ * <p>publishing to the buffer will block if the buffer is full.\n+ */\n+@Slf4j\n+public class DispatchingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<TraceBuffer> disruptor;\n+\n+  public DispatchingDisruptor(\n+      int disruptorSize,\n+      EventFactory<TraceBuffer> eventFactory,\n+      DDAgentApi api,\n+      Monitor monitor,\n+      DDAgentWriter writer) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            eventFactory,\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_WRITER,\n+            ProducerType.SINGLE,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(new TraceDispatchingHandler(api, monitor, writer));\n+  }\n+\n+  public void start() {\n+    disruptor.start();\n+  }\n+\n+  @Override\n+  public void close() {\n+    disruptor.halt();\n+  }\n+\n+  public long beginTransaction() {\n+    return disruptor.getRingBuffer().next();\n+  }\n+\n+  public TraceBuffer getTraceBuffer(long sequence) {\n+    return disruptor.getRingBuffer().get(sequence);\n+  }\n+\n+  public void commit(long sequence) {\n+    disruptor.getRingBuffer().publish(sequence);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg2MTcyMw==", "bodyText": "Wouldn't it be useful to have also the stacktrace?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    log.debug(\"Failed to send traces to the API: {}\", e.getMessage());\n          \n          \n            \n                    log.debug(\"Failed to send traces to the API: {}\", e.getMessage(), e);", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425861723", "createdAt": "2020-05-15T15:00:46Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/DispatchingDisruptor.java", "diffHunk": "@@ -0,0 +1,126 @@\n+package datadog.trace.common.writer.ddagent;\n+\n+import com.lmax.disruptor.BlockingWaitStrategy;\n+import com.lmax.disruptor.EventFactory;\n+import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.DaemonThreadFactory;\n+import datadog.trace.common.writer.DDAgentWriter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Disruptor that takes serialized traces and dispatches them to the DD agent\n+ *\n+ * <p>publishing to the buffer will block if the buffer is full.\n+ */\n+@Slf4j\n+public class DispatchingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<TraceBuffer> disruptor;\n+\n+  public DispatchingDisruptor(\n+      int disruptorSize,\n+      EventFactory<TraceBuffer> eventFactory,\n+      DDAgentApi api,\n+      Monitor monitor,\n+      DDAgentWriter writer) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            eventFactory,\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_WRITER,\n+            ProducerType.SINGLE,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(new TraceDispatchingHandler(api, monitor, writer));\n+  }\n+\n+  public void start() {\n+    disruptor.start();\n+  }\n+\n+  @Override\n+  public void close() {\n+    disruptor.halt();\n+  }\n+\n+  public long beginTransaction() {\n+    return disruptor.getRingBuffer().next();\n+  }\n+\n+  public TraceBuffer getTraceBuffer(long sequence) {\n+    return disruptor.getRingBuffer().get(sequence);\n+  }\n+\n+  public void commit(long sequence) {\n+    disruptor.getRingBuffer().publish(sequence);\n+  }\n+\n+  // Intentionally not thread safe.\n+  private static class TraceDispatchingHandler implements EventHandler<TraceBuffer> {\n+\n+    private final DDAgentApi api;\n+    private final Monitor monitor;\n+    private final DDAgentWriter writer;\n+\n+    private TraceDispatchingHandler(\n+        final DDAgentApi api, final Monitor monitor, final DDAgentWriter writer) {\n+      this.api = api;\n+      this.monitor = monitor;\n+      this.writer = writer;\n+    }\n+\n+    @Override\n+    public void onEvent(final TraceBuffer event, final long sequence, final boolean endOfBatch) {\n+      sendData(event);\n+    }\n+\n+    private void sendData(TraceBuffer traces) {\n+      if (log.isDebugEnabled()) {\n+        log.debug(\n+            \"receive id={}, rc={}, tc={}\",\n+            traces.id(),\n+            traces.representativeCount(),\n+            traces.traceCount());\n+      }\n+      try {\n+        if (traces.traceCount() > 0) {\n+          final DDAgentApi.Response response = api.sendSerializedTraces(traces);\n+          if (response.success()) {\n+            if (log.isDebugEnabled()) {\n+              log.debug(\n+                  \"Successfully sent {} traces {} to the API\", traces.traceCount(), traces.id());\n+            }\n+            monitor.onSend(writer, traces.representativeCount(), traces.sizeInBytes(), response);\n+          } else {\n+            if (log.isDebugEnabled()) {\n+              log.debug(\n+                  \"Failed to send {} traces (representing {}) of size {} bytes to the API\",\n+                  traces.traceCount(),\n+                  traces.representativeCount(),\n+                  traces.sizeInBytes());\n+            }\n+            monitor.onFailedSend(\n+                writer, traces.representativeCount(), traces.sizeInBytes(), response);\n+          }\n+        } else if (log.isDebugEnabled()) {\n+          log.debug(\"buffer {} was empty\", traces.id());\n+        }\n+      } catch (final Throwable e) {\n+        log.debug(\"Failed to send traces to the API: {}\", e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg2NTc0NQ==", "bodyText": "Nit: Could be final?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425865745", "createdAt": "2020-05-15T15:07:08Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/DisruptorUtils.java", "diffHunk": "@@ -0,0 +1,24 @@\n+package datadog.trace.common.writer.ddagent;\n+\n+import com.lmax.disruptor.EventFactory;\n+import com.lmax.disruptor.WaitStrategy;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import java.util.concurrent.ThreadFactory;\n+\n+public class DisruptorUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg3MTY3NA==", "bodyText": "Can you add a comment here explaining why both clear() and flush() must be called?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425871674", "createdAt": "2020-05-15T15:16:19Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/MsgPackStatefulSerializer.java", "diffHunk": "@@ -37,99 +44,192 @@\n \n   // reusing this within the context of each thread is handy because it\n   // caches an Encoder\n-  private final MessagePacker messagePacker =\n-      MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n+  private final MessagePacker messagePacker;\n \n   private final int[] traceSizes = new int[TRACE_HISTORY_SIZE];\n+  private final int sizeThresholdBytes;\n+  private final int bufferSize;\n \n   private int traceSizeSum;\n   private int position;\n-  private MsgPackTraceBuffer lastBuffer;\n+  private MsgPackTraceBuffer traceBuffer;\n \n-  private int lastTracesPerBuffer = 1;\n-  private int tracesPerBuffer;\n+  private int currentSerializedBytes = 0;\n \n   public MsgPackStatefulSerializer() {\n-    Arrays.fill(traceSizes, INITIAL_BUFFER_SIZE);\n-    this.traceSizeSum = INITIAL_BUFFER_SIZE * TRACE_HISTORY_SIZE;\n+    this(DEFAULT_BUFFER_THRESHOLD, DEFAULT_BUFFER_THRESHOLD * 3 / 2); // 1MB\n+  }\n+\n+  public MsgPackStatefulSerializer(int sizeThresholdBytes, int bufferSize) {\n+    Arrays.fill(traceSizes, INITIAL_TRACE_SIZE_ESTIMATE);\n+    this.traceSizeSum = INITIAL_TRACE_SIZE_ESTIMATE * TRACE_HISTORY_SIZE;\n+    this.sizeThresholdBytes = sizeThresholdBytes;\n+    this.bufferSize = bufferSize;\n+    this.messagePacker = MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n   }\n \n   @Override\n-  public void serialize(List<DDSpan> trace) throws IOException {\n-    if (null == lastBuffer) {\n-      lastBuffer = newBuffer();\n-      messagePacker.reset(lastBuffer.buffer);\n-      messagePacker.clear();\n-    }\n+  public int serialize(List<DDSpan> trace) throws IOException {\n     MSGPACK_WRITER.writeTrace(trace, messagePacker);\n-    int serializedSize = (int) messagePacker.getTotalWrittenBytes();\n-    updateTraceSize(serializedSize);\n-    lastBuffer.length += serializedSize;\n-    ++lastBuffer.traceCount;\n-    ++tracesPerBuffer;\n+    int newSerializedSize = (int) messagePacker.getTotalWrittenBytes();\n+    int serializedSize = newSerializedSize - currentSerializedBytes;\n+    currentSerializedBytes = newSerializedSize;\n+    updateTraceSizeEstimate(serializedSize);\n+    ++traceBuffer.traceCount;\n+    traceBuffer.length = newSerializedSize;\n+    return serializedSize;\n+  }\n+\n+  @Override\n+  public void dropBuffer() throws IOException {\n+    messagePacker.flush();\n+    traceBuffer = null;\n+  }\n+\n+  @Override\n+  public boolean shouldFlush() {\n+    // Return true if could not take another average trace without allocating.\n+    // There are many cases where this will lead to some amount of over allocation,\n+    // e.g. a very large trace after many very small traces, but it's a best effort\n+    // strategy to avoid buffer growth eventually.\n+    return currentSerializedBytes + avgTraceSize() >= sizeThresholdBytes;\n   }\n \n   @Override\n-  public TraceBuffer getBuffer() throws IOException {\n+  public void reset(TraceBuffer buffer) {\n+    if (buffer instanceof MsgPackTraceBuffer) {\n+      this.traceBuffer = (MsgPackTraceBuffer) buffer;\n+      this.traceBuffer.reset();\n+    } else { // i.e. if (null == buffer || unuseable)\n+      this.traceBuffer = newBuffer();\n+    }\n+    messagePacker.clear();\n     try {\n-      messagePacker.flush();\n-      return lastBuffer;\n-    } finally {\n-      lastTracesPerBuffer = tracesPerBuffer;\n-      tracesPerBuffer = 0;\n-      lastBuffer = null;\n+      messagePacker.reset(traceBuffer.buffer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg3Nzg0OQ==", "bodyText": "Can this be combined with a call to headerSize()?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  if (traceCount < (1 << 4)) {\n          \n          \n            \n                    ByteBuffer buffer = ByteBuffer.allocate(1);\n          \n          \n            \n                    buffer.put(0, (byte) (traceCount | FIXARRAY_PREFIX));\n          \n          \n            \n                    channel.write(buffer);\n          \n          \n            \n                  } else if (traceCount < (1 << 16)) {\n          \n          \n            \n                    ByteBuffer buffer = ByteBuffer.allocate(3);\n          \n          \n            \n                    buffer.put(0, ARRAY16);\n          \n          \n            \n                    buffer.putShort(1, (short) traceCount);\n          \n          \n            \n                    channel.write(buffer);\n          \n          \n            \n                  } else {\n          \n          \n            \n                    ByteBuffer buffer = ByteBuffer.allocate(5);\n          \n          \n            \n                    buffer.put(0, ARRAY32);\n          \n          \n            \n                    buffer.putInt(1, traceCount);\n          \n          \n            \n                    channel.write(buffer);\n          \n          \n            \n                  }\n          \n          \n            \n                  int headerSize = headerSize();\n          \n          \n            \n                  ByteBuffer buffer = ByteBuffer.allocate(headerSize);\n          \n          \n            \n                  switch (headerSize) {\n          \n          \n            \n                  if (traceCount < (1 << 4)) {\n          \n          \n            \n                    case 1:\n          \n          \n            \n                      buffer.put(0, (byte) (traceCount | FIXARRAY_PREFIX));\n          \n          \n            \n                      break;\n          \n          \n            \n                    case 3:\n          \n          \n            \n                      buffer.put(0, ARRAY16);\n          \n          \n            \n                      buffer.putShort(1, (short) traceCount);\n          \n          \n            \n                      break;\n          \n          \n            \n                    case 5:\n          \n          \n            \n                      buffer.put(0, ARRAY32);\n          \n          \n            \n                      buffer.putInt(1, traceCount);\n          \n          \n            \n                      break;\n          \n          \n            \n                    default:\n          \n          \n            \n                      throw new IOException(\"Unsupported header size {}\", headerSize);\n          \n          \n            \n                  }\n          \n          \n            \n                  channel.write(buffer);", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425877849", "createdAt": "2020-05-15T15:25:45Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/MsgPackStatefulSerializer.java", "diffHunk": "@@ -37,99 +44,192 @@\n \n   // reusing this within the context of each thread is handy because it\n   // caches an Encoder\n-  private final MessagePacker messagePacker =\n-      MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n+  private final MessagePacker messagePacker;\n \n   private final int[] traceSizes = new int[TRACE_HISTORY_SIZE];\n+  private final int sizeThresholdBytes;\n+  private final int bufferSize;\n \n   private int traceSizeSum;\n   private int position;\n-  private MsgPackTraceBuffer lastBuffer;\n+  private MsgPackTraceBuffer traceBuffer;\n \n-  private int lastTracesPerBuffer = 1;\n-  private int tracesPerBuffer;\n+  private int currentSerializedBytes = 0;\n \n   public MsgPackStatefulSerializer() {\n-    Arrays.fill(traceSizes, INITIAL_BUFFER_SIZE);\n-    this.traceSizeSum = INITIAL_BUFFER_SIZE * TRACE_HISTORY_SIZE;\n+    this(DEFAULT_BUFFER_THRESHOLD, DEFAULT_BUFFER_THRESHOLD * 3 / 2); // 1MB\n+  }\n+\n+  public MsgPackStatefulSerializer(int sizeThresholdBytes, int bufferSize) {\n+    Arrays.fill(traceSizes, INITIAL_TRACE_SIZE_ESTIMATE);\n+    this.traceSizeSum = INITIAL_TRACE_SIZE_ESTIMATE * TRACE_HISTORY_SIZE;\n+    this.sizeThresholdBytes = sizeThresholdBytes;\n+    this.bufferSize = bufferSize;\n+    this.messagePacker = MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n   }\n \n   @Override\n-  public void serialize(List<DDSpan> trace) throws IOException {\n-    if (null == lastBuffer) {\n-      lastBuffer = newBuffer();\n-      messagePacker.reset(lastBuffer.buffer);\n-      messagePacker.clear();\n-    }\n+  public int serialize(List<DDSpan> trace) throws IOException {\n     MSGPACK_WRITER.writeTrace(trace, messagePacker);\n-    int serializedSize = (int) messagePacker.getTotalWrittenBytes();\n-    updateTraceSize(serializedSize);\n-    lastBuffer.length += serializedSize;\n-    ++lastBuffer.traceCount;\n-    ++tracesPerBuffer;\n+    int newSerializedSize = (int) messagePacker.getTotalWrittenBytes();\n+    int serializedSize = newSerializedSize - currentSerializedBytes;\n+    currentSerializedBytes = newSerializedSize;\n+    updateTraceSizeEstimate(serializedSize);\n+    ++traceBuffer.traceCount;\n+    traceBuffer.length = newSerializedSize;\n+    return serializedSize;\n+  }\n+\n+  @Override\n+  public void dropBuffer() throws IOException {\n+    messagePacker.flush();\n+    traceBuffer = null;\n+  }\n+\n+  @Override\n+  public boolean shouldFlush() {\n+    // Return true if could not take another average trace without allocating.\n+    // There are many cases where this will lead to some amount of over allocation,\n+    // e.g. a very large trace after many very small traces, but it's a best effort\n+    // strategy to avoid buffer growth eventually.\n+    return currentSerializedBytes + avgTraceSize() >= sizeThresholdBytes;\n   }\n \n   @Override\n-  public TraceBuffer getBuffer() throws IOException {\n+  public void reset(TraceBuffer buffer) {\n+    if (buffer instanceof MsgPackTraceBuffer) {\n+      this.traceBuffer = (MsgPackTraceBuffer) buffer;\n+      this.traceBuffer.reset();\n+    } else { // i.e. if (null == buffer || unuseable)\n+      this.traceBuffer = newBuffer();\n+    }\n+    messagePacker.clear();\n     try {\n-      messagePacker.flush();\n-      return lastBuffer;\n-    } finally {\n-      lastTracesPerBuffer = tracesPerBuffer;\n-      tracesPerBuffer = 0;\n-      lastBuffer = null;\n+      messagePacker.reset(traceBuffer.buffer);\n+    } catch (IOException e) { // don't expect this to happen\n+      log.error(\"Unexpected exception resetting MessagePacker buffer\", e);\n     }\n   }\n \n-  private MsgPackTraceBuffer newBuffer() {\n-    MsgPackTraceBuffer buffer = new MsgPackTraceBuffer();\n-    int traceBufferSize = traceBufferSize();\n-    buffer.buffer = new ArrayBufferOutput(traceBufferSize);\n-    return buffer;\n+  @Override\n+  public MsgPackTraceBuffer newBuffer() {\n+    return new MsgPackTraceBuffer(new ArrayBufferOutput(bufferSize));\n   }\n \n-  private void updateTraceSize(int traceSize) {\n+  private void updateTraceSizeEstimate(int traceSize) {\n     traceSizeSum = (traceSizeSum - traceSizes[position] + traceSize);\n     traceSizes[position] = traceSize;\n     position = (position + 1) & (traceSizes.length - 1);\n   }\n \n-  private int traceBufferSize() {\n-    // round up to next KB, assumes for now that there will be one trace per buffer\n-    return ((lastTracesPerBuffer * traceSizeSum / TRACE_HISTORY_SIZE) + 1023) / 1024;\n+  private int avgTraceSize() {\n+    return traceSizeSum / TRACE_HISTORY_SIZE;\n   }\n \n-  public static class MsgPackTraceBuffer implements TraceBuffer {\n+  static class MsgPackTraceBuffer implements TraceBuffer {\n+\n+    private static final AtomicInteger BUFFER_ID = new AtomicInteger(0);\n \n-    private ArrayBufferOutput buffer;\n+    private final ArrayBufferOutput buffer;\n+    final int id;\n     private int length;\n     private int traceCount;\n+    private int representativeCount;\n+    private CountDownLatch flushLatch;\n+\n+    public MsgPackTraceBuffer(ArrayBufferOutput buffer) {\n+      this.buffer = buffer;\n+      this.id = BUFFER_ID.getAndIncrement();\n+    }\n \n     @Override\n     public void writeTo(WritableByteChannel channel) throws IOException {\n+      writeHeader(channel);\n       int remaining = length;\n       for (MessageBuffer messageBuffer : buffer.toBufferList()) {\n         int size = messageBuffer.size();\n-        ByteBuffer buffer =\n-            size > remaining\n-                ? messageBuffer.sliceAsByteBuffer(0, remaining)\n-                : messageBuffer.sliceAsByteBuffer();\n+        ByteBuffer buffer = messageBuffer.sliceAsByteBuffer(0, Math.min(size, remaining));\n         while (buffer.hasRemaining()) {\n-          channel.write(buffer);\n+          remaining -= channel.write(buffer);\n         }\n-        remaining -= size;\n       }\n       assert remaining == 0;\n     }\n \n+    private void writeHeader(WritableByteChannel channel) throws IOException {\n+      // inlines behaviour from MessagePacker.packArrayHeader\n+      if (traceCount < (1 << 4)) {\n+        ByteBuffer buffer = ByteBuffer.allocate(1);\n+        buffer.put(0, (byte) (traceCount | FIXARRAY_PREFIX));\n+        channel.write(buffer);\n+      } else if (traceCount < (1 << 16)) {\n+        ByteBuffer buffer = ByteBuffer.allocate(3);\n+        buffer.put(0, ARRAY16);\n+        buffer.putShort(1, (short) traceCount);\n+        channel.write(buffer);\n+      } else {\n+        ByteBuffer buffer = ByteBuffer.allocate(5);\n+        buffer.put(0, ARRAY32);\n+        buffer.putInt(1, traceCount);\n+        channel.write(buffer);\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg3OTk3OA==", "bodyText": "Would it be possible to change this to accept a Runnable or a custom Callback interface instead?\nWould be nice if CountdownLatch choice is not imposed on the API caller.\nAlso, can this method be called concurrently with onDispatched()?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425879978", "createdAt": "2020-05-15T15:29:15Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/MsgPackStatefulSerializer.java", "diffHunk": "@@ -37,99 +44,192 @@\n \n   // reusing this within the context of each thread is handy because it\n   // caches an Encoder\n-  private final MessagePacker messagePacker =\n-      MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n+  private final MessagePacker messagePacker;\n \n   private final int[] traceSizes = new int[TRACE_HISTORY_SIZE];\n+  private final int sizeThresholdBytes;\n+  private final int bufferSize;\n \n   private int traceSizeSum;\n   private int position;\n-  private MsgPackTraceBuffer lastBuffer;\n+  private MsgPackTraceBuffer traceBuffer;\n \n-  private int lastTracesPerBuffer = 1;\n-  private int tracesPerBuffer;\n+  private int currentSerializedBytes = 0;\n \n   public MsgPackStatefulSerializer() {\n-    Arrays.fill(traceSizes, INITIAL_BUFFER_SIZE);\n-    this.traceSizeSum = INITIAL_BUFFER_SIZE * TRACE_HISTORY_SIZE;\n+    this(DEFAULT_BUFFER_THRESHOLD, DEFAULT_BUFFER_THRESHOLD * 3 / 2); // 1MB\n+  }\n+\n+  public MsgPackStatefulSerializer(int sizeThresholdBytes, int bufferSize) {\n+    Arrays.fill(traceSizes, INITIAL_TRACE_SIZE_ESTIMATE);\n+    this.traceSizeSum = INITIAL_TRACE_SIZE_ESTIMATE * TRACE_HISTORY_SIZE;\n+    this.sizeThresholdBytes = sizeThresholdBytes;\n+    this.bufferSize = bufferSize;\n+    this.messagePacker = MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n   }\n \n   @Override\n-  public void serialize(List<DDSpan> trace) throws IOException {\n-    if (null == lastBuffer) {\n-      lastBuffer = newBuffer();\n-      messagePacker.reset(lastBuffer.buffer);\n-      messagePacker.clear();\n-    }\n+  public int serialize(List<DDSpan> trace) throws IOException {\n     MSGPACK_WRITER.writeTrace(trace, messagePacker);\n-    int serializedSize = (int) messagePacker.getTotalWrittenBytes();\n-    updateTraceSize(serializedSize);\n-    lastBuffer.length += serializedSize;\n-    ++lastBuffer.traceCount;\n-    ++tracesPerBuffer;\n+    int newSerializedSize = (int) messagePacker.getTotalWrittenBytes();\n+    int serializedSize = newSerializedSize - currentSerializedBytes;\n+    currentSerializedBytes = newSerializedSize;\n+    updateTraceSizeEstimate(serializedSize);\n+    ++traceBuffer.traceCount;\n+    traceBuffer.length = newSerializedSize;\n+    return serializedSize;\n+  }\n+\n+  @Override\n+  public void dropBuffer() throws IOException {\n+    messagePacker.flush();\n+    traceBuffer = null;\n+  }\n+\n+  @Override\n+  public boolean shouldFlush() {\n+    // Return true if could not take another average trace without allocating.\n+    // There are many cases where this will lead to some amount of over allocation,\n+    // e.g. a very large trace after many very small traces, but it's a best effort\n+    // strategy to avoid buffer growth eventually.\n+    return currentSerializedBytes + avgTraceSize() >= sizeThresholdBytes;\n   }\n \n   @Override\n-  public TraceBuffer getBuffer() throws IOException {\n+  public void reset(TraceBuffer buffer) {\n+    if (buffer instanceof MsgPackTraceBuffer) {\n+      this.traceBuffer = (MsgPackTraceBuffer) buffer;\n+      this.traceBuffer.reset();\n+    } else { // i.e. if (null == buffer || unuseable)\n+      this.traceBuffer = newBuffer();\n+    }\n+    messagePacker.clear();\n     try {\n-      messagePacker.flush();\n-      return lastBuffer;\n-    } finally {\n-      lastTracesPerBuffer = tracesPerBuffer;\n-      tracesPerBuffer = 0;\n-      lastBuffer = null;\n+      messagePacker.reset(traceBuffer.buffer);\n+    } catch (IOException e) { // don't expect this to happen\n+      log.error(\"Unexpected exception resetting MessagePacker buffer\", e);\n     }\n   }\n \n-  private MsgPackTraceBuffer newBuffer() {\n-    MsgPackTraceBuffer buffer = new MsgPackTraceBuffer();\n-    int traceBufferSize = traceBufferSize();\n-    buffer.buffer = new ArrayBufferOutput(traceBufferSize);\n-    return buffer;\n+  @Override\n+  public MsgPackTraceBuffer newBuffer() {\n+    return new MsgPackTraceBuffer(new ArrayBufferOutput(bufferSize));\n   }\n \n-  private void updateTraceSize(int traceSize) {\n+  private void updateTraceSizeEstimate(int traceSize) {\n     traceSizeSum = (traceSizeSum - traceSizes[position] + traceSize);\n     traceSizes[position] = traceSize;\n     position = (position + 1) & (traceSizes.length - 1);\n   }\n \n-  private int traceBufferSize() {\n-    // round up to next KB, assumes for now that there will be one trace per buffer\n-    return ((lastTracesPerBuffer * traceSizeSum / TRACE_HISTORY_SIZE) + 1023) / 1024;\n+  private int avgTraceSize() {\n+    return traceSizeSum / TRACE_HISTORY_SIZE;\n   }\n \n-  public static class MsgPackTraceBuffer implements TraceBuffer {\n+  static class MsgPackTraceBuffer implements TraceBuffer {\n+\n+    private static final AtomicInteger BUFFER_ID = new AtomicInteger(0);\n \n-    private ArrayBufferOutput buffer;\n+    private final ArrayBufferOutput buffer;\n+    final int id;\n     private int length;\n     private int traceCount;\n+    private int representativeCount;\n+    private CountDownLatch flushLatch;\n+\n+    public MsgPackTraceBuffer(ArrayBufferOutput buffer) {\n+      this.buffer = buffer;\n+      this.id = BUFFER_ID.getAndIncrement();\n+    }\n \n     @Override\n     public void writeTo(WritableByteChannel channel) throws IOException {\n+      writeHeader(channel);\n       int remaining = length;\n       for (MessageBuffer messageBuffer : buffer.toBufferList()) {\n         int size = messageBuffer.size();\n-        ByteBuffer buffer =\n-            size > remaining\n-                ? messageBuffer.sliceAsByteBuffer(0, remaining)\n-                : messageBuffer.sliceAsByteBuffer();\n+        ByteBuffer buffer = messageBuffer.sliceAsByteBuffer(0, Math.min(size, remaining));\n         while (buffer.hasRemaining()) {\n-          channel.write(buffer);\n+          remaining -= channel.write(buffer);\n         }\n-        remaining -= size;\n       }\n       assert remaining == 0;\n     }\n \n+    private void writeHeader(WritableByteChannel channel) throws IOException {\n+      // inlines behaviour from MessagePacker.packArrayHeader\n+      if (traceCount < (1 << 4)) {\n+        ByteBuffer buffer = ByteBuffer.allocate(1);\n+        buffer.put(0, (byte) (traceCount | FIXARRAY_PREFIX));\n+        channel.write(buffer);\n+      } else if (traceCount < (1 << 16)) {\n+        ByteBuffer buffer = ByteBuffer.allocate(3);\n+        buffer.put(0, ARRAY16);\n+        buffer.putShort(1, (short) traceCount);\n+        channel.write(buffer);\n+      } else {\n+        ByteBuffer buffer = ByteBuffer.allocate(5);\n+        buffer.put(0, ARRAY32);\n+        buffer.putInt(1, traceCount);\n+        channel.write(buffer);\n+      }\n+    }\n+\n     @Override\n     public int sizeInBytes() {\n       return length;\n     }\n \n+    @Override\n+    public int headerSize() {\n+      // Need to allocate additional to handle MessagePacker.packArrayHeader\n+      if (traceCount < (1 << 4)) {\n+        return 1;\n+      } else if (traceCount < (1 << 16)) {\n+        return 3;\n+      } else {\n+        return 5;\n+      }\n+    }\n+\n     @Override\n     public int traceCount() {\n       return traceCount;\n     }\n+\n+    @Override\n+    public int representativeCount() {\n+      return representativeCount;\n+    }\n+\n+    @Override\n+    public void setRepresentativeCount(int representativeCount) {\n+      this.representativeCount = representativeCount;\n+    }\n+\n+    @Override\n+    public int id() {\n+      return id;\n+    }\n+\n+    @Override\n+    public void setLatch(CountDownLatch latch) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4MTA1Ng==", "bodyText": "If I get this right if shouldFlush() returns true the caller should get the buffer contents and reset the buffer. The name then does not sound right - perhaps shouldReset or isCapacityExceeded()? Or something even better fitting the method purpose?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425881056", "createdAt": "2020-05-15T15:31:02Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/StatefulSerializer.java", "diffHunk": "@@ -10,17 +10,32 @@\n    * Serialises the trace into a trace buffer.\n    *\n    * @param trace a list of spans making up a trace\n+   * @return how many bytes were serialized\n    * @throws IOException\n    */\n-  void serialize(List<DDSpan> trace) throws IOException;\n+  int serialize(List<DDSpan> trace) throws IOException;\n+\n+  void dropBuffer() throws IOException;\n \n   /**\n-   * Returns a buffer containing all traces written since the last call to this method. The buffer\n-   * belongs to the caller and should no longer be referenced by the serializer after being\n-   * released.\n+   * returns a newly allocated buffer\n    *\n-   * @return the buffer into which traces have been serialized.\n-   * @throws IOException\n+   * @return a new buffer\n+   */\n+  TraceBuffer newBuffer();\n+\n+  /**\n+   * Returns true if the current buffer is near or exceeding capacity. This is advice to claim the\n+   * buffer and reset.\n+   *\n+   * @return true if the buffer should be reset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4NjkzMg==", "bodyText": "Probably the nested try is not necessary here.", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425886932", "createdAt": "2020-05-15T15:41:01Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,227 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush(millisecondTime());\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }\n       try {\n-        if (event.data != null) {\n-          // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n-          try {\n-            event.data = processor.onTraceComplete(event.data);\n-            // the intention is that batching ends up being handled here,\n-            // rather than on the BatchWritingDisruptor, which would\n-            // be responsible for sending trace buffers to the agent\n-            // synchronously, before returning the trace buffer for\n-            // reuse.\n-            serializer.serialize(event.data);\n-            TraceBuffer serializedTrace = serializer.getBuffer();\n-            int sizeInBytes = serializedTrace.sizeInBytes();\n-            batchWritingDisruptor.publish(serializedTrace, event.representativeCount);\n-            monitor.onSerialize(writer, event.data, sizeInBytes);\n-            event.representativeCount = 0; // reset in case flush is invoked below.\n-          } catch (final Throwable e) {\n-            log.debug(\"Error while serializing trace\", e);\n-            monitor.onFailedSerialize(writer, event.data, e);\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTg4OTMyNw==", "bodyText": "Is this change related to the disruptor changes?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425889327", "createdAt": "2020-05-15T15:45:07Z", "author": {"login": "jbachorik"}, "path": "dd-trace-core/src/main/java/datadog/trace/core/serialization/FormatWriter.java", "diffHunk": "@@ -85,7 +85,7 @@ public void writeStringMap(\n     writeKey(key, destination);\n     writeMapHeader(value.size(), destination);\n     for (final Map.Entry<String, String> entry : value.entrySet()) {\n-      writeString(entry.getKey(), entry.getValue(), destination);\n+      writeTag(entry.getKey(), entry.getValue(), destination);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbac1ed124fe0bf03b5017a572b2bc2dafebbbc1"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyODg0MjE1", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-412884215", "createdAt": "2020-05-15T18:43:50Z", "commit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxODo0Mzo1MFrOGWP_DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxOToyNzowNVrOGWRRLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4Mzc1Ng==", "bodyText": "Do we want to wait for flushing at all?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425983756", "createdAt": "2020-05-15T18:43:50Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,233 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4ODExMA==", "bodyText": "Something that seems to have been changed, and maybe it was intentional, but in the previous version, if there was enough flushing due to size, then the heartbeat would never actually trigger a flush.  the timeout for the flush delay was reset every flush.  With the current design, it seems you could send a bunch of large payloads, but then occasionally you'll see a smaller one.\nAn additional reason to keep a \"time since last flush\" timestamp and not just since the last heartbeat flush is I think that would make an interesting health metric to report... (how frequent are we actually flushing).", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425988110", "createdAt": "2020-05-15T18:52:44Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,233 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush(millisecondTime());\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }\n       try {\n-        if (event.data != null) {\n-          // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n-          try {\n-            event.data = processor.onTraceComplete(event.data);\n-            // the intention is that batching ends up being handled here,\n-            // rather than on the BatchWritingDisruptor, which would\n-            // be responsible for sending trace buffers to the agent\n-            // synchronously, before returning the trace buffer for\n-            // reuse.\n-            serializer.serialize(event.data);\n-            TraceBuffer serializedTrace = serializer.getBuffer();\n-            int sizeInBytes = serializedTrace.sizeInBytes();\n-            batchWritingDisruptor.publish(serializedTrace, event.representativeCount);\n-            monitor.onSerialize(writer, event.data, sizeInBytes);\n-            event.representativeCount = 0; // reset in case flush is invoked below.\n-          } catch (final Throwable e) {\n-            log.debug(\"Error while serializing trace\", e);\n-            monitor.onFailedSerialize(writer, event.data, e);\n+        if (event.force\n+            || (representativeCount > 0 && serializer.isAtCapacity())\n+            || event.flushLatch != null) {\n+          commitTransaction(event.flushLatch);\n+          beginTransaction();\n+        } else if (doTimeFlush && representativeCount > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4ODg3MA==", "bodyText": "Why not keep all units in nano?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425988870", "createdAt": "2020-05-15T18:54:18Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,233 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush(millisecondTime());\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }\n       try {\n-        if (event.data != null) {\n-          // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n-          try {\n-            event.data = processor.onTraceComplete(event.data);\n-            // the intention is that batching ends up being handled here,\n-            // rather than on the BatchWritingDisruptor, which would\n-            // be responsible for sending trace buffers to the agent\n-            // synchronously, before returning the trace buffer for\n-            // reuse.\n-            serializer.serialize(event.data);\n-            TraceBuffer serializedTrace = serializer.getBuffer();\n-            int sizeInBytes = serializedTrace.sizeInBytes();\n-            batchWritingDisruptor.publish(serializedTrace, event.representativeCount);\n-            monitor.onSerialize(writer, event.data, sizeInBytes);\n-            event.representativeCount = 0; // reset in case flush is invoked below.\n-          } catch (final Throwable e) {\n-            log.debug(\"Error while serializing trace\", e);\n-            monitor.onFailedSerialize(writer, event.data, e);\n+        if (event.force\n+            || (representativeCount > 0 && serializer.isAtCapacity())\n+            || event.flushLatch != null) {\n+          commitTransaction(event.flushLatch);\n+          beginTransaction();\n+        } else if (doTimeFlush && representativeCount > 0) {\n+          long now = millisecondTime();\n+          if (now >= nextFlushMillis) {\n+            commitTransaction();\n+            beginTransaction();\n+            scheduleNextTimeFlush(now);\n           }\n         }\n-\n-        if (event.flushLatch != null) {\n-          if (batchWritingDisruptor.running) {\n-            // propagate the flush.\n-            batchWritingDisruptor.flush(event.representativeCount, event.flushLatch);\n-          }\n-          if (!batchWritingDisruptor.running) { // check again to protect against race condition.\n-            // got shutdown early somehow?\n-            event.flushLatch.countDown();\n-          }\n+        if (event.data != null) {\n+          serialize(event.data, event.representativeCount);\n+        }\n+      } catch (final Throwable e) {\n+        if (log.isDebugEnabled()) {\n+          log.debug(\"Error while serializing trace\", e);\n         }\n+        monitor.onFailedSerialize(writer, event.data, e);\n       } finally {\n         event.reset();\n       }\n     }\n+\n+    private void serialize(List<DDSpan> trace, int representativeCount) throws IOException {\n+      // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n+      this.representativeCount += representativeCount;\n+      trace = processor.onTraceComplete(trace);\n+      int sizeInBytes = serializer.serialize(trace);\n+      monitor.onSerialize(writer, trace, sizeInBytes);\n+    }\n+\n+    private void commitTransaction() throws IOException {\n+      commitTransaction(null);\n+    }\n+\n+    private void commitTransaction(final CountDownLatch flushLatch) throws IOException {\n+      serializer.dropBuffer();\n+      TraceBuffer buffer = dispatchingDisruptor.getTraceBuffer(publicationTxn);\n+      if (null != flushLatch) {\n+        buffer.setDispatchRunnable(\n+            new Runnable() {\n+              @Override\n+              public void run() {\n+                flushLatch.countDown();\n+              }\n+            });\n+      }\n+      buffer.setRepresentativeCount(representativeCount);\n+      if (log.isDebugEnabled()) {\n+        log.debug(\n+            \"publish id={}, rc={}, tc={}\",\n+            buffer.id(),\n+            buffer.representativeCount(),\n+            buffer.traceCount());\n+      }\n+      dispatchingDisruptor.commit(publicationTxn);\n+    }\n+\n+    private void beginTransaction() {\n+      this.publicationTxn = dispatchingDisruptor.beginTransaction();\n+      this.representativeCount = 0;\n+      serializer.reset(dispatchingDisruptor.getTraceBuffer(publicationTxn));\n+    }\n+\n+    private void scheduleNextTimeFlush(long now) {\n+      nextFlushMillis = now + flushIntervalMillis;\n+    }\n+\n+    private long millisecondTime() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk4OTg2MQ==", "bodyText": "I generally prefer to let TimeUnit do my conversions to avoid silly arithmetic errors.", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425989861", "createdAt": "2020-05-15T18:56:15Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,233 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush(millisecondTime());\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }\n       try {\n-        if (event.data != null) {\n-          // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n-          try {\n-            event.data = processor.onTraceComplete(event.data);\n-            // the intention is that batching ends up being handled here,\n-            // rather than on the BatchWritingDisruptor, which would\n-            // be responsible for sending trace buffers to the agent\n-            // synchronously, before returning the trace buffer for\n-            // reuse.\n-            serializer.serialize(event.data);\n-            TraceBuffer serializedTrace = serializer.getBuffer();\n-            int sizeInBytes = serializedTrace.sizeInBytes();\n-            batchWritingDisruptor.publish(serializedTrace, event.representativeCount);\n-            monitor.onSerialize(writer, event.data, sizeInBytes);\n-            event.representativeCount = 0; // reset in case flush is invoked below.\n-          } catch (final Throwable e) {\n-            log.debug(\"Error while serializing trace\", e);\n-            monitor.onFailedSerialize(writer, event.data, e);\n+        if (event.force\n+            || (representativeCount > 0 && serializer.isAtCapacity())\n+            || event.flushLatch != null) {\n+          commitTransaction(event.flushLatch);\n+          beginTransaction();\n+        } else if (doTimeFlush && representativeCount > 0) {\n+          long now = millisecondTime();\n+          if (now >= nextFlushMillis) {\n+            commitTransaction();\n+            beginTransaction();\n+            scheduleNextTimeFlush(now);\n           }\n         }\n-\n-        if (event.flushLatch != null) {\n-          if (batchWritingDisruptor.running) {\n-            // propagate the flush.\n-            batchWritingDisruptor.flush(event.representativeCount, event.flushLatch);\n-          }\n-          if (!batchWritingDisruptor.running) { // check again to protect against race condition.\n-            // got shutdown early somehow?\n-            event.flushLatch.countDown();\n-          }\n+        if (event.data != null) {\n+          serialize(event.data, event.representativeCount);\n+        }\n+      } catch (final Throwable e) {\n+        if (log.isDebugEnabled()) {\n+          log.debug(\"Error while serializing trace\", e);\n         }\n+        monitor.onFailedSerialize(writer, event.data, e);\n       } finally {\n         event.reset();\n       }\n     }\n+\n+    private void serialize(List<DDSpan> trace, int representativeCount) throws IOException {\n+      // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n+      this.representativeCount += representativeCount;\n+      trace = processor.onTraceComplete(trace);\n+      int sizeInBytes = serializer.serialize(trace);\n+      monitor.onSerialize(writer, trace, sizeInBytes);\n+    }\n+\n+    private void commitTransaction() throws IOException {\n+      commitTransaction(null);\n+    }\n+\n+    private void commitTransaction(final CountDownLatch flushLatch) throws IOException {\n+      serializer.dropBuffer();\n+      TraceBuffer buffer = dispatchingDisruptor.getTraceBuffer(publicationTxn);\n+      if (null != flushLatch) {\n+        buffer.setDispatchRunnable(\n+            new Runnable() {\n+              @Override\n+              public void run() {\n+                flushLatch.countDown();\n+              }\n+            });\n+      }\n+      buffer.setRepresentativeCount(representativeCount);\n+      if (log.isDebugEnabled()) {\n+        log.debug(\n+            \"publish id={}, rc={}, tc={}\",\n+            buffer.id(),\n+            buffer.representativeCount(),\n+            buffer.traceCount());\n+      }\n+      dispatchingDisruptor.commit(publicationTxn);\n+    }\n+\n+    private void beginTransaction() {\n+      this.publicationTxn = dispatchingDisruptor.beginTransaction();\n+      this.representativeCount = 0;\n+      serializer.reset(dispatchingDisruptor.getTraceBuffer(publicationTxn));\n+    }\n+\n+    private void scheduleNextTimeFlush(long now) {\n+      nextFlushMillis = now + flushIntervalMillis;\n+    }\n+\n+    private long millisecondTime() {\n+      // important: nanoTime is monotonic, currentTimeMillis is not\n+      return System.nanoTime() / 1_000_000L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk5Njg4OA==", "bodyText": "beginTransaction() is always called after commitTransaction.  Perhaps the begin should be implicit (called inside commit)?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425996888", "createdAt": "2020-05-15T19:09:18Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,233 @@\n package datadog.trace.common.writer.ddagent;\n \n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, TimeUnit.MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush(millisecondTime());\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }\n       try {\n-        if (event.data != null) {\n-          // TODO populate `_sample_rate` metric in a way that accounts for lost/dropped traces\n-          try {\n-            event.data = processor.onTraceComplete(event.data);\n-            // the intention is that batching ends up being handled here,\n-            // rather than on the BatchWritingDisruptor, which would\n-            // be responsible for sending trace buffers to the agent\n-            // synchronously, before returning the trace buffer for\n-            // reuse.\n-            serializer.serialize(event.data);\n-            TraceBuffer serializedTrace = serializer.getBuffer();\n-            int sizeInBytes = serializedTrace.sizeInBytes();\n-            batchWritingDisruptor.publish(serializedTrace, event.representativeCount);\n-            monitor.onSerialize(writer, event.data, sizeInBytes);\n-            event.representativeCount = 0; // reset in case flush is invoked below.\n-          } catch (final Throwable e) {\n-            log.debug(\"Error while serializing trace\", e);\n-            monitor.onFailedSerialize(writer, event.data, e);\n+        if (event.force\n+            || (representativeCount > 0 && serializer.isAtCapacity())\n+            || event.flushLatch != null) {\n+          commitTransaction(event.flushLatch);\n+          beginTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk5ODI4Ng==", "bodyText": "Why don't we have a test for this case anymore?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425998286", "createdAt": "2020-05-15T19:12:22Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/test/groovy/datadog/trace/api/writer/DDAgentWriterTest.groovy", "diffHunk": "@@ -113,58 +121,13 @@ class DDAgentWriterTest extends DDSpecification {\n     traceCount = 100 // Shouldn't trigger payload, but bigger than the disruptor size.\n   }\n \n-  def \"test flush by size\"() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk5OTM1OQ==", "bodyText": "Did you remove this to avoid the non-determinisim, or is the comment no longer relevant?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r425999359", "createdAt": "2020-05-15T19:14:56Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/test/groovy/datadog/trace/api/writer/DDAgentWriterTest.groovy", "diffHunk": "@@ -539,11 +505,6 @@ class DDAgentWriterTest extends DDSpecification {\n     }\n \n     then:\n-    // If the in-flight request times out (we don't currently retry),\n-    // then a new batch will begin processing and many of traces will\n-    // be accepted and batched into a new failing request.\n-    // In that case, the reject number will be low.\n-    numFailedPublish.get() - priorNumFailed >= expectedRejects * 0.80", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjAwMDIxNQ==", "bodyText": "How does this test flush by time?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426000215", "createdAt": "2020-05-15T19:16:45Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/test/groovy/datadog/trace/api/writer/DDAgentWriterTest.groovy", "diffHunk": "@@ -113,58 +121,13 @@ class DDAgentWriterTest extends DDSpecification {\n     traceCount = 100 // Shouldn't trigger payload, but bigger than the disruptor size.\n   }\n \n-  def \"test flush by size\"() {\n-    setup:\n-    def writer = DDAgentWriter.builder()\n-      .agentApi(api)\n-      .traceBufferSize(DISRUPTOR_BUFFER_SIZE)\n-      .serializer(serializer)\n-      .flushFrequencySeconds(-1).build()\n-    writer.start()\n-\n-    when:\n-    (1..6).each {\n-      writer.write(trace)\n-    }\n-    // Wait for 2 flushes of 3 by size\n-    phaser.awaitAdvanceInterruptibly(phaser.arrive())\n-    phaser.awaitAdvanceInterruptibly(phaser.arriveAndDeregister())\n-\n-    then:\n-    6 * serializer.serialize(_) >> { trace -> callRealMethod() }\n-    6 * serializer.getBuffer() >> { trace -> callRealMethod() }\n-    2 * api.sendSerializedTraces(3, 3, _, { it.size() == 3 }) >> {\n-      phaser.arrive()\n-      return DDAgentApi.Response.success(200)\n-    }\n-\n-    when:\n-    (1..2).each {\n-      writer.write(trace)\n-    }\n-    // Flush the remaining 2\n-    writer.flush()\n-\n-    then:\n-    2 * serializer.serialize(_) >> { trace -> callRealMethod() }\n-    2 * serializer.getBuffer() >> { trace -> callRealMethod() }\n-    1 * api.sendSerializedTraces(2, 2, _, { it.size() == 2 }) >> DDAgentApi.Response.success(200)\n-    0 * _\n-\n-    cleanup:\n-    writer.close()\n-\n-    where:\n-    span = newSpanOf(0, \"fixed-thread-name\")\n-    trace = (0..10000).collect { span }\n-  }\n-\n   def \"test flush by time\"() {\n     setup:\n     def writer = DDAgentWriter.builder()\n       .agentApi(api)\n       .monitor(monitor)\n       .serializer(serializer)\n+      .flushFrequencySeconds(-1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjAwNDc4Mg==", "bodyText": "Can you add a comment somewhere explaining how this estimation method works?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426004782", "createdAt": "2020-05-15T19:27:05Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/MsgPackStatefulSerializer.java", "diffHunk": "@@ -37,99 +43,193 @@\n \n   // reusing this within the context of each thread is handy because it\n   // caches an Encoder\n-  private final MessagePacker messagePacker =\n-      MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n+  private final MessagePacker messagePacker;\n \n   private final int[] traceSizes = new int[TRACE_HISTORY_SIZE];\n+  private final int sizeThresholdBytes;\n+  private final int bufferSize;\n \n   private int traceSizeSum;\n   private int position;\n-  private MsgPackTraceBuffer lastBuffer;\n+  private MsgPackTraceBuffer traceBuffer;\n \n-  private int lastTracesPerBuffer = 1;\n-  private int tracesPerBuffer;\n+  private int currentSerializedBytes = 0;\n \n   public MsgPackStatefulSerializer() {\n-    Arrays.fill(traceSizes, INITIAL_BUFFER_SIZE);\n-    this.traceSizeSum = INITIAL_BUFFER_SIZE * TRACE_HISTORY_SIZE;\n+    this(DEFAULT_BUFFER_THRESHOLD, DEFAULT_BUFFER_THRESHOLD * 3 / 2); // 1MB\n+  }\n+\n+  public MsgPackStatefulSerializer(int sizeThresholdBytes, int bufferSize) {\n+    Arrays.fill(traceSizes, INITIAL_TRACE_SIZE_ESTIMATE);\n+    this.traceSizeSum = INITIAL_TRACE_SIZE_ESTIMATE * TRACE_HISTORY_SIZE;\n+    this.sizeThresholdBytes = sizeThresholdBytes;\n+    this.bufferSize = bufferSize;\n+    this.messagePacker = MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n   }\n \n   @Override\n-  public void serialize(List<DDSpan> trace) throws IOException {\n-    if (null == lastBuffer) {\n-      lastBuffer = newBuffer();\n-      messagePacker.reset(lastBuffer.buffer);\n-      messagePacker.clear();\n-    }\n+  public int serialize(List<DDSpan> trace) throws IOException {\n     MSGPACK_WRITER.writeTrace(trace, messagePacker);\n-    int serializedSize = (int) messagePacker.getTotalWrittenBytes();\n-    updateTraceSize(serializedSize);\n-    lastBuffer.length += serializedSize;\n-    ++lastBuffer.traceCount;\n-    ++tracesPerBuffer;\n+    int newSerializedSize = (int) messagePacker.getTotalWrittenBytes();\n+    int serializedSize = newSerializedSize - currentSerializedBytes;\n+    currentSerializedBytes = newSerializedSize;\n+    updateTraceSizeEstimate(serializedSize);\n+    ++traceBuffer.traceCount;\n+    traceBuffer.length = newSerializedSize;\n+    return serializedSize;\n+  }\n+\n+  @Override\n+  public void dropBuffer() throws IOException {\n+    messagePacker.flush();\n+    traceBuffer = null;\n+  }\n+\n+  @Override\n+  public boolean isAtCapacity() {\n+    // Return true if could not take another average trace without allocating.\n+    // There are many cases where this will lead to some amount of over allocation,\n+    // e.g. a very large trace after many very small traces, but it's a best effort\n+    // strategy to avoid buffer growth eventually.\n+    return currentSerializedBytes + avgTraceSize() >= sizeThresholdBytes;\n   }\n \n   @Override\n-  public TraceBuffer getBuffer() throws IOException {\n+  public void reset(TraceBuffer buffer) {\n+    if (buffer instanceof MsgPackTraceBuffer) {\n+      this.traceBuffer = (MsgPackTraceBuffer) buffer;\n+      this.traceBuffer.reset();\n+    } else { // i.e. if (null == buffer || unuseable)\n+      this.traceBuffer = newBuffer();\n+    }\n+    // reset the packer's position to zero\n+    messagePacker.clear();\n     try {\n-      messagePacker.flush();\n-      return lastBuffer;\n-    } finally {\n-      lastTracesPerBuffer = tracesPerBuffer;\n-      tracesPerBuffer = 0;\n-      lastBuffer = null;\n+      messagePacker.reset(traceBuffer.buffer);\n+    } catch (IOException e) { // don't expect this to happen\n+      log.error(\"Unexpected exception resetting MessagePacker buffer\", e);\n     }\n   }\n \n-  private MsgPackTraceBuffer newBuffer() {\n-    MsgPackTraceBuffer buffer = new MsgPackTraceBuffer();\n-    int traceBufferSize = traceBufferSize();\n-    buffer.buffer = new ArrayBufferOutput(traceBufferSize);\n-    return buffer;\n+  @Override\n+  public MsgPackTraceBuffer newBuffer() {\n+    return new MsgPackTraceBuffer(new ArrayBufferOutput(bufferSize));\n   }\n \n-  private void updateTraceSize(int traceSize) {\n+  private void updateTraceSizeEstimate(int traceSize) {\n     traceSizeSum = (traceSizeSum - traceSizes[position] + traceSize);\n     traceSizes[position] = traceSize;\n     position = (position + 1) & (traceSizes.length - 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "originalPosition": 142}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMDk1MDQ4", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-413095048", "createdAt": "2020-05-16T17:44:02Z", "commit": {"oid": "336bec7e307043d5abfe428d69b48c4f1cba1ce3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "749a8833bcfe5a0e48547801924e25b1eb8175cf", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/749a8833bcfe5a0e48547801924e25b1eb8175cf", "committedDate": "2020-05-18T11:29:27Z", "message": "move batching to trace processor to ease memory buffer reuse\n\n Date:      Wed May 13 11:00:00 2020 +0100"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c2adca9b3842c4a5904a52c0f65b549e2290831", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3c2adca9b3842c4a5904a52c0f65b549e2290831", "committedDate": "2020-05-18T11:30:25Z", "message": "make revapi happy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03a8c83bf0daf377859bf7112a584db891fedc65", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/03a8c83bf0daf377859bf7112a584db891fedc65", "committedDate": "2020-05-18T11:30:35Z", "message": "make verifyGoogleJavaFormat happy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "92aa000eb91a5beb0a5bf10efe903aa99ab86de0", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/92aa000eb91a5beb0a5bf10efe903aa99ab86de0", "committedDate": "2020-05-18T11:30:35Z", "message": "review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8d28480c7eacd11d35e5ce3c15a6128fb94a13a", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/e8d28480c7eacd11d35e5ce3c15a6128fb94a13a", "committedDate": "2020-05-18T11:30:35Z", "message": "restore heartbeat semantics (don't force a flush unless the time threshold has been breached)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5209f82714f66f1f91889c7f05032c295900f07", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/a5209f82714f66f1f91889c7f05032c295900f07", "committedDate": "2020-05-18T11:30:35Z", "message": "document the trace size moving average calculation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3d4b102be3893746448bcbd7c6e4e6ae468f4178", "committedDate": "2020-05-18T11:35:17Z", "message": "rebase revapi breaks"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4d95c36e738ac59d1bb353608f92b5c788e85d3b", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/4d95c36e738ac59d1bb353608f92b5c788e85d3b", "committedDate": "2020-05-18T11:09:56Z", "message": "document the trace size moving average calculation"}, "afterCommit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3d4b102be3893746448bcbd7c6e4e6ae468f4178", "committedDate": "2020-05-18T11:35:17Z", "message": "rebase revapi breaks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzNjEwNTI4", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-413610528", "createdAt": "2020-05-18T13:47:35Z", "commit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzNjc5OTI1", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-413679925", "createdAt": "2020-05-18T15:01:46Z", "commit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNTowMTo0NlrOGW7M5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOFQxNjozMDoyOVrOGW-5vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MTgxNQ==", "bodyText": "I wonder if we should limit this to 4 instead... 8 fully serialized buffers seems like a significant backlog.", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426691815", "createdAt": "2020-05-18T15:01:46Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/DDAgentWriter.java", "diffHunk": "@@ -34,12 +37,14 @@\n public class DDAgentWriter implements Writer {\n \n   private static final int DISRUPTOR_BUFFER_SIZE = 1024;\n+  private static final int OUTSTANDING_REQUESTS = 8;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MzMwNA==", "bodyText": "Do we want to check the closed state at all in here?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426693304", "createdAt": "2020-05-18T15:03:53Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/DDAgentWriter.java", "diffHunk": "@@ -166,20 +183,21 @@ public DDAgentApi getApi() {\n \n   @Override\n   public void start() {\n-    batchWritingDisruptor.start();\n+    dispatchingDisruptor.start();\n     traceProcessingDisruptor.start();\n     monitor.onStart(this);\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTY4OQ==", "bodyText": "Would it be better to shift the heartbeat logic into the constructor instead of exposing as a parameter?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426699689", "createdAt": "2020-05-18T15:12:42Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,234 @@\n package datadog.trace.common.writer.ddagent;\n \n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+\n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator<>();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc0NjU2OQ==", "bodyText": "Should this be called in the constructor instead?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426746569", "createdAt": "2020-05-18T16:21:01Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/TraceProcessingDisruptor.java", "diffHunk": "@@ -1,103 +1,234 @@\n package datadog.trace.common.writer.ddagent;\n \n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+\n+import com.lmax.disruptor.BlockingWaitStrategy;\n import com.lmax.disruptor.EventHandler;\n+import com.lmax.disruptor.dsl.Disruptor;\n+import com.lmax.disruptor.dsl.ProducerType;\n+import datadog.common.exec.CommonTaskExecutor;\n import datadog.common.exec.DaemonThreadFactory;\n import datadog.trace.common.writer.DDAgentWriter;\n import datadog.trace.core.DDSpan;\n import datadog.trace.core.processor.TraceProcessor;\n+import java.io.IOException;\n import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ScheduledFuture;\n+import java.util.concurrent.TimeUnit;\n import lombok.extern.slf4j.Slf4j;\n \n /**\n  * Disruptor that takes completed traces and applies processing to them. Upon completion, the\n- * serialized trace is published to {@link BatchWritingDisruptor}.\n+ * serialized trace is published to {@link DispatchingDisruptor}.\n  *\n  * <p>publishing to the buffer will not block the calling thread, but instead will return false if\n  * the buffer is full. This is to avoid impacting an application thread.\n  */\n @Slf4j\n-public class TraceProcessingDisruptor extends AbstractDisruptor<List<DDSpan>> {\n+public class TraceProcessingDisruptor implements AutoCloseable {\n+\n+  private final Disruptor<DisruptorEvent<List<DDSpan>>> disruptor;\n+  private final DisruptorEvent.DataTranslator<List<DDSpan>> dataTranslator;\n+  private final DisruptorEvent.FlushTranslator<List<DDSpan>> flushTranslator;\n+  private final DisruptorEvent.HeartbeatTranslator<List<DDSpan>> heartbeatTranslator =\n+      new DisruptorEvent.HeartbeatTranslator<>();\n+  private final boolean doHeartbeat;\n+\n+  private volatile ScheduledFuture<?> heartbeat;\n \n   public TraceProcessingDisruptor(\n       final int disruptorSize,\n-      final BatchWritingDisruptor batchWritingDisruptor,\n+      final DispatchingDisruptor dispatchingDisruptor,\n       final Monitor monitor,\n       final DDAgentWriter writer,\n-      final StatefulSerializer serializer) {\n-    // TODO: add config to enable control over serialization overhead.\n-    super(\n-        disruptorSize,\n-        new TraceSerializingHandler(batchWritingDisruptor, monitor, writer, serializer));\n+      final StatefulSerializer serializer,\n+      final long flushInterval,\n+      final TimeUnit timeUnit,\n+      final boolean heartbeat) {\n+    this.disruptor =\n+        DisruptorUtils.create(\n+            new DisruptorEvent.Factory<List<DDSpan>>(),\n+            disruptorSize,\n+            DaemonThreadFactory.TRACE_PROCESSOR,\n+            ProducerType.MULTI,\n+            new BlockingWaitStrategy());\n+    disruptor.handleEventsWith(\n+        new TraceSerializingHandler(\n+            dispatchingDisruptor, monitor, writer, serializer, flushInterval, timeUnit));\n+    this.dataTranslator = new DisruptorEvent.DataTranslator<>();\n+    this.flushTranslator = new DisruptorEvent.FlushTranslator<>();\n+    this.doHeartbeat = heartbeat;\n   }\n \n-  @Override\n-  protected DaemonThreadFactory getThreadFactory() {\n-    return DaemonThreadFactory.TRACE_PROCESSOR;\n+  public void start() {\n+    if (doHeartbeat) {\n+      // This provides a steady stream of events to enable flushing with a low throughput.\n+      heartbeat =\n+          CommonTaskExecutor.INSTANCE.scheduleAtFixedRate(\n+              new HeartbeatTask(), this, 100, 100, MILLISECONDS, \"disruptor heartbeat\");\n+    }\n+    disruptor.start();\n+  }\n+\n+  public boolean flush(long timeout, TimeUnit timeUnit) {\n+    CountDownLatch latch = new CountDownLatch(1);\n+    disruptor.publishEvent(flushTranslator, 0, latch);\n+    try {\n+      return latch.await(timeout, timeUnit);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      return false;\n+    }\n   }\n \n   @Override\n+  public void close() {\n+    if (null != heartbeat) {\n+      heartbeat.cancel(true);\n+    }\n+    disruptor.halt();\n+  }\n+\n   public boolean publish(final List<DDSpan> data, final int representativeCount) {\n     return disruptor.getRingBuffer().tryPublishEvent(dataTranslator, data, representativeCount);\n   }\n \n-  // This class is threadsafe if we want to enable more processors.\n+  void heartbeat() {\n+    disruptor.getRingBuffer().tryPublishEvent(heartbeatTranslator);\n+  }\n+\n+  public int getDisruptorCapacity() {\n+    return disruptor.getRingBuffer().getBufferSize();\n+  }\n+\n+  public long getDisruptorRemainingCapacity() {\n+    return disruptor.getRingBuffer().remainingCapacity();\n+  }\n+\n   public static class TraceSerializingHandler\n       implements EventHandler<DisruptorEvent<List<DDSpan>>> {\n     private final TraceProcessor processor = new TraceProcessor();\n-    private final BatchWritingDisruptor batchWritingDisruptor;\n+    private final DispatchingDisruptor dispatchingDisruptor;\n     private final Monitor monitor;\n     private final DDAgentWriter writer;\n     private final StatefulSerializer serializer;\n+    private final long flushIntervalMillis;\n+    private final boolean doTimeFlush;\n+\n+    private long publicationTxn = -1;\n+    private int representativeCount = 0;\n+    private long nextFlushMillis;\n \n     public TraceSerializingHandler(\n-        final BatchWritingDisruptor batchWritingDisruptor,\n+        final DispatchingDisruptor dispatchingDisruptor,\n         final Monitor monitor,\n         final DDAgentWriter writer,\n-        final StatefulSerializer serializer) {\n-      this.batchWritingDisruptor = batchWritingDisruptor;\n+        final StatefulSerializer serializer,\n+        final long flushInterval,\n+        final TimeUnit timeUnit) {\n+      this.dispatchingDisruptor = dispatchingDisruptor;\n       this.monitor = monitor;\n       this.writer = writer;\n       this.serializer = serializer;\n+      this.doTimeFlush = flushInterval > 0;\n+      if (doTimeFlush) {\n+        this.flushIntervalMillis = timeUnit.toMillis(flushInterval);\n+        scheduleNextTimeFlush();\n+      } else {\n+        this.flushIntervalMillis = Long.MAX_VALUE;\n+      }\n     }\n \n     @Override\n     public void onEvent(\n         final DisruptorEvent<List<DDSpan>> event, final long sequence, final boolean endOfBatch) {\n+      if (-1L == publicationTxn) {\n+        beginTransaction();\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc1MjQ0Nw==", "bodyText": "Should this class be made generic instead to ensure the proper argument type?", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#discussion_r426752447", "createdAt": "2020-05-18T16:30:29Z", "author": {"login": "tylerbenson"}, "path": "dd-trace-core/src/main/java/datadog/trace/common/writer/ddagent/MsgPackStatefulSerializer.java", "diffHunk": "@@ -37,99 +43,205 @@\n \n   // reusing this within the context of each thread is handy because it\n   // caches an Encoder\n-  private final MessagePacker messagePacker =\n-      MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n+  private final MessagePacker messagePacker;\n \n-  private final int[] traceSizes = new int[TRACE_HISTORY_SIZE];\n+  private final int[] traceSizeHistory = new int[TRACE_HISTORY_SIZE];\n+  private final int sizeThresholdBytes;\n+  private final int bufferSize;\n \n-  private int traceSizeSum;\n+  private int runningTraceSizeSum;\n   private int position;\n-  private MsgPackTraceBuffer lastBuffer;\n+  private MsgPackTraceBuffer traceBuffer;\n \n-  private int lastTracesPerBuffer = 1;\n-  private int tracesPerBuffer;\n+  private int currentSerializedBytes = 0;\n \n   public MsgPackStatefulSerializer() {\n-    Arrays.fill(traceSizes, INITIAL_BUFFER_SIZE);\n-    this.traceSizeSum = INITIAL_BUFFER_SIZE * TRACE_HISTORY_SIZE;\n+    this(DEFAULT_BUFFER_THRESHOLD, DEFAULT_BUFFER_THRESHOLD * 3 / 2); // 1MB\n+  }\n+\n+  public MsgPackStatefulSerializer(int sizeThresholdBytes, int bufferSize) {\n+    Arrays.fill(traceSizeHistory, INITIAL_TRACE_SIZE_ESTIMATE);\n+    this.runningTraceSizeSum = INITIAL_TRACE_SIZE_ESTIMATE * TRACE_HISTORY_SIZE;\n+    this.sizeThresholdBytes = sizeThresholdBytes;\n+    this.bufferSize = bufferSize;\n+    this.messagePacker = MESSAGE_PACKER_CONFIG.newPacker(new ArrayBufferOutput(0));\n   }\n \n   @Override\n-  public void serialize(List<DDSpan> trace) throws IOException {\n-    if (null == lastBuffer) {\n-      lastBuffer = newBuffer();\n-      messagePacker.reset(lastBuffer.buffer);\n-      messagePacker.clear();\n-    }\n+  public int serialize(List<DDSpan> trace) throws IOException {\n     MSGPACK_WRITER.writeTrace(trace, messagePacker);\n-    int serializedSize = (int) messagePacker.getTotalWrittenBytes();\n-    updateTraceSize(serializedSize);\n-    lastBuffer.length += serializedSize;\n-    ++lastBuffer.traceCount;\n-    ++tracesPerBuffer;\n+    int newSerializedSize = (int) messagePacker.getTotalWrittenBytes();\n+    int serializedSize = newSerializedSize - currentSerializedBytes;\n+    currentSerializedBytes = newSerializedSize;\n+    updateTraceSizeEstimate(serializedSize);\n+    ++traceBuffer.traceCount;\n+    traceBuffer.length = newSerializedSize;\n+    return serializedSize;\n+  }\n+\n+  @Override\n+  public void dropBuffer() throws IOException {\n+    messagePacker.flush();\n+    traceBuffer = null;\n+  }\n+\n+  @Override\n+  public boolean isAtCapacity() {\n+    // Return true if could not take another average trace without allocating.\n+    // There are many cases where this will lead to some amount of over allocation,\n+    // e.g. a very large trace after many very small traces, but it's a best effort\n+    // strategy to avoid buffer growth eventually.\n+    return currentSerializedBytes + avgTraceSize() >= sizeThresholdBytes;\n   }\n \n   @Override\n-  public TraceBuffer getBuffer() throws IOException {\n+  public void reset(TraceBuffer buffer) {\n+    if (buffer instanceof MsgPackTraceBuffer) {\n+      this.traceBuffer = (MsgPackTraceBuffer) buffer;\n+      this.traceBuffer.reset();\n+    } else { // i.e. if (null == buffer || unuseable)\n+      this.traceBuffer = newBuffer();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d4b102be3893746448bcbd7c6e4e6ae468f4178"}, "originalPosition": 114}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3f8b6ec05ec24e43edc953b522286fca3e1e065", "author": {"user": {"login": "richardstartin", "name": "Richard Startin"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/c3f8b6ec05ec24e43edc953b522286fca3e1e065", "committedDate": "2020-05-18T17:08:56Z", "message": "review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzODE0MzMw", "url": "https://github.com/DataDog/dd-trace-java/pull/1454#pullrequestreview-413814330", "createdAt": "2020-05-18T17:53:00Z", "commit": {"oid": "c3f8b6ec05ec24e43edc953b522286fca3e1e065"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2495, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}