{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQwOTc2OTE4", "number": 2209, "title": "Try reducing allocation in stream processing", "bodyText": "This change is introducing a RequestBody subclass which can do direct stream-to-stream compression without going through the intermediary state of reading in all data into a byte array. This should lead to decreased allocation pressure coming from the profile uploads.\nThanks to handling the compression in Okio specific manner the generic StreamUtils class which was providing compression helper methods was removed.\nBecause the upload data size is only known after the input stream has been fully compressed it is not possible to send it in the header and/or log before starting the upload request. Instead, the request is using chunked transfer with the request size set to -1 (== unknown) and the request debug logging is done in the request callback once the request has finished/failed.", "createdAt": "2020-12-16T08:10:52Z", "url": "https://github.com/DataDog/dd-trace-java/pull/2209", "merged": true, "mergeCommit": {"oid": "a8647bb2d605b6eb78c056e49641bc11908c73c6"}, "closed": true, "closedAt": "2021-03-16T10:40:06Z", "author": {"login": "jbachorik"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABeCek2FAH2gAyNTQwOTc2OTE4OjNiMDI5YTI5OTk4NjkzZDg1YTYxY2M5MTA0OThiNDYzY2YzOTkxNTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABeDqQjJgFqTYxMzA3MjY4OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3b029a29998693d85a61cc910498b463cf399159", "author": {"user": {"login": "jbachorik", "name": "Jaroslav Bachorik"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3b029a29998693d85a61cc910498b463cf399159", "committedDate": "2021-03-12T18:13:38Z", "message": "Try reducing allocation in stream processing"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "64e531dc37feb90c3a982287b05d01bae07a2887", "author": {"user": {"login": "jbachorik", "name": "Jaroslav Bachorik"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/64e531dc37feb90c3a982287b05d01bae07a2887", "committedDate": "2020-12-16T08:05:00Z", "message": "Spotless!"}, "afterCommit": {"oid": "3b029a29998693d85a61cc910498b463cf399159", "author": {"user": {"login": "jbachorik", "name": "Jaroslav Bachorik"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/3b029a29998693d85a61cc910498b463cf399159", "committedDate": "2021-03-12T18:13:38Z", "message": "Try reducing allocation in stream processing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a79ba0c8fdf348cd627aa462d7337e44074b446d", "author": {"user": {"login": "jbachorik", "name": "Jaroslav Bachorik"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/a79ba0c8fdf348cd627aa462d7337e44074b446d", "committedDate": "2021-03-15T11:50:46Z", "message": "Docs cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6ee868ae29772ce4e5f3eb4e068609b8b6239a2", "author": {"user": {"login": "jbachorik", "name": "Jaroslav Bachorik"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/f6ee868ae29772ce4e5f3eb4e068609b8b6239a2", "committedDate": "2021-03-15T14:56:47Z", "message": "Track and log the read/written bytes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEyMzgyNTA1", "url": "https://github.com/DataDog/dd-trace-java/pull/2209#pullrequestreview-612382505", "createdAt": "2021-03-15T16:14:12Z", "commit": {"oid": "f6ee868ae29772ce4e5f3eb4e068609b8b6239a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQxNjoxNDoxMlrOI28E6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQxNjoxNDoxMlrOI28E6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDQ3ODMxMg==", "bodyText": "try-with-resources?", "url": "https://github.com/DataDog/dd-trace-java/pull/2209#discussion_r594478312", "createdAt": "2021-03-15T16:14:12Z", "author": {"login": "jpbempel"}, "path": "dd-java-agent/agent-profiling/profiling-uploader/src/main/java/com/datadog/profiling/uploader/CompressingRequestBody.java", "diffHunk": "@@ -0,0 +1,340 @@\n+package com.datadog.profiling.uploader;\n+\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.zip.GZIPOutputStream;\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import net.jpountz.lz4.LZ4FrameOutputStream;\n+import okhttp3.MediaType;\n+import okhttp3.RequestBody;\n+import okio.BufferedSink;\n+import okio.Okio;\n+import okio.Source;\n+import org.openjdk.jmc.common.io.IOToolkit;\n+\n+/**\n+ * A specialized {@linkplain RequestBody} subclass performing on-the fly compression of the uploaded\n+ * data.\n+ */\n+final class CompressingRequestBody extends RequestBody {\n+  /** A simple functional supplier throwing an {@linkplain IOException} */\n+  @FunctionalInterface\n+  interface InputStreamSupplier {\n+    InputStream get() throws IOException;\n+  }\n+\n+  /** A simple functional mapper allowing to throw {@linkplain IOException} */\n+  @FunctionalInterface\n+  interface OutputStreamMappingFunction {\n+    OutputStream apply(OutputStream param) throws IOException;\n+  }\n+\n+  /**\n+   * A data upload retry policy. By using this policy it is possible to customize how many times the\n+   * data upload will be reattempted if the input data stream is unavailable.\n+   */\n+  @FunctionalInterface\n+  interface RetryPolicy {\n+    /**\n+     * @param ordinal number of data upload attempts so far\n+     * @return {@literal true} if the data upload should be retried\n+     */\n+    boolean shouldRetry(int ordinal);\n+  }\n+\n+  /**\n+   * A data upload retry backoff policy. This policy will be used to obtain the delay before the\n+   * next retry.\n+   */\n+  @FunctionalInterface\n+  interface RetryBackoff {\n+    /**\n+     * @param ordinal number of data upload attempts so far\n+     * @return the required delay in milliscenods before next retry\n+     */\n+    int backoff(int ordinal);\n+  }\n+\n+  static final MediaType OCTET_STREAM = MediaType.parse(\"application/octet-stream\");\n+\n+  // https://github.com/lz4/lz4/blob/dev/doc/lz4_Frame_format.md#general-structure-of-lz4-frame-format\n+  private static final int[] LZ4_MAGIC = new int[] {0x04, 0x22, 0x4D, 0x18};\n+\n+  // JMC's IOToolkit hides this from us...\n+  private static final int ZIP_MAGIC[] = new int[] {80, 75, 3, 4};\n+  private static final int GZ_MAGIC[] = new int[] {31, 139};\n+\n+  private final InputStreamSupplier inputStreamSupplier;\n+  private final OutputStreamMappingFunction outputStreamMapper;\n+  private final RetryPolicy retryPolicy;\n+  private final RetryBackoff retryBackoff;\n+\n+  private long readBytes = 0;\n+  private long writtenBytes = 0;\n+\n+  /**\n+   * Create a new instance configured with 1 retry and constant 10ms backoff delay.\n+   *\n+   * @param compressionType {@linkplain CompressionType} value\n+   * @param inputStreamSupplier supplier of the data input stream\n+   */\n+  CompressingRequestBody(\n+      @Nonnull CompressionType compressionType, @Nonnull InputStreamSupplier inputStreamSupplier) {\n+    this(compressionType, inputStreamSupplier, r -> r <= 1, r -> 10);\n+  }\n+\n+  /**\n+   * Create a new instance configured with constant 10ms backoff delay.\n+   *\n+   * @param compressionType {@linkplain CompressionType} value\n+   * @param inputStreamSupplier supplier of the data input stream\n+   * @param retryPolicy {@linkplain RetryPolicy} instance\n+   */\n+  CompressingRequestBody(\n+      @Nonnull CompressionType compressionType,\n+      @Nonnull InputStreamSupplier inputStreamSupplier,\n+      @Nonnull RetryPolicy retryPolicy) {\n+    this(compressionType, inputStreamSupplier, retryPolicy, r -> 10);\n+  }\n+\n+  /**\n+   * Create a new instance.\n+   *\n+   * @param compressionType {@linkplain CompressionType} value\n+   * @param inputStreamSupplier supplier of the data input stream\n+   * @param retryPolicy {@linkplain RetryPolicy} instance\n+   * @param retryBackoff {@linkplain RetryBackoff} instance\n+   */\n+  CompressingRequestBody(\n+      @Nonnull CompressionType compressionType,\n+      @Nonnull InputStreamSupplier inputStreamSupplier,\n+      @Nonnull RetryPolicy retryPolicy,\n+      @Nonnull RetryBackoff retryBackoff) {\n+    this.inputStreamSupplier = () -> ensureMarkSupported(inputStreamSupplier.get());\n+    this.outputStreamMapper = getOutputStreamMapper(compressionType);\n+    this.retryPolicy = retryPolicy;\n+    this.retryBackoff = retryBackoff;\n+  }\n+\n+  @Override\n+  public long contentLength() throws IOException {\n+    // uploading chunked streaming data -> the length is unknown\n+    return -1;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public MediaType contentType() {\n+    return OCTET_STREAM;\n+  }\n+\n+  @Override\n+  public void writeTo(BufferedSink bufferedSink) throws IOException {\n+    Throwable lastException = null;\n+    boolean shouldRetry = false;\n+    int retry = 1;\n+    do {\n+      /*\n+       * Here we do attempt a simple retry functionality in case the input stream can not be obtained -\n+       * which is usually caused by the JFR recording being not finalized on disk in our case.\n+       * The number of times this should be re-attempted as well as the backoff between the attempts\n+       * can be defined per CompressingRequestBody instance.\n+       *\n+       * However, the failures in reading the input stream and writing the data out to the Okio sink\n+       * will not be retried because doing so can result in corrupted data uploads. Instead, the client\n+       * should use the OkHttpClient callback to get notified about failed requests and handle the retries\n+       * at the request level.\n+       */\n+      try (ByteCountingInputStream inputStream =\n+          new ByteCountingInputStream(inputStreamSupplier.get())) {\n+        // Got the input stream so clear the 'lastException'\n+        lastException = null;\n+        try {\n+          ByteCountingOutputStream outputStream =\n+              new ByteCountingOutputStream(bufferedSink.outputStream());\n+          attemptWrite(inputStream, outputStream);\n+          readBytes = inputStream.getReadBytes();\n+          writtenBytes = outputStream.getWrittenBytes();\n+        } catch (Throwable t) {\n+          // Only the failures while obtaining the input stream are retriable.\n+          // Any failure during reading that input stream must make this write to fail as well.\n+          lastException = t;\n+          shouldRetry = false;\n+        }\n+      } catch (Throwable t) {\n+        // Only the failures while obtaining the input stream are retriable.\n+        lastException = t;\n+        shouldRetry = true;\n+      }\n+      if (shouldRetry && retryPolicy.shouldRetry(retry)) {\n+        long backoffMs = retryBackoff.backoff(retry);\n+        try {\n+          Thread.sleep(backoffMs);\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+          throw new IOException(e);\n+        }\n+        retry++;\n+      } else {\n+        shouldRetry = false;\n+      }\n+    } while (shouldRetry);\n+    if (lastException != null) {\n+      throw lastException instanceof IOException\n+          ? (IOException) lastException\n+          : new IOException(lastException);\n+    }\n+  }\n+\n+  long getReadBytes() {\n+    return readBytes;\n+  }\n+\n+  long getWrittenBytes() {\n+    return writtenBytes;\n+  }\n+\n+  private void attemptWrite(@Nonnull InputStream inputStream, @Nonnull OutputStream outputStream)\n+      throws IOException {\n+    OutputStream sinkStream =\n+        isCompressed(inputStream)\n+            ? outputStream\n+            : new BufferedOutputStream(\n+                outputStreamMapper.apply(\n+                    new BufferedOutputStream(outputStream) {\n+                      @Override\n+                      public void close() throws IOException {\n+                        // Do not propagate close; call 'flush()' instead.\n+                        // Compression streams must be 'closed' because they finalize the\n+                        // compression\n+                        // in that method.\n+                        flush();\n+                      }\n+                    }));\n+    BufferedSink sink = Okio.buffer(Okio.sink(sinkStream));\n+    try (Source source = Okio.buffer(Okio.source(inputStream))) {\n+      sink.writeAll(source);\n+    }\n+    // a bit of cargo-culting to make sure that all writes have really-really been flushed\n+    sink.emit();\n+    sink.flush();\n+    sinkStream.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6ee868ae29772ce4e5f3eb4e068609b8b6239a2"}, "originalPosition": 225}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe007159518919347e2871197f8c582e999825bc", "author": {"user": {"login": "jbachorik", "name": "Jaroslav Bachorik"}}, "url": "https://github.com/DataDog/dd-trace-java/commit/fe007159518919347e2871197f8c582e999825bc", "committedDate": "2021-03-15T17:15:17Z", "message": "Try-with-resources"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEyNDcwMTkw", "url": "https://github.com/DataDog/dd-trace-java/pull/2209#pullrequestreview-612470190", "createdAt": "2021-03-15T17:36:12Z", "commit": {"oid": "fe007159518919347e2871197f8c582e999825bc"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjEzMDcyNjg4", "url": "https://github.com/DataDog/dd-trace-java/pull/2209#pullrequestreview-613072688", "createdAt": "2021-03-16T10:24:15Z", "commit": {"oid": "fe007159518919347e2871197f8c582e999825bc"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2768, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}