{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxNDYwOTIz", "number": 1906, "title": "Spring batch integration", "bodyText": "Initial pass of Spring batch integration\nHighlights:\n\nnew project, hapi-fhir-jpaserver-batch\nResourceless batch job running, all jobs are held in memory. This means there is no durability by default. This can be overridden by providing a data source/platform transaction manager.\nIBatchJobSubmitteradded, which accepts Job definitions for running in batch. This is the point at which other modules can interface with batch.\n\nFirst converted task: Bulk Export.\nBulk export has been converted from a single-threaded in-line job, to a spring-batch style Job, which makes use of the Read/Process/Write methodology. One step is started for each resource requested in the Job, providing a modest parallelization.\nHere's a rough breakdown of the components\n\nJobExistsParameterValidator: ensures the job UUID is valid before starting the job\nResourceTypePartitioner : Breaks the task into partitions, each defined by a single resource type.\nBulkItemReader : Given the resource type, load all the pids of that resource type, and feed them 10 at a time to the processor\nPidToResourceProcessor: Given a list of pids, convert it to a list of IBaseResources\nResourceToFileWriter: Given a list of IBaseResources, write them to a binary object and attach it to the job entity.", "createdAt": "2020-06-09T00:11:48Z", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906", "merged": true, "mergeCommit": {"oid": "e365e643baa48290fc0c0478d45b2e80c84d80a2"}, "closed": true, "closedAt": "2020-06-28T23:10:44Z", "author": {"login": "tadgh"}, "timelineItems": {"totalCount": 70, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABck93VIAH2gAyNDMxNDYwOTIzOjM1YjE1MzNkYjFhYTZiM2QyOTM3MzZkYmU0MWJmMjBmYmY4YTBmYTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcvfRbcgH2gAyNDMxNDYwOTIzOmJlMjNlYjNjZmM1OGY3MmRmMTgyMmVlNWZlMDBmOWFmZmEzNDhjZDc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "35b1533db1aa6b3d293736dbe41bf20fbf8a0fa2", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/35b1533db1aa6b3d293736dbe41bf20fbf8a0fa2", "committedDate": "2020-05-26T05:32:00Z", "message": "Add new project for spring batch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48fcf8a39da192883193d8423504c4b6060b13f1", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/48fcf8a39da192883193d8423504c4b6060b13f1", "committedDate": "2020-05-26T19:08:00Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81d5ff8f5a0722ec2468da13116d04f2e1142608", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/81d5ff8f5a0722ec2468da13116d04f2e1142608", "committedDate": "2020-05-28T00:06:52Z", "message": "Add schema initialization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a30682562552871603975689ef149606ba326bca", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/a30682562552871603975689ef149606ba326bca", "committedDate": "2020-05-28T19:10:52Z", "message": "moving dependencies around. Get non-persisted batch working"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa7d1cbcb7f9454173ebf6df7f53b95d5f34adeb", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/aa7d1cbcb7f9454173ebf6df7f53b95d5f34adeb", "committedDate": "2020-06-01T18:32:36Z", "message": "Wip getting tasks built"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fc2e04e650f9ee850767716c1c42da2498918d6", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/0fc2e04e650f9ee850767716c1c42da2498918d6", "committedDate": "2020-06-01T19:13:55Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8d699e13b07756079eb93bfb34a34d1a04af4b6", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/f8d699e13b07756079eb93bfb34a34d1a04af4b6", "committedDate": "2020-06-01T22:51:09Z", "message": "Initial working commit passing jobs to spring batch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb863f26fab72566042c0e446abc484d1e2ae546", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/bb863f26fab72566042c0e446abc484d1e2ae546", "committedDate": "2020-06-02T20:44:23Z", "message": "Initial run through the first part of batch integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31b7e862d43728f0553ee946df2f648381bd83fa", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/31b7e862d43728f0553ee946df2f648381bd83fa", "committedDate": "2020-06-02T22:00:56Z", "message": "Remove extra beans"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8970bce465603e1a1fe641875c03d9d2a98bc8b", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/e8970bce465603e1a1fe641875c03d9d2a98bc8b", "committedDate": "2020-06-02T23:08:19Z", "message": "Running batch job"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d46b3cc10122f4d2db2d6a349c70198f941719c", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/7d46b3cc10122f4d2db2d6a349c70198f941719c", "committedDate": "2020-06-03T21:58:42Z", "message": "Partitioning Based on resource type complete. Still no threading"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7ed65b0080bed4d498b99b0757b819257110959d", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/7ed65b0080bed4d498b99b0757b819257110959d", "committedDate": "2020-06-05T03:58:06Z", "message": "partially done writer, working out size-based chunking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4405e50db9e8bb17f2ef87ee44dd969857bced29", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/4405e50db9e8bb17f2ef87ee44dd969857bced29", "committedDate": "2020-06-05T22:53:06Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44aa688a20cd216560c164a32c6c81962da6be47", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/44aa688a20cd216560c164a32c6c81962da6be47", "committedDate": "2020-06-08T22:59:08Z", "message": "Final config layout of Bulk Export Batch job"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "864c9c2cb81ec736aa334f353bcca519c7dc81e4", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/864c9c2cb81ec736aa334f353bcca519c7dc81e4", "committedDate": "2020-06-08T23:35:15Z", "message": "Add task executor to partition step"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25fa0377df579fa64163474ca657fa59accc3cd0", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/25fa0377df579fa64163474ca657fa59accc3cd0", "committedDate": "2020-06-09T00:02:37Z", "message": "tidying"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "278a1070aeac0d82175c629a08b93a272a36c611", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/278a1070aeac0d82175c629a08b93a272a36c611", "committedDate": "2020-06-09T00:16:11Z", "message": ":Merge branch 'master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b995f8be7700da93de43631308e6602f5d9a202", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/8b995f8be7700da93de43631308e6602f5d9a202", "committedDate": "2020-06-09T00:16:22Z", "message": "Remove dead test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c153527dd862d64bc4d363379df894b5d80596f", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/9c153527dd862d64bc4d363379df894b5d80596f", "committedDate": "2020-06-09T16:03:56Z", "message": "remove dead imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7273457ea76aa4ee482c126d2408721567aa7ae6", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/7273457ea76aa4ee482c126d2408721567aa7ae6", "committedDate": "2020-06-09T16:55:10Z", "message": "Rework to read and process larger chunks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4f59c38692fd9650c196150a035917d702e9c0c8", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/4f59c38692fd9650c196150a035917d702e9c0c8", "committedDate": "2020-06-09T17:20:41Z", "message": "Remove comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a25a1064f843a13e0d1c7c07d779c688ceb1942f", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/a25a1064f843a13e0d1c7c07d779c688ceb1942f", "committedDate": "2020-06-09T17:22:31Z", "message": "remove constant"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/b1d2ab7619e86d84952b69bb0ff16e6af1b6513c", "committedDate": "2020-06-09T18:02:37Z", "message": "Add invalid parameter validator"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57f2002b841b81edce2f4d254e9a673a1bbca616", "author": {"user": {"login": "fil512", "name": "Ken Stevens"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/57f2002b841b81edce2f4d254e9a673a1bbca616", "committedDate": "2020-06-09T19:49:11Z", "message": "organize imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10", "author": {"user": {"login": "fil512", "name": "Ken Stevens"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/7fdd0f17531bff1323e81303888a7584638e1e10", "committedDate": "2020-06-09T19:52:54Z", "message": "Merge remote-tracking branch 'remotes/origin/master' into spring-batch-integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3NDg0OTQz", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#pullrequestreview-427484943", "createdAt": "2020-06-09T19:43:28Z", "commit": {"oid": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 34, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQxOTo0MzoyOFrOGhZjmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQyMDo0NTozMlrOGhbibw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NDkwNQ==", "bodyText": "Javadoc on interfaces", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437674905", "createdAt": "2020-06-09T19:43:28Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-batch/src/main/java/ca/uhn/fhir/jpa/batch/api/IBatchJobSubmitter.java", "diffHunk": "@@ -0,0 +1,11 @@\n+package ca.uhn.fhir.jpa.batch.api;\n+\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobExecution;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+\n+public interface IBatchJobSubmitter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NjcwNw==", "bodyText": "This claims to be a Spring Config but doesn't contain any beans?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437676707", "createdAt": "2020-06-09T19:46:53Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-batch/src/main/java/ca/uhn/fhir/jpa/batch/config/InMemoryJobRepositoryBatchConfig.java", "diffHunk": "@@ -0,0 +1,66 @@\n+package ca.uhn.fhir.jpa.batch.config;\n+\n+import org.springframework.batch.core.configuration.annotation.BatchConfigurer;\n+import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.explore.JobExplorer;\n+import org.springframework.batch.core.explore.support.MapJobExplorerFactoryBean;\n+import org.springframework.batch.core.launch.JobLauncher;\n+import org.springframework.batch.core.launch.support.SimpleJobLauncher;\n+import org.springframework.batch.core.repository.JobRepository;\n+import org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean;\n+import org.springframework.batch.support.transaction.ResourcelessTransactionManager;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.transaction.PlatformTransactionManager;\n+\n+import javax.annotation.PostConstruct;\n+\n+@Configuration", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4Mzc3OA==", "bodyText": "move to common?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437683778", "createdAt": "2020-06-09T20:00:09Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4MzkwOQ==", "bodyText": "rename", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437683909", "createdAt": "2020-06-09T20:00:22Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {\n+\t\treturn new JobExistsParameterValidator();\n+\t}\n+\n+\n+\t@Bean\n+\tpublic Step bulkExportGenerateResourceFilesStep() {\n+\t\treturn myStepBuilderFactory.get(\"bulkExportGenerateResourceFilesStep\")\n+\t\t\t.<List<ResourcePersistentId>, List<IBaseResource>> chunk(100) //1000 resources per generated file, as the reader returns 10 resources at a time.\n+\t\t\t.reader(bulkItemReader(null))\n+\t\t\t.processor(myPidToIBaseResourceProcessor)\n+\t\t\t.writer(resourceToFileWriter())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic BulkExportJobStatusChangeListener bulkExportJobCompletionListener() {\n+\t\treturn new BulkExportJobStatusChangeListener();\n+\t}\n+\n+\t@Bean\n+\tpublic Step partitionStep() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NDA0NA==", "bodyText": "move to common?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437684044", "createdAt": "2020-06-09T20:00:40Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {\n+\t\treturn new JobExistsParameterValidator();\n+\t}\n+\n+\n+\t@Bean\n+\tpublic Step bulkExportGenerateResourceFilesStep() {\n+\t\treturn myStepBuilderFactory.get(\"bulkExportGenerateResourceFilesStep\")\n+\t\t\t.<List<ResourcePersistentId>, List<IBaseResource>> chunk(100) //1000 resources per generated file, as the reader returns 10 resources at a time.\n+\t\t\t.reader(bulkItemReader(null))\n+\t\t\t.processor(myPidToIBaseResourceProcessor)\n+\t\t\t.writer(resourceToFileWriter())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic BulkExportJobStatusChangeListener bulkExportJobCompletionListener() {\n+\t\treturn new BulkExportJobStatusChangeListener();\n+\t}\n+\n+\t@Bean\n+\tpublic Step partitionStep() {\n+\t\treturn myStepBuilderFactory.get(\"partitionStep\")\n+\t\t\t.partitioner(\"bulkExportGenerateResourceFilesStep\", partitioner(null))\n+\t\t\t.step(bulkExportGenerateResourceFilesStep())\n+\t\t\t.taskExecutor(myTaskExecutor)\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@StepScope\n+\tpublic BulkItemReader bulkItemReader(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {\n+\t\tBulkItemReader bulkItemReader = new BulkItemReader();\n+\t\tbulkItemReader.setJobUUID(theJobUUID);\n+\t\treturn bulkItemReader;\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic ResourceTypePartitioner partitioner(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NDM5OQ==", "bodyText": "move to common?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437684399", "createdAt": "2020-06-09T20:01:16Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {\n+\t\treturn new JobExistsParameterValidator();\n+\t}\n+\n+\n+\t@Bean\n+\tpublic Step bulkExportGenerateResourceFilesStep() {\n+\t\treturn myStepBuilderFactory.get(\"bulkExportGenerateResourceFilesStep\")\n+\t\t\t.<List<ResourcePersistentId>, List<IBaseResource>> chunk(100) //1000 resources per generated file, as the reader returns 10 resources at a time.\n+\t\t\t.reader(bulkItemReader(null))\n+\t\t\t.processor(myPidToIBaseResourceProcessor)\n+\t\t\t.writer(resourceToFileWriter())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic BulkExportJobStatusChangeListener bulkExportJobCompletionListener() {\n+\t\treturn new BulkExportJobStatusChangeListener();\n+\t}\n+\n+\t@Bean\n+\tpublic Step partitionStep() {\n+\t\treturn myStepBuilderFactory.get(\"partitionStep\")\n+\t\t\t.partitioner(\"bulkExportGenerateResourceFilesStep\", partitioner(null))\n+\t\t\t.step(bulkExportGenerateResourceFilesStep())\n+\t\t\t.taskExecutor(myTaskExecutor)\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@StepScope\n+\tpublic BulkItemReader bulkItemReader(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {\n+\t\tBulkItemReader bulkItemReader = new BulkItemReader();\n+\t\tbulkItemReader.setJobUUID(theJobUUID);\n+\t\treturn bulkItemReader;\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic ResourceTypePartitioner partitioner(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {\n+\t\treturn new ResourceTypePartitioner(theJobUUID);\n+\t}\n+\n+\t@Bean\n+\t@StepScope\n+\tpublic ItemWriter<List<IBaseResource>> resourceToFileWriter() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NTE2NA==", "bodyText": "Won't it just keep failing endlessly?  I wonder if failed jobs should move to an error state with maybe a way to retry them?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437685164", "createdAt": "2020-06-09T20:02:45Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobStatusChangeListener.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.springframework.batch.core.BatchStatus;\n+import org.springframework.batch.core.JobExecution;\n+import org.springframework.batch.core.JobExecutionListener;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+/**\n+ * Will run before and after a job to set the status to whatever is appropriate.\n+ */\n+public class BulkExportJobStatusChangeListener implements JobExecutionListener {\n+\n+\t@Value(\"#{jobParameters['jobUUID']}\")\n+\tprivate String myJobUUID;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\t@Override\n+\tpublic void beforeJob(JobExecution theJobExecution) {\n+\t\tif (theJobExecution.getStatus() == BatchStatus.STARTING) {\n+\t\t\tmyBulkExportDaoSvc.setJobToStatus(myJobUUID, BulkJobStatusEnum.BUILDING);\n+\t\t}\n+\n+\t}\n+\n+\t@Override\n+\tpublic void afterJob(JobExecution theJobExecution) {\n+\t\tif (theJobExecution.getStatus() == BatchStatus.COMPLETED) {\n+\t\t\tmyBulkExportDaoSvc.setJobToStatus(myJobUUID, BulkJobStatusEnum.COMPLETE);\n+\t\t} else {\n+\t\t\t//If the job didn't complete successfully, just set it back to submitted so that it gets picked up again by the scheduler.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NTkwMQ==", "bodyText": "should this be configurable?  (e.g. one of the job params...?)", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437685901", "createdAt": "2020-06-09T20:04:20Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.interceptor.model.RequestPartitionId;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.dao.IResultIterator;\n+import ca.uhn.fhir.jpa.dao.ISearchBuilder;\n+import ca.uhn.fhir.jpa.dao.SearchBuilderFactory;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import ca.uhn.fhir.rest.param.DateRangeParam;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.NonTransientResourceException;\n+import org.springframework.batch.item.ParseException;\n+import org.springframework.batch.item.UnexpectedInputException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n+\tprivate static final Logger ourLog = LoggerFactory.getLogger(BulkItemReader.class);\n+\n+\tprivate static final int READ_CHUNK_SIZE = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NjM0MA==", "bodyText": "warn", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437686340", "createdAt": "2020-06-09T20:05:09Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.interceptor.model.RequestPartitionId;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.dao.IResultIterator;\n+import ca.uhn.fhir.jpa.dao.ISearchBuilder;\n+import ca.uhn.fhir.jpa.dao.SearchBuilderFactory;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import ca.uhn.fhir.rest.param.DateRangeParam;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.NonTransientResourceException;\n+import org.springframework.batch.item.ParseException;\n+import org.springframework.batch.item.UnexpectedInputException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n+\tprivate static final Logger ourLog = LoggerFactory.getLogger(BulkItemReader.class);\n+\n+\tprivate static final int READ_CHUNK_SIZE = 10;\n+\n+\t@Autowired\n+\tprivate IBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tprivate DaoRegistry myDaoRegistry;\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;\n+\n+\t@Autowired\n+\tprivate SearchBuilderFactory mySearchBuilderFactory;\n+\n+\tprivate BulkExportJobEntity myJobEntity;\n+\n+\tprivate String myJobUUID;\n+\n+\t@Value(\"#{stepExecutionContext['resourceType']}\")\n+\tprivate String myResourceType;\n+\n+\tIterator<ResourcePersistentId> myPidIterator;\n+\n+\tprivate void loadResourcePids() {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(myJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NzAzMw==", "bodyText": "feels like we'll probably want a separate logger for batch jobs so they don't get swamped by fhir request logs", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437687033", "createdAt": "2020-06-09T20:06:32Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.interceptor.model.RequestPartitionId;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.dao.IResultIterator;\n+import ca.uhn.fhir.jpa.dao.ISearchBuilder;\n+import ca.uhn.fhir.jpa.dao.SearchBuilderFactory;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import ca.uhn.fhir.rest.param.DateRangeParam;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.NonTransientResourceException;\n+import org.springframework.batch.item.ParseException;\n+import org.springframework.batch.item.UnexpectedInputException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n+\tprivate static final Logger ourLog = LoggerFactory.getLogger(BulkItemReader.class);\n+\n+\tprivate static final int READ_CHUNK_SIZE = 10;\n+\n+\t@Autowired\n+\tprivate IBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tprivate DaoRegistry myDaoRegistry;\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;\n+\n+\t@Autowired\n+\tprivate SearchBuilderFactory mySearchBuilderFactory;\n+\n+\tprivate BulkExportJobEntity myJobEntity;\n+\n+\tprivate String myJobUUID;\n+\n+\t@Value(\"#{stepExecutionContext['resourceType']}\")\n+\tprivate String myResourceType;\n+\n+\tIterator<ResourcePersistentId> myPidIterator;\n+\n+\tprivate void loadResourcePids() {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(myJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");\n+\t\t\treturn;\n+\t\t}\n+\t\tmyJobEntity = jobOpt.get();\n+\t\tourLog.info(\"Bulk export starting generation for batch export job: {}\", myJobEntity);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4ODkxOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * This class will prevent job running if the UUID is found to be non-existent, or invalid.\n          \n          \n            \n             * This class will prevent a job from running if the UUID does not exist or is invalid.", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437688918", "createdAt": "2020-06-09T20:10:10Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.apache.commons.lang3.StringUtils;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.Optional;\n+\n+/**\n+ * This class will prevent job running if the UUID is found to be non-existent, or invalid.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4OTU0Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tthrow new JobParametersInvalidException(\"You did not pass a jobUUID to this job!\");\n          \n          \n            \n            \t\t\tthrow new JobParametersInvalidException(\"Missing jobUUID job parameter\");", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437689546", "createdAt": "2020-06-09T20:11:25Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.apache.commons.lang3.StringUtils;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.Optional;\n+\n+/**\n+ * This class will prevent job running if the UUID is found to be non-existent, or invalid.\n+ */\n+public class JobExistsParameterValidator implements JobParametersValidator {\n+\t@Autowired\n+\tprivate IBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Override\n+\tpublic void validate(JobParameters theJobParameters) throws JobParametersInvalidException {\n+\t\tString jobUUID = theJobParameters.getString(\"jobUUID\");\n+\t\tif (StringUtils.isBlank(jobUUID)) {\n+\t\t\tthrow new JobParametersInvalidException(\"You did not pass a jobUUID to this job!\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MDA5MA==", "bodyText": "I like the way the steps are separated out by Spring Batch.  Feels like it cleans up and organizes our batch jobs--plus provides for better re-use", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437690090", "createdAt": "2020-06-09T20:12:29Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceToFileWriter.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.parser.IParser;\n+import ca.uhn.fhir.rest.api.Constants;\n+import ca.uhn.fhir.util.BinaryUtil;\n+import org.hl7.fhir.instance.model.api.IBaseBinary;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.hl7.fhir.instance.model.api.IIdType;\n+import org.slf4j.Logger;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import javax.annotation.PostConstruct;\n+import java.io.ByteArrayOutputStream;\n+import java.io.OutputStreamWriter;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceToFileWriter implements ItemWriter<List<IBaseResource>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MDk3Ng==", "bodyText": "nitpick: there are so many contexts, I like to call these myFhirContext", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437690976", "createdAt": "2020-06-09T20:14:11Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceToFileWriter.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.parser.IParser;\n+import ca.uhn.fhir.rest.api.Constants;\n+import ca.uhn.fhir.util.BinaryUtil;\n+import org.hl7.fhir.instance.model.api.IBaseBinary;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.hl7.fhir.instance.model.api.IIdType;\n+import org.slf4j.Logger;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import javax.annotation.PostConstruct;\n+import java.io.ByteArrayOutputStream;\n+import java.io.OutputStreamWriter;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceToFileWriter implements ItemWriter<List<IBaseResource>> {\n+\tprivate static final Logger ourLog = getLogger(ResourceToFileWriter.class);\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MjY2Nw==", "bodyText": "nitpick: simple if statements are more approachable to junior developers", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437692667", "createdAt": "2020-06-09T20:17:19Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceToFileWriter.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.parser.IParser;\n+import ca.uhn.fhir.rest.api.Constants;\n+import ca.uhn.fhir.util.BinaryUtil;\n+import org.hl7.fhir.instance.model.api.IBaseBinary;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.hl7.fhir.instance.model.api.IIdType;\n+import org.slf4j.Logger;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import javax.annotation.PostConstruct;\n+import java.io.ByteArrayOutputStream;\n+import java.io.OutputStreamWriter;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceToFileWriter implements ItemWriter<List<IBaseResource>> {\n+\tprivate static final Logger ourLog = getLogger(ResourceToFileWriter.class);\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;\n+\n+\t@Autowired\n+\tprivate DaoRegistry myDaoRegistry;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\tprivate ByteArrayOutputStream myOutputStream;\n+\tprivate OutputStreamWriter myWriter;\n+\tprivate IParser myParser;\n+\n+\t@Value(\"#{stepExecutionContext['bulkExportCollectionEntityId']}\")\n+\tprivate Long myBulkExportCollectionEntityId;\n+\n+\tprivate IFhirResourceDao<IBaseBinary> myBinaryDao;\n+\n+\n+\tpublic ResourceToFileWriter() {\n+\t\tmyOutputStream = new ByteArrayOutputStream();\n+\t\tmyWriter = new OutputStreamWriter(myOutputStream, Constants.CHARSET_UTF8);\n+\t}\n+\n+\t@PostConstruct\n+\tpublic void start() {\n+\t\tmyParser = myContext.newJsonParser().setPrettyPrint(false);\n+\t\tmyBinaryDao = getBinaryDao();\n+\t}\n+\n+\tprivate Optional<IIdType> flushToFiles() {\n+\t\tif (myOutputStream.size() > 0) {\n+\t\t\tIIdType createdId = createBinaryFromOutputStream();\n+\t\t\tBulkExportCollectionFileEntity file = new BulkExportCollectionFileEntity();\n+\t\t\tfile.setResource(createdId.getIdPart());\n+\n+\t\t\tmyBulkExportDaoSvc.addFileToCollectionWithId(myBulkExportCollectionEntityId, file);\n+\n+\t\t\tmyOutputStream.reset();\n+\n+\t\t\treturn Optional.of(createdId);\n+\t\t}\n+\n+\t\treturn Optional.empty();\n+\t}\n+\n+\tprivate IIdType createBinaryFromOutputStream() {\n+\t\tIBaseBinary binary = BinaryUtil.newBinary(myContext);\n+\t\tbinary.setContentType(Constants.CT_FHIR_NDJSON);\n+\t\tbinary.setContent(myOutputStream.toByteArray());\n+\n+\t\treturn myBinaryDao.create(binary).getResource().getIdElement();\n+\t}\n+\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate IFhirResourceDao<IBaseBinary> getBinaryDao() {\n+\t\treturn myDaoRegistry.getResourceDao(\"Binary\");\n+\t}\n+\n+\t@Override\n+\tpublic void write(List<? extends List<IBaseResource>> theList) throws Exception {\n+\n+\t\tfor (List<IBaseResource> resourceList : theList) {\n+\t\t\tfor (IBaseResource nextFileResource : resourceList) {\n+\t\t\t\tmyParser.encodeResourceToWriter(nextFileResource, myWriter);\n+\t\t\t\tmyWriter.append(\"\\n\");\n+\t\t\t}\n+\t\t}\n+\n+\t\tOptional<IIdType> createdId = flushToFiles();\n+\t\tcreatedId.ifPresent(theIIdType -> ourLog.warn(\"Created resources for bulk export file containing {} resources of type \", theIIdType.toUnqualifiedVersionless().getValue()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MzA3MQ==", "bodyText": "commented out code", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437693071", "createdAt": "2020-06-09T20:18:07Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceTypePartitioner.java", "diffHunk": "@@ -0,0 +1,60 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.slf4j.Logger;\n+import org.springframework.batch.core.partition.support.Partitioner;\n+import org.springframework.batch.item.ExecutionContext;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceTypePartitioner implements Partitioner {\n+\tprivate static final Logger ourLog = getLogger(ResourceTypePartitioner.class);\n+\n+\tprivate String myJobUUID;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\tpublic ResourceTypePartitioner(String theJobUUID) {\n+\t\tmyJobUUID = theJobUUID;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, ExecutionContext> partition(int gridSize) {\n+\t\tMap<String, ExecutionContext> partitionContextMap = new HashMap<>();\n+\n+\t\tMap<Long, String> idToResourceType = myBulkExportDaoSvc.getBulkJobCollectionIdToResourceTypeMap(\tmyJobUUID);\n+\t\t//observation -> obs1.json, obs2.json, obs3.json BulkJobCollectionEntity", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MzU4Ng==", "bodyText": "nitpick: replace large block with method reference", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437693586", "createdAt": "2020-06-09T20:19:13Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceTypePartitioner.java", "diffHunk": "@@ -0,0 +1,60 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.slf4j.Logger;\n+import org.springframework.batch.core.partition.support.Partitioner;\n+import org.springframework.batch.item.ExecutionContext;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceTypePartitioner implements Partitioner {\n+\tprivate static final Logger ourLog = getLogger(ResourceTypePartitioner.class);\n+\n+\tprivate String myJobUUID;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\tpublic ResourceTypePartitioner(String theJobUUID) {\n+\t\tmyJobUUID = theJobUUID;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, ExecutionContext> partition(int gridSize) {\n+\t\tMap<String, ExecutionContext> partitionContextMap = new HashMap<>();\n+\n+\t\tMap<Long, String> idToResourceType = myBulkExportDaoSvc.getBulkJobCollectionIdToResourceTypeMap(\tmyJobUUID);\n+\t\t//observation -> obs1.json, obs2.json, obs3.json BulkJobCollectionEntity\n+\t\t//bulk Collection Entity ID -> patient\n+\n+\t\t// 123123-> Patient\n+\t\t// 91876389126-> Observation\n+\t\tidToResourceType.entrySet().stream()\n+\t\t\t.forEach(entry -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NDQzNw==", "bodyText": "nice to see this class shrink in size!  :-)", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437694437", "createdAt": "2020-06-09T20:20:46Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkDataExportSvcImpl.java", "diffHunk": "@@ -429,7 +320,7 @@ private void updateExpiry(BulkExportJobEntity theJob) {\n \n \t@Transactional\n \t@Override\n-\tpublic JobInfo getJobStatusOrThrowResourceNotFound(String theJobId) {\n+\tpublic JobInfo getJobInfoOrThrowResourceNotFound(String theJobId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTI1MA==", "bodyText": "warn", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437695250", "createdAt": "2020-06-09T20:22:10Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package ca.uhn.fhir.jpa.bulk.svc;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionFileDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.slf4j.Logger;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Service;\n+\n+import javax.transaction.Transactional;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+@Service\n+public class BulkExportDaoSvc {\n+\tprivate static final Logger ourLog = getLogger(BulkExportDaoSvc.class);\n+\n+\t@Autowired\n+\tIBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionDao myBulkExportCollectionDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n+\n+\t@Transactional\n+\tpublic void addFileToCollectionWithId(Long theCollectionEntityId, BulkExportCollectionFileEntity theFile) {\n+\t\tOptional<BulkExportCollectionEntity> byId = myBulkExportCollectionDao.findById(theCollectionEntityId);\n+\t\tif (byId.isPresent()) {\n+\t\t\tBulkExportCollectionEntity exportCollectionEntity = byId.get();\n+\t\t\ttheFile.setCollection(exportCollectionEntity);;\n+\t\t\tmyBulkExportCollectionFileDao.saveAndFlush(theFile);\n+\t\t\tmyBulkExportCollectionDao.saveAndFlush(exportCollectionEntity);\n+\t\t}\n+\n+\t}\n+\n+\t@Transactional\n+\tpublic Map<Long, String> getBulkJobCollectionIdToResourceTypeMap(String theJobUUID) {\n+\t\tBulkExportJobEntity bulkExportJobEntity = loadJob(theJobUUID);\n+\t\tCollection<BulkExportCollectionEntity> collections = bulkExportJobEntity.getCollections();\n+\t\treturn collections.stream()\n+\t\t\t.collect(Collectors.toMap(\n+\t\t\t\tBulkExportCollectionEntity::getId,\n+\t\t\t\tBulkExportCollectionEntity::getResourceType\n+\t\t\t));\n+\t}\n+\n+\tprivate BulkExportJobEntity loadJob(String theJobUUID) {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTMzNQ==", "bodyText": "and name the uuid", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437695335", "createdAt": "2020-06-09T20:22:22Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package ca.uhn.fhir.jpa.bulk.svc;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionFileDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.slf4j.Logger;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Service;\n+\n+import javax.transaction.Transactional;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+@Service\n+public class BulkExportDaoSvc {\n+\tprivate static final Logger ourLog = getLogger(BulkExportDaoSvc.class);\n+\n+\t@Autowired\n+\tIBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionDao myBulkExportCollectionDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n+\n+\t@Transactional\n+\tpublic void addFileToCollectionWithId(Long theCollectionEntityId, BulkExportCollectionFileEntity theFile) {\n+\t\tOptional<BulkExportCollectionEntity> byId = myBulkExportCollectionDao.findById(theCollectionEntityId);\n+\t\tif (byId.isPresent()) {\n+\t\t\tBulkExportCollectionEntity exportCollectionEntity = byId.get();\n+\t\t\ttheFile.setCollection(exportCollectionEntity);;\n+\t\t\tmyBulkExportCollectionFileDao.saveAndFlush(theFile);\n+\t\t\tmyBulkExportCollectionDao.saveAndFlush(exportCollectionEntity);\n+\t\t}\n+\n+\t}\n+\n+\t@Transactional\n+\tpublic Map<Long, String> getBulkJobCollectionIdToResourceTypeMap(String theJobUUID) {\n+\t\tBulkExportJobEntity bulkExportJobEntity = loadJob(theJobUUID);\n+\t\tCollection<BulkExportCollectionEntity> collections = bulkExportJobEntity.getCollections();\n+\t\treturn collections.stream()\n+\t\t\t.collect(Collectors.toMap(\n+\t\t\t\tBulkExportCollectionEntity::getId,\n+\t\t\t\tBulkExportCollectionEntity::getResourceType\n+\t\t\t));\n+\t}\n+\n+\tprivate BulkExportJobEntity loadJob(String theJobUUID) {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTI1MA=="}, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTUyMA==", "bodyText": "warn and name it.", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437695520", "createdAt": "2020-06-09T20:22:43Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package ca.uhn.fhir.jpa.bulk.svc;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionFileDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.slf4j.Logger;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Service;\n+\n+import javax.transaction.Transactional;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+@Service\n+public class BulkExportDaoSvc {\n+\tprivate static final Logger ourLog = getLogger(BulkExportDaoSvc.class);\n+\n+\t@Autowired\n+\tIBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionDao myBulkExportCollectionDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n+\n+\t@Transactional\n+\tpublic void addFileToCollectionWithId(Long theCollectionEntityId, BulkExportCollectionFileEntity theFile) {\n+\t\tOptional<BulkExportCollectionEntity> byId = myBulkExportCollectionDao.findById(theCollectionEntityId);\n+\t\tif (byId.isPresent()) {\n+\t\t\tBulkExportCollectionEntity exportCollectionEntity = byId.get();\n+\t\t\ttheFile.setCollection(exportCollectionEntity);;\n+\t\t\tmyBulkExportCollectionFileDao.saveAndFlush(theFile);\n+\t\t\tmyBulkExportCollectionDao.saveAndFlush(exportCollectionEntity);\n+\t\t}\n+\n+\t}\n+\n+\t@Transactional\n+\tpublic Map<Long, String> getBulkJobCollectionIdToResourceTypeMap(String theJobUUID) {\n+\t\tBulkExportJobEntity bulkExportJobEntity = loadJob(theJobUUID);\n+\t\tCollection<BulkExportCollectionEntity> collections = bulkExportJobEntity.getCollections();\n+\t\treturn collections.stream()\n+\t\t\t.collect(Collectors.toMap(\n+\t\t\t\tBulkExportCollectionEntity::getId,\n+\t\t\t\tBulkExportCollectionEntity::getResourceType\n+\t\t\t));\n+\t}\n+\n+\tprivate BulkExportJobEntity loadJob(String theJobUUID) {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");\n+\t\t\treturn null;\n+\t\t}\n+\t\treturn jobOpt.get();\n+\t}\n+\n+\t@Transactional\n+\tpublic void setJobToStatus(String theJobUUID, BulkJobStatusEnum theStatus) {\n+\t\tOptional<BulkExportJobEntity> oJob = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!oJob.isPresent()) {\n+\t\t\tourLog.error(\"Job doesn't exist!\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NjE2NA==", "bodyText": "nitpick: I'd be inclined to return here and drop the else", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437696164", "createdAt": "2020-06-09T20:24:00Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package ca.uhn.fhir.jpa.bulk.svc;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionFileDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.slf4j.Logger;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Service;\n+\n+import javax.transaction.Transactional;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+@Service\n+public class BulkExportDaoSvc {\n+\tprivate static final Logger ourLog = getLogger(BulkExportDaoSvc.class);\n+\n+\t@Autowired\n+\tIBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionDao myBulkExportCollectionDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n+\n+\t@Transactional\n+\tpublic void addFileToCollectionWithId(Long theCollectionEntityId, BulkExportCollectionFileEntity theFile) {\n+\t\tOptional<BulkExportCollectionEntity> byId = myBulkExportCollectionDao.findById(theCollectionEntityId);\n+\t\tif (byId.isPresent()) {\n+\t\t\tBulkExportCollectionEntity exportCollectionEntity = byId.get();\n+\t\t\ttheFile.setCollection(exportCollectionEntity);;\n+\t\t\tmyBulkExportCollectionFileDao.saveAndFlush(theFile);\n+\t\t\tmyBulkExportCollectionDao.saveAndFlush(exportCollectionEntity);\n+\t\t}\n+\n+\t}\n+\n+\t@Transactional\n+\tpublic Map<Long, String> getBulkJobCollectionIdToResourceTypeMap(String theJobUUID) {\n+\t\tBulkExportJobEntity bulkExportJobEntity = loadJob(theJobUUID);\n+\t\tCollection<BulkExportCollectionEntity> collections = bulkExportJobEntity.getCollections();\n+\t\treturn collections.stream()\n+\t\t\t.collect(Collectors.toMap(\n+\t\t\t\tBulkExportCollectionEntity::getId,\n+\t\t\t\tBulkExportCollectionEntity::getResourceType\n+\t\t\t));\n+\t}\n+\n+\tprivate BulkExportJobEntity loadJob(String theJobUUID) {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");\n+\t\t\treturn null;\n+\t\t}\n+\t\treturn jobOpt.get();\n+\t}\n+\n+\t@Transactional\n+\tpublic void setJobToStatus(String theJobUUID, BulkJobStatusEnum theStatus) {\n+\t\tOptional<BulkExportJobEntity> oJob = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!oJob.isPresent()) {\n+\t\t\tourLog.error(\"Job doesn't exist!\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTUyMA=="}, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5Njk1NA==", "bodyText": "commented code", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437696954", "createdAt": "2020-06-09T20:25:32Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/dao/data/IBulkExportCollectionDao.java", "diffHunk": "@@ -36,4 +36,6 @@\n \t@Query(\"DELETE FROM BulkExportCollectionEntity t WHERE t.myId = :pid\")\n \tvoid deleteByPid(@Param(\"pid\") Long theId);\n \n+//\t@Query(\"SELECT BulkExportCollectionEntity \")\n+//\tvoid findByJobId(Long theId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NzQ4NA==", "bodyText": "Would it make sense for us to subclass Job and autowire by type?  (Would save the @Qualifier)", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437697484", "createdAt": "2020-06-09T20:26:37Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -41,6 +52,12 @@\n \tprivate IBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n \t@Autowired\n \tprivate IBulkDataExportSvc myBulkDataExportSvc;\n+\t@Autowired\n+\tprivate IBatchJobSubmitter myBatchJobSubmitter;\n+\n+\t@Autowired\n+\t@Qualifier(\"bulkExportJob\")\n+\tprivate Job myBulkJob;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMDA2Nw==", "bodyText": "This is a nice test.  Demonstrates how simple and clean it is to create and submit a job.", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437700067", "createdAt": "2020-06-09T20:31:42Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -234,7 +251,45 @@ public void testSubmitReusesExisting() {\n \t}\n \n \n-\t\t@Test\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", jobDetails.getJobId());\n+\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMDM0OA==", "bodyText": "Feels like needing to create a uuid is such a common thing all the services are going to want to do, I wonder if we should hide this by default and just do it for them.", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437700348", "createdAt": "2020-06-09T20:32:15Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -234,7 +251,45 @@ public void testSubmitReusesExisting() {\n \t}\n \n \n-\t\t@Test\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", jobDetails.getJobId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMTI5Mw==", "bodyText": "await().until(() -> myBulkDataExportSvc.getJobInfoOrThrowResourceNotFound(theJobId).getStatus() == BulkJobStatusEnum.COMPLETE)", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437701293", "createdAt": "2020-06-09T20:34:10Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -234,7 +251,45 @@ public void testSubmitReusesExisting() {\n \t}\n \n \n-\t\t@Test\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", jobDetails.getJobId());\n+\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\n+\t\tIBulkDataExportSvc.JobInfo jobInfo = awaitJobCompletion(jobDetails.getJobId());\n+\t\tassertThat(jobInfo.getStatus(), equalTo(BulkJobStatusEnum.COMPLETE));\n+\t}\n+\n+\t@Test\n+\tpublic void testJobParametersValidatorRejectsInvalidParameters() {\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", \"I'm not real!\");\n+\t\ttry {\n+\t\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\t\t\tfail(\"Should have had invalid parameter execption!\");\n+\t\t} catch (JobParametersInvalidException e) {\n+\n+\t\t}\n+\n+\t}\n+\n+\tpublic IBulkDataExportSvc.JobInfo awaitJobCompletion(String theJobId) throws InterruptedException {\n+\t\twhile(true) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjYyMg==", "bodyText": "Change this comment so it makes sense outside the context of this commit:\nPlease do not rename this bean to \"transactionManager\" as that will conflict with the Spring Batch transactionManager.", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437702622", "createdAt": "2020-06-09T20:36:51Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/config/TestJPAConfig.java", "diffHunk": "@@ -40,8 +40,12 @@ public ModelConfig modelConfig() {\n \t\treturn daoConfig().getModelConfig();\n \t}\n \n+\t/*\n+\tI had to rename this bean as it was clashing with Spring Batch `transactionManager` in SimpleBatchConfiguration", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjc4MQ==", "bodyText": "or something like that", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437702781", "createdAt": "2020-06-09T20:37:09Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/config/TestJPAConfig.java", "diffHunk": "@@ -40,8 +40,12 @@ public ModelConfig modelConfig() {\n \t\treturn daoConfig().getModelConfig();\n \t}\n \n+\t/*\n+\tI had to rename this bean as it was clashing with Spring Batch `transactionManager` in SimpleBatchConfiguration", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjYyMg=="}, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMzA2MQ==", "bodyText": "is it possible to call it jpaTransactionManager ?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437703061", "createdAt": "2020-06-09T20:37:39Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/config/TestJPAConfig.java", "diffHunk": "@@ -40,8 +40,12 @@ public ModelConfig modelConfig() {\n \t\treturn daoConfig().getModelConfig();\n \t}\n \n+\t/*\n+\tI had to rename this bean as it was clashing with Spring Batch `transactionManager` in SimpleBatchConfiguration\n+\t */\n \t@Bean\n-\tpublic JpaTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {\n+\t@Primary\n+\tpublic JpaTransactionManager hapiTransactionManager(EntityManagerFactory entityManagerFactory) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMzc2OQ==", "bodyText": "add another version where JobParameters is optional and we auto-create a uuid for the caller?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437703769", "createdAt": "2020-06-09T20:38:56Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-batch/src/main/java/ca/uhn/fhir/jpa/batch/svc/BatchJobSubmitterImpl.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package ca.uhn.fhir.jpa.batch.svc;\n+\n+import ca.uhn.fhir.jpa.batch.api.IBatchJobSubmitter;\n+import org.slf4j.Logger;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobExecution;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.launch.JobLauncher;\n+import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;\n+import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;\n+import org.springframework.batch.core.repository.JobRepository;\n+import org.springframework.batch.core.repository.JobRestartException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class BatchJobSubmitterImpl implements IBatchJobSubmitter {\n+\n+\tprivate static final Logger ourLog = getLogger(BatchJobSubmitterImpl.class);\n+\n+\t@Autowired\n+\tprivate JobLauncher myJobLauncher;\n+\n+\t@Autowired\n+\tprivate JobRepository myJobRepository;\n+\n+\t@Override\n+\tpublic JobExecution runJob(Job theJob, JobParameters theJobParameters) throws JobParametersInvalidException{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNTQ5MA==", "bodyText": "this latch is never used...?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437705490", "createdAt": "2020-06-09T20:42:13Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-batch/src/test/java/ca/uhn/fhir/jpa/batch/config/BatchJobConfig.java", "diffHunk": "@@ -0,0 +1,84 @@\n+package ca.uhn.fhir.jpa.batch.config;\n+\n+import ca.uhn.fhir.interceptor.api.HookParams;\n+import ca.uhn.test.concurrency.IPointcutLatch;\n+import ca.uhn.test.concurrency.PointcutLatch;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.core.step.tasklet.Tasklet;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+\n+import java.util.List;\n+\n+@Configuration\n+public class BatchJobConfig implements IPointcutLatch {\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNzIyMg==", "bodyText": "This job appears to run forever", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437707222", "createdAt": "2020-06-09T20:45:20Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-batch/src/test/java/ca/uhn/fhir/jpa/batch/svc/BatchSvcTest.java", "diffHunk": "@@ -0,0 +1,24 @@\n+package ca.uhn.fhir.jpa.batch.svc;\n+\n+import ca.uhn.fhir.jpa.batch.BaseBatchR4Test;\n+import ca.uhn.fhir.jpa.batch.config.BatchJobConfig;\n+import org.junit.Test;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;\n+import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;\n+import org.springframework.batch.core.repository.JobRestartException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+public class BatchSvcTest extends BaseBatchR4Test {\n+\t@Autowired\n+\tprivate BatchJobConfig myBatchJobConfig;\n+\n+\t@Test\n+\tpublic void testApplicationContextLoads() throws JobParametersInvalidException, JobExecutionAlreadyRunningException, JobRestartException, JobInstanceAlreadyCompleteException, InterruptedException {\n+\t\tmyBatchJobConfig.setExpectedCount(1);\n+\t\tmyJobLauncher.run(myJob, new JobParameters());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNzM3NQ==", "bodyText": "I don't see how this could ever happen...?", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437707375", "createdAt": "2020-06-09T20:45:32Z", "author": {"login": "fil512"}, "path": "hapi-fhir-jpaserver-batch/src/test/java/ca/uhn/fhir/jpa/batch/svc/BatchSvcTest.java", "diffHunk": "@@ -0,0 +1,24 @@\n+package ca.uhn.fhir.jpa.batch.svc;\n+\n+import ca.uhn.fhir.jpa.batch.BaseBatchR4Test;\n+import ca.uhn.fhir.jpa.batch.config.BatchJobConfig;\n+import org.junit.Test;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;\n+import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;\n+import org.springframework.batch.core.repository.JobRestartException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+public class BatchSvcTest extends BaseBatchR4Test {\n+\t@Autowired\n+\tprivate BatchJobConfig myBatchJobConfig;\n+\n+\t@Test\n+\tpublic void testApplicationContextLoads() throws JobParametersInvalidException, JobExecutionAlreadyRunningException, JobRestartException, JobInstanceAlreadyCompleteException, InterruptedException {\n+\t\tmyBatchJobConfig.setExpectedCount(1);\n+\t\tmyJobLauncher.run(myJob, new JobParameters());\n+\t\tmyBatchJobConfig.awaitExpected();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7cea688f6acd1ccd514fae4838db628338b4171", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/b7cea688f6acd1ccd514fae4838db628338b4171", "committedDate": "2020-06-09T22:52:37Z", "message": "Update hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java\n\nCo-authored-by: Ken Stevens <khstevens@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d90987368a9dfab00435a1303d5d09df15380b70", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/d90987368a9dfab00435a1303d5d09df15380b70", "committedDate": "2020-06-09T23:29:15Z", "message": "Update hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java\n\nCo-authored-by: Ken Stevens <khstevens@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "942fc313ec19635fb69635ec73f01d3fd1d6416a", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/942fc313ec19635fb69635ec73f01d3fd1d6416a", "committedDate": "2020-06-10T02:00:41Z", "message": "Code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c36e1d28995da74200b39d56aef009211fbd5c0c", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/c36e1d28995da74200b39d56aef009211fbd5c0c", "committedDate": "2020-06-10T02:18:06Z", "message": "More code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fc5b753e5d36f1d9277373b6bd5f5d1295e2d22", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/6fc5b753e5d36f1d9277373b6bd5f5d1295e2d22", "committedDate": "2020-06-10T02:20:43Z", "message": "Readding imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e43c5348c99f759f1c6e33eba3368229f8f189e", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/6e43c5348c99f759f1c6e33eba3368229f8f189e", "committedDate": "2020-06-10T02:27:16Z", "message": "Add missing imports from import reorganization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d698ad71801945ef83eba5579092d85bd7115632", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/d698ad71801945ef83eba5579092d85bd7115632", "committedDate": "2020-06-10T17:46:28Z", "message": "Code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e42a59ad4b3ebe0a37426db2b916111585fc9e9", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/4e42a59ad4b3ebe0a37426db2b916111585fc9e9", "committedDate": "2020-06-10T18:52:45Z", "message": "Add licenses"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9c704c06e34c66b007a326147a5a802127988d1", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/a9c704c06e34c66b007a326147a5a802127988d1", "committedDate": "2020-06-10T19:24:19Z", "message": "Merge branch 'master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f469c1a56c4196f3fcd1d91d7f1744604d3f5a8", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/0f469c1a56c4196f3fcd1d91d7f1744604d3f5a8", "committedDate": "2020-06-12T21:20:50Z", "message": "Rework to allow for job creation inside of batch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "928aafba1e406c0e912d7c9508ec442cdf0da312", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/928aafba1e406c0e912d7c9508ec442cdf0da312", "committedDate": "2020-06-12T23:18:04Z", "message": "Rename bean"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4270636d434ee60e97404156ad17e85959002ac6", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/4270636d434ee60e97404156ad17e85959002ac6", "committedDate": "2020-06-12T23:20:54Z", "message": "Merge branch 'master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "798fdb9cc2196f30407666a6bf23d29266da844f", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/798fdb9cc2196f30407666a6bf23d29266da844f", "committedDate": "2020-06-15T16:10:41Z", "message": "Merge branch 'master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eccd280d711856de69e297462fa08ce27d6ce3b7", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/eccd280d711856de69e297462fa08ce27d6ce3b7", "committedDate": "2020-06-15T18:24:39Z", "message": "Fix test error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9222fa3f59b22ecee2884e61928f2f82df4ed38a", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/9222fa3f59b22ecee2884e61928f2f82df4ed38a", "committedDate": "2020-06-15T22:17:25Z", "message": "change transaction call"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d32ad744021b34af102aa01068a88687decd0403", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/d32ad744021b34af102aa01068a88687decd0403", "committedDate": "2020-06-16T04:49:34Z", "message": "some renaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e86732599c178ebb2400c85df1b70273c4ac579", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/0e86732599c178ebb2400c85df1b70273c4ac579", "committedDate": "2020-06-19T21:03:22Z", "message": "Add changelog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb7a773f41378c690a4d427e4970541df50688f9", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/bb7a773f41378c690a4d427e4970541df50688f9", "committedDate": "2020-06-23T23:02:41Z", "message": "Merge branch 'master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4cccc9804bd411893d64f060919db61316b9a697", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/4cccc9804bd411893d64f060919db61316b9a697", "committedDate": "2020-06-23T23:29:36Z", "message": "nop for CI"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bd17806c48f0ad756d639f26ac6e22be23f092e", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/2bd17806c48f0ad756d639f26ac6e22be23f092e", "committedDate": "2020-06-24T16:46:56Z", "message": "Update changelog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59659b280245da10edd1d8d3aff55b0dba48c48d", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/59659b280245da10edd1d8d3aff55b0dba48c48d", "committedDate": "2020-06-24T17:11:30Z", "message": "Move order of modules"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7740922f9ecf0142cdddd6387afcd7a4d99bd6f", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/a7740922f9ecf0142cdddd6387afcd7a4d99bd6f", "committedDate": "2020-06-24T18:02:52Z", "message": "Specify version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99fe66bb589507236a51c37a97af7073f3754fb1", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/99fe66bb589507236a51c37a97af7073f3754fb1", "committedDate": "2020-06-24T18:52:05Z", "message": "Fix pom to use project.version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64dbd038c96dd45292f44691051650a030b39972", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/64dbd038c96dd45292f44691051650a030b39972", "committedDate": "2020-06-24T20:16:11Z", "message": "Increase min thread count for batch. Add new  which waits for all bulkexport jobs to be done"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "571589d00de013712b24e25fbdf7b1adfb221b24", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/571589d00de013712b24e25fbdf7b1adfb221b24", "committedDate": "2020-06-24T20:46:13Z", "message": "Merge branch 'master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a69c1478a234f31723323d3b613341ded2535e2b", "author": {"user": {"login": "fil512", "name": "Ken Stevens"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/a69c1478a234f31723323d3b613341ded2535e2b", "committedDate": "2020-06-25T02:07:38Z", "message": "Merge remote-tracking branch 'remotes/origin/master' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3da4ad280bda3d89d3041ad670cefb3525a00cd8", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/3da4ad280bda3d89d3041ad670cefb3525a00cd8", "committedDate": "2020-06-25T03:00:52Z", "message": "undoes merge issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6875740178255d1b71fea76bb4bbd77c58e35ce5", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/6875740178255d1b71fea76bb4bbd77c58e35ce5", "committedDate": "2020-06-25T03:15:47Z", "message": "Merge branch 'spring-batch-integration' of github.com:jamesagnew/hapi-fhir into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06e1368388a72615a392f92d2c90b635fdad18bc", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/06e1368388a72615a392f92d2c90b635fdad18bc", "committedDate": "2020-06-25T04:00:45Z", "message": "Re-add comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e51c5b52630f11298112bde7f721ca80c0cc5e96", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/e51c5b52630f11298112bde7f721ca80c0cc5e96", "committedDate": "2020-06-25T04:03:02Z", "message": "change transactionmanager name so it doesnt shadow the batchprocessing one"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93da042d19138c328006f04b8534338ef6f05b1f", "author": {"user": {"login": "fil512", "name": "Ken Stevens"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/93da042d19138c328006f04b8534338ef6f05b1f", "committedDate": "2020-06-25T15:14:42Z", "message": "fix empi tests so they work with Spring Batch\n(all but one test fixed)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d9641a7361178d1c72cf37e6aa78da662ffaed3", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/1d9641a7361178d1c72cf37e6aa78da662ffaed3", "committedDate": "2020-06-25T17:28:03Z", "message": "fix empilinks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed155e7657ad108b09e38f967fad2cddbabc7082", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/ed155e7657ad108b09e38f967fad2cddbabc7082", "committedDate": "2020-06-25T17:37:02Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21567fdc43955c1d60eb68cd7ffa4203e1574806", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/21567fdc43955c1d60eb68cd7ffa4203e1574806", "committedDate": "2020-06-25T18:42:17Z", "message": "Tidy await"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c11b6eeab6274cf9830ff432c19e05598070c39d", "author": {"user": {"login": "fil512", "name": "Ken Stevens"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/c11b6eeab6274cf9830ff432c19e05598070c39d", "committedDate": "2020-06-25T19:27:31Z", "message": "back out Tx wrap\nuse qualifier"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7322760f5d09a19bbd1aeff975193db328012e28", "author": {"user": {"login": "fil512", "name": "Ken Stevens"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/7322760f5d09a19bbd1aeff975193db328012e28", "committedDate": "2020-06-25T19:27:46Z", "message": "Merge remote-tracking branch 'origin/spring-batch-integration' into spring-batch-integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "484f390db07bc87ffb5df7c270e2893fc0e4735e", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/484f390db07bc87ffb5df7c270e2893fc0e4735e", "committedDate": "2020-06-25T20:00:59Z", "message": "Fix batch test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13bbdde45890fab645c96d2e921b24e47482f2dc", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/13bbdde45890fab645c96d2e921b24e47482f2dc", "committedDate": "2020-06-25T22:38:15Z", "message": "pipeline fun"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0b622597b4c6fccb3422f5643e5f9235c5638fe3", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/0b622597b4c6fccb3422f5643e5f9235c5638fe3", "committedDate": "2020-06-26T17:13:03Z", "message": "Muck with initial loading to allow EMF and platformtransactionmanager to be available"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b31b6652a956d18687fe3a104486a54d9f02b09f", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/b31b6652a956d18687fe3a104486a54d9f02b09f", "committedDate": "2020-06-26T18:51:01Z", "message": "Add async to the job launcher, fix test to reflect this"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f6aee64b132853d72cf6a0c4036289fd6dc6fea", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/8f6aee64b132853d72cf6a0c4036289fd6dc6fea", "committedDate": "2020-06-26T19:22:23Z", "message": "Fix batch project to now use an async task launcher"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16039af110ca3cf84fb569c2a4d5a02a79a212bb", "author": {"user": {"login": "tadgh", "name": "Tadgh"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/16039af110ca3cf84fb569c2a4d5a02a79a212bb", "committedDate": "2020-06-26T20:15:31Z", "message": "Fix up samlpe starters"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NjIxOTI2", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#pullrequestreview-438621926", "createdAt": "2020-06-26T22:07:51Z", "commit": {"oid": "16039af110ca3cf84fb569c2a4d5a02a79a212bb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be23eb3cfc58f72df1822ee5fe00f9affa348cd7", "author": {"user": {"login": "jamesagnew", "name": "James Agnew"}}, "url": "https://github.com/hapifhir/hapi-fhir/commit/be23eb3cfc58f72df1822ee5fe00f9affa348cd7", "committedDate": "2020-06-27T22:06:37Z", "message": "Move the flush into the existing transaction"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3942, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}