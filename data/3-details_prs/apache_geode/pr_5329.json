{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxOTE4MzA5", "number": 5329, "title": "GEODE-8029: Allow OplogEntryIdSet to Overflow", "bodyText": "Do not delete drf files during member startup as that should be only\ndone by the compactor thread. Instead, allow the OplogEntryIdSet to\ngrow over the default capacity and log a warning message instructing\nthe user to manually compact the disk-stores.\n\nAdded unit tests.\nReplaced usages of 'junit.Assert' by 'assertj'.\nModified DiskStoreImpl.deadRecordCount to return long instead of int.\nAdded internal overflow implementation to the OplogEntryIdSet so it can\ngrow above the default limit.\n\nThank you for submitting a contribution to Apache Geode.\nIn order to streamline the review of the contribution we ask you\nto ensure the following steps have been taken:\nFor all changes:\n\n\n Is there a JIRA ticket associated with this PR? Is it referenced in the commit message?\n\n\n Has your PR been rebased against the latest commit within the target branch (typically develop)?\n\n\n Is your initial contribution a single, squashed commit?\n\n\n Does gradlew build run cleanly?\n\n\n Have you written or updated unit tests to verify your changes?\n\n\n If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under ASF 2.0?\n\n\nNote:\nPlease ensure that once the PR is submitted, check Concourse for build issues and\nsubmit an update to your PR as soon as possible. If you need help, please send an\nemail to dev@geode.apache.org.", "createdAt": "2020-06-30T09:58:32Z", "url": "https://github.com/apache/geode/pull/5329", "merged": true, "mergeCommit": {"oid": "fdc440131f0d562d97f2340d2e7ba5aacf935d62"}, "closed": true, "closedAt": "2020-07-02T12:37:15Z", "author": {"login": "jujoramos"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwSixTgH2gAyNDQxOTE4MzA5OjNhNTllNGY4YmNhNTM2NjEwNDI4MTM0YjgyZDA4MTZhMTFiY2VlMGY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcw8Z5LAH2gAyNDQxOTE4MzA5OjYwNDM0NmM3Yzg5ZDJkY2NiZjg4ZDJjYzY1MzQzODE4OTEyZGUzNWU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f", "author": {"user": {"login": "jujoramos", "name": "Juan Jos\u00e9 Ramos"}}, "url": "https://github.com/apache/geode/commit/3a59e4f8bca536610428134b82d0816a11bcee0f", "committedDate": "2020-06-30T09:50:43Z", "message": "GEODE-8029: Allow OplogEntryIdSet to Overflow\n\nDo not delete drf files during member startup as that should be only\ndone by the compactor thread. Instead, allow the OplogEntryIdSet to\ngrow over the default capacity and log a warning message instructing\nthe user to manually compact the disk-stores.\n\n- Added unit tests.\n- Replaced usages of 'junit.Assert' by 'assertj'.\n- Modified DiskStoreImpl.deadRecordCount to return long instead of int.\n- Added internal overflow implementation to the OplogEntryIdSet so it can\n  grow above the default limit."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMDczMzU1", "url": "https://github.com/apache/geode/pull/5329#pullrequestreview-441073355", "createdAt": "2020-07-01T17:40:06Z", "commit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo0MDowNlrOGrva1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo0MDowNlrOGrva1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxODg2OQ==", "bodyText": "change \"my\" to \"may\"", "url": "https://github.com/apache/geode/pull/5329#discussion_r448518869", "createdAt": "2020-07-01T17:40:06Z", "author": {"login": "dschneider-pivotal"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java", "diffHunk": "@@ -937,17 +937,11 @@ void initAfterRecovery(boolean offline) {\n         // this.crf.raf.seek(this.crf.currSize);\n       } else if (!offline) {\n         // drf exists but crf has been deleted (because it was empty).\n+        // I don't think the drf needs to be opened. It is only used during\n+        // recovery.\n+        // At some point the compacter my identify that it can be deleted.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMTI1OTA5", "url": "https://github.com/apache/geode/pull/5329#pullrequestreview-441125909", "createdAt": "2020-07-01T19:03:15Z", "commit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTowMzoxNlrOGrx88w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTo0MDo0N1rOGrzApg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ==", "bodyText": "\"Too many entries\", this is referring to deleted entries, right? In that case how about changing it\nto \"large number of deleted entries\"?", "url": "https://github.com/apache/geode/pull/5329#discussion_r448560371", "createdAt": "2020-07-01T19:03:16Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDkwMg==", "bodyText": "Do we need to pass \"illegalArgumentExcpetion\" to warning message? It may not be useful for end user.", "url": "https://github.com/apache/geode/pull/5329#discussion_r448560902", "createdAt": "2020-07-01T19:04:26Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg==", "bodyText": "@dschneider-pivotal\nIs there a scenario where the same ID is added many times? In that case, earlier we had only one id set, and there was no chance of duplicate. Now with the new approach, the same ID could be present in multiple id sets.\nOne of the caller of add is \"offlineCompact()\" its iterating over live entries and calling add...Will there be two live entries with the same id?", "url": "https://github.com/apache/geode/pull/5329#discussion_r448577702", "createdAt": "2020-07-01T19:40:47Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",\n+            illegalArgumentException);\n+\n+        // Overflow to the next [Int|Long]OpenHashSet and continue.\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMTg2MDg4", "url": "https://github.com/apache/geode/pull/5329#pullrequestreview-441186088", "createdAt": "2020-07-01T20:47:22Z", "commit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "604346c7c89d2dccbf88d2cc65343818912de35e", "author": {"user": {"login": "jujoramos", "name": "Juan Jos\u00e9 Ramos"}}, "url": "https://github.com/apache/geode/commit/604346c7c89d2dccbf88d2cc65343818912de35e", "committedDate": "2020-07-02T10:37:02Z", "message": "- Changes requested by reviewers."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4343, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}