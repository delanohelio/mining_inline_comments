{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxOTE4MzA5", "number": 5329, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo0MDowNlrOEKmmoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTo0MDo0N1rOEKo2Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NTUzNjk2OnYy", "diffSide": "RIGHT", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo0MDowNlrOGrva1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxMDoxNDozMFrOGsGkFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxODg2OQ==", "bodyText": "change \"my\" to \"may\"", "url": "https://github.com/apache/geode/pull/5329#discussion_r448518869", "createdAt": "2020-07-01T17:40:06Z", "author": {"login": "dschneider-pivotal"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java", "diffHunk": "@@ -937,17 +937,11 @@ void initAfterRecovery(boolean offline) {\n         // this.crf.raf.seek(this.crf.currSize);\n       } else if (!offline) {\n         // drf exists but crf has been deleted (because it was empty).\n+        // I don't think the drf needs to be opened. It is only used during\n+        // recovery.\n+        // At some point the compacter my identify that it can be deleted.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5ODA2OQ==", "bodyText": "Thanks @dschneider-pivotal!. I'll fix the type and create another ticket for the long term solution.", "url": "https://github.com/apache/geode/pull/5329#discussion_r448898069", "createdAt": "2020-07-02T10:14:30Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java", "diffHunk": "@@ -937,17 +937,11 @@ void initAfterRecovery(boolean offline) {\n         // this.crf.raf.seek(this.crf.currSize);\n       } else if (!offline) {\n         // drf exists but crf has been deleted (because it was empty).\n+        // I don't think the drf needs to be opened. It is only used during\n+        // recovery.\n+        // At some point the compacter my identify that it can be deleted.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxODg2OQ=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NTc5NDQ1OnYy", "diffSide": "RIGHT", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTowMzoxNlrOGrx88w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxMDoxNDo1N1rOGsGlEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ==", "bodyText": "\"Too many entries\", this is referring to deleted entries, right? In that case how about changing it\nto \"large number of deleted entries\"?", "url": "https://github.com/apache/geode/pull/5329#discussion_r448560371", "createdAt": "2020-07-01T19:03:16Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDkwMg==", "bodyText": "Do we need to pass \"illegalArgumentExcpetion\" to warning message? It may not be useful for end user.", "url": "https://github.com/apache/geode/pull/5329#discussion_r448560902", "createdAt": "2020-07-01T19:04:26Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5ODMyMA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/geode/pull/5329#discussion_r448898320", "createdAt": "2020-07-02T10:14:57Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NTkwNDIzOnYy", "diffSide": "RIGHT", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTo0MDo0N1rOGrzApg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNzo0OTo0NFrOGsXsYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg==", "bodyText": "@dschneider-pivotal\nIs there a scenario where the same ID is added many times? In that case, earlier we had only one id set, and there was no chance of duplicate. Now with the new approach, the same ID could be present in multiple id sets.\nOne of the caller of add is \"offlineCompact()\" its iterating over live entries and calling add...Will there be two live entries with the same id?", "url": "https://github.com/apache/geode/pull/5329#discussion_r448577702", "createdAt": "2020-07-01T19:40:47Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",\n+            illegalArgumentException);\n+\n+        // Overflow to the next [Int|Long]OpenHashSet and continue.\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODYxNTIzOQ==", "bodyText": "I don't think so. The ids are unique and should only be deleted once from a drf. If this did happen it would just cause the size to be larger than it should be. So you could review the code that uses the size of this class.\nYou could prevent this from having each add first call contains which shouldn't slow down the add too much.", "url": "https://github.com/apache/geode/pull/5329#discussion_r448615239", "createdAt": "2020-07-01T21:04:10Z", "author": {"login": "dschneider-pivotal"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",\n+            illegalArgumentException);\n+\n+        // Overflow to the next [Int|Long]OpenHashSet and continue.\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODkwNzYxMQ==", "bodyText": "@agingade: I think we're good on this regard, ids are unique and the actual size of the OplogEntryIdSet is only used by offline-validate / offline-compact commands. That is, if we ever report a bigger number for deadRecordCount (set through OplogEntryIdSet.size()) because there are duplicated ids (shouldn't happen), the user would know that the disk-store should be compacted anyways.\nThe real problem would happen if we ever report there are not compact-able entries when there actually are, which is not the case here.", "url": "https://github.com/apache/geode/pull/5329#discussion_r448907611", "createdAt": "2020-07-02T10:33:00Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",\n+            illegalArgumentException);\n+\n+        // Overflow to the next [Int|Long]OpenHashSet and continue.\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3ODcyMg==", "bodyText": "Thanks for the clarification.", "url": "https://github.com/apache/geode/pull/5329#discussion_r449178722", "createdAt": "2020-07-02T17:49:44Z", "author": {"login": "agingade"}, "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",\n+            illegalArgumentException);\n+\n+        // Overflow to the next [Int|Long]OpenHashSet and continue.\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg=="}, "originalCommit": {"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f"}, "originalPosition": 73}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3896, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}