{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA3NTk0Njc3", "number": 5647, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNTo0MTo0MFrOEwUS3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNTo0MzoxNVrOEwUWNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MDk5NjE1OnYy", "diffSide": "RIGHT", "path": "geode-redis/src/distributedTest/java/org/apache/geode/redis/internal/executor/hash/HdelDUnitTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNTo0MTo0MFrOHlzFIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOToyNzoxMVrOHmAAvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM5NjI1OA==", "bodyText": "Since you'll have to re-trigger anyway, the name could be changed to correct some grammatical errors/fit in with the test name below it: testConcurrentHDel_returnsExpectedNumberOfDeletions", "url": "https://github.com/apache/geode/pull/5647#discussion_r509396258", "createdAt": "2020-10-21T15:41:40Z", "author": {"login": "sabbey37"}, "path": "geode-redis/src/distributedTest/java/org/apache/geode/redis/internal/executor/hash/HdelDUnitTest.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.geode.redis.internal.executor.hash;\n+\n+import static org.apache.geode.distributed.ConfigurationProperties.REDIS_PORT;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import io.lettuce.core.ClientOptions;\n+import io.lettuce.core.RedisClient;\n+import io.lettuce.core.api.StatefulRedisConnection;\n+import io.lettuce.core.api.sync.RedisCommands;\n+import io.lettuce.core.resource.ClientResources;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+\n+import org.apache.geode.internal.AvailablePortHelper;\n+import org.apache.geode.redis.ConcurrentLoopingThreads;\n+import org.apache.geode.redis.session.springRedisTestApplication.config.DUnitSocketAddressResolver;\n+import org.apache.geode.test.dunit.rules.MemberVM;\n+import org.apache.geode.test.dunit.rules.RedisClusterStartupRule;\n+import org.apache.geode.test.junit.rules.ExecutorServiceRule;\n+\n+public class HdelDUnitTest {\n+\n+  @ClassRule\n+  public static RedisClusterStartupRule cluster = new RedisClusterStartupRule();\n+\n+  @ClassRule\n+  public static ExecutorServiceRule executor = new ExecutorServiceRule();\n+\n+  private static final int HASH_SIZE = 50000;\n+  private static MemberVM locator;\n+  private static MemberVM server1;\n+  private static MemberVM server2;\n+  private static int[] redisPorts;\n+  private static RedisCommands<String, String> lettuce;\n+  private static StatefulRedisConnection<String, String> connection;\n+  private static ClientResources resources;\n+\n+  @BeforeClass\n+  public static void classSetup() {\n+    redisPorts = AvailablePortHelper.getRandomAvailableTCPPorts(3);\n+\n+    String redisPort1 = \"\" + redisPorts[0];\n+    String redisPort2 = \"\" + redisPorts[1];\n+\n+    locator = cluster.startLocatorVM(0);\n+\n+    server1 = startRedisVM(1, redisPorts[0]);\n+    server2 = startRedisVM(2, redisPorts[1]);\n+\n+    DUnitSocketAddressResolver dnsResolver =\n+        new DUnitSocketAddressResolver(new String[] {redisPort2, redisPort1});\n+\n+    resources = ClientResources.builder()\n+        .socketAddressResolver(dnsResolver)\n+        .build();\n+\n+    RedisClient redisClient = RedisClient.create(resources, \"redis://localhost\");\n+    redisClient.setOptions(ClientOptions.builder()\n+        .autoReconnect(true)\n+        .build());\n+    connection = redisClient.connect();\n+    lettuce = connection.sync();\n+  }\n+\n+  private static MemberVM startRedisVM(int vmID, int redisPort) {\n+    int locatorPort = locator.getPort();\n+\n+    return cluster.startRedisVM(vmID, x -> x\n+        .withConnectionToLocator(locatorPort)\n+        .withProperty(REDIS_PORT, \"\" + redisPort));\n+  }\n+\n+  @Before\n+  public void testSetup() {\n+    lettuce.flushall();\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+    resources.shutdown().get();\n+    connection.close();\n+    server1.stop();\n+    server2.stop();\n+  }\n+\n+  @Test\n+  public void testConcurrentHDelReturnExceptedNumberOfDeletions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d091b7357c95589020b96e9e2f30a9967c538b"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwODEyNA==", "bodyText": "Thanks! Done.", "url": "https://github.com/apache/geode/pull/5647#discussion_r509608124", "createdAt": "2020-10-21T19:27:11Z", "author": {"login": "jdeppe-pivotal"}, "path": "geode-redis/src/distributedTest/java/org/apache/geode/redis/internal/executor/hash/HdelDUnitTest.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.geode.redis.internal.executor.hash;\n+\n+import static org.apache.geode.distributed.ConfigurationProperties.REDIS_PORT;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import io.lettuce.core.ClientOptions;\n+import io.lettuce.core.RedisClient;\n+import io.lettuce.core.api.StatefulRedisConnection;\n+import io.lettuce.core.api.sync.RedisCommands;\n+import io.lettuce.core.resource.ClientResources;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+\n+import org.apache.geode.internal.AvailablePortHelper;\n+import org.apache.geode.redis.ConcurrentLoopingThreads;\n+import org.apache.geode.redis.session.springRedisTestApplication.config.DUnitSocketAddressResolver;\n+import org.apache.geode.test.dunit.rules.MemberVM;\n+import org.apache.geode.test.dunit.rules.RedisClusterStartupRule;\n+import org.apache.geode.test.junit.rules.ExecutorServiceRule;\n+\n+public class HdelDUnitTest {\n+\n+  @ClassRule\n+  public static RedisClusterStartupRule cluster = new RedisClusterStartupRule();\n+\n+  @ClassRule\n+  public static ExecutorServiceRule executor = new ExecutorServiceRule();\n+\n+  private static final int HASH_SIZE = 50000;\n+  private static MemberVM locator;\n+  private static MemberVM server1;\n+  private static MemberVM server2;\n+  private static int[] redisPorts;\n+  private static RedisCommands<String, String> lettuce;\n+  private static StatefulRedisConnection<String, String> connection;\n+  private static ClientResources resources;\n+\n+  @BeforeClass\n+  public static void classSetup() {\n+    redisPorts = AvailablePortHelper.getRandomAvailableTCPPorts(3);\n+\n+    String redisPort1 = \"\" + redisPorts[0];\n+    String redisPort2 = \"\" + redisPorts[1];\n+\n+    locator = cluster.startLocatorVM(0);\n+\n+    server1 = startRedisVM(1, redisPorts[0]);\n+    server2 = startRedisVM(2, redisPorts[1]);\n+\n+    DUnitSocketAddressResolver dnsResolver =\n+        new DUnitSocketAddressResolver(new String[] {redisPort2, redisPort1});\n+\n+    resources = ClientResources.builder()\n+        .socketAddressResolver(dnsResolver)\n+        .build();\n+\n+    RedisClient redisClient = RedisClient.create(resources, \"redis://localhost\");\n+    redisClient.setOptions(ClientOptions.builder()\n+        .autoReconnect(true)\n+        .build());\n+    connection = redisClient.connect();\n+    lettuce = connection.sync();\n+  }\n+\n+  private static MemberVM startRedisVM(int vmID, int redisPort) {\n+    int locatorPort = locator.getPort();\n+\n+    return cluster.startRedisVM(vmID, x -> x\n+        .withConnectionToLocator(locatorPort)\n+        .withProperty(REDIS_PORT, \"\" + redisPort));\n+  }\n+\n+  @Before\n+  public void testSetup() {\n+    lettuce.flushall();\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+    resources.shutdown().get();\n+    connection.close();\n+    server1.stop();\n+    server2.stop();\n+  }\n+\n+  @Test\n+  public void testConcurrentHDelReturnExceptedNumberOfDeletions() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM5NjI1OA=="}, "originalCommit": {"oid": "64d091b7357c95589020b96e9e2f30a9967c538b"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTAwNDcxOnYy", "diffSide": "RIGHT", "path": "geode-redis/src/distributedTest/java/org/apache/geode/redis/internal/executor/hash/HdelDUnitTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNTo0MzoxNVrOHlzKVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOToyNzoxOFrOHmABCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM5NzU4OQ==", "bodyText": "Since you have to re-trigger anyway, you could change the name to HDel instead of Del and better fit with the test name above: testConcurrentHDel_whenServerCrashesAndRestarts_deletesAllHashFieldsAndValues", "url": "https://github.com/apache/geode/pull/5647#discussion_r509397589", "createdAt": "2020-10-21T15:43:15Z", "author": {"login": "sabbey37"}, "path": "geode-redis/src/distributedTest/java/org/apache/geode/redis/internal/executor/hash/HdelDUnitTest.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.geode.redis.internal.executor.hash;\n+\n+import static org.apache.geode.distributed.ConfigurationProperties.REDIS_PORT;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import io.lettuce.core.ClientOptions;\n+import io.lettuce.core.RedisClient;\n+import io.lettuce.core.api.StatefulRedisConnection;\n+import io.lettuce.core.api.sync.RedisCommands;\n+import io.lettuce.core.resource.ClientResources;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+\n+import org.apache.geode.internal.AvailablePortHelper;\n+import org.apache.geode.redis.ConcurrentLoopingThreads;\n+import org.apache.geode.redis.session.springRedisTestApplication.config.DUnitSocketAddressResolver;\n+import org.apache.geode.test.dunit.rules.MemberVM;\n+import org.apache.geode.test.dunit.rules.RedisClusterStartupRule;\n+import org.apache.geode.test.junit.rules.ExecutorServiceRule;\n+\n+public class HdelDUnitTest {\n+\n+  @ClassRule\n+  public static RedisClusterStartupRule cluster = new RedisClusterStartupRule();\n+\n+  @ClassRule\n+  public static ExecutorServiceRule executor = new ExecutorServiceRule();\n+\n+  private static final int HASH_SIZE = 50000;\n+  private static MemberVM locator;\n+  private static MemberVM server1;\n+  private static MemberVM server2;\n+  private static int[] redisPorts;\n+  private static RedisCommands<String, String> lettuce;\n+  private static StatefulRedisConnection<String, String> connection;\n+  private static ClientResources resources;\n+\n+  @BeforeClass\n+  public static void classSetup() {\n+    redisPorts = AvailablePortHelper.getRandomAvailableTCPPorts(3);\n+\n+    String redisPort1 = \"\" + redisPorts[0];\n+    String redisPort2 = \"\" + redisPorts[1];\n+\n+    locator = cluster.startLocatorVM(0);\n+\n+    server1 = startRedisVM(1, redisPorts[0]);\n+    server2 = startRedisVM(2, redisPorts[1]);\n+\n+    DUnitSocketAddressResolver dnsResolver =\n+        new DUnitSocketAddressResolver(new String[] {redisPort2, redisPort1});\n+\n+    resources = ClientResources.builder()\n+        .socketAddressResolver(dnsResolver)\n+        .build();\n+\n+    RedisClient redisClient = RedisClient.create(resources, \"redis://localhost\");\n+    redisClient.setOptions(ClientOptions.builder()\n+        .autoReconnect(true)\n+        .build());\n+    connection = redisClient.connect();\n+    lettuce = connection.sync();\n+  }\n+\n+  private static MemberVM startRedisVM(int vmID, int redisPort) {\n+    int locatorPort = locator.getPort();\n+\n+    return cluster.startRedisVM(vmID, x -> x\n+        .withConnectionToLocator(locatorPort)\n+        .withProperty(REDIS_PORT, \"\" + redisPort));\n+  }\n+\n+  @Before\n+  public void testSetup() {\n+    lettuce.flushall();\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+    resources.shutdown().get();\n+    connection.close();\n+    server1.stop();\n+    server2.stop();\n+  }\n+\n+  @Test\n+  public void testConcurrentHDelReturnExceptedNumberOfDeletions() {\n+    AtomicLong client1Deletes = new AtomicLong();\n+    AtomicLong client2Deletes = new AtomicLong();\n+\n+    String key = \"HSET\";\n+\n+    Map<String, String> setUpData =\n+        makeHashMap(HASH_SIZE, \"field\", \"value\");\n+\n+    lettuce.hset(key, setUpData);\n+\n+    new ConcurrentLoopingThreads(HASH_SIZE,\n+        i -> {\n+          long deleted = lettuce.hdel(key, \"field\" + i, \"value\" + i);\n+          client1Deletes.addAndGet(deleted);\n+        },\n+        i -> {\n+          long deleted = lettuce.hdel(key, \"field\" + i, \"value\" + i);\n+          client2Deletes.addAndGet(deleted);\n+        })\n+            .run();\n+\n+    assertThat(client1Deletes.get() + client2Deletes.get()).isEqualTo(HASH_SIZE);\n+  }\n+\n+  @Test\n+  public void testConcurrentDel_whenServerCrashesAndRestarts() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d091b7357c95589020b96e9e2f30a9967c538b"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYwODIwMQ==", "bodyText": "Done", "url": "https://github.com/apache/geode/pull/5647#discussion_r509608201", "createdAt": "2020-10-21T19:27:18Z", "author": {"login": "jdeppe-pivotal"}, "path": "geode-redis/src/distributedTest/java/org/apache/geode/redis/internal/executor/hash/HdelDUnitTest.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package org.apache.geode.redis.internal.executor.hash;\n+\n+import static org.apache.geode.distributed.ConfigurationProperties.REDIS_PORT;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import io.lettuce.core.ClientOptions;\n+import io.lettuce.core.RedisClient;\n+import io.lettuce.core.api.StatefulRedisConnection;\n+import io.lettuce.core.api.sync.RedisCommands;\n+import io.lettuce.core.resource.ClientResources;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+\n+import org.apache.geode.internal.AvailablePortHelper;\n+import org.apache.geode.redis.ConcurrentLoopingThreads;\n+import org.apache.geode.redis.session.springRedisTestApplication.config.DUnitSocketAddressResolver;\n+import org.apache.geode.test.dunit.rules.MemberVM;\n+import org.apache.geode.test.dunit.rules.RedisClusterStartupRule;\n+import org.apache.geode.test.junit.rules.ExecutorServiceRule;\n+\n+public class HdelDUnitTest {\n+\n+  @ClassRule\n+  public static RedisClusterStartupRule cluster = new RedisClusterStartupRule();\n+\n+  @ClassRule\n+  public static ExecutorServiceRule executor = new ExecutorServiceRule();\n+\n+  private static final int HASH_SIZE = 50000;\n+  private static MemberVM locator;\n+  private static MemberVM server1;\n+  private static MemberVM server2;\n+  private static int[] redisPorts;\n+  private static RedisCommands<String, String> lettuce;\n+  private static StatefulRedisConnection<String, String> connection;\n+  private static ClientResources resources;\n+\n+  @BeforeClass\n+  public static void classSetup() {\n+    redisPorts = AvailablePortHelper.getRandomAvailableTCPPorts(3);\n+\n+    String redisPort1 = \"\" + redisPorts[0];\n+    String redisPort2 = \"\" + redisPorts[1];\n+\n+    locator = cluster.startLocatorVM(0);\n+\n+    server1 = startRedisVM(1, redisPorts[0]);\n+    server2 = startRedisVM(2, redisPorts[1]);\n+\n+    DUnitSocketAddressResolver dnsResolver =\n+        new DUnitSocketAddressResolver(new String[] {redisPort2, redisPort1});\n+\n+    resources = ClientResources.builder()\n+        .socketAddressResolver(dnsResolver)\n+        .build();\n+\n+    RedisClient redisClient = RedisClient.create(resources, \"redis://localhost\");\n+    redisClient.setOptions(ClientOptions.builder()\n+        .autoReconnect(true)\n+        .build());\n+    connection = redisClient.connect();\n+    lettuce = connection.sync();\n+  }\n+\n+  private static MemberVM startRedisVM(int vmID, int redisPort) {\n+    int locatorPort = locator.getPort();\n+\n+    return cluster.startRedisVM(vmID, x -> x\n+        .withConnectionToLocator(locatorPort)\n+        .withProperty(REDIS_PORT, \"\" + redisPort));\n+  }\n+\n+  @Before\n+  public void testSetup() {\n+    lettuce.flushall();\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+    resources.shutdown().get();\n+    connection.close();\n+    server1.stop();\n+    server2.stop();\n+  }\n+\n+  @Test\n+  public void testConcurrentHDelReturnExceptedNumberOfDeletions() {\n+    AtomicLong client1Deletes = new AtomicLong();\n+    AtomicLong client2Deletes = new AtomicLong();\n+\n+    String key = \"HSET\";\n+\n+    Map<String, String> setUpData =\n+        makeHashMap(HASH_SIZE, \"field\", \"value\");\n+\n+    lettuce.hset(key, setUpData);\n+\n+    new ConcurrentLoopingThreads(HASH_SIZE,\n+        i -> {\n+          long deleted = lettuce.hdel(key, \"field\" + i, \"value\" + i);\n+          client1Deletes.addAndGet(deleted);\n+        },\n+        i -> {\n+          long deleted = lettuce.hdel(key, \"field\" + i, \"value\" + i);\n+          client2Deletes.addAndGet(deleted);\n+        })\n+            .run();\n+\n+    assertThat(client1Deletes.get() + client2Deletes.get()).isEqualTo(HASH_SIZE);\n+  }\n+\n+  @Test\n+  public void testConcurrentDel_whenServerCrashesAndRestarts() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM5NzU4OQ=="}, "originalCommit": {"oid": "64d091b7357c95589020b96e9e2f30a9967c538b"}, "originalPosition": 135}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4562, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}