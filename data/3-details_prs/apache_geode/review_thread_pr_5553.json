{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzMTgxNzk4", "number": 5553, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMTozOTo1NVrOEpo30w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMTozOTo1NVrOEpo30w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDk2NzIzOnYy", "diffSide": "RIGHT", "path": "geode-lucene/src/main/java/org/apache/geode/cache/lucene/internal/IndexRepositoryFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMTozOTo1NVrOHbbqXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxOTozMDozN1rOHb5RbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUyNjgxNQ==", "bodyText": "Do you think the number of retries is enough?\nBased on the original ticket description, the IOException thrown is caused by \"LuceneEventListener is asynchronously updating the fileAndChunkRegion\". Do we know if the wait is enough for the updating to finish? Is it a problem if IndexWriter creation needs to wait longer for the resources to be freed?\nDo we know \"updating the fileAndChunkRegion\" usually do not require a minute to finish, or we actually hit this issue due to different threads keep updating the fileAndChunkRegion? If so, we can decide the number of attempts and whether the wait needs to be using different intervals.\nIf we do not know answers for these, I think this code change is fine to fix the StackOverflowError.", "url": "https://github.com/apache/geode/pull/5553#discussion_r498526815", "createdAt": "2020-10-01T21:39:55Z", "author": {"login": "pivotal-eshu"}, "path": "geode-lucene/src/main/java/org/apache/geode/cache/lucene/internal/IndexRepositoryFactory.java", "diffHunk": "@@ -44,6 +44,7 @@\n   private static final Logger logger = LogService.getLogger();\n   public static final String FILE_REGION_LOCK_FOR_BUCKET_ID = \"FileRegionLockForBucketId:\";\n   public static final String APACHE_GEODE_INDEX_COMPLETE = \"APACHE_GEODE_INDEX_COMPLETE\";\n+  protected static final int GET_INDEX_WRITER_MAX_ATTEMPTS = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c54594d47bf8a1fd19114e18cd73ef3ba5e789bb"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxMTk0OQ==", "bodyText": "As I understand it, the timing window to hit the IOException is quite small and difficult to hit, since this problem only shows up in about 1 in 1000 runs of the test I used to diagnose the issue. If the fileAndChunkRegion was unavailable for a long period of time, I would expect to see this issue reproduce more often. After running some experiments, I was able to increase the number of retries to 200 without any noticeable negative effects, which would increase the time window during which IOExceptions would have to be consistently encountered and an exception thrown to 1 second, which should help reduce the chances of encountering it. However, I don't think it's possible to know for certain how long the fileAndChunkRegion might be unavailable, since that could change based on the operation being used on it, the size of the region, current system resources etc.", "url": "https://github.com/apache/geode/pull/5553#discussion_r499011949", "createdAt": "2020-10-02T19:30:37Z", "author": {"login": "DonalEvans"}, "path": "geode-lucene/src/main/java/org/apache/geode/cache/lucene/internal/IndexRepositoryFactory.java", "diffHunk": "@@ -44,6 +44,7 @@\n   private static final Logger logger = LogService.getLogger();\n   public static final String FILE_REGION_LOCK_FOR_BUCKET_ID = \"FileRegionLockForBucketId:\";\n   public static final String APACHE_GEODE_INDEX_COMPLETE = \"APACHE_GEODE_INDEX_COMPLETE\";\n+  protected static final int GET_INDEX_WRITER_MAX_ATTEMPTS = 10;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUyNjgxNQ=="}, "originalCommit": {"oid": "c54594d47bf8a1fd19114e18cd73ef3ba5e789bb"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4645, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}