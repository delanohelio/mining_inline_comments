{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1MDY3OTI0", "number": 4970, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxODoxODo1NlrODz3DNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQyMjozODoxNFrOD0d_bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NzA1OTA5OnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxODoxODo1NlrOGIhkEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODoyODozN1rOGI5XAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5MTY5Nw==", "bodyText": "Is this comment entirely correct? I'm not sure I see where auto-reconnects are being cancelled.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411591697", "createdAt": "2020-04-20T18:18:56Z", "author": {"login": "DonalEvans"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk4MTU2OQ==", "bodyText": "Good catch!. I added the \"disable auto-reconnect\" part when developing the tests but I realised I actually want the member to reconnect afterwards, forgot to update the comments. Thanks.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411981569", "createdAt": "2020-04-21T08:28:37Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5MTY5Nw=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 293}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NzMwMTEyOnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxOToxOToxOVrOGIj0Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODozNDozNlrOGI5nfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTYyODYzMA==", "bodyText": "Is it possible that due to slow execution somewhere, enough time passes before the region is fully cleared that some expiration tasks trigger? Perhaps you could set the expiration timeout for this test to a larger value (possibly just larger than the default test timeout), so that we know that the test will either not expire anything, or timeout because it's taking too long, with no chance of a weird middle ground where expiry might be triggered.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411628630", "createdAt": "2020-04-20T19:19:19Z", "author": {"login": "DonalEvans"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 328}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk4NTc4OQ==", "bodyText": "I've set up 30 seconds as the expiration time so we should be fine, clear for this particular region (small) shouldn't take that long... will increase the timeout anyway, as you stated, it will reduce the chances of flakiness. Thanks for catching this.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411985789", "createdAt": "2020-04-21T08:34:36Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTYyODYzMA=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 328}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NzQ0Njk2OnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxOTo1Njo1MFrOGIlK9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODozNzo0MFrOGI5wig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MDgwNA==", "bodyText": "The wording here is a little confusing. Saying that the expiration tasks were executed might imply that they actually evicted/invalidated entries, rather than just expiring due to no entry being present. Perhaps saying that the tasks were not cancelled, but instead expired, would make things clearer?", "url": "https://github.com/apache/geode/pull/4970#discussion_r411650804", "createdAt": "2020-04-20T19:56:50Z", "author": {"login": "DonalEvans"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed.\n+    asList(server1, server2).forEach(vm -> vm.invoke(() -> {\n+      ExpirationListener listener = (ExpirationListener) EntryExpiryTask.expiryTaskListener;\n+      assertThat(listener.tasksRan.get()).isEqualTo(0);\n+      assertThat(listener.tasksCanceled.get()).isEqualTo(listener.tasksScheduled.get());\n+\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions\n+          .forEach(bucketRegion -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue());\n+    }));\n+\n+    // Assert Region Buckets are consistent and region is empty,\n+    accessor.invoke(this::assertRegionBucketsConsistency);\n+    assertRegionIsEmpty(asList(accessor, server1, server1));\n+  }\n+\n+  /**\n+   * The test does the following (expiration action is parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Sets the {@link MemberKiller} as a {@link CacheWriter} to stop the coordinator VM while the\n+   * clear is in progress.\n+   * - Clears the Partition Region (at this point the coordinator is restarted).\n+   * - Asserts that, after the clear is finished and the expiration time is reached:\n+   * . No expiration tasks were cancelled.\n+   * . All entries were removed due to the expiration.\n+   * . The Partition Region Buckets are consistent on all members.\n+   */\n+  @Test\n+  @TestCaseName(\"[{index}] {method}(RegionType:{0}, ExpirationAction:{1})\")\n+  @Parameters(method = \"regionTypesAndExpirationActions\")\n+  public void clearShouldFailWhenCoordinatorMemberIsBouncedAndExpirationTasksShouldSurvive(\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 1000;\n+    ExpirationAttributes expirationAttributes =\n+        new ExpirationAttributes(EXPIRATION_TIME, expirationAction);\n+    parametrizedSetup(regionShortcut, expirationAttributes);\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+    registerVMKillerAsCacheWriter(Collections.singletonList(server1));\n+\n+    // Clear the region (it should fail).\n+    server1.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      assertThatThrownBy(region::clear)\n+          .isInstanceOf(DistributedSystemDisconnectedException.class)\n+          .hasCauseInstanceOf(ForcedDisconnectException.class);\n+    });\n+\n+    // Wait for member to get back online and assign all buckets.\n+    server1.invoke(() -> {\n+      cacheRule.createCache();\n+      initDataStore(regionShortcut, expirationAttributes);\n+      await().untilAsserted(\n+          () -> assertThat(InternalDistributedSystem.getConnectedInstance()).isNotNull());\n+      PartitionRegionHelper.assignBucketsToPartitions(cacheRule.getCache().getRegion(REGION_NAME));\n+    });\n+\n+    // Wait until all expiration tasks are executed.\n+    asList(server1, server2).forEach(vm -> vm.invoke(() -> {\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions.forEach(bucketRegion -> await()\n+          .untilAsserted(() -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue()));\n+    }));\n+\n+    // At this point the entries should be either invalidated or destroyed (expiration tasks ran).\n+    asList(accessor, server1, server2).forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entries).forEach(i -> {\n+        String key = String.valueOf(i);\n+        assertThat(region.get(key)).isNull();\n+      });\n+    }));\n+\n+    // Assert Region Buckets are consistent.\n+    accessor.invoke(this::assertRegionBucketsConsistency);\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Sets the {@link MemberKiller} as a {@link CacheWriter} to stop a non-coordinator VM while the\n+   * clear is in progress (the member has primary buckets, though, so participates on\n+   * the clear operation).\n+   * - Clears the Partition Region (at this point the non-coordinator is restarted).\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed on the non-restarted members.\n+   * . All expiration tasks were cancelled on the non-restarted members.\n+   * . Map of expiry tasks per bucket is empty on the non-restarted members.\n+   * . All expiration tasks were executed and all expired on the restarted members.\n+   * . The Partition Region is empty and buckets are consistent across all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldSucceedAndRemoveRegisteredExpirationTasksWhenNonCoordinatorMemberIsBounced(\n+      TestVM coordinatorVM, RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 1500;\n+    ExpirationAttributes expirationAttributes =\n+        new ExpirationAttributes(EXPIRATION_TIME, expirationAction);\n+    parametrizedSetup(regionShortcut, expirationAttributes);\n+    registerVMKillerAsCacheWriter(Collections.singletonList(server2));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Wait for member to get back online and assign buckets.\n+    server2.invoke(() -> {\n+      cacheRule.createCache();\n+      initDataStore(regionShortcut, expirationAttributes);\n+      await().untilAsserted(\n+          () -> assertThat(InternalDistributedSystem.getConnectedInstance()).isNotNull());\n+      PartitionRegionHelper.assignBucketsToPartitions(cacheRule.getCache().getRegion(REGION_NAME));\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed (surviving members).\n+    server1.invoke(() -> {\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions\n+          .forEach(bucketRegion -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue());\n+\n+      ExpirationListener listener = (ExpirationListener) EntryExpiryTask.expiryTaskListener;\n+      assertThat(listener.tasksRan.get()).isEqualTo(0);\n+      assertThat(listener.tasksCanceled.get()).isEqualTo(listener.tasksScheduled.get());\n+    });\n+\n+    // Assert all expiration tasks were executed and expired (restarted member).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 464}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk4ODEwNg==", "bodyText": "Done!.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411988106", "createdAt": "2020-04-21T08:37:40Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed.\n+    asList(server1, server2).forEach(vm -> vm.invoke(() -> {\n+      ExpirationListener listener = (ExpirationListener) EntryExpiryTask.expiryTaskListener;\n+      assertThat(listener.tasksRan.get()).isEqualTo(0);\n+      assertThat(listener.tasksCanceled.get()).isEqualTo(listener.tasksScheduled.get());\n+\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions\n+          .forEach(bucketRegion -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue());\n+    }));\n+\n+    // Assert Region Buckets are consistent and region is empty,\n+    accessor.invoke(this::assertRegionBucketsConsistency);\n+    assertRegionIsEmpty(asList(accessor, server1, server1));\n+  }\n+\n+  /**\n+   * The test does the following (expiration action is parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Sets the {@link MemberKiller} as a {@link CacheWriter} to stop the coordinator VM while the\n+   * clear is in progress.\n+   * - Clears the Partition Region (at this point the coordinator is restarted).\n+   * - Asserts that, after the clear is finished and the expiration time is reached:\n+   * . No expiration tasks were cancelled.\n+   * . All entries were removed due to the expiration.\n+   * . The Partition Region Buckets are consistent on all members.\n+   */\n+  @Test\n+  @TestCaseName(\"[{index}] {method}(RegionType:{0}, ExpirationAction:{1})\")\n+  @Parameters(method = \"regionTypesAndExpirationActions\")\n+  public void clearShouldFailWhenCoordinatorMemberIsBouncedAndExpirationTasksShouldSurvive(\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 1000;\n+    ExpirationAttributes expirationAttributes =\n+        new ExpirationAttributes(EXPIRATION_TIME, expirationAction);\n+    parametrizedSetup(regionShortcut, expirationAttributes);\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+    registerVMKillerAsCacheWriter(Collections.singletonList(server1));\n+\n+    // Clear the region (it should fail).\n+    server1.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      assertThatThrownBy(region::clear)\n+          .isInstanceOf(DistributedSystemDisconnectedException.class)\n+          .hasCauseInstanceOf(ForcedDisconnectException.class);\n+    });\n+\n+    // Wait for member to get back online and assign all buckets.\n+    server1.invoke(() -> {\n+      cacheRule.createCache();\n+      initDataStore(regionShortcut, expirationAttributes);\n+      await().untilAsserted(\n+          () -> assertThat(InternalDistributedSystem.getConnectedInstance()).isNotNull());\n+      PartitionRegionHelper.assignBucketsToPartitions(cacheRule.getCache().getRegion(REGION_NAME));\n+    });\n+\n+    // Wait until all expiration tasks are executed.\n+    asList(server1, server2).forEach(vm -> vm.invoke(() -> {\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions.forEach(bucketRegion -> await()\n+          .untilAsserted(() -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue()));\n+    }));\n+\n+    // At this point the entries should be either invalidated or destroyed (expiration tasks ran).\n+    asList(accessor, server1, server2).forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entries).forEach(i -> {\n+        String key = String.valueOf(i);\n+        assertThat(region.get(key)).isNull();\n+      });\n+    }));\n+\n+    // Assert Region Buckets are consistent.\n+    accessor.invoke(this::assertRegionBucketsConsistency);\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Sets the {@link MemberKiller} as a {@link CacheWriter} to stop a non-coordinator VM while the\n+   * clear is in progress (the member has primary buckets, though, so participates on\n+   * the clear operation).\n+   * - Clears the Partition Region (at this point the non-coordinator is restarted).\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed on the non-restarted members.\n+   * . All expiration tasks were cancelled on the non-restarted members.\n+   * . Map of expiry tasks per bucket is empty on the non-restarted members.\n+   * . All expiration tasks were executed and all expired on the restarted members.\n+   * . The Partition Region is empty and buckets are consistent across all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldSucceedAndRemoveRegisteredExpirationTasksWhenNonCoordinatorMemberIsBounced(\n+      TestVM coordinatorVM, RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 1500;\n+    ExpirationAttributes expirationAttributes =\n+        new ExpirationAttributes(EXPIRATION_TIME, expirationAction);\n+    parametrizedSetup(regionShortcut, expirationAttributes);\n+    registerVMKillerAsCacheWriter(Collections.singletonList(server2));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Wait for member to get back online and assign buckets.\n+    server2.invoke(() -> {\n+      cacheRule.createCache();\n+      initDataStore(regionShortcut, expirationAttributes);\n+      await().untilAsserted(\n+          () -> assertThat(InternalDistributedSystem.getConnectedInstance()).isNotNull());\n+      PartitionRegionHelper.assignBucketsToPartitions(cacheRule.getCache().getRegion(REGION_NAME));\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed (surviving members).\n+    server1.invoke(() -> {\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions\n+          .forEach(bucketRegion -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue());\n+\n+      ExpirationListener listener = (ExpirationListener) EntryExpiryTask.expiryTaskListener;\n+      assertThat(listener.tasksRan.get()).isEqualTo(0);\n+      assertThat(listener.tasksCanceled.get()).isEqualTo(listener.tasksScheduled.get());\n+    });\n+\n+    // Assert all expiration tasks were executed and expired (restarted member).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY1MDgwNA=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 464}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1Nzc2ODQ5OnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMToxODozOFrOGIoHaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODo0MzowMlrOGI6AOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY5OTA1MQ==", "bodyText": "This rule is not used.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411699051", "createdAt": "2020-04-20T21:18:38Z", "author": {"login": "jchen21"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk5MjEyMA==", "bodyText": "It's used to launch the DistributedTest VMs. See this for further details.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411992120", "createdAt": "2020-04-21T08:43:02Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY5OTA1MQ=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1Nzc3MDA4OnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMToxODo1NlrOGIoIRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODo0NTowMVrOGI6GVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY5OTI3MA==", "bodyText": "This rule is not used as well.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411699270", "createdAt": "2020-04-20T21:18:56Z", "author": {"login": "jchen21"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk5MzY4Ng==", "bodyText": "It's used to ensure all VMs use their own disk-store, specifically to avoid flakiness with the PERSISTENT regions. It works in conjunction with the DistributedRule.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411993686", "createdAt": "2020-04-21T08:45:01Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY5OTI3MA=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1Nzg2NzkxOnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMTo0NTo0OFrOGIpBFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODo0ODowOVrOGI6PfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxMzgxNQ==", "bodyText": "asList(server1, server2) should be enough.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411713815", "createdAt": "2020-04-20T21:45:48Z", "author": {"login": "jchen21"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk5NjAyOQ==", "bodyText": "The populateRegion methods uses the accessor to insert data into the region and, afterwards, it verifies that the entries are in sync in all three VMs. It's true that only server1 and server2 are needed as those are the only members hosting data, but I prefer to use all members for verification purposes (the accessor ultimately gets the data from other members, but it's harmless to execute the verification anyway).", "url": "https://github.com/apache/geode/pull/4970#discussion_r411996029", "createdAt": "2020-04-21T08:48:09Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxMzgxNQ=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 320}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NzkxMDA0OnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMTo1Nzo0OVrOGIpZFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODo0ODoxNVrOGI6PwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxOTk1OQ==", "bodyText": "asList(server1, server2) should be enough.", "url": "https://github.com/apache/geode/pull/4970#discussion_r411719959", "createdAt": "2020-04-20T21:57:49Z", "author": {"login": "jchen21"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed.\n+    asList(server1, server2).forEach(vm -> vm.invoke(() -> {\n+      ExpirationListener listener = (ExpirationListener) EntryExpiryTask.expiryTaskListener;\n+      assertThat(listener.tasksRan.get()).isEqualTo(0);\n+      assertThat(listener.tasksCanceled.get()).isEqualTo(listener.tasksScheduled.get());\n+\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions\n+          .forEach(bucketRegion -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue());\n+    }));\n+\n+    // Assert Region Buckets are consistent and region is empty,\n+    accessor.invoke(this::assertRegionBucketsConsistency);\n+    assertRegionIsEmpty(asList(accessor, server1, server1));\n+  }\n+\n+  /**\n+   * The test does the following (expiration action is parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Sets the {@link MemberKiller} as a {@link CacheWriter} to stop the coordinator VM while the\n+   * clear is in progress.\n+   * - Clears the Partition Region (at this point the coordinator is restarted).\n+   * - Asserts that, after the clear is finished and the expiration time is reached:\n+   * . No expiration tasks were cancelled.\n+   * . All entries were removed due to the expiration.\n+   * . The Partition Region Buckets are consistent on all members.\n+   */\n+  @Test\n+  @TestCaseName(\"[{index}] {method}(RegionType:{0}, ExpirationAction:{1})\")\n+  @Parameters(method = \"regionTypesAndExpirationActions\")\n+  public void clearShouldFailWhenCoordinatorMemberIsBouncedAndExpirationTasksShouldSurvive(\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 1000;\n+    ExpirationAttributes expirationAttributes =\n+        new ExpirationAttributes(EXPIRATION_TIME, expirationAction);\n+    parametrizedSetup(regionShortcut, expirationAttributes);\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 367}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk5NjA5Nw==", "bodyText": "The populateRegion methods uses the accessor to insert data into the region and, afterwards, it verifies that the entries are in sync in all three VMs. It's true that only server1 and server2 are needed as those are the only members hosting data, but I prefer to use all members for verification purposes (the accessor ultimately gets the data from other members, but it's harmless to execute the verification anyway).", "url": "https://github.com/apache/geode/pull/4970#discussion_r411996097", "createdAt": "2020-04-21T08:48:15Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,535 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license\n+ * agreements. See the NOTICE file distributed with this work for additional information regarding\n+ * copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License. You may obtain a\n+ * copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License\n+ * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n+ * or implied. See the License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+package org.apache.geode.internal.cache;\n+\n+import static org.apache.geode.cache.ExpirationAction.DESTROY;\n+import static org.apache.geode.cache.ExpirationAction.INVALIDATE;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_OVERFLOW;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT;\n+import static org.apache.geode.cache.RegionShortcut.PARTITION_REDUNDANT_PERSISTENT_OVERFLOW;\n+import static org.apache.geode.internal.util.ArrayUtils.asList;\n+import static org.apache.geode.test.awaitility.GeodeAwaitility.await;\n+import static org.apache.geode.test.dunit.VM.getVM;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import junitparams.JUnitParamsRunner;\n+import junitparams.Parameters;\n+import junitparams.naming.TestCaseName;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+\n+import org.apache.geode.ForcedDisconnectException;\n+import org.apache.geode.cache.Cache;\n+import org.apache.geode.cache.CacheWriter;\n+import org.apache.geode.cache.CacheWriterException;\n+import org.apache.geode.cache.ExpirationAction;\n+import org.apache.geode.cache.ExpirationAttributes;\n+import org.apache.geode.cache.PartitionAttributes;\n+import org.apache.geode.cache.PartitionAttributesFactory;\n+import org.apache.geode.cache.Region;\n+import org.apache.geode.cache.RegionEvent;\n+import org.apache.geode.cache.RegionShortcut;\n+import org.apache.geode.cache.partition.PartitionRegionHelper;\n+import org.apache.geode.cache.util.CacheWriterAdapter;\n+import org.apache.geode.distributed.DistributedSystemDisconnectedException;\n+import org.apache.geode.distributed.internal.DMStats;\n+import org.apache.geode.distributed.internal.InternalDistributedSystem;\n+import org.apache.geode.distributed.internal.membership.api.MembershipManagerHelper;\n+import org.apache.geode.test.dunit.VM;\n+import org.apache.geode.test.dunit.rules.CacheRule;\n+import org.apache.geode.test.dunit.rules.DistributedDiskDirRule;\n+import org.apache.geode.test.dunit.rules.DistributedRule;\n+\n+/**\n+ * Tests to verify that {@link PartitionedRegion#clear()} cancels all remaining expiration tasks\n+ * on the {@link PartitionedRegion} once the operation is executed.\n+ */\n+@RunWith(JUnitParamsRunner.class)\n+public class PartitionedRegionClearWithExpirationDUnitTest implements Serializable {\n+  private static final Integer BUCKETS = 13;\n+  private static final Integer EXPIRATION_TIME = 30;\n+  private static final String REGION_NAME = \"PartitionedRegion\";\n+  private static final String TEST_CASE_NAME =\n+      \"[{index}] {method}(Coordinator:{0}, RegionType:{1}, ExpirationAction:{2})\";\n+\n+  @Rule\n+  public DistributedRule distributedRule = new DistributedRule(3);\n+\n+  @Rule\n+  public CacheRule cacheRule = CacheRule.builder().createCacheInAll().build();\n+\n+  @Rule\n+  public DistributedDiskDirRule distributedDiskDirRule = new DistributedDiskDirRule();\n+\n+  private VM accessor, server1, server2;\n+\n+  private enum TestVM {\n+    ACCESSOR(0), SERVER1(1), SERVER2(2);\n+\n+    final int vmNumber;\n+\n+    TestVM(int vmNumber) {\n+      this.vmNumber = vmNumber;\n+    }\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static RegionShortcut[] regionTypes() {\n+    return new RegionShortcut[] {\n+        PARTITION,\n+        PARTITION_OVERFLOW,\n+        PARTITION_REDUNDANT,\n+        PARTITION_REDUNDANT_OVERFLOW,\n+\n+        PARTITION_PERSISTENT,\n+        PARTITION_PERSISTENT_OVERFLOW,\n+        PARTITION_REDUNDANT_PERSISTENT,\n+        PARTITION_REDUNDANT_PERSISTENT_OVERFLOW\n+    };\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] regionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {regionShortcut, DESTROY});\n+      parameters.add(new Object[] {regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  static Object[] vmsRegionTypesAndExpirationActions() {\n+    ArrayList<Object[]> parameters = new ArrayList<>();\n+    RegionShortcut[] regionShortcuts = regionTypes();\n+\n+    Arrays.stream(regionShortcuts).forEach(regionShortcut -> {\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.SERVER1, regionShortcut, INVALIDATE});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, DESTROY});\n+      parameters.add(new Object[] {TestVM.ACCESSOR, regionShortcut, INVALIDATE});\n+    });\n+\n+    return parameters.toArray();\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    server1 = getVM(TestVM.SERVER1.vmNumber);\n+    server2 = getVM(TestVM.SERVER2.vmNumber);\n+    accessor = getVM(TestVM.ACCESSOR.vmNumber);\n+  }\n+\n+  private RegionShortcut getRegionAccessorShortcut(RegionShortcut dataStoreRegionShortcut) {\n+    if (dataStoreRegionShortcut.isPersistent()) {\n+      switch (dataStoreRegionShortcut) {\n+        case PARTITION_PERSISTENT:\n+          return PARTITION;\n+        case PARTITION_PERSISTENT_OVERFLOW:\n+          return PARTITION_OVERFLOW;\n+        case PARTITION_REDUNDANT_PERSISTENT:\n+          return PARTITION_REDUNDANT;\n+        case PARTITION_REDUNDANT_PERSISTENT_OVERFLOW:\n+          return PARTITION_REDUNDANT_OVERFLOW;\n+      }\n+    }\n+\n+    return dataStoreRegionShortcut;\n+  }\n+\n+  private void initAccessor(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    RegionShortcut accessorShortcut = getRegionAccessorShortcut(regionShortcut);\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .setLocalMaxMemory(0)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(accessorShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+  }\n+\n+  private void initDataStore(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    PartitionAttributes<String, String> attributes =\n+        new PartitionAttributesFactory<String, String>()\n+            .setTotalNumBuckets(BUCKETS)\n+            .create();\n+\n+    cacheRule.getCache()\n+        .<String, String>createRegionFactory(regionShortcut)\n+        .setPartitionAttributes(attributes)\n+        .setEntryTimeToLive(expirationAttributes)\n+        .setEntryIdleTimeout(expirationAttributes)\n+        .create(REGION_NAME);\n+\n+    ExpiryTask.expiryTaskListener = new ExpirationListener();\n+  }\n+\n+  private void parametrizedSetup(RegionShortcut regionShortcut,\n+      ExpirationAttributes expirationAttributes) {\n+    server1.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    server2.invoke(() -> initDataStore(regionShortcut, expirationAttributes));\n+    accessor.invoke(() -> initAccessor(regionShortcut, expirationAttributes));\n+  }\n+\n+  private void waitForSilence() {\n+    DMStats dmStats = cacheRule.getSystem().getDistributionManager().getStats();\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    PartitionedRegionStats partitionedRegionStats = region.getPrStats();\n+\n+    await().untilAsserted(() -> {\n+      assertThat(dmStats.getReplyWaitsInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getVolunteeringInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getPrimaryTransfersInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalanceBucketCreatesInProgress()).isEqualTo(0);\n+      assertThat(partitionedRegionStats.getRebalancePrimaryTransfersInProgress()).isEqualTo(0);\n+    });\n+  }\n+\n+  /**\n+   * Populates the region and verifies the data on the selected VMs.\n+   */\n+  private void populateRegion(VM feeder, int entryCount, List<VM> vms) {\n+    feeder.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      IntStream.range(0, entryCount).forEach(i -> region.put(String.valueOf(i), \"Value_\" + i));\n+    });\n+\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      IntStream.range(0, entryCount)\n+          .forEach(i -> assertThat(region.get(String.valueOf(i))).isEqualTo(\"Value_\" + i));\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region is empty on requested VMs.\n+   */\n+  private void assertRegionIsEmpty(List<VM> vms) {\n+    vms.forEach(vm -> vm.invoke(() -> {\n+      waitForSilence();\n+      PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+\n+      assertThat(region.getLocalSize()).isEqualTo(0);\n+    }));\n+  }\n+\n+  /**\n+   * Asserts that the region data is consistent across buckets.\n+   */\n+  private void assertRegionBucketsConsistency() throws ForceReattemptException {\n+    waitForSilence();\n+    List<BucketDump> bucketDumps;\n+    PartitionedRegion region = (PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME);\n+    // Redundant copies + 1 primary.\n+    int expectedCopies = region.getRedundantCopies() + 1;\n+\n+    for (int bucketId = 0; bucketId < BUCKETS; bucketId++) {\n+      bucketDumps = region.getAllBucketEntries(bucketId);\n+      assertThat(bucketDumps.size()).as(\"Bucket \" + bucketId + \" should have \" + expectedCopies\n+          + \" copies, but has \" + bucketDumps.size()).isEqualTo(expectedCopies);\n+\n+      // Check that all copies of the bucket have the same data.\n+      if (bucketDumps.size() > 1) {\n+        BucketDump firstDump = bucketDumps.get(0);\n+\n+        for (int j = 1; j < bucketDumps.size(); j++) {\n+          BucketDump otherDump = bucketDumps.get(j);\n+          assertThat(otherDump.getValues())\n+              .as(\"Values for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getValues());\n+          assertThat(otherDump.getVersions())\n+              .as(\"Versions for bucket \" + bucketId + \" on member \" + otherDump.getMember()\n+                  + \" are not consistent with member \" + firstDump.getMember())\n+              .isEqualTo(firstDump.getVersions());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Register the MemberKiller CacheWriter on the given vms and cancel auto-reconnects.\n+   */\n+  private void registerVMKillerAsCacheWriter(List<VM> vmsToBounce) {\n+    vmsToBounce.forEach(vm -> vm.invoke(() -> {\n+      Region<String, String> region = cacheRule.getCache().getRegion(REGION_NAME);\n+      region.getAttributesMutator().setCacheWriter(new MemberKiller());\n+    }));\n+  }\n+\n+  /**\n+   * The test does the following (clear coordinator and expiration action are parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Clears the Partition Region once.\n+   * - Asserts that, after the clear is finished:\n+   * . No expiration tasks were executed.\n+   * . All expiration tasks were cancelled.\n+   * . Map of expiry tasks per bucket is empty.\n+   * . The Partition Region is empty on all members.\n+   */\n+  @Test\n+  @TestCaseName(TEST_CASE_NAME)\n+  @Parameters(method = \"vmsRegionTypesAndExpirationActions\")\n+  public void clearShouldRemoveRegisteredExpirationTasks(TestVM coordinatorVM,\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 500;\n+    parametrizedSetup(regionShortcut, new ExpirationAttributes(EXPIRATION_TIME, expirationAction));\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));\n+\n+    // Clear the region.\n+    getVM(coordinatorVM.vmNumber).invoke(() -> {\n+      Cache cache = cacheRule.getCache();\n+      cache.getRegion(REGION_NAME).clear();\n+    });\n+\n+    // Assert all expiration tasks were cancelled and none were executed.\n+    asList(server1, server2).forEach(vm -> vm.invoke(() -> {\n+      ExpirationListener listener = (ExpirationListener) EntryExpiryTask.expiryTaskListener;\n+      assertThat(listener.tasksRan.get()).isEqualTo(0);\n+      assertThat(listener.tasksCanceled.get()).isEqualTo(listener.tasksScheduled.get());\n+\n+      PartitionedRegionDataStore dataStore =\n+          ((PartitionedRegion) cacheRule.getCache().getRegion(REGION_NAME)).getDataStore();\n+      Set<BucketRegion> bucketRegions = dataStore.getAllLocalBucketRegions();\n+      bucketRegions\n+          .forEach(bucketRegion -> assertThat(bucketRegion.entryExpiryTasks.isEmpty()).isTrue());\n+    }));\n+\n+    // Assert Region Buckets are consistent and region is empty,\n+    accessor.invoke(this::assertRegionBucketsConsistency);\n+    assertRegionIsEmpty(asList(accessor, server1, server1));\n+  }\n+\n+  /**\n+   * The test does the following (expiration action is parametrized):\n+   * - Populates the Partition Region (entries have expiration).\n+   * - Verifies that the entries are synchronized on all members.\n+   * - Sets the {@link MemberKiller} as a {@link CacheWriter} to stop the coordinator VM while the\n+   * clear is in progress.\n+   * - Clears the Partition Region (at this point the coordinator is restarted).\n+   * - Asserts that, after the clear is finished and the expiration time is reached:\n+   * . No expiration tasks were cancelled.\n+   * . All entries were removed due to the expiration.\n+   * . The Partition Region Buckets are consistent on all members.\n+   */\n+  @Test\n+  @TestCaseName(\"[{index}] {method}(RegionType:{0}, ExpirationAction:{1})\")\n+  @Parameters(method = \"regionTypesAndExpirationActions\")\n+  public void clearShouldFailWhenCoordinatorMemberIsBouncedAndExpirationTasksShouldSurvive(\n+      RegionShortcut regionShortcut, ExpirationAction expirationAction) {\n+    final int entries = 1000;\n+    ExpirationAttributes expirationAttributes =\n+        new ExpirationAttributes(EXPIRATION_TIME, expirationAction);\n+    parametrizedSetup(regionShortcut, expirationAttributes);\n+    populateRegion(accessor, entries, asList(accessor, server1, server2));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTcxOTk1OQ=="}, "originalCommit": {"oid": "8fdb8f10f4040b61777a6e91f0ff04581b8d41cf"}, "originalPosition": 367}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU2MzQzOTE5OnYy", "diffSide": "RIGHT", "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQyMjozODoxNFrOGJbZRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwODo0ODoxNFrOGJq9Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjUzOTIwNQ==", "bodyText": "The test needs 30 minutes to finish all combination. And many tests took more than 30 seconds each.\nMany combinations are unnecessary, for example we want to see the expiration tasks are cancelled, we don't care what expiration. Only when we want to see an expiration to be triggered, then we need some (not all) expiration types. In my opinion, we can hard code to use DESTROY expiration type only in this test.\nWe don't have to verify clear from accessor or server. Some other tests have verified that. You can just use server to do clear.\nRegion types can also be reduced to a few. We have other tests  to verify that all the combination of region type can do clear successfully. We only need to verify the expiration tasks are cleared in this test.", "url": "https://github.com/apache/geode/pull/4970#discussion_r412539205", "createdAt": "2020-04-21T22:38:14Z", "author": {"login": "gesterzhou"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,537 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3b7da7ff4f7eedcf0f1d28b39f85b158a047f8a"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjc5NDE4Mg==", "bodyText": "The test needs 30 minutes to finish all combination. And many tests took more than 30 seconds each.\n\nThis is not a bad thing per se, I wanted to test all possible combinations so it's expected for the distributed test to take some time to finish. I'll remove some combinations, though, so the overall time will be reduced.\n\nMany combinations are unnecessary, for example we want to see the expiration tasks are cancelled, we don't care what expiration. Only when we want to see an expiration to be triggered, then we need some (not all) expiration types. In my opinion, we can hard code to use DESTROY expiration type only in this test.\nWe don't have to verify clear from accessor or server. Some other tests have verified that. You can just use server to do clear.\nRegion types can also be reduced to a few. We have other tests to verify that all the combination of region type can do clear successfully. We only need to verify the expiration tasks are cleared in this test.\n\nHaving tests to verify several possible combinations is better than having no tests at all, specially for the region types as we might change the implementation class in the future... if we do, this test might be able to catch possible regressions introduced,  so I'd prefer to keep them all. Regarding the ExpirationAction, I agree, will remove the INVALIDATE one and just use DESTROY.", "url": "https://github.com/apache/geode/pull/4970#discussion_r412794182", "createdAt": "2020-04-22T08:48:14Z", "author": {"login": "jujoramos"}, "path": "geode-core/src/distributedTest/java/org/apache/geode/internal/cache/PartitionedRegionClearWithExpirationDUnitTest.java", "diffHunk": "@@ -0,0 +1,537 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjUzOTIwNQ=="}, "originalCommit": {"oid": "e3b7da7ff4f7eedcf0f1d28b39f85b158a047f8a"}, "originalPosition": 1}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4335, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}