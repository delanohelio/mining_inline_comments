{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0NjIyNjA0", "number": 3717, "title": "[Spark load][Fe 6/6] Fe process etl and loading state job", "bodyText": "Fe checks the status of etl job regularly\n1.1 If status is RUNNING, update etl job progress\n1.2 If status is CANCELLED, cancel load job\n1.3 If status is FINISHED, get the etl output file paths, update job state to LOADING and log job update info\n\n\nFe sends PushTask to Be and commits transaction after all push tasks execute successfully\n\n\n#3433", "createdAt": "2020-05-28T17:14:04Z", "url": "https://github.com/apache/incubator-doris/pull/3717", "merged": true, "mergeCommit": {"oid": "a63fa882947345b7b97643f6346ff9360e210c37"}, "closed": true, "closedAt": "2020-06-21T14:17:04Z", "author": {"login": "wyb"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcmVyzVAFqTQyMTM5OTU4Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcs9aJTgFqTQzNDM5MjgwNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMzk5NTg3", "url": "https://github.com/apache/incubator-doris/pull/3717#pullrequestreview-421399587", "createdAt": "2020-05-30T11:47:19Z", "commit": {"oid": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxMTo0NzoxOVrOGcyJ7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQxMTo1Njo1MlrOGcyMQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTA1Mw==", "bodyText": "Why is this ScanNode needed?\nSeems that this scanNode will be destruct after this function.", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r432835053", "createdAt": "2020-05-30T11:47:19Z", "author": {"login": "imay"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,700 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.Analyzer;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.CastExpr;\n+import org.apache.doris.analysis.DescriptorTable;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.analysis.Expr;\n+import org.apache.doris.analysis.SlotDescriptor;\n+import org.apache.doris.analysis.SlotRef;\n+import org.apache.doris.analysis.TupleDescriptor;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Column;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.FsBroker;\n+import org.apache.doris.catalog.MaterializedIndex;\n+import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n+import org.apache.doris.catalog.OlapTable;\n+import org.apache.doris.catalog.Partition;\n+import org.apache.doris.catalog.PrimitiveType;\n+import org.apache.doris.catalog.Replica;\n+import org.apache.doris.catalog.ScalarType;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.catalog.Tablet;\n+import org.apache.doris.catalog.Type;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.EtlStatus;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.planner.PlanNodeId;\n+import org.apache.doris.planner.ScanNode;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.system.Backend;\n+import org.apache.doris.task.AgentBatchTask;\n+import org.apache.doris.task.AgentTaskExecutor;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+import org.apache.doris.thrift.TBrokerRangeDesc;\n+import org.apache.doris.thrift.TBrokerScanRange;\n+import org.apache.doris.thrift.TBrokerScanRangeParams;\n+import org.apache.doris.thrift.TDescriptorTable;\n+import org.apache.doris.thrift.TFileFormatType;\n+import org.apache.doris.thrift.TFileType;\n+import org.apache.doris.thrift.TNetworkAddress;\n+import org.apache.doris.thrift.TPlanNode;\n+import org.apache.doris.thrift.TPriority;\n+import org.apache.doris.thrift.TPushType;\n+import org.apache.doris.thrift.TScanRangeLocations;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TabletQuorumFailedException;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.gson.annotations.SerializedName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    private static class PushBrokerScannerParams {\n+        TBrokerScanRange tBrokerScanRange;\n+        TDescriptorTable tDescriptorTable;\n+\n+        public void init(List<Column> columns, BrokerDesc brokerDesc) throws UserException {\n+            Analyzer analyzer = new Analyzer(null, null);\n+            // Generate tuple descriptor\n+            DescriptorTable descTable = analyzer.getDescTbl();\n+            TupleDescriptor destTupleDesc = descTable.createTupleDescriptor();\n+            // use index schema to fill the descriptor table\n+            for (Column column : columns) {\n+                SlotDescriptor destSlotDesc = descTable.addSlotDescriptor(destTupleDesc);\n+                destSlotDesc.setIsMaterialized(true);\n+                destSlotDesc.setColumn(column);\n+                if (column.isAllowNull()) {\n+                    destSlotDesc.setIsNullable(true);\n+                } else {\n+                    destSlotDesc.setIsNullable(false);\n+                }\n+            }\n+            // Push broker scan node\n+            PushBrokerScanNode scanNode = new PushBrokerScanNode(destTupleDesc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTI5NA==", "bodyText": "When is this assigned?", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r432835294", "createdAt": "2020-05-30T11:51:04Z", "author": {"login": "imay"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,700 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.Analyzer;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.CastExpr;\n+import org.apache.doris.analysis.DescriptorTable;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.analysis.Expr;\n+import org.apache.doris.analysis.SlotDescriptor;\n+import org.apache.doris.analysis.SlotRef;\n+import org.apache.doris.analysis.TupleDescriptor;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Column;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.FsBroker;\n+import org.apache.doris.catalog.MaterializedIndex;\n+import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n+import org.apache.doris.catalog.OlapTable;\n+import org.apache.doris.catalog.Partition;\n+import org.apache.doris.catalog.PrimitiveType;\n+import org.apache.doris.catalog.Replica;\n+import org.apache.doris.catalog.ScalarType;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.catalog.Tablet;\n+import org.apache.doris.catalog.Type;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.EtlStatus;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.planner.PlanNodeId;\n+import org.apache.doris.planner.ScanNode;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.system.Backend;\n+import org.apache.doris.task.AgentBatchTask;\n+import org.apache.doris.task.AgentTaskExecutor;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+import org.apache.doris.thrift.TBrokerRangeDesc;\n+import org.apache.doris.thrift.TBrokerScanRange;\n+import org.apache.doris.thrift.TBrokerScanRangeParams;\n+import org.apache.doris.thrift.TDescriptorTable;\n+import org.apache.doris.thrift.TFileFormatType;\n+import org.apache.doris.thrift.TFileType;\n+import org.apache.doris.thrift.TNetworkAddress;\n+import org.apache.doris.thrift.TPlanNode;\n+import org.apache.doris.thrift.TPriority;\n+import org.apache.doris.thrift.TPushType;\n+import org.apache.doris.thrift.TScanRangeLocations;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TabletQuorumFailedException;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.gson.annotations.SerializedName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTY0OQ==", "bodyText": "why not get lock in the outer public function? In most scenario, there is only one writer.\nIf you put this lock in a private function, there is a case that multiple threads will do proceeEtlFinish concurrently.", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r432835649", "createdAt": "2020-05-30T11:56:52Z", "author": {"login": "imay"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,700 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.Analyzer;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.CastExpr;\n+import org.apache.doris.analysis.DescriptorTable;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.analysis.Expr;\n+import org.apache.doris.analysis.SlotDescriptor;\n+import org.apache.doris.analysis.SlotRef;\n+import org.apache.doris.analysis.TupleDescriptor;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Column;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.FsBroker;\n+import org.apache.doris.catalog.MaterializedIndex;\n+import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n+import org.apache.doris.catalog.OlapTable;\n+import org.apache.doris.catalog.Partition;\n+import org.apache.doris.catalog.PrimitiveType;\n+import org.apache.doris.catalog.Replica;\n+import org.apache.doris.catalog.ScalarType;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.catalog.Tablet;\n+import org.apache.doris.catalog.Type;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.EtlStatus;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.planner.PlanNodeId;\n+import org.apache.doris.planner.ScanNode;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.system.Backend;\n+import org.apache.doris.task.AgentBatchTask;\n+import org.apache.doris.task.AgentTaskExecutor;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+import org.apache.doris.thrift.TBrokerRangeDesc;\n+import org.apache.doris.thrift.TBrokerScanRange;\n+import org.apache.doris.thrift.TBrokerScanRangeParams;\n+import org.apache.doris.thrift.TDescriptorTable;\n+import org.apache.doris.thrift.TFileFormatType;\n+import org.apache.doris.thrift.TFileType;\n+import org.apache.doris.thrift.TNetworkAddress;\n+import org.apache.doris.thrift.TPlanNode;\n+import org.apache.doris.thrift.TPriority;\n+import org.apache.doris.thrift.TPushType;\n+import org.apache.doris.thrift.TScanRangeLocations;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TabletQuorumFailedException;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.gson.annotations.SerializedName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    private static class PushBrokerScannerParams {\n+        TBrokerScanRange tBrokerScanRange;\n+        TDescriptorTable tDescriptorTable;\n+\n+        public void init(List<Column> columns, BrokerDesc brokerDesc) throws UserException {\n+            Analyzer analyzer = new Analyzer(null, null);\n+            // Generate tuple descriptor\n+            DescriptorTable descTable = analyzer.getDescTbl();\n+            TupleDescriptor destTupleDesc = descTable.createTupleDescriptor();\n+            // use index schema to fill the descriptor table\n+            for (Column column : columns) {\n+                SlotDescriptor destSlotDesc = descTable.addSlotDescriptor(destTupleDesc);\n+                destSlotDesc.setIsMaterialized(true);\n+                destSlotDesc.setColumn(column);\n+                if (column.isAllowNull()) {\n+                    destSlotDesc.setIsNullable(true);\n+                } else {\n+                    destSlotDesc.setIsNullable(false);\n+                }\n+            }\n+            // Push broker scan node\n+            PushBrokerScanNode scanNode = new PushBrokerScanNode(destTupleDesc);\n+            scanNode.setLoadInfo(columns, brokerDesc);\n+            scanNode.init(analyzer);\n+            tBrokerScanRange = scanNode.getTBrokerScanRange();\n+\n+            // descTable\n+            descTable.computeMemLayout();\n+            tDescriptorTable = descTable.toThrift();\n+        }\n+    }\n+\n+    private static class PushBrokerScanNode extends ScanNode {\n+        private TBrokerScanRange tBrokerScanRange;\n+        private List<Column> columns;\n+        private BrokerDesc brokerDesc;\n+\n+        public PushBrokerScanNode(TupleDescriptor destTupleDesc) {\n+            super(new PlanNodeId(0), destTupleDesc, \"PushBrokerScanNode\");\n+            this.tBrokerScanRange = new TBrokerScanRange();\n+        }\n+\n+        public void setLoadInfo(List<Column> columns, BrokerDesc brokerDesc) {\n+            this.columns = columns;\n+            this.brokerDesc = brokerDesc;\n+        }\n+\n+        public void init(Analyzer analyzer) throws UserException {\n+            super.init(analyzer);\n+\n+            // scan range params\n+            TBrokerScanRangeParams params = new TBrokerScanRangeParams();\n+            params.setStrict_mode(false);\n+            params.setProperties(brokerDesc.getProperties());\n+            TupleDescriptor srcTupleDesc = analyzer.getDescTbl().createTupleDescriptor();\n+            Map<String, SlotDescriptor> srcSlotDescByName = Maps.newHashMap();\n+            for (Column column : columns) {\n+                SlotDescriptor srcSlotDesc = analyzer.getDescTbl().addSlotDescriptor(srcTupleDesc);\n+                srcSlotDesc.setType(ScalarType.createType(PrimitiveType.VARCHAR));\n+                srcSlotDesc.setIsMaterialized(true);\n+                srcSlotDesc.setIsNullable(true);\n+                srcSlotDesc.setColumn(new Column(column.getName(), PrimitiveType.VARCHAR));\n+                params.addToSrc_slot_ids(srcSlotDesc.getId().asInt());\n+                srcSlotDescByName.put(column.getName(), srcSlotDesc);\n+            }\n+\n+            TupleDescriptor destTupleDesc = desc;\n+            Map<Integer, Integer> destSidToSrcSidWithoutTrans = Maps.newHashMap();\n+            for (SlotDescriptor destSlotDesc : destTupleDesc.getSlots()) {\n+                if (!destSlotDesc.isMaterialized()) {\n+                    continue;\n+                }\n+\n+                SlotDescriptor srcSlotDesc = srcSlotDescByName.get(destSlotDesc.getColumn().getName());\n+                destSidToSrcSidWithoutTrans.put(destSlotDesc.getId().asInt(), srcSlotDesc.getId().asInt());\n+                Expr expr = new SlotRef(srcSlotDesc);\n+                if (destSlotDesc.getType().getPrimitiveType() == PrimitiveType.BOOLEAN) {\n+                    // there is no cast string to boolean function\n+                    // so we cast string to tinyint first, then cast tinyint to boolean\n+                    expr = new CastExpr(Type.BOOLEAN, new CastExpr(Type.TINYINT, expr));\n+                } else {\n+                    expr = castToSlot(destSlotDesc, expr);\n+                }\n+                params.putToExpr_of_dest_slot(destSlotDesc.getId().asInt(), expr.treeToThrift());\n+            }\n+            params.setDest_sid_to_src_sid_without_trans(destSidToSrcSidWithoutTrans);\n+            params.setSrc_tuple_id(srcTupleDesc.getId().asInt());\n+            params.setDest_tuple_id(destTupleDesc.getId().asInt());\n+            tBrokerScanRange.setParams(params);\n+\n+            // broker address updated for each replica\n+            tBrokerScanRange.setBroker_addresses(Lists.newArrayList());\n+\n+            // broker range desc\n+            TBrokerRangeDesc tBrokerRangeDesc = new TBrokerRangeDesc();\n+            tBrokerRangeDesc.setFile_type(TFileType.FILE_BROKER);\n+            tBrokerRangeDesc.setFormat_type(TFileFormatType.FORMAT_PARQUET);\n+            tBrokerRangeDesc.setSplittable(false);\n+            tBrokerRangeDesc.setStart_offset(0);\n+            tBrokerRangeDesc.setSize(-1);\n+            // path and file size updated for each replica\n+            tBrokerScanRange.setRanges(Lists.newArrayList(tBrokerRangeDesc));\n+        }\n+\n+        public TBrokerScanRange getTBrokerScanRange() {\n+            return tBrokerScanRange;\n+        }\n+\n+        @Override\n+        public List<TScanRangeLocations> getScanRangeLocations(long maxScanRangeLength) {\n+            return null;\n+        }\n+\n+        @Override\n+        protected void toThrift(TPlanNode msg) {}\n+    }\n+\n+    // only for log replay\n+    public SparkLoadJob() {\n+        super();\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public SparkLoadJob(long dbId, String label, ResourceDesc resourceDesc, OriginStatement originStmt)\n+            throws MetaNotFoundException {\n+        super(dbId, label, originStmt);\n+        this.resourceDesc = resourceDesc;\n+        timeoutSecond = Config.spark_load_default_timeout_second;\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    private boolean checkState(JobState expectState) {\n+        readLock();\n+        try {\n+            if (state == expectState) {\n+                return true;\n+            }\n+            return false;\n+        } finally {\n+            readUnlock();\n+        }\n+    }\n+\n+    public void updateEtlStatus() throws Exception {\n+        if (!checkState(JobState.ETL)) {\n+            return;\n+        }\n+\n+        // get etl status\n+        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n+        EtlStatus status = handler.getEtlJobStatus(sparkAppHandle, appId, id, etlOutputPath, sparkResource, brokerDesc);\n+        switch (status.getState()) {\n+            case RUNNING:\n+                updateEtlStatusInternal(status);\n+                break;\n+            case FINISHED:\n+                processEtlFinish(status, handler);\n+                break;\n+            case CANCELLED:\n+                throw new LoadException(\"spark etl job failed. msg: \" + status.getFailMsg());\n+            default:\n+                LOG.warn(\"unknown etl state: {}\", status.getState().name());\n+                break;\n+        }\n+    }\n+\n+    private void updateEtlStatusInternal(EtlStatus etlStatus) {\n+        writeLock();\n+        try {\n+            loadingStatus = etlStatus;\n+            progress = etlStatus.getProgress();\n+            if (!sparkResource.isYarnMaster()) {\n+                loadingStatus.setTrackingUrl(appId);\n+            }\n+\n+            // TODO(wyb): spark-load\n+            /*\n+            DppResult dppResult = etlStatus.getDppResult();\n+            if (dppResult != null) {\n+                // update load statistic and counters when spark etl job finished\n+                // fe gets these infos from spark dpp, so we use dummy load id and dummy backend id here\n+                loadStatistic.fileNum = (int) dppResult.fileNumber;\n+                loadStatistic.totalFileSizeB = dppResult.fileSize;\n+                TUniqueId dummyId = new TUniqueId(0, 0);\n+                long dummyBackendId = -1L;\n+                loadStatistic.initLoad(dummyId, Sets.newHashSet(dummyId), Lists.newArrayList(dummyBackendId));\n+                loadStatistic.updateLoadProgress(dummyBackendId, dummyId, dummyId, dppResult.scannedRows, true);\n+\n+                Map<String, String> counters = loadingStatus.getCounters();\n+                counters.put(DPP_NORMAL_ALL, String.valueOf(dppResult.normalRows));\n+                counters.put(DPP_ABNORMAL_ALL, String.valueOf(dppResult.abnormalRows));\n+                counters.put(UNSELECTED_ROWS, String.valueOf(dppResult.unselectRows));\n+            }\n+            */\n+        } finally {\n+            writeUnlock();\n+        }\n+    }\n+\n+    private void processEtlFinish(EtlStatus etlStatus, SparkEtlJobHandler handler) throws Exception {\n+        updateEtlStatusInternal(etlStatus);\n+        // checkDataQuality\n+        if (!checkDataQuality()) {\n+            cancelJobWithoutCheck(new FailMsg(FailMsg.CancelType.ETL_QUALITY_UNSATISFIED, QUALITY_FAIL_MSG),\n+                                  true, true);\n+            return;\n+        }\n+\n+        // get etl output files and update loading state\n+        updateToLoadingState(etlStatus, handler.getEtlFilePaths(etlOutputPath, brokerDesc));\n+        // log loading state\n+        logUpdateStateInfo();\n+\n+        // create push tasks\n+        prepareLoadingInfos();\n+        submitPushTasks();\n+    }\n+\n+    private void updateToLoadingState(EtlStatus etlStatus, Map<String, Long> filePathToSize) throws LoadException {\n+        writeLock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182"}, "originalPosition": 354}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d7c944aa6d7786f4e330e35e97d53e86acd2a195", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/d7c944aa6d7786f4e330e35e97d53e86acd2a195", "committedDate": "2020-06-18T06:03:11Z", "message": "Add some comments and logs"}, "afterCommit": {"oid": "b0aaf1ef45b058c74b42ddc450bcb4f8dcb0ab11", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/b0aaf1ef45b058c74b42ddc450bcb4f8dcb0ab11", "committedDate": "2020-06-19T10:40:45Z", "message": "Fe process etl and loading state job"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1da49652c852457e44e425ded669c2d95a05dfbc", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/1da49652c852457e44e425ded669c2d95a05dfbc", "committedDate": "2020-06-19T11:06:27Z", "message": "Fe process etl and loading state job"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b0aaf1ef45b058c74b42ddc450bcb4f8dcb0ab11", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/b0aaf1ef45b058c74b42ddc450bcb4f8dcb0ab11", "committedDate": "2020-06-19T10:40:45Z", "message": "Fe process etl and loading state job"}, "afterCommit": {"oid": "1da49652c852457e44e425ded669c2d95a05dfbc", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/1da49652c852457e44e425ded669c2d95a05dfbc", "committedDate": "2020-06-19T11:06:27Z", "message": "Fe process etl and loading state job"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2e7e1280631fe642a5c8975de4d031d2b43f4b6", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/a2e7e1280631fe642a5c8975de4d031d2b43f4b6", "committedDate": "2020-06-19T11:16:19Z", "message": "Add missing class import"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MTY4NjIy", "url": "https://github.com/apache/incubator-doris/pull/3717#pullrequestreview-434168622", "createdAt": "2020-06-19T15:32:33Z", "commit": {"oid": "a2e7e1280631fe642a5c8975de4d031d2b43f4b6"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c57352ee8768da107dd48c2883f9ddbb9b8cc697", "author": {"user": {"login": "wyb", "name": "wyb"}}, "url": "https://github.com/apache/incubator-doris/commit/c57352ee8768da107dd48c2883f9ddbb9b8cc697", "committedDate": "2020-06-19T15:53:10Z", "message": "Fix ut"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MzkyODA1", "url": "https://github.com/apache/incubator-doris/pull/3717#pullrequestreview-434392805", "createdAt": "2020-06-20T01:31:31Z", "commit": {"oid": "c57352ee8768da107dd48c2883f9ddbb9b8cc697"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2824, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}