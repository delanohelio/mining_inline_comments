{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0NDUxNDY1", "number": 3715, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMToyNzo1MFrOEAuiXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwMjozMToxMFrOEDNC3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTk3OTE5OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/JobState.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMToyNzo1MFrOGcL8SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMToyNTowN1rOGcv2lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwODk2OQ==", "bodyText": "Is this OK to put a new State in the middle of original states?", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432208969", "createdAt": "2020-05-29T01:27:50Z", "author": {"login": "imay"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/JobState.java", "diffHunk": "@@ -21,6 +21,7 @@\n public enum JobState {\n     UNKNOWN, // this is only for ISSUE #2354\n     PENDING, // init state\n+    ETL,     // load data partition, sort and aggregation with etl cluster", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5315bd408b27c6b06c34a39bf4e5a69f6d9bcd71"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5NzMzNQ==", "bodyText": "JobState will be persisted in meta data by name, so the order of these state is not important", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432797335", "createdAt": "2020-05-30T01:25:07Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/JobState.java", "diffHunk": "@@ -21,6 +21,7 @@\n public enum JobState {\n     UNKNOWN, // this is only for ISSUE #2354\n     PENDING, // init state\n+    ETL,     // load data partition, sort and aggregation with etl cluster", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwODk2OQ=="}, "originalCommit": {"oid": "5315bd408b27c6b06c34a39bf4e5a69f6d9bcd71"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTk4MjA3OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/analysis/ResourceDesc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMToyOTo0OVrOGcL-MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTozMToxMFrOGcv43Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwOTQ1Nw==", "bodyText": "use JSON to serialize", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432209457", "createdAt": "2020-05-29T01:29:49Z", "author": {"login": "imay"}, "path": "fe/src/main/java/org/apache/doris/analysis/ResourceDesc.java", "diffHunk": "@@ -0,0 +1,121 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.analysis;\n+\n+import com.google.common.collect.Maps;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.common.AnalysisException;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.common.io.Writable;\n+import org.apache.doris.common.util.PrintableMap;\n+import org.apache.doris.load.EtlJobType;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+// Resource descriptor\n+//\n+// Spark example:\n+// WITH RESOURCE \"spark0\"\n+// (\n+//   \"spark.jars\" = \"xxx.jar,yyy.jar\",\n+//   \"spark.files\" = \"/tmp/aaa,/tmp/bbb\",\n+//   \"spark.executor.memory\" = \"1g\",\n+//   \"spark.yarn.queue\" = \"queue0\"\n+// )\n+public class ResourceDesc implements Writable {\n+    protected String name;\n+    protected Map<String, String> properties;\n+    protected EtlJobType etlJobType;\n+\n+    // Only used for recovery\n+    private ResourceDesc() {\n+    }\n+\n+    public ResourceDesc(String name, Map<String, String> properties) {\n+        this.name = name;\n+        this.properties = properties;\n+        if (this.properties == null) {\n+            this.properties = Maps.newHashMap();\n+        }\n+        this.etlJobType = EtlJobType.UNKNOWN;\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public Map<String, String> getProperties() {\n+        return properties;\n+    }\n+\n+    public EtlJobType getEtlJobType() {\n+        return etlJobType;\n+    }\n+\n+    public void analyze() throws AnalysisException {\n+        // check resource exist or not\n+        Resource resource = Catalog.getCurrentCatalog().getResourceMgr().getResource(getName());\n+        if (resource == null) {\n+            throw new AnalysisException(\"Resource does not exist. name: \" + getName());\n+        }\n+        if (resource.getType() == Resource.ResourceType.SPARK) {\n+            etlJobType = EtlJobType.SPARK;\n+        }\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5315bd408b27c6b06c34a39bf4e5a69f6d9bcd71"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5NzkxNw==", "bodyText": "I remove this serialization, because it is not used", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432797917", "createdAt": "2020-05-30T01:31:10Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/analysis/ResourceDesc.java", "diffHunk": "@@ -0,0 +1,121 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.analysis;\n+\n+import com.google.common.collect.Maps;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.common.AnalysisException;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.common.io.Writable;\n+import org.apache.doris.common.util.PrintableMap;\n+import org.apache.doris.load.EtlJobType;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+// Resource descriptor\n+//\n+// Spark example:\n+// WITH RESOURCE \"spark0\"\n+// (\n+//   \"spark.jars\" = \"xxx.jar,yyy.jar\",\n+//   \"spark.files\" = \"/tmp/aaa,/tmp/bbb\",\n+//   \"spark.executor.memory\" = \"1g\",\n+//   \"spark.yarn.queue\" = \"queue0\"\n+// )\n+public class ResourceDesc implements Writable {\n+    protected String name;\n+    protected Map<String, String> properties;\n+    protected EtlJobType etlJobType;\n+\n+    // Only used for recovery\n+    private ResourceDesc() {\n+    }\n+\n+    public ResourceDesc(String name, Map<String, String> properties) {\n+        this.name = name;\n+        this.properties = properties;\n+        if (this.properties == null) {\n+            this.properties = Maps.newHashMap();\n+        }\n+        this.etlJobType = EtlJobType.UNKNOWN;\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public Map<String, String> getProperties() {\n+        return properties;\n+    }\n+\n+    public EtlJobType getEtlJobType() {\n+        return etlJobType;\n+    }\n+\n+    public void analyze() throws AnalysisException {\n+        // check resource exist or not\n+        Resource resource = Catalog.getCurrentCatalog().getResourceMgr().getResource(getName());\n+        if (resource == null) {\n+            throw new AnalysisException(\"Resource does not exist. name: \" + getName());\n+        }\n+        if (resource.getType() == Resource.ResourceType.SPARK) {\n+            etlJobType = EtlJobType.SPARK;\n+        }\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwOTQ1Nw=="}, "originalCommit": {"oid": "5315bd408b27c6b06c34a39bf4e5a69f6d9bcd71"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTk4NDU0OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/analysis/BrokerDesc.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMTozMToyMlrOGcL_wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMToyNTo0M1rOGcv2xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwOTg1OA==", "bodyText": "This is not a good idea to make Broker to extends Resource. I think they are two things", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432209858", "createdAt": "2020-05-29T01:31:22Z", "author": {"login": "imay"}, "path": "fe/src/main/java/org/apache/doris/analysis/BrokerDesc.java", "diffHunk": "@@ -17,61 +17,36 @@\n \n package org.apache.doris.analysis;\n \n-import org.apache.doris.common.io.Text;\n-import org.apache.doris.common.io.Writable;\n+import org.apache.doris.common.AnalysisException;\n+import org.apache.doris.load.EtlJobType;\n import org.apache.doris.common.util.PrintableMap;\n \n-import com.google.common.collect.Maps;\n-\n import java.io.DataInput;\n-import java.io.DataOutput;\n import java.io.IOException;\n import java.util.Map;\n \n // Broker descriptor\n-public class BrokerDesc implements Writable {\n-    private String name;\n-    private Map<String, String> properties;\n-\n+//\n+// Broker example:\n+// WITH BROKER \"broker0\"\n+// (\n+//   \"username\" = \"user0\",\n+//   \"password\" = \"password0\"\n+// )\n+public class BrokerDesc extends ResourceDesc {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5315bd408b27c6b06c34a39bf4e5a69f6d9bcd71"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5NzM4Mg==", "bodyText": "ok", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432797382", "createdAt": "2020-05-30T01:25:43Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/analysis/BrokerDesc.java", "diffHunk": "@@ -17,61 +17,36 @@\n \n package org.apache.doris.analysis;\n \n-import org.apache.doris.common.io.Text;\n-import org.apache.doris.common.io.Writable;\n+import org.apache.doris.common.AnalysisException;\n+import org.apache.doris.load.EtlJobType;\n import org.apache.doris.common.util.PrintableMap;\n \n-import com.google.common.collect.Maps;\n-\n import java.io.DataInput;\n-import java.io.DataOutput;\n import java.io.IOException;\n import java.util.Map;\n \n // Broker descriptor\n-public class BrokerDesc implements Writable {\n-    private String name;\n-    private Map<String, String> properties;\n-\n+//\n+// Broker example:\n+// WITH BROKER \"broker0\"\n+// (\n+//   \"username\" = \"user0\",\n+//   \"password\" = \"password0\"\n+// )\n+public class BrokerDesc extends ResourceDesc {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIwOTg1OA=="}, "originalCommit": {"oid": "5315bd408b27c6b06c34a39bf4e5a69f6d9bcd71"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTY0MjkwOnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/common/Config.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTozOTo0NVrOGcv7cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTozOTo0NVrOGcv7cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5ODU3OQ==", "bodyText": "I think 1 day is long enough~", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432798579", "createdAt": "2020-05-30T01:39:45Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/common/Config.java", "diffHunk": "@@ -491,6 +491,12 @@\n     @ConfField(mutable = true, masterOnly = true)\n     public static int hadoop_load_default_timeout_second = 86400 * 3; // 3 day\n \n+    /*\n+     * Default spark load timeout\n+     */\n+    @ConfField(mutable = true, masterOnly = true)\n+    public static int spark_load_default_timeout_second = 86400 * 3; // 3 days", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTY0NjQyOnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/BulkLoadJob.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTo0NjoyOVrOGcv9Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxNTozMzo1MFrOGdPF0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTA0Nw==", "bodyText": "I think brokerDesc should be in the subclass of BulkLoadJob.\nAlthough currently both broker load and spark load need a broker, but for spark load, it may not be required in future.", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432799047", "createdAt": "2020-05-30T01:46:29Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/BulkLoadJob.java", "diffHunk": "@@ -0,0 +1,328 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.DataDescription;\n+import org.apache.doris.analysis.LoadStmt;\n+import org.apache.doris.analysis.SqlParser;\n+import org.apache.doris.analysis.SqlScanner;\n+import org.apache.doris.catalog.AuthorizationInfo;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.Table;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.FeMetaVersion;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.common.util.SqlParserUtils;\n+import org.apache.doris.load.BrokerFileGroup;\n+import org.apache.doris.load.BrokerFileGroupAggInfo;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.ConnectContext;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.qe.SessionVariable;\n+import org.apache.doris.qe.SqlModeHelper;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TransactionState;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * parent class of BrokerLoadJob and SparkLoadJob from load stmt\n+ */\n+public abstract class BulkLoadJob extends LoadJob {\n+    private static final Logger LOG = LogManager.getLogger(BulkLoadJob.class);\n+\n+    // input params\n+    protected BrokerDesc brokerDesc;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMwOTEzOA==", "bodyText": "After discussion with @morningman, I will improve this later, including persistence with json", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r433309138", "createdAt": "2020-06-01T15:33:50Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/BulkLoadJob.java", "diffHunk": "@@ -0,0 +1,328 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.DataDescription;\n+import org.apache.doris.analysis.LoadStmt;\n+import org.apache.doris.analysis.SqlParser;\n+import org.apache.doris.analysis.SqlScanner;\n+import org.apache.doris.catalog.AuthorizationInfo;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.Table;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.FeMetaVersion;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.common.util.SqlParserUtils;\n+import org.apache.doris.load.BrokerFileGroup;\n+import org.apache.doris.load.BrokerFileGroupAggInfo;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.ConnectContext;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.qe.SessionVariable;\n+import org.apache.doris.qe.SqlModeHelper;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TransactionState;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * parent class of BrokerLoadJob and SparkLoadJob from load stmt\n+ */\n+public abstract class BulkLoadJob extends LoadJob {\n+    private static final Logger LOG = LogManager.getLogger(BulkLoadJob.class);\n+\n+    // input params\n+    protected BrokerDesc brokerDesc;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTA0Nw=="}, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTY0OTE5OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTo1MTozOVrOGcv-tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxNTozODoyNFrOGdPWqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTQxMw==", "bodyText": "this property is hard to understand and is coupled with the detail implementation of the global dict.\nHow about changing it to a more abstract nouns?", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432799413", "createdAt": "2020-05-30T01:51:39Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,249 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Strings;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMxMzQ1MA==", "bodyText": "This is for temporary use. I am investigating load from hive table, and i will update it recently.", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r433313450", "createdAt": "2020-06-01T15:38:24Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,249 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Strings;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTQxMw=="}, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTY0OTc4OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTo1Mjo0OVrOGcv_Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxMToyNzozNlrOGdHSAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTQ5OA==", "bodyText": "Why using Preconditions here?\nIt may be some other kind of resource.", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432799498", "createdAt": "2020-05-30T01:52:49Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,249 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Strings;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    //private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    // only for log replay\n+    public SparkLoadJob() {\n+        super();\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public SparkLoadJob(long dbId, String label, ResourceDesc resourceDesc, OriginStatement originStmt)\n+            throws MetaNotFoundException {\n+        super(dbId, label, originStmt);\n+        this.resourceDesc = resourceDesc;\n+        timeoutSecond = Config.spark_load_default_timeout_second;\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public String getHiveTableName() {\n+        return hiveTableName;\n+    }\n+\n+    @Override\n+    protected void setJobProperties(Map<String, String> properties) throws DdlException {\n+        super.setJobProperties(properties);\n+\n+        // set spark resource and broker desc\n+        setResourceInfo();\n+\n+        // global dict\n+        if (properties != null) {\n+            if (properties.containsKey(BITMAP_DATA_PROPERTY)) {\n+                hiveTableName = properties.get(BITMAP_DATA_PROPERTY);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * merge system conf with load stmt\n+     * @throws DdlException\n+     */\n+    private void setResourceInfo() throws DdlException {\n+        // spark resource\n+        String resourceName = resourceDesc.getName();\n+        Resource oriResource = Catalog.getCurrentCatalog().getResourceMgr().getResource(resourceName);\n+        if (oriResource == null) {\n+            throw new DdlException(\"Resource does not exist. name: \" + resourceName);\n+        }\n+        Preconditions.checkState(oriResource instanceof SparkResource);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzE4MTE4Nw==", "bodyText": "remove this check", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r433181187", "createdAt": "2020-06-01T11:27:36Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,249 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Strings;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    //private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    // only for log replay\n+    public SparkLoadJob() {\n+        super();\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public SparkLoadJob(long dbId, String label, ResourceDesc resourceDesc, OriginStatement originStmt)\n+            throws MetaNotFoundException {\n+        super(dbId, label, originStmt);\n+        this.resourceDesc = resourceDesc;\n+        timeoutSecond = Config.spark_load_default_timeout_second;\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public String getHiveTableName() {\n+        return hiveTableName;\n+    }\n+\n+    @Override\n+    protected void setJobProperties(Map<String, String> properties) throws DdlException {\n+        super.setJobProperties(properties);\n+\n+        // set spark resource and broker desc\n+        setResourceInfo();\n+\n+        // global dict\n+        if (properties != null) {\n+            if (properties.containsKey(BITMAP_DATA_PROPERTY)) {\n+                hiveTableName = properties.get(BITMAP_DATA_PROPERTY);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * merge system conf with load stmt\n+     * @throws DdlException\n+     */\n+    private void setResourceInfo() throws DdlException {\n+        // spark resource\n+        String resourceName = resourceDesc.getName();\n+        Resource oriResource = Catalog.getCurrentCatalog().getResourceMgr().getResource(resourceName);\n+        if (oriResource == null) {\n+            throw new DdlException(\"Resource does not exist. name: \" + resourceName);\n+        }\n+        Preconditions.checkState(oriResource instanceof SparkResource);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTQ5OA=="}, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NTY1MDYzOnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwMTo1NDo0MVrOGcv_gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxNTo0ODozOFrOGdP7VA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTYxOQ==", "bodyText": "Save the error msg somewhere for user to get?", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r432799619", "createdAt": "2020-05-30T01:54:41Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,249 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Strings;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    //private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    // only for log replay\n+    public SparkLoadJob() {\n+        super();\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public SparkLoadJob(long dbId, String label, ResourceDesc resourceDesc, OriginStatement originStmt)\n+            throws MetaNotFoundException {\n+        super(dbId, label, originStmt);\n+        this.resourceDesc = resourceDesc;\n+        timeoutSecond = Config.spark_load_default_timeout_second;\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public String getHiveTableName() {\n+        return hiveTableName;\n+    }\n+\n+    @Override\n+    protected void setJobProperties(Map<String, String> properties) throws DdlException {\n+        super.setJobProperties(properties);\n+\n+        // set spark resource and broker desc\n+        setResourceInfo();\n+\n+        // global dict\n+        if (properties != null) {\n+            if (properties.containsKey(BITMAP_DATA_PROPERTY)) {\n+                hiveTableName = properties.get(BITMAP_DATA_PROPERTY);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * merge system conf with load stmt\n+     * @throws DdlException\n+     */\n+    private void setResourceInfo() throws DdlException {\n+        // spark resource\n+        String resourceName = resourceDesc.getName();\n+        Resource oriResource = Catalog.getCurrentCatalog().getResourceMgr().getResource(resourceName);\n+        if (oriResource == null) {\n+            throw new DdlException(\"Resource does not exist. name: \" + resourceName);\n+        }\n+        Preconditions.checkState(oriResource instanceof SparkResource);\n+        sparkResource = ((SparkResource) oriResource).getCopiedResource();\n+        sparkResource.update(resourceDesc);\n+\n+        // broker desc\n+        Map<String, String> brokerProperties = sparkResource.getBrokerPropertiesWithoutPrefix();\n+        brokerDesc = new BrokerDesc(sparkResource.getBroker(), brokerProperties);\n+    }\n+\n+    /**\n+     * load job already cancelled or finished, clear job below:\n+     * 1. kill etl job and delete etl files\n+     * 2. clear push tasks and infos that not persist\n+     */\n+    private void clearJob() {\n+        Preconditions.checkState(state == JobState.FINISHED || state == JobState.CANCELLED);\n+\n+        LOG.debug(\"kill etl job and delete etl files. id: {}, state: {}\", id, state);\n+        // TODO(wyb): spark-load\n+        //SparkEtlJobHandler handler = new SparkEtlJobHandler();\n+        if (state == JobState.CANCELLED) {\n+            if ((!Strings.isNullOrEmpty(appId) && sparkResource.isYarnMaster()) || sparkAppHandle != null) {\n+                try {\n+                    // TODO(wyb): spark-load\n+                    //handler.killEtlJob(sparkAppHandle, appId, id, sparkResource);\n+                } catch (Exception e) {\n+                    LOG.warn(\"kill etl job failed. id: {}, state: {}\", id, state, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMyMjgzNg==", "bodyText": "I think it\u2019s not necessary, because clear job is just trying to kill etl job as much as possible.", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r433322836", "createdAt": "2020-06-01T15:48:38Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,249 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import com.google.common.base.Strings;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Resource;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.io.Text;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    //private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    // only for log replay\n+    public SparkLoadJob() {\n+        super();\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public SparkLoadJob(long dbId, String label, ResourceDesc resourceDesc, OriginStatement originStmt)\n+            throws MetaNotFoundException {\n+        super(dbId, label, originStmt);\n+        this.resourceDesc = resourceDesc;\n+        timeoutSecond = Config.spark_load_default_timeout_second;\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public String getHiveTableName() {\n+        return hiveTableName;\n+    }\n+\n+    @Override\n+    protected void setJobProperties(Map<String, String> properties) throws DdlException {\n+        super.setJobProperties(properties);\n+\n+        // set spark resource and broker desc\n+        setResourceInfo();\n+\n+        // global dict\n+        if (properties != null) {\n+            if (properties.containsKey(BITMAP_DATA_PROPERTY)) {\n+                hiveTableName = properties.get(BITMAP_DATA_PROPERTY);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * merge system conf with load stmt\n+     * @throws DdlException\n+     */\n+    private void setResourceInfo() throws DdlException {\n+        // spark resource\n+        String resourceName = resourceDesc.getName();\n+        Resource oriResource = Catalog.getCurrentCatalog().getResourceMgr().getResource(resourceName);\n+        if (oriResource == null) {\n+            throw new DdlException(\"Resource does not exist. name: \" + resourceName);\n+        }\n+        Preconditions.checkState(oriResource instanceof SparkResource);\n+        sparkResource = ((SparkResource) oriResource).getCopiedResource();\n+        sparkResource.update(resourceDesc);\n+\n+        // broker desc\n+        Map<String, String> brokerProperties = sparkResource.getBrokerPropertiesWithoutPrefix();\n+        brokerDesc = new BrokerDesc(sparkResource.getBroker(), brokerProperties);\n+    }\n+\n+    /**\n+     * load job already cancelled or finished, clear job below:\n+     * 1. kill etl job and delete etl files\n+     * 2. clear push tasks and infos that not persist\n+     */\n+    private void clearJob() {\n+        Preconditions.checkState(state == JobState.FINISHED || state == JobState.CANCELLED);\n+\n+        LOG.debug(\"kill etl job and delete etl files. id: {}, state: {}\", id, state);\n+        // TODO(wyb): spark-load\n+        //SparkEtlJobHandler handler = new SparkEtlJobHandler();\n+        if (state == JobState.CANCELLED) {\n+            if ((!Strings.isNullOrEmpty(appId) && sparkResource.isYarnMaster()) || sparkAppHandle != null) {\n+                try {\n+                    // TODO(wyb): spark-load\n+                    //handler.killEtlJob(sparkAppHandle, appId, id, sparkResource);\n+                } catch (Exception e) {\n+                    LOG.warn(\"kill etl job failed. id: {}, state: {}\", id, state, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc5OTYxOQ=="}, "originalCommit": {"oid": "79b3089908e163e21d2a168a118e0ff13278bb63"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNzk0OTExOnYy", "diffSide": "LEFT", "path": "fe/src/main/cup/sql_parser.cup", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wN1QwMjozMToxMFrOGgGvgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOVQwODoxMDoxMVrOGg9nZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMxODA4MQ==", "bodyText": "Did you just removed this grammar?", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r436318081", "createdAt": "2020-06-07T02:31:10Z", "author": {"login": "morningman"}, "path": "fe/src/main/cup/sql_parser.cup", "diffHunk": "@@ -1364,15 +1371,16 @@ opt_broker ::=\n     :}\n     ;\n \n-opt_cluster ::=", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f6a7c6807334cfd4ee9042bbaa41576a55f3cf5"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzIxNzEyNQ==", "bodyText": "hadoop uses opt_system, opt_cluster is no longer used", "url": "https://github.com/apache/incubator-doris/pull/3715#discussion_r437217125", "createdAt": "2020-06-09T08:10:11Z", "author": {"login": "wyb"}, "path": "fe/src/main/cup/sql_parser.cup", "diffHunk": "@@ -1364,15 +1371,16 @@ opt_broker ::=\n     :}\n     ;\n \n-opt_cluster ::=", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjMxODA4MQ=="}, "originalCommit": {"oid": "7f6a7c6807334cfd4ee9042bbaa41576a55f3cf5"}, "originalPosition": 28}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1651, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}