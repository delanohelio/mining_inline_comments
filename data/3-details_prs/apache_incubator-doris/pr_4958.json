{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI3ODY2OTI5", "number": 4958, "title": "Optimized the read performance of the table when have multi versions", "bodyText": "changed the merge method of the unique table,\nmerged the cumulative version data first, and then merged with the base version.\nFor the data with only one base version, read directly without merging\nProposed changes\nOptimize the read performance of AGG and UNIQUE table with too much versions\nthe benchmark as follows\nBy the way, in most cases, the meaning of merge in our code is merge sort, so to avoid ambiguity I renamed some functions and variables\nMergeHeap -> MergeSortHeap\nMergeIterator -> MergeSortIterator\nnew_merge_iterator -> new_sort_iterator\nTest Data\nThis test data set is the catalog_sales data in the tpcds 10G data set. The data is divided into two parts, one is the complete data (corresponding to the big table) 3G, a total of 28802522 (after unqiue 14401261) rows, one only has 200M sampling data (corresponding to test The table) has a total of 1,000,000 rows, only one partition is used, and the complete data is divided into 10 buckets.\nAll tables are in segment V2 format\n+-------+--------------+------+-------+---------+---------+\n| Field | Type         | Null | Key   | Default | Extra   |\n+-------+--------------+------+-------+---------+---------+\n| k1    | BIGINT       | No   | true  | NULL    |         |\n| k2    | BIGINT       | No   | true  | NULL    |         |\n| k3    | BIGINT       | Yes  | true  | NULL    |         |\n| k4    | BIGINT       | Yes  | true  | NULL    |         |\n| k5    | BIGINT       | Yes  | true  | NULL    |         |\n| k6    | BIGINT       | Yes  | true  | NULL    |         |\n| k7    | BIGINT       | Yes  | true  | NULL    |         |\n| k8    | BIGINT       | Yes  | true  | NULL    |         |\n| k9    | BIGINT       | Yes  | true  | NULL    |         |\n| k10   | BIGINT       | Yes  | true  | NULL    |         |\n| v1    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v2    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v3    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v4    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v5    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v6    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v7    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v8    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v9    | BIGINT       | Yes  | false | NULL    | REPLACE |\n| v10   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v11   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v12   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v13   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v14   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v15   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v16   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v17   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v18   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v19   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v20   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v21   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v22   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v23   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n| v24   | DECIMAL(7,2) | Yes  | false | NULL    | REPLACE |\n+-------+--------------+------+-------+---------+---------+\n\nThis test is mainly to test the read performance, especially when there are a large number of small versions that are merged, so the test query for this time is select count(*) from (select k1,k2,k3,k4,k5 ,k6,k7,k8,k9,k10 from table_name) a;\nUNIQUE_KEY and UNIQUE_KEY comparison\nFirst of all, this test compares the difference in read performance between the UNIQUE_KEY table and the DUPLICTE_KEY table. The first version of the two versions is an empty version\n\n\n\nTable\nData size\nNumber of partitions\nNumber of buckets\nNumber of rows\nNumber of versions\nQuery time (s)\n\n\n\n\ntest_uniq\n93.585 MB\n1\n1\n1,000,000\n2\n9\n\n\ntest_uniq_big\n1.327 GB\n10\n10\n14,401,261\n2\n13.5\n\n\ntest_dup\n93.724 MB\n1\n1\n1,000,000\n2\n4.8\n\n\ntest_dup_big\n2.571 G\n10\n10\n28,802,522\n2\n7\n\n\n\nIt can be seen that the read speed of the duplicate table is about 1 times that of unique\nOptimized data\n\n\n\nTable\nData size\nNumber of partitions\nNumber of buckets\nNumber of rows\nNumber of versions\nQuery time (s)\n\n\n\n\ntest_uniq\n93.585 MB\n1\n1\n1,000,000\n2\n4.8\n\n\ntest_uniq_big\n1.327 GB\n10\n10\n14,401,261\n2\n7.4\n\n\ntest_dup\n93.724 MB\n1\n1\n1,000,000\n2\n4.7\n\n\ntest_dup_big\n2.571 G\n10\n10\n28,802,522\n2\n7\n\n\n\nAfter optimization, when the number of base versions is relatively small, the query performance is not much different.\nUNIQUE_KEY multi-version reading optimization comparison\nSince the data imported in multiple versions is random data, the data of the non-full version is a bit different, the test query is select count(*) from table_name\n\n\n\nTable\nData size\nNumber of partitions\nNumber of buckets\nNumber of rows\nNumber of versions\nQuery time (s)\n\n\n\n\ntest_uniq(before)\n136.288 MB\n1\n1\n1008592\n10000\n11.7\n\n\ntest_uniq(after)\n136.266 MB\n1\n1\n1008635\n10000\n7.1\n\n\ntest_uniq_big(before\uff09\n1.368 GB\n1\n10\n14401261\n10000\n15.5\n\n\ntest_uniq_big(after\uff09\n1.368 GB\n1\n10\n14401261\n10000\n12\n\n\ntest_uniq_base(before)\n94.252 MB\n1\n1\n1000000\n1\n4\n\n\ntest_uniq_base(after)\n94.252 MB\n1\n1\n1000000\n1\n3.75\n\n\ntest_uniq_big_base(before)\n1.327 GB\n1\n10\n14401261\n1\n6\n\n\ntest_uniq_big_base(after)\n1.327 GB\n1\n10\n14401261\n1\n5.6\n\n\ntest_uniq(before)\n96.348 MB\n1\n1\n1008592\n500\n7.1\n\n\ntest_uniq(after)\n96.349 MB\n1\n1\n1008635\n500\n4.5\n\n\ntest_uniq_big(before\uff09\n1.329 GB\n1\n10\n14401261\n500\n8.8\n\n\ntest_uniq_big(after\uff09\n1.327 GB\n1\n10\n14401261\n500\n6.7\n\n\n\nWhen there are many versions of a table, we have a lot of time spent on sorting and merging rowset\nIn our scenario, this sorting is actually a merge sorting of multiple ordered queues, and usually due to the existence of compapction, we will have a relatively large base rowset and several small rowset, so we can combine The small rowset is sorted first and merged with the large rowset to optimize the read performance\nTypes of changes\nWhat types of changes does your code introduce to Doris?\nPut an x in the boxes that apply\n\n[] Bugfix (non-breaking change which fixes an issue)\n New feature (non-breaking change which adds functionality)\n[] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n[] Documentation Update (if none of the other choices apply)\n[] Code refactor (Modify the code structure, format the code, etc...)\n\nChecklist\nPut an x in the boxes that apply. You can also fill these out after creating the PR. If you're unsure about any of them, don't hesitate to ask. We're here to help! This is simply a reminder of what we are going to look for before merging your code.\n\n I have create an issue on (Fix #4957 ), and have described the bug/feature there in detail\n[] Compiling and unit tests pass locally with my changes\n I have added tests that prove my fix is effective or that my feature works\n[] If this change need a document change, I have updated the document\n[] Any dependent changes have been merged\n\nFurther comments\nIf this is a relatively large or complex change, kick off the discussion at dev@doris.apache.org by explaining why you chose the solution you did and what alternatives you considered, etc...", "createdAt": "2020-11-26T06:53:08Z", "url": "https://github.com/apache/incubator-doris/pull/4958", "merged": true, "mergeCommit": {"oid": "df1f06e60b1339ef6e2756d0c4cb492cb64986c7"}, "closed": true, "closedAt": "2020-12-01T04:25:12Z", "author": {"login": "yangzhg"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdgkZD3ABqjQwNDUzNjU1MzM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdhfeKGAFqTU0MDYyOTA0Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "872f402c1b4b4a307acc79d65549f03b9cdc8828", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/872f402c1b4b4a307acc79d65549f03b9cdc8828", "committedDate": "2020-11-27T08:26:14Z", "message": "rename"}, "afterCommit": {"oid": "195c39c713eea3a229afe7360191fcbdf32ddfac", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/195c39c713eea3a229afe7360191fcbdf32ddfac", "committedDate": "2020-11-27T09:46:31Z", "message": "remove overlaping check"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwMzA4MzA2", "url": "https://github.com/apache/incubator-doris/pull/4958#pullrequestreview-540308306", "createdAt": "2020-11-28T03:41:17Z", "commit": {"oid": "195c39c713eea3a229afe7360191fcbdf32ddfac"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOFQwMzo0MToxN1rOH7MThQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOFQwMzo0MToxN1rOH7MThQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTgyOTYzNw==", "bodyText": "We can first check the size of _children, then decide whether to find the base rowset.\nAnd there is a memory leak of cumu_iter", "url": "https://github.com/apache/incubator-doris/pull/4958#discussion_r531829637", "createdAt": "2020-11-28T03:41:17Z", "author": {"login": "morningman"}, "path": "be/src/olap/collect_iterator.cpp", "diffHunk": "@@ -0,0 +1,328 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"olap/collect_iterator.h\"\n+\n+#include \"olap/reader.h\"\n+#include \"olap/row.h\"\n+#include \"olap/row_block.h\"\n+#include \"olap/row_cursor.h\"\n+\n+namespace doris {\n+\n+CollectIterator::~CollectIterator() {\n+    for (auto child : _children) {\n+        if (child != nullptr) {\n+            delete child;\n+            child = nullptr;\n+        }\n+    }\n+}\n+\n+void CollectIterator::init(Reader* reader) {\n+    _reader = reader;\n+    // when aggregate is enabled or key_type is DUP_KEYS, we don't merge\n+    // multiple data to aggregate for performance in user fetch\n+    if (_reader->_reader_type == READER_QUERY &&\n+        (_reader->_aggregation || _reader->_tablet->keys_type() == KeysType::DUP_KEYS)) {\n+        _merge = false;\n+    }\n+}\n+\n+OLAPStatus CollectIterator::add_child(RowsetReaderSharedPtr rs_reader) {\n+    std::unique_ptr<LevelIterator> child(new Level0Iterator(rs_reader, _reader));\n+    RETURN_NOT_OK(child->init());\n+    if (child->current_row() == nullptr) {\n+        return OLAP_SUCCESS;\n+    }\n+\n+    LevelIterator* child_ptr = child.release();\n+    _children.push_back(child_ptr);\n+    _rs_readers.push_back(rs_reader);\n+    return OLAP_SUCCESS;\n+}\n+\n+// Build a merge heap. If _merge is true, a rowset with the max rownum\n+// status will be used as the base rowset, and the other rowsets will be merged first and\n+// then merged with the base rowset.\n+void CollectIterator::build_heap() {\n+    DCHECK(_rs_readers.size() == _children.size());\n+    _reverse = _reader->_tablet->tablet_schema().keys_type() == KeysType::UNIQUE_KEYS;\n+    if (_children.empty()) {\n+        _inner_iter.reset(nullptr);\n+        return;\n+    } else if (_merge) {\n+        DCHECK(!_rs_readers.empty());\n+        // find base rowset(max rownum),\n+        RowsetReaderSharedPtr base_reader = _rs_readers[0];\n+        int base_reader_idx = 0;\n+        for (size_t i = 1; i < _rs_readers.size(); ++i) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "195c39c713eea3a229afe7360191fcbdf32ddfac"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c88fd4c15b44ae567a28f9f96cba6b8a04025f3f", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/c88fd4c15b44ae567a28f9f96cba6b8a04025f3f", "committedDate": "2020-11-30T02:21:42Z", "message": "Optimized the read performance of the table when have multi versions,\nchanged the merge method of the unique table,\nmerged the cumulative version data first, and then merged with the base version.\nFor the data with only one base version, read directly without merging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51d402b3e44774012485f76c25df7f17793362ca", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/51d402b3e44774012485f76c25df7f17793362ca", "committedDate": "2020-11-30T02:21:42Z", "message": "remove overlaping check"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "195c39c713eea3a229afe7360191fcbdf32ddfac", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/195c39c713eea3a229afe7360191fcbdf32ddfac", "committedDate": "2020-11-27T09:46:31Z", "message": "remove overlaping check"}, "afterCommit": {"oid": "51d402b3e44774012485f76c25df7f17793362ca", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/51d402b3e44774012485f76c25df7f17793362ca", "committedDate": "2020-11-30T02:21:42Z", "message": "remove overlaping check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "672d8945867416ceb44782b6c3db94c26241787a", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/672d8945867416ceb44782b6c3db94c26241787a", "committedDate": "2020-11-30T02:29:40Z", "message": "format"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "70a118011e12b84bf1c2a0071c036c6c93da9aa5", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/70a118011e12b84bf1c2a0071c036c6c93da9aa5", "committedDate": "2020-11-30T02:40:57Z", "message": "format"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1037647de161405a04cb5a892ef9a8e8e8eba352", "author": {"user": {"login": "yangzhg", "name": "Zhengguo Yang"}}, "url": "https://github.com/apache/incubator-doris/commit/1037647de161405a04cb5a892ef9a8e8e8eba352", "committedDate": "2020-11-30T05:51:26Z", "message": "fix mem leak"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwNjI5MDQz", "url": "https://github.com/apache/incubator-doris/pull/4958#pullrequestreview-540629043", "createdAt": "2020-11-30T06:36:44Z", "commit": {"oid": "1037647de161405a04cb5a892ef9a8e8e8eba352"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4634, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}