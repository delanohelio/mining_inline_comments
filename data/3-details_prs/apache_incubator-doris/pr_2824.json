{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY3OTE1MDc0", "number": 2824, "title": "[Load] Fix bug of wrong file group aggregation when handling broker load job", "bodyText": "Describe the bug\nFirst, In the broker load, we allow users to add multiple data descriptions. Each data description\nrepresents a description of a file (or set of files). Including file path, delimiter, table and\npartitions to be loaded, and other information.\nWhen the user specifies multiple data descriptions, Doris currently aggregates the data\ndescriptions belonging to the same table and generates a unified load task.\nThe problem here is that although different data descriptions point to the same table,\nthey may specify different partitions. Therefore, the aggregation of data description\nshould not only consider the table level, but also the partition level.\nExamples are as follows:\ndata description 1 is:\nDATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\nINTO TABLE `tbl1`\nPARTITION (p1, p2)\n\ndata description 2 is:\nDATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\nINTO TABLE `tbl1`\nPARTITION (p3, p4)\n\nWhat user expects is to load file1 into partition p1 and p2 of tbl1, and load file2 into paritition\np3 and p4 of same table. But currently, it will be aggregated together, which result in loading\nfile1 and file2 into all partitions p1, p2, p3 and p4.\nSecond, the following 2 data descriptions are not allowed:\nDATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\nINTO TABLE `tbl1`\nPARTITION (p1, p2)\nDATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\nINTO TABLE `tbl1`\nPARTITION (p2, p3)\n\nThey have overlapping partition(p2), which is not support yet. And we should throw an Exception\nto cancel this load job.\nThird, there is a problem with the code implementation. In the constructor of\nOlapTableSink.java, we pass in a string of partition names separated by commas.\nBut at the OlapTableSink level, we should be able to pass in a list of partition ids directly,\ninstead of names.\nISSUE: #2823", "createdAt": "2020-01-28T09:45:21Z", "url": "https://github.com/apache/incubator-doris/pull/2824", "merged": true, "mergeCommit": {"oid": "bb00f7e656fdcd4e90ac23b56c62591fd0813917"}, "closed": true, "closedAt": "2020-02-03T12:15:13Z", "author": {"login": "morningman"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb-tocMAH2gAyMzY3OTE1MDc0OmVkMWIwOWI0MWQ4OGE4YzEzNGZkYzdiMmMzYzRmN2UwMjlkZWEyMjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcAreZUAFqTM1MjE5NTg5Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ed1b09b41d88a8c134fdc7b2c3c4f7e029dea229", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/ed1b09b41d88a8c134fdc7b2c3c4f7e029dea229", "committedDate": "2020-01-28T09:08:08Z", "message": "first"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f681655711d379e2c4e07b97fae41ca00e97691b", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/f681655711d379e2c4e07b97fae41ca00e97691b", "committedDate": "2020-01-29T02:21:27Z", "message": "fix partition empty bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c22c30813cbfadf5325d96fd044641eee5e558f8", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/c22c30813cbfadf5325d96fd044641eee5e558f8", "committedDate": "2020-01-29T02:34:35Z", "message": "remove unused code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35cf28d2e69cd885c582c857a750763834bdca63", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/35cf28d2e69cd885c582c857a750763834bdca63", "committedDate": "2020-01-29T09:49:13Z", "message": "fix overlapping problem"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1529831a0e84f63137a652eefc9d51d5a3fda2d7", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/1529831a0e84f63137a652eefc9d51d5a3fda2d7", "committedDate": "2020-01-29T10:43:59Z", "message": "add missing ut file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8b786d2e4bf52d3c20f274670792579f7f5d70a", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/e8b786d2e4bf52d3c20f274670792579f7f5d70a", "committedDate": "2020-01-29T12:40:13Z", "message": "fix bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/063543a16e9d39e94f0dba6361c1ef1f7ad91093", "committedDate": "2020-01-29T12:43:44Z", "message": "fix ut"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyMDEwNDA2", "url": "https://github.com/apache/incubator-doris/pull/2824#pullrequestreview-352010406", "createdAt": "2020-02-03T03:56:27Z", "commit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwMzo1NjoyN1rOFkl3iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QwODoxODoxM1rOFkpJ5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkxMzQ4MA==", "bodyText": "Private contructor could be mocked also.", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373913480", "createdAt": "2020-02-03T03:56:27Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroup.java", "diffHunk": "@@ -77,8 +77,8 @@\n     // filter the data which has been conformed\n     private Expr whereExpr;\n \n-    // Used for recovery from edit log\n-    private BrokerFileGroup() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkxNTU1Mg==", "bodyText": "The max filter ratio belongs to the whole broker load instead of single data desc . So it does not change the \"max_filter_ratio\" when the system aggregate data desc automatically. Right?", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373915552", "createdAt": "2020-02-03T04:10:36Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk1NjI0Nw==", "bodyText": "Maybe public class is better.", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373956247", "createdAt": "2020-02-03T07:41:57Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();\n+    // auxiliary structure, tbl id -> set of partition ids.\n+    // used to exam the overlapping partitions of same table.\n+    private Map<Long, Set<Long>> tableIdToPartitioIds = Maps.newHashMap();\n+\n+    // this inner class This class is used to distinguish different combinations of table and partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk1OTE0Mg==", "bodyText": "It can be omitted.", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373959142", "createdAt": "2020-02-03T07:51:50Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();\n+    // auxiliary structure, tbl id -> set of partition ids.\n+    // used to exam the overlapping partitions of same table.\n+    private Map<Long, Set<Long>> tableIdToPartitioIds = Maps.newHashMap();\n+\n+    // this inner class This class is used to distinguish different combinations of table and partitions\n+    public static class FileGroupAggKey {\n+        private long tableId;\n+        private Set<Long> partitionIds; // empty means partition is not specified\n+\n+        public FileGroupAggKey(long tableId, List<Long> partitionIds) {\n+            this.tableId = tableId;\n+            if (partitionIds != null) {\n+                this.partitionIds = Sets.newHashSet(partitionIds);\n+            } else {\n+                this.partitionIds = Sets.newHashSet();\n+            }\n+        }\n+\n+        public long getTableId() {\n+            return tableId;\n+        }\n+\n+        public Set<Long> getPartitionIds() {\n+            return partitionIds;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (this == obj) {\n+                return true;\n+            }\n+            if (!(obj instanceof FileGroupAggKey)) {\n+                return false;\n+            }\n+\n+            FileGroupAggKey other = (FileGroupAggKey) obj;\n+            return other.tableId == this.tableId && other.partitionIds.equals(this.partitionIds);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(tableId, partitionIds);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(\"[\").append(tableId).append(\": \").append(partitionIds).append(\"]\");\n+            return sb.toString();\n+        }\n+    }\n+\n+    public BrokerFileGroupAggInfo() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NDcyMg==", "bodyText": "This class has been consistent in the old version. Maybe you should keep it .", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373964722", "createdAt": "2020-02-03T08:09:42Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/PullLoadSourceInfo.java", "diffHunk": "@@ -1,119 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NjQ3OA==", "bodyText": "Maybe in is a PullLoadSourceInfo??", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373966478", "createdAt": "2020-02-03T08:15:33Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();\n+    // auxiliary structure, tbl id -> set of partition ids.\n+    // used to exam the overlapping partitions of same table.\n+    private Map<Long, Set<Long>> tableIdToPartitioIds = Maps.newHashMap();\n+\n+    // this inner class This class is used to distinguish different combinations of table and partitions\n+    public static class FileGroupAggKey {\n+        private long tableId;\n+        private Set<Long> partitionIds; // empty means partition is not specified\n+\n+        public FileGroupAggKey(long tableId, List<Long> partitionIds) {\n+            this.tableId = tableId;\n+            if (partitionIds != null) {\n+                this.partitionIds = Sets.newHashSet(partitionIds);\n+            } else {\n+                this.partitionIds = Sets.newHashSet();\n+            }\n+        }\n+\n+        public long getTableId() {\n+            return tableId;\n+        }\n+\n+        public Set<Long> getPartitionIds() {\n+            return partitionIds;\n+        }\n+\n+        @Override\n+        public boolean equals(Object obj) {\n+            if (this == obj) {\n+                return true;\n+            }\n+            if (!(obj instanceof FileGroupAggKey)) {\n+                return false;\n+            }\n+\n+            FileGroupAggKey other = (FileGroupAggKey) obj;\n+            return other.tableId == this.tableId && other.partitionIds.equals(this.partitionIds);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hash(tableId, partitionIds);\n+        }\n+\n+        @Override\n+        public String toString() {\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(\"[\").append(tableId).append(\": \").append(partitionIds).append(\"]\");\n+            return sb.toString();\n+        }\n+    }\n+\n+    public BrokerFileGroupAggInfo() {\n+\n+    }\n+\n+    public void addFileGroup(BrokerFileGroup fileGroup) throws DdlException {\n+        FileGroupAggKey fileGroupAggKey = new FileGroupAggKey(fileGroup.getTableId(), fileGroup.getPartitionIds());\n+        List<BrokerFileGroup> fileGroupList = aggKeyToFileGroups.get(fileGroupAggKey);\n+        if (fileGroupList == null) {\n+            // check if there are overlapping partitions of same table\n+            if (tableIdToPartitioIds.containsKey(fileGroup.getTableId()) \n+                    && tableIdToPartitioIds.get(fileGroup.getTableId()).stream().anyMatch(id -> fileGroup.getPartitionIds().contains(id))) {\n+                throw new DdlException(\"There are overlapping partitions of same table in data descrition of load job stmt\");\n+            }\n+            \n+            fileGroupList = Lists.newArrayList();\n+            aggKeyToFileGroups.put(fileGroupAggKey, fileGroupList);\n+        }\n+        // exist, aggregate them\n+        fileGroupList.add(fileGroup);\n+\n+        // update tableIdToPartitioIds\n+        Set<Long> partitionIds = tableIdToPartitioIds.get(fileGroup.getTableId());\n+        if (partitionIds == null) {\n+            partitionIds = Sets.newHashSet();\n+            tableIdToPartitioIds.put(fileGroup.getTableId(), partitionIds);\n+        }\n+        if (fileGroup.getPartitionIds() != null) {\n+            partitionIds.addAll(fileGroup.getPartitionIds());\n+        }\n+    }\n+\n+    public Set<Long> getAllTableIds() {\n+        return aggKeyToFileGroups.keySet().stream().map(k -> k.tableId).collect(Collectors.toSet());\n+    }\n+\n+    public Map<FileGroupAggKey, List<BrokerFileGroup>> getAggKeyToFileGroups() {\n+        return aggKeyToFileGroups;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(aggKeyToFileGroups);\n+        return sb.toString();\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+        // The pull load source info doesn't need to be persisted.\n+        // It will be recreated by origin stmt in prepare of load job.\n+        // write 0 just for compatibility\n+        out.writeInt(0);\n+    }\n+\n+    public void readFields(DataInput in) throws IOException {\n+        in.readInt();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzk2NzMzNA==", "bodyText": "Maybe Map<tableId, List> is useful.", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r373967334", "createdAt": "2020-02-03T08:18:13Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -0,0 +1,239 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load;\n+\n+import org.apache.doris.common.DdlException;\n+import org.apache.doris.common.io.Writable;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/*\n+ * This class is mainly used to aggregate information of multiple DataDescriptors.\n+ * When the table name and specified partitions in the two DataDescriptors are same,\n+ * the BrokerFileGroup information corresponding to the two DataDescriptors will be aggregated together.\n+ * eg1\uff1a\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ *\n+ *  will be aggregated together, because they have same table name and specified partitions\n+ *  =>\n+ *  FileGroupAggKey(tbl1, [p1, p2]) => List(file1, file2);\n+ * \n+ * eg2:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ * \n+ *  will NOT be aggregated together, because they have same table name but different specified partitions\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file2);\n+ * \n+ * eg3:\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1, p2)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2, p3)\n+ * \n+ *  will throw an Exception, because there is an overlap partition(p2) between 2 data descriptions. And we\n+ *  currently not allow this. You can rewrite the data descriptions like this:\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p1)\n+ * \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p3) \n+ *  \n+ *  and\n+ * \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file1\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2) \n+ *  \n+ *  and\n+ *  \n+ *  DATA INFILE(\"hdfs://hdfs_host:hdfs_port/input/file2\")\n+ *  INTO TABLE `tbl1`\n+ *  PARTITION (p2)\n+ *  \n+ *  they will be aggregate like:\n+ *  FileGroupAggKey(tbl1, [p1]) => List(file1);\n+ *  FileGroupAggKey(tbl1, [p3]) => List(file2);\n+ *  FileGroupAggKey(tbl1, [p2]) => List(file1, file2);\n+ *  \n+ *  Although this transformation can be done automatically by system, but it change the \"max_filter_ratio\".\n+ *  So we have to let user decide what to do.\n+ */\n+public class BrokerFileGroupAggInfo implements Writable {\n+    private static final Logger LOG = LogManager.getLogger(BrokerFileGroupAggInfo.class);\n+\n+    private Map<FileGroupAggKey, List<BrokerFileGroup>> aggKeyToFileGroups = Maps.newHashMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "063543a16e9d39e94f0dba6361c1ef1f7ad91093"}, "originalPosition": 123}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "814537a575b701e7fe1d306b26dc40ac00c1a042", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/814537a575b701e7fe1d306b26dc40ac00c1a042", "committedDate": "2020-02-03T09:26:58Z", "message": "fix by review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4", "author": {"user": {"login": "morningman", "name": "Mingyu Chen"}}, "url": "https://github.com/apache/incubator-doris/commit/f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4", "committedDate": "2020-02-03T11:20:24Z", "message": "add compatible code"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyMTg0ODM0", "url": "https://github.com/apache/incubator-doris/pull/2824#pullrequestreview-352184834", "createdAt": "2020-02-03T11:23:37Z", "commit": {"oid": "f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxMToyMzozN1rOFkuMEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QxMToyMzozN1rOFkuMEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDA0OTgxMQ==", "bodyText": "A warning log is better ~", "url": "https://github.com/apache/incubator-doris/pull/2824#discussion_r374049811", "createdAt": "2020-02-03T11:23:37Z", "author": {"login": "EmmyMiao87"}, "path": "fe/src/main/java/org/apache/doris/load/BrokerFileGroupAggInfo.java", "diffHunk": "@@ -224,7 +224,15 @@ public void write(DataOutput out) throws IOException {\n     }\n \n     public void readFields(DataInput in) throws IOException {\n-        in.readInt();\n+        int mapSize = in.readInt();\n+        // just for compatibility, the following read objects are useless", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUyMTk1ODkz", "url": "https://github.com/apache/incubator-doris/pull/2824#pullrequestreview-352195893", "createdAt": "2020-02-03T11:45:12Z", "commit": {"oid": "f3bf8b552943d2f5d40f2d6d4638436a7a1ba6e4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3666, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}