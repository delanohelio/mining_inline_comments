{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1NTMwNjkz", "number": 3063, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQwNjo0ODoxNFrODoNCwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDozMzo0OFrOEFFLSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzNDgzMzMwOnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/BuildGlobalDict.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQwNjo0ODoxNFrOF2nJGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNjozNTozMlrOF75ATg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgwODcyOQ==", "bodyText": "How about GlobalDictBuilder?", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r392808729", "createdAt": "2020-03-16T06:48:14Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/BuildGlobalDict.java", "diffHunk": "@@ -0,0 +1,272 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class BuildGlobalDict {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f085241d895664298443fda908ff564f93b7b12c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NDI3MA==", "bodyText": "sounds great", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r398344270", "createdAt": "2020-03-26T06:35:32Z", "author": {"login": "wangbo"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/BuildGlobalDict.java", "diffHunk": "@@ -0,0 +1,272 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class BuildGlobalDict {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgwODcyOQ=="}, "originalCommit": {"oid": "f085241d895664298443fda908ff564f93b7b12c"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzE2NTk0OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxMTo1NVrOGjBtaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxMTo1NVrOGjBtaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MTM1NQ==", "bodyText": "if column name of doris table is upper, this will be wrong.\nMaybe you can use TreeMap for source hive table columns.", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439381355", "createdAt": "2020-06-12T12:11:55Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class GlobalDictBuilder {\n+\n+    protected static final Logger LOG = LoggerFactory.getLogger(GlobalDictBuilder.class);\n+\n+    // name of the column in doris table which need to build global dict\n+    // for example: some dict columns a,b,c\n+    // case 1: all dict columns has no relation, then the map is as below\n+    //     [a=null, b=null, c=null]\n+    // case 2: column a's value can reuse column b's value which means column a's value is a subset of column b's value\n+    //  [b=a,c=null]\n+    private MultiValueMap dictColumn;\n+    // target doris table columns in current spark load job\n+    private List<String> dorisOlapTableColumnList;\n+\n+    // distinct columns which need to use map join to solve data skew in encodeDorisIntermediateHiveTable()\n+    // we needn't to specify it until data skew happends\n+    private List<String> mapSideJoinColumns;\n+\n+    // hive table datasource,format is db.table\n+    private String sourceHiveDBTableName;\n+    // user-specified filter when query sourceHiveDBTable\n+    private String sourceHiveFilter;\n+    // intermediate hive table to store the distinct value of distinct column\n+    private String distinctKeyTableName;\n+    // current doris table's global dict hive table\n+    private String globalDictTableName;\n+\n+    // used for next step to read\n+    private String dorisIntermediateHiveTable;\n+    private SparkSession spark;\n+\n+    // key=doris column name,value=column type\n+    private Map<String, String> dorisColumnNameTypeMap = new HashMap<>();\n+\n+    // column in this list means need split distinct value and then encode respectively\n+    // to avoid the performance bottleneck to transfer origin value to dict value\n+    private List<String> veryHighCardinalityColumn;\n+    // determine the split num of new distinct value,better can be divisible by 1\n+    private int veryHighCardinalityColumnSplitNum;\n+\n+    private ExecutorService pool;\n+\n+    private StructType distinctValueSchema;\n+\n+    public GlobalDictBuilder(MultiValueMap dictColumn,\n+                             List<String> dorisOlapTableColumnList,\n+                             List<String> mapSideJoinColumns,\n+                             String sourceHiveDBTableName,\n+                             String sourceHiveFilter,\n+                             String dorisHiveDB,\n+                             String distinctKeyTableName,\n+                             String globalDictTableName,\n+                             String dorisIntermediateHiveTable,\n+                             int buildConcurrency,\n+                             List<String> veryHighCardinalityColumn,\n+                             int veryHighCardinalityColumnSplitNum,\n+                             SparkSession spark) {\n+        this.dictColumn = dictColumn;\n+        this.dorisOlapTableColumnList = dorisOlapTableColumnList;\n+        this.mapSideJoinColumns = mapSideJoinColumns;\n+        this.sourceHiveDBTableName = sourceHiveDBTableName;\n+        this.sourceHiveFilter = sourceHiveFilter;\n+        this.distinctKeyTableName = distinctKeyTableName;\n+        this.globalDictTableName = globalDictTableName;\n+        this.dorisIntermediateHiveTable = dorisIntermediateHiveTable;\n+        this.spark = spark;\n+        this.pool = Executors.newFixedThreadPool(buildConcurrency < 0 ? 1 : buildConcurrency);\n+        this.veryHighCardinalityColumn = veryHighCardinalityColumn;\n+        this.veryHighCardinalityColumnSplitNum = veryHighCardinalityColumnSplitNum;\n+\n+        spark.sql(\"use \" + dorisHiveDB);\n+    }\n+\n+    public void createHiveIntermediateTable() throws AnalysisException {\n+        Map<String, String> sourceHiveTableColumn = spark.catalog()\n+                .listColumns(sourceHiveDBTableName)\n+                .collectAsList()\n+                .stream().collect(Collectors.toMap(Column::name, Column::dataType));\n+\n+        Map<String, String> sourceHiveTableColumnInLowercase = new HashMap<>();\n+        for (Map.Entry<String, String> entry : sourceHiveTableColumn.entrySet()) {\n+            sourceHiveTableColumnInLowercase.put(entry.getKey().toLowerCase(), entry.getValue().toLowerCase());\n+        }\n+\n+        // check and get doris column type in hive\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            String columnType = sourceHiveTableColumnInLowercase.get(columnName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53bfc0239d8e9e557af0028a8525b816d242e01a"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzE3MDA1OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxMzoyMVrOGjBv7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxMzoyMVrOGjBv7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MTk5Nw==", "bodyText": "two spaces", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439381997", "createdAt": "2020-06-12T12:13:21Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class GlobalDictBuilder {\n+\n+    protected static final Logger LOG = LoggerFactory.getLogger(GlobalDictBuilder.class);\n+\n+    // name of the column in doris table which need to build global dict\n+    // for example: some dict columns a,b,c\n+    // case 1: all dict columns has no relation, then the map is as below\n+    //     [a=null, b=null, c=null]\n+    // case 2: column a's value can reuse column b's value which means column a's value is a subset of column b's value\n+    //  [b=a,c=null]\n+    private MultiValueMap dictColumn;\n+    // target doris table columns in current spark load job\n+    private List<String> dorisOlapTableColumnList;\n+\n+    // distinct columns which need to use map join to solve data skew in encodeDorisIntermediateHiveTable()\n+    // we needn't to specify it until data skew happends\n+    private List<String> mapSideJoinColumns;\n+\n+    // hive table datasource,format is db.table\n+    private String sourceHiveDBTableName;\n+    // user-specified filter when query sourceHiveDBTable\n+    private String sourceHiveFilter;\n+    // intermediate hive table to store the distinct value of distinct column\n+    private String distinctKeyTableName;\n+    // current doris table's global dict hive table\n+    private String globalDictTableName;\n+\n+    // used for next step to read\n+    private String dorisIntermediateHiveTable;\n+    private SparkSession spark;\n+\n+    // key=doris column name,value=column type\n+    private Map<String, String> dorisColumnNameTypeMap = new HashMap<>();\n+\n+    // column in this list means need split distinct value and then encode respectively\n+    // to avoid the performance bottleneck to transfer origin value to dict value\n+    private List<String> veryHighCardinalityColumn;\n+    // determine the split num of new distinct value,better can be divisible by 1\n+    private int veryHighCardinalityColumnSplitNum;\n+\n+    private ExecutorService pool;\n+\n+    private StructType distinctValueSchema;\n+\n+    public GlobalDictBuilder(MultiValueMap dictColumn,\n+                             List<String> dorisOlapTableColumnList,\n+                             List<String> mapSideJoinColumns,\n+                             String sourceHiveDBTableName,\n+                             String sourceHiveFilter,\n+                             String dorisHiveDB,\n+                             String distinctKeyTableName,\n+                             String globalDictTableName,\n+                             String dorisIntermediateHiveTable,\n+                             int buildConcurrency,\n+                             List<String> veryHighCardinalityColumn,\n+                             int veryHighCardinalityColumnSplitNum,\n+                             SparkSession spark) {\n+        this.dictColumn = dictColumn;\n+        this.dorisOlapTableColumnList = dorisOlapTableColumnList;\n+        this.mapSideJoinColumns = mapSideJoinColumns;\n+        this.sourceHiveDBTableName = sourceHiveDBTableName;\n+        this.sourceHiveFilter = sourceHiveFilter;\n+        this.distinctKeyTableName = distinctKeyTableName;\n+        this.globalDictTableName = globalDictTableName;\n+        this.dorisIntermediateHiveTable = dorisIntermediateHiveTable;\n+        this.spark = spark;\n+        this.pool = Executors.newFixedThreadPool(buildConcurrency < 0 ? 1 : buildConcurrency);\n+        this.veryHighCardinalityColumn = veryHighCardinalityColumn;\n+        this.veryHighCardinalityColumnSplitNum = veryHighCardinalityColumnSplitNum;\n+\n+        spark.sql(\"use \" + dorisHiveDB);\n+    }\n+\n+    public void createHiveIntermediateTable() throws AnalysisException {\n+        Map<String, String> sourceHiveTableColumn = spark.catalog()\n+                .listColumns(sourceHiveDBTableName)\n+                .collectAsList()\n+                .stream().collect(Collectors.toMap(Column::name, Column::dataType));\n+\n+        Map<String, String> sourceHiveTableColumnInLowercase = new HashMap<>();\n+        for (Map.Entry<String, String> entry : sourceHiveTableColumn.entrySet()) {\n+            sourceHiveTableColumnInLowercase.put(entry.getKey().toLowerCase(), entry.getValue().toLowerCase());\n+        }\n+\n+        // check and get doris column type in hive\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            String columnType = sourceHiveTableColumnInLowercase.get(columnName);\n+            if (StringUtils.isEmpty(columnType)) {\n+                throw new RuntimeException(String.format(\"doris column %s not in source hive table\", columnName));\n+            }\n+            dorisColumnNameTypeMap.put(columnName, columnType);\n+        });\n+\n+        spark.sql(String.format(\"drop table if exists %s \", dorisIntermediateHiveTable));\n+        // create IntermediateHiveTable\n+        spark.sql(getCreateIntermediateHiveTableSql());\n+\n+        // insert data to IntermediateHiveTable\n+        spark.sql(getInsertIntermediateHiveTableSql());\n+    }\n+\n+    public void extractDistinctColumn() {\n+        // create distinct tables\n+        spark.sql(getCreateDistinctKeyTableSql());\n+\n+        // extract distinct column\n+        List<GlobalDictBuildWorker> workerList = new ArrayList<>();\n+        // For the column in dictColumns's valueSet, their value is a subset of column in keyset,\n+        // so we don't need to extract distinct value of column in valueSet\n+        for (Object column : dictColumn.keySet()) {\n+            workerList.add(()->{\n+                spark.sql(getInsertDistinctKeyTableSql(column.toString(), dorisIntermediateHiveTable));\n+            });\n+        }\n+\n+        submitWorker(workerList);\n+    }\n+\n+    public void buildGlobalDict() throws ExecutionException, InterruptedException {\n+        // create global dict hive table\n+        spark.sql(getCreateGlobalDictHiveTableSql());\n+\n+        List<GlobalDictBuildWorker> globalDictBuildWorkers = new ArrayList<>();\n+        for (Object distinctColumnNameOrigin : dictColumn.keySet()) {\n+            String distinctColumnNameTmp = distinctColumnNameOrigin.toString();\n+            globalDictBuildWorkers.add(()->{\n+                // get global dict max value\n+                List<Row> maxGlobalDictValueRow = spark.sql(getMaxGlobalDictValueSql(distinctColumnNameTmp)).collectAsList();\n+                if (maxGlobalDictValueRow.size() == 0) {\n+                    throw new RuntimeException(String.format(\"get max dict value failed: %s\", distinctColumnNameTmp));\n+                }\n+\n+                long maxDictValue = 0;\n+                long minDictValue = 0;\n+                Row row = maxGlobalDictValueRow.get(0);\n+                if (row != null && row.get(0) != null) {\n+                    maxDictValue = (long)row.get(0);\n+                    minDictValue = (long)row.get(1);\n+                }\n+                LOG.info(\" column {} 's max value in dict is {} , min value is {}\", distinctColumnNameTmp, maxDictValue, minDictValue);\n+                // maybe never happened, but we need detect it\n+                if (minDictValue < 0) {\n+                    throw new RuntimeException(String.format(\" column %s 's cardinality has exceed bigint's max value\", distinctColumnNameTmp));\n+                }\n+\n+                if (veryHighCardinalityColumn.contains(distinctColumnNameTmp) && veryHighCardinalityColumnSplitNum > 1) {\n+                    // split distinct key first and then encode with count\n+                    buildGlobalDictBySplit(maxDictValue, distinctColumnNameTmp);\n+                } else {\n+                    // build global dict directly\n+                    spark.sql(getBuildGlobalDictSql(maxDictValue, distinctColumnNameTmp));\n+                }\n+\n+            });\n+        }\n+        submitWorker(globalDictBuildWorkers);\n+    }\n+\n+    // encode dorisIntermediateHiveTable's distinct column\n+    public void encodeDorisIntermediateHiveTable() {\n+        for (Object distinctColumnObj  : dictColumn.keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53bfc0239d8e9e557af0028a8525b816d242e01a"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzE3MzYzOnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxNDo1NVrOGjByQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMjoxNDo1NVrOGjByQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MjU5NQ==", "bodyText": "better using List<String> childColumn", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439382595", "createdAt": "2020-06-12T12:14:55Z", "author": {"login": "wyb"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class GlobalDictBuilder {\n+\n+    protected static final Logger LOG = LoggerFactory.getLogger(GlobalDictBuilder.class);\n+\n+    // name of the column in doris table which need to build global dict\n+    // for example: some dict columns a,b,c\n+    // case 1: all dict columns has no relation, then the map is as below\n+    //     [a=null, b=null, c=null]\n+    // case 2: column a's value can reuse column b's value which means column a's value is a subset of column b's value\n+    //  [b=a,c=null]\n+    private MultiValueMap dictColumn;\n+    // target doris table columns in current spark load job\n+    private List<String> dorisOlapTableColumnList;\n+\n+    // distinct columns which need to use map join to solve data skew in encodeDorisIntermediateHiveTable()\n+    // we needn't to specify it until data skew happends\n+    private List<String> mapSideJoinColumns;\n+\n+    // hive table datasource,format is db.table\n+    private String sourceHiveDBTableName;\n+    // user-specified filter when query sourceHiveDBTable\n+    private String sourceHiveFilter;\n+    // intermediate hive table to store the distinct value of distinct column\n+    private String distinctKeyTableName;\n+    // current doris table's global dict hive table\n+    private String globalDictTableName;\n+\n+    // used for next step to read\n+    private String dorisIntermediateHiveTable;\n+    private SparkSession spark;\n+\n+    // key=doris column name,value=column type\n+    private Map<String, String> dorisColumnNameTypeMap = new HashMap<>();\n+\n+    // column in this list means need split distinct value and then encode respectively\n+    // to avoid the performance bottleneck to transfer origin value to dict value\n+    private List<String> veryHighCardinalityColumn;\n+    // determine the split num of new distinct value,better can be divisible by 1\n+    private int veryHighCardinalityColumnSplitNum;\n+\n+    private ExecutorService pool;\n+\n+    private StructType distinctValueSchema;\n+\n+    public GlobalDictBuilder(MultiValueMap dictColumn,\n+                             List<String> dorisOlapTableColumnList,\n+                             List<String> mapSideJoinColumns,\n+                             String sourceHiveDBTableName,\n+                             String sourceHiveFilter,\n+                             String dorisHiveDB,\n+                             String distinctKeyTableName,\n+                             String globalDictTableName,\n+                             String dorisIntermediateHiveTable,\n+                             int buildConcurrency,\n+                             List<String> veryHighCardinalityColumn,\n+                             int veryHighCardinalityColumnSplitNum,\n+                             SparkSession spark) {\n+        this.dictColumn = dictColumn;\n+        this.dorisOlapTableColumnList = dorisOlapTableColumnList;\n+        this.mapSideJoinColumns = mapSideJoinColumns;\n+        this.sourceHiveDBTableName = sourceHiveDBTableName;\n+        this.sourceHiveFilter = sourceHiveFilter;\n+        this.distinctKeyTableName = distinctKeyTableName;\n+        this.globalDictTableName = globalDictTableName;\n+        this.dorisIntermediateHiveTable = dorisIntermediateHiveTable;\n+        this.spark = spark;\n+        this.pool = Executors.newFixedThreadPool(buildConcurrency < 0 ? 1 : buildConcurrency);\n+        this.veryHighCardinalityColumn = veryHighCardinalityColumn;\n+        this.veryHighCardinalityColumnSplitNum = veryHighCardinalityColumnSplitNum;\n+\n+        spark.sql(\"use \" + dorisHiveDB);\n+    }\n+\n+    public void createHiveIntermediateTable() throws AnalysisException {\n+        Map<String, String> sourceHiveTableColumn = spark.catalog()\n+                .listColumns(sourceHiveDBTableName)\n+                .collectAsList()\n+                .stream().collect(Collectors.toMap(Column::name, Column::dataType));\n+\n+        Map<String, String> sourceHiveTableColumnInLowercase = new HashMap<>();\n+        for (Map.Entry<String, String> entry : sourceHiveTableColumn.entrySet()) {\n+            sourceHiveTableColumnInLowercase.put(entry.getKey().toLowerCase(), entry.getValue().toLowerCase());\n+        }\n+\n+        // check and get doris column type in hive\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            String columnType = sourceHiveTableColumnInLowercase.get(columnName);\n+            if (StringUtils.isEmpty(columnType)) {\n+                throw new RuntimeException(String.format(\"doris column %s not in source hive table\", columnName));\n+            }\n+            dorisColumnNameTypeMap.put(columnName, columnType);\n+        });\n+\n+        spark.sql(String.format(\"drop table if exists %s \", dorisIntermediateHiveTable));\n+        // create IntermediateHiveTable\n+        spark.sql(getCreateIntermediateHiveTableSql());\n+\n+        // insert data to IntermediateHiveTable\n+        spark.sql(getInsertIntermediateHiveTableSql());\n+    }\n+\n+    public void extractDistinctColumn() {\n+        // create distinct tables\n+        spark.sql(getCreateDistinctKeyTableSql());\n+\n+        // extract distinct column\n+        List<GlobalDictBuildWorker> workerList = new ArrayList<>();\n+        // For the column in dictColumns's valueSet, their value is a subset of column in keyset,\n+        // so we don't need to extract distinct value of column in valueSet\n+        for (Object column : dictColumn.keySet()) {\n+            workerList.add(()->{\n+                spark.sql(getInsertDistinctKeyTableSql(column.toString(), dorisIntermediateHiveTable));\n+            });\n+        }\n+\n+        submitWorker(workerList);\n+    }\n+\n+    public void buildGlobalDict() throws ExecutionException, InterruptedException {\n+        // create global dict hive table\n+        spark.sql(getCreateGlobalDictHiveTableSql());\n+\n+        List<GlobalDictBuildWorker> globalDictBuildWorkers = new ArrayList<>();\n+        for (Object distinctColumnNameOrigin : dictColumn.keySet()) {\n+            String distinctColumnNameTmp = distinctColumnNameOrigin.toString();\n+            globalDictBuildWorkers.add(()->{\n+                // get global dict max value\n+                List<Row> maxGlobalDictValueRow = spark.sql(getMaxGlobalDictValueSql(distinctColumnNameTmp)).collectAsList();\n+                if (maxGlobalDictValueRow.size() == 0) {\n+                    throw new RuntimeException(String.format(\"get max dict value failed: %s\", distinctColumnNameTmp));\n+                }\n+\n+                long maxDictValue = 0;\n+                long minDictValue = 0;\n+                Row row = maxGlobalDictValueRow.get(0);\n+                if (row != null && row.get(0) != null) {\n+                    maxDictValue = (long)row.get(0);\n+                    minDictValue = (long)row.get(1);\n+                }\n+                LOG.info(\" column {} 's max value in dict is {} , min value is {}\", distinctColumnNameTmp, maxDictValue, minDictValue);\n+                // maybe never happened, but we need detect it\n+                if (minDictValue < 0) {\n+                    throw new RuntimeException(String.format(\" column %s 's cardinality has exceed bigint's max value\", distinctColumnNameTmp));\n+                }\n+\n+                if (veryHighCardinalityColumn.contains(distinctColumnNameTmp) && veryHighCardinalityColumnSplitNum > 1) {\n+                    // split distinct key first and then encode with count\n+                    buildGlobalDictBySplit(maxDictValue, distinctColumnNameTmp);\n+                } else {\n+                    // build global dict directly\n+                    spark.sql(getBuildGlobalDictSql(maxDictValue, distinctColumnNameTmp));\n+                }\n+\n+            });\n+        }\n+        submitWorker(globalDictBuildWorkers);\n+    }\n+\n+    // encode dorisIntermediateHiveTable's distinct column\n+    public void encodeDorisIntermediateHiveTable() {\n+        for (Object distinctColumnObj  : dictColumn.keySet()) {\n+            spark.sql(getEncodeDorisIntermediateHiveTableSql(distinctColumnObj.toString(), (ArrayList)dictColumn.get(distinctColumnObj.toString())));\n+        }\n+    }\n+\n+    private String getCreateIntermediateHiveTableSql() {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"create table if not exists \" + dorisIntermediateHiveTable + \" ( \");\n+\n+        Set<String> allDictColumn = new HashSet<>();\n+        allDictColumn.addAll(dictColumn.keySet());\n+        allDictColumn.addAll(dictColumn.values());\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            sql.append(columnName).append(\" \");\n+            if (allDictColumn.contains(columnName)) {\n+                sql.append(\" string ,\");\n+            } else {\n+                sql.append(dorisColumnNameTypeMap.get(columnName)).append(\" ,\");\n+            }\n+        });\n+        return sql.deleteCharAt(sql.length() - 1).append(\" )\").append(\" stored as sequencefile \").toString();\n+    }\n+\n+    private String getInsertIntermediateHiveTableSql() {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"insert overwrite table \").append(dorisIntermediateHiveTable).append(\" select \");\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            sql.append(columnName).append(\" ,\");\n+        });\n+        sql.deleteCharAt(sql.length() - 1)\n+                .append(\" from \").append(sourceHiveDBTableName);\n+        if (!StringUtils.isEmpty(sourceHiveFilter)) {\n+            sql.append(\" where \").append(sourceHiveFilter);\n+        }\n+        return sql.toString();\n+    }\n+\n+    private String getCreateDistinctKeyTableSql() {\n+        return \"create table if not exists \" + distinctKeyTableName + \"(dict_key string) partitioned by (dict_column string) stored as sequencefile \";\n+    }\n+\n+    private String getInsertDistinctKeyTableSql(String distinctColumnName, String sourceHiveTable) {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"insert overwrite table \").append(distinctKeyTableName)\n+                .append(\" partition(dict_column='\").append(distinctColumnName).append(\"')\")\n+                .append(\" select \").append(distinctColumnName)\n+                .append(\" from \").append(sourceHiveTable)\n+                .append(\" group by \").append(distinctColumnName);\n+        return sql.toString();\n+    }\n+\n+    private String getCreateGlobalDictHiveTableSql() {\n+        return \"create table if not exists \" + globalDictTableName\n+                + \"(dict_key string, dict_value bigint) partitioned by(dict_column string) stored as sequencefile \";\n+    }\n+\n+    private String getMaxGlobalDictValueSql(String distinctColumnName) {\n+        return \"select max(dict_value) as max_value,min(dict_value) as min_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"'\";\n+    }\n+\n+    private void buildGlobalDictBySplit(long maxGlobalDictValue, String distinctColumnName) {\n+        // 1. get distinct value\n+        Dataset<Row> newDistinctValue = spark.sql(getNewDistinctValue(distinctColumnName));\n+\n+        // 2. split the newDistinctValue to avoid window functions' single node bottleneck\n+        Dataset<Row>[] splitedDistinctValue = newDistinctValue.randomSplit(getRandomSplitWeights());\n+        long currentMaxDictValue = maxGlobalDictValue;\n+        Map<String, Long> distinctKeyMap = new HashMap<>();\n+\n+        for (int i = 0; i < splitedDistinctValue.length; i++) {\n+            long currentDatasetStartDictValue = currentMaxDictValue;\n+            long splitDistinctValueCount = splitedDistinctValue[i].count();\n+            currentMaxDictValue += splitDistinctValueCount;\n+            String tmpDictTableName = String.format(\"%s_%s_tmp_dict_%s\", i, currentDatasetStartDictValue, distinctColumnName);\n+            distinctKeyMap.put(tmpDictTableName, currentDatasetStartDictValue);\n+            Dataset<Row> distinctValueFrame = spark.createDataFrame(splitedDistinctValue[i].toJavaRDD(), getDistinctValueSchema());\n+            distinctValueFrame.createOrReplaceTempView(tmpDictTableName);\n+        }\n+\n+        spark.sql(getSplitBuildGlobalDictSql(distinctKeyMap, distinctColumnName));\n+\n+    }\n+\n+    private String getSplitBuildGlobalDictSql(Map<String, Long> distinctKeyMap, String distinctColumnName) {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"insert overwrite table \").append(globalDictTableName).append(\" partition(dict_column='\").append(distinctColumnName).append(\"') \")\n+                .append(\" select dict_key,dict_value from \").append(globalDictTableName).append(\" where dict_column='\").append(distinctColumnName).append(\"' \");\n+        for (Map.Entry<String, Long> entry : distinctKeyMap.entrySet()) {\n+            sql.append(\" union all select dict_key, (row_number() over(order by dict_key)) \")\n+                    .append(String.format(\" +(%s) as dict_value from %s\", entry.getValue(), entry.getKey()));\n+        }\n+        return sql.toString();\n+    }\n+\n+    private StructType getDistinctValueSchema() {\n+        if (distinctValueSchema == null) {\n+            List<StructField> fieldList = new ArrayList<>();\n+            fieldList.add(DataTypes.createStructField(\"dict_key\", DataTypes.StringType, false));\n+            distinctValueSchema = DataTypes.createStructType(fieldList);\n+        }\n+        return distinctValueSchema;\n+    }\n+\n+    private double[] getRandomSplitWeights() {\n+        double[] weights = new double[veryHighCardinalityColumnSplitNum];\n+        double weight = 1 / Double.parseDouble(String.valueOf(veryHighCardinalityColumnSplitNum));\n+        Arrays.fill(weights, weight);\n+        return weights;\n+    }\n+\n+    private String getBuildGlobalDictSql(long maxGlobalDictValue, String distinctColumnName) {\n+        return \"insert overwrite table \" + globalDictTableName + \" partition(dict_column='\" + distinctColumnName + \"') \"\n+                + \" select dict_key,dict_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"' \"\n+                + \" union all select t1.dict_key as dict_key,(row_number() over(order by t1.dict_key)) + (\" + maxGlobalDictValue + \") as dict_value from \"\n+                + \"(select dict_key from \" + distinctKeyTableName + \" where dict_column='\" + distinctColumnName + \"' and dict_key is not null)t1 left join \"\n+                + \" (select dict_key,dict_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"' )t2 \" +\n+                \"on t1.dict_key = t2.dict_key where t2.dict_value is null\";\n+    }\n+\n+    private String getNewDistinctValue(String distinctColumnName) {\n+        return  \"select t1.dict_key from \" +\n+                \" (select dict_key from \" + distinctKeyTableName + \" where dict_column='\" + distinctColumnName + \"' and dict_key is not null)t1 left join \" +\n+                \" (select dict_key,dict_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"' )t2 \" +\n+                \"on t1.dict_key = t2.dict_key where t2.dict_value is null\";\n+\n+    }\n+\n+    private String getEncodeDorisIntermediateHiveTableSql(String dictColumn, ArrayList<String> childColumn) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53bfc0239d8e9e557af0028a8525b816d242e01a"}, "originalPosition": 351}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzYyMTE0OnYy", "diffSide": "LEFT", "path": "fe/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDozMDo1MVrOGjGL0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDozMDo1MVrOGjGL0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ1NDY3NQ==", "bodyText": "Why removing this?", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439454675", "createdAt": "2020-06-12T14:30:51Z", "author": {"login": "morningman"}, "path": "fe/pom.xml", "diffHunk": "@@ -38,7 +38,6 @@ under the License.\n         <maven.compiler.target>1.8</maven.compiler.target>\n         <jprotobuf.version>2.2.11</jprotobuf.version>\n         <skip.plugin>false</skip.plugin>\n-        <fe_ut_parallel>${env.FE_UT_PARALLEL}</fe_ut_parallel>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53bfc0239d8e9e557af0028a8525b816d242e01a"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzYzMTQ2OnYy", "diffSide": "RIGHT", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDozMzo0OFrOGjGSqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDozMzo0OFrOGjGSqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ1NjQyNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *      BuildGlobalDict.createHiveIntermediateTable()\n          \n          \n            \n             *      GlobalDictBuilder.createHiveIntermediateTable()", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439456426", "createdAt": "2020-06-12T14:33:48Z", "author": {"login": "morningman"}, "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53bfc0239d8e9e557af0028a8525b816d242e01a"}, "originalPosition": 55}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2088, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}