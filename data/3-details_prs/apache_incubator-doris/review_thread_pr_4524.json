{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc3ODE4MDk1", "number": 4524, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNVQwOTowOToxM1rOEglgow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxMjoyNzowNVrOEgsX_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNjA0NDUxOnYy", "diffSide": "RIGHT", "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNVQwOTowOToxM1rOHNgx_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMjo1MTo0NFrOHNurQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzMDYyMA==", "bodyText": "If the column definition is k1 decimal(4,3), than the value range of k1 should be\n[-9.999, 9.999]\nAnd your code will give: [-9999.999, 9999.999], which is not right.", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r483930620", "createdAt": "2020-09-05T09:09:13Z", "author": {"login": "morningman"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -186,4 +192,61 @@ public boolean parse(String value) {\n             throw new RuntimeException(\"string check failed \", e);\n         }\n     }\n+}\n+\n+class DecimalParser extends ColumnParser {\n+\n+    public static int PRECISION = 27;\n+    public static int SCALE = 9;\n+\n+    private BigDecimal maxValue;\n+    private BigDecimal minValue;\n+\n+    public DecimalParser(EtlJobConfig.EtlColumn etlColumn) {\n+        StringBuilder precisionStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.precision; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzNDU3Mg==", "bodyText": "Better to refer to the logic in DecimalLiteral class.\ncheckPrecisionAndScale(int precision, int scale)", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r483934572", "createdAt": "2020-09-05T09:56:40Z", "author": {"login": "wyb"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -186,4 +192,61 @@ public boolean parse(String value) {\n             throw new RuntimeException(\"string check failed \", e);\n         }\n     }\n+}\n+\n+class DecimalParser extends ColumnParser {\n+\n+    public static int PRECISION = 27;\n+    public static int SCALE = 9;\n+\n+    private BigDecimal maxValue;\n+    private BigDecimal minValue;\n+\n+    public DecimalParser(EtlJobConfig.EtlColumn etlColumn) {\n+        StringBuilder precisionStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.precision; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzMDYyMA=="}, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE1ODI3NA==", "bodyText": "\ud83d\udc4c", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r484158274", "createdAt": "2020-09-07T02:51:44Z", "author": {"login": "wangbo"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -186,4 +192,61 @@ public boolean parse(String value) {\n             throw new RuntimeException(\"string check failed \", e);\n         }\n     }\n+}\n+\n+class DecimalParser extends ColumnParser {\n+\n+    public static int PRECISION = 27;\n+    public static int SCALE = 9;\n+\n+    private BigDecimal maxValue;\n+    private BigDecimal minValue;\n+\n+    public DecimalParser(EtlJobConfig.EtlColumn etlColumn) {\n+        StringBuilder precisionStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.precision; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzMDYyMA=="}, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNjA1MDU0OnYy", "diffSide": "RIGHT", "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNVQwOToxODoxOFrOHNg00w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMjo1MzozNlrOHNusrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzMTM0Nw==", "bodyText": "missing DEFAULT?", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r483931347", "createdAt": "2020-09-05T09:18:18Z", "author": {"login": "morningman"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -358,12 +362,44 @@ private void processRollupTree(RollupTreeNode rootNode,\n         return Pair.of(keyMap.toArray(new Integer[keyMap.size()]), valueMap.toArray(new Integer[valueMap.size()]));\n     }\n \n-    // repartition dataframe by partitionid_bucketid\n-    // so data in the same bucket will be consecutive.\n-    private JavaPairRDD<List<Object>, Object[]> fillTupleWithPartitionColumn(SparkSession spark, Dataset<Row> dataframe,\n+    /**\n+     *   check decimal,char/varchar\n+     */\n+    private boolean validateData(Object srcValue, EtlJobConfig.EtlColumn etlColumn, ColumnParser columnParser,Row row) {\n+\n+        switch (etlColumn.columnType.toUpperCase()) {\n+            case \"DECIMALV2\":\n+                // TODO(wb):  support decimal round; see be DecimalV2Value::round\n+                DecimalParser decimalParser = (DecimalParser) columnParser;\n+                BigDecimal srcBigDecimal = (BigDecimal) srcValue;\n+                if (srcValue != null && (decimalParser.getMaxValue().compareTo(srcBigDecimal) < 0 || decimalParser.getMinValue().compareTo(srcBigDecimal) > 0)) {\n+                    LOG.warn(String.format(\"decimal value is not valid for defination, column=%s, value=%s,precision=%s,scale=%s\",\n+                            etlColumn.columnName, srcValue.toString(), srcBigDecimal.precision(), srcBigDecimal.scale()));\n+                    abnormalRowAcc.add(1);\n+                    return false;\n+                }\n+                break;\n+            case \"CHAR\":\n+            case \"VARCHAR\":\n+                // TODO(wb) padding char type\n+                if (srcValue != null && srcValue.toString().length() > etlColumn.stringLength) {\n+                    LOG.warn(String.format(\"the length of input is too long than schema. column_name:%s,input_str[%s],schema length:%s,actual length:%s\",\n+                            etlColumn.columnName, row.toString(), etlColumn.stringLength, srcValue.toString().length()));\n+                    return false;\n+                }\n+                break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE1ODYzOA==", "bodyText": "we just validate char/varchar and decimal here,so it't not necessary", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r484158638", "createdAt": "2020-09-07T02:53:36Z", "author": {"login": "wangbo"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -358,12 +362,44 @@ private void processRollupTree(RollupTreeNode rootNode,\n         return Pair.of(keyMap.toArray(new Integer[keyMap.size()]), valueMap.toArray(new Integer[valueMap.size()]));\n     }\n \n-    // repartition dataframe by partitionid_bucketid\n-    // so data in the same bucket will be consecutive.\n-    private JavaPairRDD<List<Object>, Object[]> fillTupleWithPartitionColumn(SparkSession spark, Dataset<Row> dataframe,\n+    /**\n+     *   check decimal,char/varchar\n+     */\n+    private boolean validateData(Object srcValue, EtlJobConfig.EtlColumn etlColumn, ColumnParser columnParser,Row row) {\n+\n+        switch (etlColumn.columnType.toUpperCase()) {\n+            case \"DECIMALV2\":\n+                // TODO(wb):  support decimal round; see be DecimalV2Value::round\n+                DecimalParser decimalParser = (DecimalParser) columnParser;\n+                BigDecimal srcBigDecimal = (BigDecimal) srcValue;\n+                if (srcValue != null && (decimalParser.getMaxValue().compareTo(srcBigDecimal) < 0 || decimalParser.getMinValue().compareTo(srcBigDecimal) > 0)) {\n+                    LOG.warn(String.format(\"decimal value is not valid for defination, column=%s, value=%s,precision=%s,scale=%s\",\n+                            etlColumn.columnName, srcValue.toString(), srcBigDecimal.precision(), srcBigDecimal.scale()));\n+                    abnormalRowAcc.add(1);\n+                    return false;\n+                }\n+                break;\n+            case \"CHAR\":\n+            case \"VARCHAR\":\n+                // TODO(wb) padding char type\n+                if (srcValue != null && srcValue.toString().length() > etlColumn.stringLength) {\n+                    LOG.warn(String.format(\"the length of input is too long than schema. column_name:%s,input_str[%s],schema length:%s,actual length:%s\",\n+                            etlColumn.columnName, row.toString(), etlColumn.stringLength, srcValue.toString().length()));\n+                    return false;\n+                }\n+                break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzMTM0Nw=="}, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNjA5MDQxOnYy", "diffSide": "RIGHT", "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNVQxMDoxNjozMFrOHNhHZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMjo1OToyN1rOHNuxhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzNjEwMg==", "bodyText": "Check largeint type range", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r483936102", "createdAt": "2020-09-05T10:16:30Z", "author": {"login": "wyb"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -186,4 +192,61 @@ public boolean parse(String value) {\n             throw new RuntimeException(\"string check failed \", e);\n         }\n     }\n+}\n+\n+class DecimalParser extends ColumnParser {\n+\n+    public static int PRECISION = 27;\n+    public static int SCALE = 9;\n+\n+    private BigDecimal maxValue;\n+    private BigDecimal minValue;\n+\n+    public DecimalParser(EtlJobConfig.EtlColumn etlColumn) {\n+        StringBuilder precisionStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.precision; i++) {\n+            precisionStr.append(\"9\");\n+        }\n+        StringBuilder scaleStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.scale; i++) {\n+            scaleStr.append(\"9\");\n+        }\n+        maxValue = new BigDecimal(precisionStr.toString() + \".\" + scaleStr.toString());\n+        minValue = new BigDecimal(\"-\" + precisionStr.toString() + \".\" + scaleStr.toString());\n+    }\n+\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            BigDecimal bigDecimal = new BigDecimal(value);\n+            return bigDecimal.precision() - bigDecimal.scale() <= PRECISION - SCALE && bigDecimal.scale() <= SCALE;\n+        } catch (NumberFormatException e) {\n+            return false;\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"decimal parse failed \", e);\n+        }\n+    }\n+\n+    public BigDecimal getMaxValue() {\n+        return maxValue;\n+    }\n+\n+    public BigDecimal getMinValue() {\n+        return minValue;\n+    }\n+}\n+\n+class LargeIntParser extends ColumnParser {\n+\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            BigInteger bigInteger = new BigInteger(value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE1OTg3Ng==", "bodyText": "BigInteger constructors and operations throw {@code ArithmeticException} when\nthe result is out of the supported range of\n-2 Integer.MAX_VALUE}(exclusive) to\n+2Integer.MAX_VALUE}(exclusive)\n\nSo I think add a ArithmeticException here is enough", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r484159876", "createdAt": "2020-09-07T02:59:27Z", "author": {"login": "wangbo"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/ColumnParser.java", "diffHunk": "@@ -186,4 +192,61 @@ public boolean parse(String value) {\n             throw new RuntimeException(\"string check failed \", e);\n         }\n     }\n+}\n+\n+class DecimalParser extends ColumnParser {\n+\n+    public static int PRECISION = 27;\n+    public static int SCALE = 9;\n+\n+    private BigDecimal maxValue;\n+    private BigDecimal minValue;\n+\n+    public DecimalParser(EtlJobConfig.EtlColumn etlColumn) {\n+        StringBuilder precisionStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.precision; i++) {\n+            precisionStr.append(\"9\");\n+        }\n+        StringBuilder scaleStr = new StringBuilder();\n+        for (int i = 0; i < etlColumn.scale; i++) {\n+            scaleStr.append(\"9\");\n+        }\n+        maxValue = new BigDecimal(precisionStr.toString() + \".\" + scaleStr.toString());\n+        minValue = new BigDecimal(\"-\" + precisionStr.toString() + \".\" + scaleStr.toString());\n+    }\n+\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            BigDecimal bigDecimal = new BigDecimal(value);\n+            return bigDecimal.precision() - bigDecimal.scale() <= PRECISION - SCALE && bigDecimal.scale() <= SCALE;\n+        } catch (NumberFormatException e) {\n+            return false;\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"decimal parse failed \", e);\n+        }\n+    }\n+\n+    public BigDecimal getMaxValue() {\n+        return maxValue;\n+    }\n+\n+    public BigDecimal getMinValue() {\n+        return minValue;\n+    }\n+}\n+\n+class LargeIntParser extends ColumnParser {\n+\n+    @Override\n+    public boolean parse(String value) {\n+        try {\n+            BigInteger bigInteger = new BigInteger(value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzkzNjEwMg=="}, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzE2OTI3OnYy", "diffSide": "RIGHT", "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxMjoyNzowNVrOHNo6XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMjo1OTozNFrOHNuxjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA2MzgzNg==", "bodyText": "byte length", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r484063836", "createdAt": "2020-09-06T12:27:05Z", "author": {"login": "wyb"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -358,12 +362,44 @@ private void processRollupTree(RollupTreeNode rootNode,\n         return Pair.of(keyMap.toArray(new Integer[keyMap.size()]), valueMap.toArray(new Integer[valueMap.size()]));\n     }\n \n-    // repartition dataframe by partitionid_bucketid\n-    // so data in the same bucket will be consecutive.\n-    private JavaPairRDD<List<Object>, Object[]> fillTupleWithPartitionColumn(SparkSession spark, Dataset<Row> dataframe,\n+    /**\n+     *   check decimal,char/varchar\n+     */\n+    private boolean validateData(Object srcValue, EtlJobConfig.EtlColumn etlColumn, ColumnParser columnParser,Row row) {\n+\n+        switch (etlColumn.columnType.toUpperCase()) {\n+            case \"DECIMALV2\":\n+                // TODO(wb):  support decimal round; see be DecimalV2Value::round\n+                DecimalParser decimalParser = (DecimalParser) columnParser;\n+                BigDecimal srcBigDecimal = (BigDecimal) srcValue;\n+                if (srcValue != null && (decimalParser.getMaxValue().compareTo(srcBigDecimal) < 0 || decimalParser.getMinValue().compareTo(srcBigDecimal) > 0)) {\n+                    LOG.warn(String.format(\"decimal value is not valid for defination, column=%s, value=%s,precision=%s,scale=%s\",\n+                            etlColumn.columnName, srcValue.toString(), srcBigDecimal.precision(), srcBigDecimal.scale()));\n+                    abnormalRowAcc.add(1);\n+                    return false;\n+                }\n+                break;\n+            case \"CHAR\":\n+            case \"VARCHAR\":\n+                // TODO(wb) padding char type\n+                if (srcValue != null && srcValue.toString().length() > etlColumn.stringLength) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDE1OTg4NQ==", "bodyText": "\ud83d\udc4c", "url": "https://github.com/apache/incubator-doris/pull/4524#discussion_r484159885", "createdAt": "2020-09-07T02:59:34Z", "author": {"login": "wangbo"}, "path": "fe/spark-dpp/src/main/java/org/apache/doris/load/loadv2/dpp/SparkDpp.java", "diffHunk": "@@ -358,12 +362,44 @@ private void processRollupTree(RollupTreeNode rootNode,\n         return Pair.of(keyMap.toArray(new Integer[keyMap.size()]), valueMap.toArray(new Integer[valueMap.size()]));\n     }\n \n-    // repartition dataframe by partitionid_bucketid\n-    // so data in the same bucket will be consecutive.\n-    private JavaPairRDD<List<Object>, Object[]> fillTupleWithPartitionColumn(SparkSession spark, Dataset<Row> dataframe,\n+    /**\n+     *   check decimal,char/varchar\n+     */\n+    private boolean validateData(Object srcValue, EtlJobConfig.EtlColumn etlColumn, ColumnParser columnParser,Row row) {\n+\n+        switch (etlColumn.columnType.toUpperCase()) {\n+            case \"DECIMALV2\":\n+                // TODO(wb):  support decimal round; see be DecimalV2Value::round\n+                DecimalParser decimalParser = (DecimalParser) columnParser;\n+                BigDecimal srcBigDecimal = (BigDecimal) srcValue;\n+                if (srcValue != null && (decimalParser.getMaxValue().compareTo(srcBigDecimal) < 0 || decimalParser.getMinValue().compareTo(srcBigDecimal) > 0)) {\n+                    LOG.warn(String.format(\"decimal value is not valid for defination, column=%s, value=%s,precision=%s,scale=%s\",\n+                            etlColumn.columnName, srcValue.toString(), srcBigDecimal.precision(), srcBigDecimal.scale()));\n+                    abnormalRowAcc.add(1);\n+                    return false;\n+                }\n+                break;\n+            case \"CHAR\":\n+            case \"VARCHAR\":\n+                // TODO(wb) padding char type\n+                if (srcValue != null && srcValue.toString().length() > etlColumn.stringLength) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA2MzgzNg=="}, "originalCommit": {"oid": "07942ac364d185906fb5d2a6e748a2b01f37f4d2"}, "originalPosition": 52}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1077, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}