{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwMDE5MjE0", "number": 3391, "reviewThreads": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMDo1Mzo1N1rOD3YFgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzoxMzo0MlrOD4IgBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MzkyODk2OnYy", "diffSide": "RIGHT", "path": "splice_access_api/src/main/java/com/splicemachine/access/configuration/OlapConfigurations.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMDo1Mzo1N1rOGNmfxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyMjozMVrOGOYVpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjkxNTM5OQ==", "bodyText": "The same parameter is defined twice in HbaseConfigurations & OlapConfigurations, we probably only need one", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r416915399", "createdAt": "2020-04-28T20:53:57Z", "author": {"login": "arnaud-splice"}, "path": "splice_access_api/src/main/java/com/splicemachine/access/configuration/OlapConfigurations.java", "diffHunk": "@@ -117,6 +117,10 @@\n     public static final String SPARK_COMPACTION_BLOCKING = \"spark.compaction.blocking\";\n     public static final boolean DEFAULT_SPARK_COMPACTION_BLOCKING = true;\n \n+    // Kafka Bootstrap Servers\n+    public static final String KAFKA_BOOTSTRAP_SERVERS = \"splice.kafka.bootstrapServers\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMjAwNg==", "bodyText": "Removed param from OlapConfigurations in new commit ce45a6e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417732006", "createdAt": "2020-04-30T03:22:31Z", "author": {"login": "jpanko1"}, "path": "splice_access_api/src/main/java/com/splicemachine/access/configuration/OlapConfigurations.java", "diffHunk": "@@ -117,6 +117,10 @@\n     public static final String SPARK_COMPACTION_BLOCKING = \"spark.compaction.blocking\";\n     public static final boolean DEFAULT_SPARK_COMPACTION_BLOCKING = true;\n \n+    // Kafka Bootstrap Servers\n+    public static final String KAFKA_BOOTSTRAP_SERVERS = \"splice.kafka.bootstrapServers\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjkxNTM5OQ=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NzQ1NzIwOnYy", "diffSide": "RIGHT", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNjo0NzowOVrOGOHyZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyODozN1rOGOYasQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MDgzNg==", "bodyText": "change 2019 to 2020", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417460836", "createdAt": "2020-04-29T16:47:09Z", "author": {"login": "jyuanca"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzI5Nw==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733297", "createdAt": "2020-04-30T03:28:37Z", "author": {"login": "jpanko1"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MDgzNg=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NzQ1ODM5OnYy", "diffSide": "RIGHT", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNjo0NzozM1rOGOHzVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyODowOFrOGOYaPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MTA3Ng==", "bodyText": "KAFKA_EXPORT?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417461076", "createdAt": "2020-04-29T16:47:33Z", "author": {"login": "jyuanca"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.db.impl.sql.compile;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.reference.ClassName;\n+import com.splicemachine.db.iapi.reference.SQLState;\n+import com.splicemachine.db.iapi.services.classfile.VMOpcode;\n+import com.splicemachine.db.iapi.services.compiler.MethodBuilder;\n+import com.splicemachine.db.iapi.sql.ResultColumnDescriptor;\n+import com.splicemachine.db.iapi.sql.ResultDescription;\n+import com.splicemachine.db.iapi.sql.compile.Visitor;\n+import com.splicemachine.db.iapi.types.DataTypeDescriptor;\n+import com.splicemachine.db.iapi.types.TypeId;\n+import com.splicemachine.db.impl.sql.GenericColumnDescriptor;\n+\n+import java.util.List;\n+\n+/**\n+ * Export Node\n+ * <p/>\n+ * EXAMPLE:\n+ * <p/>\n+ * BINARY_EXPORT('/dir', true, 'parquet') select a, b, sqrt(c) from table1 where a > 100;\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzE4MQ==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733181", "createdAt": "2020-04-30T03:28:08Z", "author": {"login": "jpanko1"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.db.impl.sql.compile;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.reference.ClassName;\n+import com.splicemachine.db.iapi.reference.SQLState;\n+import com.splicemachine.db.iapi.services.classfile.VMOpcode;\n+import com.splicemachine.db.iapi.services.compiler.MethodBuilder;\n+import com.splicemachine.db.iapi.sql.ResultColumnDescriptor;\n+import com.splicemachine.db.iapi.sql.ResultDescription;\n+import com.splicemachine.db.iapi.sql.compile.Visitor;\n+import com.splicemachine.db.iapi.types.DataTypeDescriptor;\n+import com.splicemachine.db.iapi.types.TypeId;\n+import com.splicemachine.db.impl.sql.GenericColumnDescriptor;\n+\n+import java.util.List;\n+\n+/**\n+ * Export Node\n+ * <p/>\n+ * EXAMPLE:\n+ * <p/>\n+ * BINARY_EXPORT('/dir', true, 'parquet') select a, b, sqrt(c) from table1 where a > 100;\n+ */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MTA3Ng=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NzQ4ODU4OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNjo1NDo0MFrOGOIGhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyNzozN1rOGOYZ7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2NTk5MQ==", "bodyText": "License header missing", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417465991", "createdAt": "2020-04-29T16:54:40Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.splicemachine.derby.stream.spark;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzEwMg==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733102", "createdAt": "2020-04-30T03:27:37Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.splicemachine.derby.stream.spark;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2NTk5MQ=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NzYyNzE2OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNzozMDoxOVrOGOJeSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyNzoyNVrOGOYZxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ4ODQ1OA==", "bodyText": "remove this?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417488458", "createdAt": "2020-04-29T17:30:19Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {\n+    private String topicName;\n+    private String bootstrapServers;\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction() {\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName) {\n+        this(context, topicName, SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName, String bootstrapServers) {\n+        super(context);\n+        this.topicName = topicName;\n+        this.bootstrapServers = bootstrapServers;\n+    }\n+\n+    @Override\n+    public Iterator<ExecRow> call(Iterator<Integer> partitions) throws Exception {\n+        Properties props = new Properties();\n+\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+\n+        String group_id = \"spark-consumer-dss-ekoiersfmf-\"+UUID.randomUUID();\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG,group_id);\n+        props.put(ConsumerConfig.CLIENT_ID_CONFIG, group_id);\n+\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+        //For Debug\n+        System.out.println(\"System print group_id:\"+group_id);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzA2Mg==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733062", "createdAt": "2020-04-30T03:27:25Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {\n+    private String topicName;\n+    private String bootstrapServers;\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction() {\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName) {\n+        this(context, topicName, SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName, String bootstrapServers) {\n+        super(context);\n+        this.topicName = topicName;\n+        this.bootstrapServers = bootstrapServers;\n+    }\n+\n+    @Override\n+    public Iterator<ExecRow> call(Iterator<Integer> partitions) throws Exception {\n+        Properties props = new Properties();\n+\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+\n+        String group_id = \"spark-consumer-dss-ekoiersfmf-\"+UUID.randomUUID();\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG,group_id);\n+        props.put(ConsumerConfig.CLIENT_ID_CONFIG, group_id);\n+\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+        //For Debug\n+        System.out.println(\"System print group_id:\"+group_id);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ4ODQ1OA=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5Nzc0MDczOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNzo1OTozOVrOGOKm0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyNzowOFrOGOYZiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUwNzAyNw==", "bodyText": "partition is never used, so it can be removed", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417507027", "createdAt": "2020-04-29T17:59:39Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private int partition;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzAwMQ==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733001", "createdAt": "2020-04-30T03:27:08Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private int partition;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUwNzAyNw=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NzgxNTQwOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxODoyMTowM1rOGOLXRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMDozMTozMFrOGOVxfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUxOTQyOA==", "bodyText": "Why this is necessary?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417519428", "createdAt": "2020-04-29T18:21:03Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.db.iapi.types.SQLLongint;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.impl.SpliceSpark;\n+import com.splicemachine.derby.stream.iapi.DataSet;\n+import com.splicemachine.derby.stream.output.DataSetWriter;\n+import com.splicemachine.si.api.txn.TxnView;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Collections;\n+import java.util.Optional;\n+\n+\n+public class SparkKafkaDataSetWriter<V> implements DataSetWriter{\n+    private String topicName;\n+    private JavaRDD<V> rdd;\n+    private Optional<StructType> schema;\n+\n+    public SparkKafkaDataSetWriter(){\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName){\n+        this(rdd, topicName, Optional.empty());\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName,\n+                                   Optional<StructType> schema){\n+        this.rdd = rdd;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void writeExternal(ObjectOutput out) throws IOException{\n+        out.writeUTF(topicName);\n+        out.writeObject(rdd);\n+        out.writeObject(schema.orElse(new StructType()));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException{\n+        topicName = in.readUTF();\n+        rdd = (JavaRDD)in.readObject();\n+        StructType structType = (StructType)in.readObject();\n+        schema = structType.length() > 0 ? Optional.of(structType) : Optional.empty();\n+    }\n+\n+    @Override\n+    public DataSet<ExecRow> write() throws StandardException{\n+        long start = System.currentTimeMillis();\n+\n+        CountFunction countFunction = new CountFunction<>();\n+        KafkaStreamer kafkaStreamer = new KafkaStreamer(rdd.getNumPartitions(), topicName, schema);\n+        JavaRDD streamed = rdd.map(countFunction).mapPartitionsWithIndex(kafkaStreamer, true);\n+        streamed.collect();\n+\n+        Long count = countFunction.getCount().value();\n+        if(count == 0) {\n+            try {\n+                kafkaStreamer.noData();\n+            } catch(Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4OTk4Mw==", "bodyText": "Previously, if a select statement legitimately returned no rows, the code block before this one would do nothing, and the client (KafkaToDF) would time out.  With this fix, the noData function sends an empty object with a schema so that the client is notified the query returned no rows and it also gets the schema.\u2028", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417689983", "createdAt": "2020-04-30T00:31:30Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.db.iapi.types.SQLLongint;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.impl.SpliceSpark;\n+import com.splicemachine.derby.stream.iapi.DataSet;\n+import com.splicemachine.derby.stream.output.DataSetWriter;\n+import com.splicemachine.si.api.txn.TxnView;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Collections;\n+import java.util.Optional;\n+\n+\n+public class SparkKafkaDataSetWriter<V> implements DataSetWriter{\n+    private String topicName;\n+    private JavaRDD<V> rdd;\n+    private Optional<StructType> schema;\n+\n+    public SparkKafkaDataSetWriter(){\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName){\n+        this(rdd, topicName, Optional.empty());\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName,\n+                                   Optional<StructType> schema){\n+        this.rdd = rdd;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void writeExternal(ObjectOutput out) throws IOException{\n+        out.writeUTF(topicName);\n+        out.writeObject(rdd);\n+        out.writeObject(schema.orElse(new StructType()));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException{\n+        topicName = in.readUTF();\n+        rdd = (JavaRDD)in.readObject();\n+        StructType structType = (StructType)in.readObject();\n+        schema = structType.length() > 0 ? Optional.of(structType) : Optional.empty();\n+    }\n+\n+    @Override\n+    public DataSet<ExecRow> write() throws StandardException{\n+        long start = System.currentTimeMillis();\n+\n+        CountFunction countFunction = new CountFunction<>();\n+        KafkaStreamer kafkaStreamer = new KafkaStreamer(rdd.getNumPartitions(), topicName, schema);\n+        JavaRDD streamed = rdd.map(countFunction).mapPartitionsWithIndex(kafkaStreamer, true);\n+        streamed.collect();\n+\n+        Long count = countFunction.getCount().value();\n+        if(count == 0) {\n+            try {\n+                kafkaStreamer.noData();\n+            } catch(Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUxOTQyOA=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5Nzg0ODc1OnYy", "diffSide": "RIGHT", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxODozMDoyNlrOGOLs2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyNjo0MFrOGOYZIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNDk1NQ==", "bodyText": "remove them?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417524955", "createdAt": "2020-04-29T18:30:26Z", "author": {"login": "jyuanca"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "diffHunk": "@@ -41,20 +41,38 @@\n import com.splicemachine.storage.Partition;\n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import org.apache.commons.collections.iterators.SingletonIterator;\n+//import org.apache.kafka.clients.consumer.CommitFailedException;\n+//import org.apache.kafka.clients.consumer.ConsumerConfig;\n+//import org.apache.kafka.clients.consumer.ConsumerRecords;\n+//import org.apache.kafka.clients.consumer.ConsumerRecord;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMjg5OQ==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417732899", "createdAt": "2020-04-30T03:26:40Z", "author": {"login": "jpanko1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "diffHunk": "@@ -41,20 +41,38 @@\n import com.splicemachine.storage.Partition;\n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import org.apache.commons.collections.iterators.SingletonIterator;\n+//import org.apache.kafka.clients.consumer.CommitFailedException;\n+//import org.apache.kafka.clients.consumer.ConsumerConfig;\n+//import org.apache.kafka.clients.consumer.ConsumerRecords;\n+//import org.apache.kafka.clients.consumer.ConsumerRecord;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNDk1NQ=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5Nzg1MDQwOnYy", "diffSide": "RIGHT", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxODozMDo1NVrOGOLt6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMzoyNjoyM1rOGOYY3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNTIyNQ==", "bodyText": "Remove them?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417525225", "createdAt": "2020-04-29T18:30:55Z", "author": {"login": "jyuanca"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "diffHunk": "@@ -404,4 +422,45 @@ public TableChecker getTableChecker(String schemaName, String tableName, DataSet\n     @Override public void incrementOpDepth() { }\n     @Override public void decrementOpDepth() { }\n     @Override public void resetOpDepth() { }\n+\n+    @Override\n+    public <V> DataSet<ExecRow> readKafkaTopic(String topicName, OperationContext context) throws StandardException {\n+//        Properties props = new Properties();\n+//        props.put(\"bootstrap.servers\", SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+//        props.put(\"enable.auto.commit\", false);\n+//        props.put(\"auto.commit.interval.ms\", \"1000\");\n+//        props.put(\"session.timeout.ms\", \"30000\");\n+//        props.put(\"auto.offset.reset\", \"latest\");\n+//\n+//        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+////        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+//        //ExternalizableDeserializer was implemented in package com.splicemachine.derby.stream.spark\n+//\n+//        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+//        consumer.subscribe(Arrays.asList(topicName));\n+//\n+//        try {\n+//            ConsumerRecords<Integer, Externalizable> msgList = consumer.poll(1000);\n+//            for (ConsumerRecord<Integer, Externalizable> record : msgList) {\n+//                if (LOG.isInfoEnabled())\n+//                    LOG.info(\"KAFKA==receive: key = \" + record.key() + \", value = \" + record.value() + \" offset===\" + record.offset());\n+//            }\n+//\n+//            PairDataSet<Integer, InputStream> streamSet;\n+//\n+//            consumer.commitAsync();\n+//\n+//            Dataset<Row> table = null;\n+//        } catch (CommitFailedException e){\n+//            throw new StandardException();\n+//        }\n+//        finally {\n+//            try {\n+//                consumer.commitSync();\n+//            }finally {\n+//                consumer.close();\n+//            }\n+//        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMjgyOA==", "bodyText": "Fixed in new commit e15bf8e", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417732828", "createdAt": "2020-04-30T03:26:23Z", "author": {"login": "jpanko1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "diffHunk": "@@ -404,4 +422,45 @@ public TableChecker getTableChecker(String schemaName, String tableName, DataSet\n     @Override public void incrementOpDepth() { }\n     @Override public void decrementOpDepth() { }\n     @Override public void resetOpDepth() { }\n+\n+    @Override\n+    public <V> DataSet<ExecRow> readKafkaTopic(String topicName, OperationContext context) throws StandardException {\n+//        Properties props = new Properties();\n+//        props.put(\"bootstrap.servers\", SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+//        props.put(\"enable.auto.commit\", false);\n+//        props.put(\"auto.commit.interval.ms\", \"1000\");\n+//        props.put(\"session.timeout.ms\", \"30000\");\n+//        props.put(\"auto.offset.reset\", \"latest\");\n+//\n+//        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+////        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+//        //ExternalizableDeserializer was implemented in package com.splicemachine.derby.stream.spark\n+//\n+//        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+//        consumer.subscribe(Arrays.asList(topicName));\n+//\n+//        try {\n+//            ConsumerRecords<Integer, Externalizable> msgList = consumer.poll(1000);\n+//            for (ConsumerRecord<Integer, Externalizable> record : msgList) {\n+//                if (LOG.isInfoEnabled())\n+//                    LOG.info(\"KAFKA==receive: key = \" + record.key() + \", value = \" + record.value() + \" offset===\" + record.offset());\n+//            }\n+//\n+//            PairDataSet<Integer, InputStream> streamSet;\n+//\n+//            consumer.commitAsync();\n+//\n+//            Dataset<Row> table = null;\n+//        } catch (CommitFailedException e){\n+//            throw new StandardException();\n+//        }\n+//        finally {\n+//            try {\n+//                consumer.commitSync();\n+//            }finally {\n+//                consumer.close();\n+//            }\n+//        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNTIyNQ=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5Nzg5NjY1OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/org/apache/spark/sql/execution/datasources/jdbc/SpliceRelation2.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxODo0NDoxMFrOGOMLwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMDoyNjozMlrOGOVquQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUzMjg2NA==", "bodyText": "This has to be named as SpliceRelation2?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417532864", "createdAt": "2020-04-29T18:44:10Z", "author": {"login": "jyuanca"}, "path": "splice_spark2/src/main/spark2.2/org/apache/spark/sql/execution/datasources/jdbc/SpliceRelation2.scala", "diffHunk": "@@ -0,0 +1,68 @@\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import com.splicemachine.spark2.splicemachine.{SpliceJDBCOptions, SpliceJDBCUtil, SplicemachineContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.jdbc.JdbcDialects\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext, SparkSession}\n+import org.apache.spark.sql.sources._\n+\n+/**\n+  * Created by jleach on 4/7/17.\n+  */\n+\n+case class SpliceRelation2(jdbcOptions: JDBCOptions)(@transient val sqlContext: SQLContext, @transient var userSchema: Option[StructType]) extends BaseRelation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODI0OQ==", "bodyText": "Renamed it so it wouldn\u2019t collide with SpliceRelation in the splice_spark folder.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417688249", "createdAt": "2020-04-30T00:26:32Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/org/apache/spark/sql/execution/datasources/jdbc/SpliceRelation2.scala", "diffHunk": "@@ -0,0 +1,68 @@\n+package org.apache.spark.sql.execution.datasources.jdbc\n+\n+import com.splicemachine.spark2.splicemachine.{SpliceJDBCOptions, SpliceJDBCUtil, SplicemachineContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.jdbc.JdbcDialects\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.{DataFrame, Row, SQLContext, SparkSession}\n+import org.apache.spark.sql.sources._\n+\n+/**\n+  * Created by jleach on 4/7/17.\n+  */\n+\n+case class SpliceRelation2(jdbcOptions: JDBCOptions)(@transient val sqlContext: SQLContext, @transient var userSchema: Option[StructType]) extends BaseRelation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUzMjg2NA=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NzkxNDE2OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/KafkaToDF.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxODo0ODo0NlrOGOMWmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMDoyNzozMlrOGOVsVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUzNTY0Mg==", "bodyText": "Is KafkaToDF platform dependent and has to duplicated for spark 2.2, 2.3, and 2.4?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417535642", "createdAt": "2020-04-29T18:48:46Z", "author": {"login": "jyuanca"}, "path": "splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/KafkaToDF.scala", "diffHunk": "@@ -0,0 +1,81 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODY2Mw==", "bodyText": "KafkaToDF passes data into Spark classes and they may be subject to change from one Spark version to another.  The Kafka classes being used may change between Kafka versions.  KafkaConsumer.poll(long) was recently deprecated in favor of KafkaConsumer.poll(Duration).", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417688663", "createdAt": "2020-04-30T00:27:32Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/KafkaToDF.scala", "diffHunk": "@@ -0,0 +1,81 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUzNTY0Mg=="}, "originalCommit": {"oid": "206063e762cfb202071910106a7f804e4f4d99df"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMDc2MjI1OnYy", "diffSide": "RIGHT", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/execute/ValueRow.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMjo0OToxNFrOGOntpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwNzozMjo0NlrOGP1Vyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA==", "bodyText": "I feel that serializing the schema for every single ValueRow is too much overhead, where is this needed?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417983908", "createdAt": "2020-04-30T12:49:14Z", "author": {"login": "dgomezferro"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/execute/ValueRow.java", "diffHunk": "@@ -400,6 +411,8 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n \t\tfor (int i = 0; i < ncols; i++) {\n \t\t\tcolumn[i] = (DataValueDescriptor) in.readObject();\n \t\t}\n+\t\tStructType structType = (StructType)in.readObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE3NDUwMg==", "bodyText": "The schema was added so that the dataframe returned from SplicemachineContext.df() would contain a schema having the correct column names instead of the C0, C1, etc, names generated by ValueRow.getNamedColumn().  Since individual rows are returned through Kafka rather than one collection object of rows, I put the schema on the row level.  org.apache.spark.sql.Row implemented by ValueRow also has a method for returning the schema on the row level.\nWould it be acceptable if ValueRow stores and serializes the schema as a string, and converts it to StructType only when it\u2019s needed in ValueRow.schema() ?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418174502", "createdAt": "2020-04-30T17:31:47Z", "author": {"login": "jpanko1"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/execute/ValueRow.java", "diffHunk": "@@ -400,6 +411,8 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n \t\tfor (int i = 0; i < ncols; i++) {\n \t\t\tcolumn[i] = (DataValueDescriptor) in.readObject();\n \t\t}\n+\t\tStructType structType = (StructType)in.readObject();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE4ODY1MQ==", "bodyText": "I would try to get the schema separately, maybe through the JDBC connection, and feed it to KafkaToDF() so that it has it from the beginning rather than expecting it on every ValueRow", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418188651", "createdAt": "2020-04-30T17:55:53Z", "author": {"login": "dgomezferro"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/execute/ValueRow.java", "diffHunk": "@@ -400,6 +411,8 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n \t\tfor (int i = 0; i < ncols; i++) {\n \t\t\tcolumn[i] = (DataValueDescriptor) in.readObject();\n \t\t}\n+\t\tStructType structType = (StructType)in.readObject();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTI1NTc1NA==", "bodyText": "Fixed in new commit 5eb097b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419255754", "createdAt": "2020-05-04T07:32:46Z", "author": {"login": "jpanko1"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/execute/ValueRow.java", "diffHunk": "@@ -400,6 +411,8 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n \t\tfor (int i = 0; i < ncols; i++) {\n \t\t\tcolumn[i] = (DataValueDescriptor) in.readObject();\n \t\t}\n+\t\tStructType structType = (StructType)in.readObject();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMDc2NDk5OnYy", "diffSide": "RIGHT", "path": "hbase_sql/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxMjo0OTo0N1rOGOnvJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzowMjowOVrOGOyRfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4NDI5NQ==", "bodyText": "update year", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417984295", "createdAt": "2020-04-30T12:49:47Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/pom.xml", "diffHunk": "@@ -1,6 +1,6 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <!--\n-  ~ Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+  ~ Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE1NjkyNA==", "bodyText": "Fixed in new commit 82fba10", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418156924", "createdAt": "2020-04-30T17:02:09Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/pom.xml", "diffHunk": "@@ -1,6 +1,6 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <!--\n-  ~ Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+  ~ Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4NDI5NQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTI3MDA3OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNDo0Njo0NFrOGOsv1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoxNToyNFrOGPDA6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2NjM4OQ==", "bodyText": "Nitpick: can we rename it to KafkaReadFunction or something similar, a bit more descriptive ?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418066389", "createdAt": "2020-04-30T14:46:44Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTIwOQ==", "bodyText": "Renamed in new commits 5c1565b and ca75b78.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418431209", "createdAt": "2020-05-01T06:15:24Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2NjM4OQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTI4NjgxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNDo1MDoyNlrOGOs6mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMTo0NDoyM1rOGPWiwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2OTE0Nw==", "bodyText": "I think you expect a single value here, in that case I would make this function a SpliceFlatMapFunction<ExportKafkaOperation, Integer, ExecRow> and call it with rdd.map(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));\n(instead of mapPartitions)", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418069147", "createdAt": "2020-04-30T14:50:26Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {\n+    private String topicName;\n+    private String bootstrapServers;\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction() {\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName) {\n+        this(context, topicName, SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName, String bootstrapServers) {\n+        super(context);\n+        this.topicName = topicName;\n+        this.bootstrapServers = bootstrapServers;\n+    }\n+\n+    @Override\n+    public Iterator<ExecRow> call(Iterator<Integer> partitions) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1MTE3MA==", "bodyText": "Fixed in new commit c8b039b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418751170", "createdAt": "2020-05-01T21:44:23Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {\n+    private String topicName;\n+    private String bootstrapServers;\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction() {\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName) {\n+        this(context, topicName, SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName, String bootstrapServers) {\n+        super(context);\n+        this.topicName = topicName;\n+        this.bootstrapServers = bootstrapServers;\n+    }\n+\n+    @Override\n+    public Iterator<ExecRow> call(Iterator<Integer> partitions) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2OTE0Nw=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTI5NzUxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNDo1MzowMlrOGOtBkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMzozODozOFrOGO98Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MDkyOQ==", "bodyText": "We need a follow up Jira to improve serialization, relying on Java's serialization is too slow.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418070929", "createdAt": "2020-04-30T14:53:02Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private volatile TaskContext taskContext;\n+\n+    // Serialization\n+    public KafkaStreamer(){\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName) {\n+        this(numPartitions, topicName, Optional.empty());\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName, Optional<StructType> schema) {\n+        this.bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        this.numPartitions = numPartitions;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void noData() throws Exception {\n+        call(0, (Iterator<T>)Arrays.asList(new ValueRow(0)).iterator());\n+    }\n+\n+    @Override\n+    public Iterator<String> call(Integer partition, Iterator<T> locatedRowIterator) throws Exception {\n+        taskContext = TaskContext.get();\n+\n+        if (taskContext != null && taskContext.attemptNumber() > 0) {\n+            LOG.trace(\"KS.c attempts \"+taskContext.attemptNumber());\n+            long entriesInKafka = KafkaUtils.messageCount(bootstrapServers, topicName, partition);\n+            LOG.trace(\"KS.c entries \"+entriesInKafka);\n+            for (long i = 0; i < entriesInKafka; ++i) {\n+                locatedRowIterator.next();\n+            }\n+        }\n+\n+        Properties props = new Properties();\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-dss-ks-\"+UUID.randomUUID() );\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ExternalizableSerializer.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM0ODAzNA==", "bodyText": "Added DB-9474.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418348034", "createdAt": "2020-04-30T23:38:38Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private volatile TaskContext taskContext;\n+\n+    // Serialization\n+    public KafkaStreamer(){\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName) {\n+        this(numPartitions, topicName, Optional.empty());\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName, Optional<StructType> schema) {\n+        this.bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        this.numPartitions = numPartitions;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void noData() throws Exception {\n+        call(0, (Iterator<T>)Arrays.asList(new ValueRow(0)).iterator());\n+    }\n+\n+    @Override\n+    public Iterator<String> call(Integer partition, Iterator<T> locatedRowIterator) throws Exception {\n+        taskContext = TaskContext.get();\n+\n+        if (taskContext != null && taskContext.attemptNumber() > 0) {\n+            LOG.trace(\"KS.c attempts \"+taskContext.attemptNumber());\n+            long entriesInKafka = KafkaUtils.messageCount(bootstrapServers, topicName, partition);\n+            LOG.trace(\"KS.c entries \"+entriesInKafka);\n+            for (long i = 0; i < entriesInKafka; ++i) {\n+                locatedRowIterator.next();\n+            }\n+        }\n+\n+        Properties props = new Properties();\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-dss-ks-\"+UUID.randomUUID() );\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ExternalizableSerializer.class.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MDkyOQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTMxMjgzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNDo1NjoxOFrOGOtLZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoxNjo0NFrOGPDB-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MzQ0Ng==", "bodyText": "Remove comments", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418073446", "createdAt": "2020-04-30T14:56:18Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private volatile TaskContext taskContext;\n+\n+    // Serialization\n+    public KafkaStreamer(){\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName) {\n+        this(numPartitions, topicName, Optional.empty());\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName, Optional<StructType> schema) {\n+        this.bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        this.numPartitions = numPartitions;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void noData() throws Exception {\n+        call(0, (Iterator<T>)Arrays.asList(new ValueRow(0)).iterator());\n+    }\n+\n+    @Override\n+    public Iterator<String> call(Integer partition, Iterator<T> locatedRowIterator) throws Exception {\n+        taskContext = TaskContext.get();\n+\n+        if (taskContext != null && taskContext.attemptNumber() > 0) {\n+            LOG.trace(\"KS.c attempts \"+taskContext.attemptNumber());\n+            long entriesInKafka = KafkaUtils.messageCount(bootstrapServers, topicName, partition);\n+            LOG.trace(\"KS.c entries \"+entriesInKafka);\n+            for (long i = 0; i < entriesInKafka; ++i) {\n+                locatedRowIterator.next();\n+            }\n+        }\n+\n+        Properties props = new Properties();\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-dss-ks-\"+UUID.randomUUID() );\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ExternalizableSerializer.class.getName());\n+        KafkaProducer<Integer, Externalizable> producer = new KafkaProducer<>(props);\n+        int count = 0 ;\n+        while (locatedRowIterator.hasNext()) {\n+            T lr = locatedRowIterator.next();\n+\n+            if(schema.isPresent() && lr instanceof ValueRow) {\n+                lr = ((T)new ValueRow((ValueRow)lr, schema));\n+            }\n+\n+            ProducerRecord<Integer, Externalizable> record = new ProducerRecord(topicName,\n+                    /*partition.intValue(),*/ count++, lr);\n+            producer.send(record);\n+            LOG.trace(\"KS.c sent \"+partition.intValue()+\" \"+count+\" \"+lr);\n+        }\n+        LOG.trace(\"KS.c count \"+partition.intValue()+\" \"+count);\n+\n+//        ProducerRecord<Integer, Externalizable> record = new ProducerRecord(topicName,\n+//                partition.intValue(), -1, new ValueRow());\n+//        producer.send(record); // termination marker", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTQ4MA==", "bodyText": "Removed in new commit d83e2a6.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418431480", "createdAt": "2020-05-01T06:16:44Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private volatile TaskContext taskContext;\n+\n+    // Serialization\n+    public KafkaStreamer(){\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName) {\n+        this(numPartitions, topicName, Optional.empty());\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName, Optional<StructType> schema) {\n+        this.bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        this.numPartitions = numPartitions;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void noData() throws Exception {\n+        call(0, (Iterator<T>)Arrays.asList(new ValueRow(0)).iterator());\n+    }\n+\n+    @Override\n+    public Iterator<String> call(Integer partition, Iterator<T> locatedRowIterator) throws Exception {\n+        taskContext = TaskContext.get();\n+\n+        if (taskContext != null && taskContext.attemptNumber() > 0) {\n+            LOG.trace(\"KS.c attempts \"+taskContext.attemptNumber());\n+            long entriesInKafka = KafkaUtils.messageCount(bootstrapServers, topicName, partition);\n+            LOG.trace(\"KS.c entries \"+entriesInKafka);\n+            for (long i = 0; i < entriesInKafka; ++i) {\n+                locatedRowIterator.next();\n+            }\n+        }\n+\n+        Properties props = new Properties();\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-dss-ks-\"+UUID.randomUUID() );\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ExternalizableSerializer.class.getName());\n+        KafkaProducer<Integer, Externalizable> producer = new KafkaProducer<>(props);\n+        int count = 0 ;\n+        while (locatedRowIterator.hasNext()) {\n+            T lr = locatedRowIterator.next();\n+\n+            if(schema.isPresent() && lr instanceof ValueRow) {\n+                lr = ((T)new ValueRow((ValueRow)lr, schema));\n+            }\n+\n+            ProducerRecord<Integer, Externalizable> record = new ProducerRecord(topicName,\n+                    /*partition.intValue(),*/ count++, lr);\n+            producer.send(record);\n+            LOG.trace(\"KS.c sent \"+partition.intValue()+\" \"+count+\" \"+lr);\n+        }\n+        LOG.trace(\"KS.c count \"+partition.intValue()+\" \"+count);\n+\n+//        ProducerRecord<Integer, Externalizable> record = new ProducerRecord(topicName,\n+//                partition.intValue(), -1, new ValueRow());\n+//        producer.send(record); // termination marker", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MzQ0Ng=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYwNzMxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjowNToyNlrOGOwI0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMTo0NDozNlrOGPWjHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMTkzNw==", "bodyText": "Following up from the other comment, I think this should be return rdd.map(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418121937", "createdAt": "2020-04-30T16:05:26Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1144,4 +1156,26 @@ public void decrementOpDepth() {\n \n     @Override\n     public void resetOpDepth() { opDepth = 0; }\n+\n+    public <V> DataSet<ExecRow> readKafkaTopic(String topicName, OperationContext context) throws StandardException {\n+        Properties props = new Properties();\n+        String consumerId = \"spark-consumer-dss-sdsp-\"+UUID.randomUUID();\n+        String bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerId);\n+        props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumerId);\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+\n+        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        List ps = consumer.partitionsFor(topicName);\n+        List<Integer> partitions = new ArrayList<>(ps.size());\n+        for (int i = 0; i < ps.size(); ++i) {\n+            partitions.add(i);\n+        }\n+        consumer.close();\n+\n+        SparkDataSet rdd = new SparkDataSet(SpliceSpark.getContext().parallelize(partitions, partitions.size()));\n+        return rdd.mapPartitions(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1MTI2MA==", "bodyText": "Tried to convert to map and had an issue, but changing to flatMap worked.  Fixed in new commit c8b039b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418751260", "createdAt": "2020-05-01T21:44:36Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1144,4 +1156,26 @@ public void decrementOpDepth() {\n \n     @Override\n     public void resetOpDepth() { opDepth = 0; }\n+\n+    public <V> DataSet<ExecRow> readKafkaTopic(String topicName, OperationContext context) throws StandardException {\n+        Properties props = new Properties();\n+        String consumerId = \"spark-consumer-dss-sdsp-\"+UUID.randomUUID();\n+        String bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerId);\n+        props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumerId);\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+\n+        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        List ps = consumer.partitionsFor(topicName);\n+        List<Integer> partitions = new ArrayList<>(ps.size());\n+        for (int i = 0; i < ps.size(); ++i) {\n+            partitions.add(i);\n+        }\n+        consumer.close();\n+\n+        SparkDataSet rdd = new SparkDataSet(SpliceSpark.getContext().parallelize(partitions, partitions.size()));\n+        return rdd.mapPartitions(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMTkzNw=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYxNTAwOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjowNzoxNVrOGOwNpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoxOToyN1rOGPDEUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzE3Mw==", "bodyText": "Does this get used? If not remove.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418123173", "createdAt": "2020-04-30T16:07:15Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.db.iapi.types.SQLLongint;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.impl.SpliceSpark;\n+import com.splicemachine.derby.stream.iapi.DataSet;\n+import com.splicemachine.derby.stream.output.DataSetWriter;\n+import com.splicemachine.si.api.txn.TxnView;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Collections;\n+import java.util.Optional;\n+\n+\n+public class SparkKafkaDataSetWriter<V> implements DataSetWriter{\n+    private String topicName;\n+    private JavaRDD<V> rdd;\n+    private Optional<StructType> schema;\n+\n+    public SparkKafkaDataSetWriter(){\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName){\n+        this(rdd, topicName, Optional.empty());\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName,\n+                                   Optional<StructType> schema){\n+        this.rdd = rdd;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void writeExternal(ObjectOutput out) throws IOException{\n+        out.writeUTF(topicName);\n+        out.writeObject(rdd);\n+        out.writeObject(schema.orElse(new StructType()));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException{\n+        topicName = in.readUTF();\n+        rdd = (JavaRDD)in.readObject();\n+        StructType structType = (StructType)in.readObject();\n+        schema = structType.length() > 0 ? Optional.of(structType) : Optional.empty();\n+    }\n+\n+    @Override\n+    public DataSet<ExecRow> write() throws StandardException{\n+        long start = System.currentTimeMillis();\n+\n+        CountFunction countFunction = new CountFunction<>();\n+        KafkaStreamer kafkaStreamer = new KafkaStreamer(rdd.getNumPartitions(), topicName, schema);\n+        JavaRDD streamed = rdd.map(countFunction).mapPartitionsWithIndex(kafkaStreamer, true);\n+        streamed.collect();\n+\n+        Long count = countFunction.getCount().value();\n+        if(count == 0) {\n+            try {\n+                kafkaStreamer.noData();\n+            } catch(Exception e) {\n+                throw StandardException.newException(\"\", e);\n+            }\n+        }\n+\n+        long end = System.currentTimeMillis();\n+        ValueRow valueRow=new ValueRow(2);\n+        valueRow.setColumn(1,new SQLLongint(count));\n+        valueRow.setColumn(2,new SQLLongint(end-start));\n+        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n+    @Override\n+    public void setTxn(TxnView childTxn){\n+        throw new UnsupportedOperationException(\"IMPLEMENT\");\n+    }\n+\n+    @Override\n+    public TxnView getTxn(){\n+        throw new UnsupportedOperationException(\"IMPLEMENT\");\n+    }\n+\n+    @Override\n+    public byte[] getDestinationTable(){\n+        throw new UnsupportedOperationException();\n+    }\n+\n+\n+    public static class NullFunction<V> implements org.apache.spark.api.java.function.Function<V, Object>{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjA4Mw==", "bodyText": "Fixed in new commit 8d9468c.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432083", "createdAt": "2020-05-01T06:19:27Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.db.iapi.types.SQLLongint;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.impl.SpliceSpark;\n+import com.splicemachine.derby.stream.iapi.DataSet;\n+import com.splicemachine.derby.stream.output.DataSetWriter;\n+import com.splicemachine.si.api.txn.TxnView;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Collections;\n+import java.util.Optional;\n+\n+\n+public class SparkKafkaDataSetWriter<V> implements DataSetWriter{\n+    private String topicName;\n+    private JavaRDD<V> rdd;\n+    private Optional<StructType> schema;\n+\n+    public SparkKafkaDataSetWriter(){\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName){\n+        this(rdd, topicName, Optional.empty());\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName,\n+                                   Optional<StructType> schema){\n+        this.rdd = rdd;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void writeExternal(ObjectOutput out) throws IOException{\n+        out.writeUTF(topicName);\n+        out.writeObject(rdd);\n+        out.writeObject(schema.orElse(new StructType()));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException{\n+        topicName = in.readUTF();\n+        rdd = (JavaRDD)in.readObject();\n+        StructType structType = (StructType)in.readObject();\n+        schema = structType.length() > 0 ? Optional.of(structType) : Optional.empty();\n+    }\n+\n+    @Override\n+    public DataSet<ExecRow> write() throws StandardException{\n+        long start = System.currentTimeMillis();\n+\n+        CountFunction countFunction = new CountFunction<>();\n+        KafkaStreamer kafkaStreamer = new KafkaStreamer(rdd.getNumPartitions(), topicName, schema);\n+        JavaRDD streamed = rdd.map(countFunction).mapPartitionsWithIndex(kafkaStreamer, true);\n+        streamed.collect();\n+\n+        Long count = countFunction.getCount().value();\n+        if(count == 0) {\n+            try {\n+                kafkaStreamer.noData();\n+            } catch(Exception e) {\n+                throw StandardException.newException(\"\", e);\n+            }\n+        }\n+\n+        long end = System.currentTimeMillis();\n+        ValueRow valueRow=new ValueRow(2);\n+        valueRow.setColumn(1,new SQLLongint(count));\n+        valueRow.setColumn(2,new SQLLongint(end-start));\n+        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n+    @Override\n+    public void setTxn(TxnView childTxn){\n+        throw new UnsupportedOperationException(\"IMPLEMENT\");\n+    }\n+\n+    @Override\n+    public TxnView getTxn(){\n+        throw new UnsupportedOperationException(\"IMPLEMENT\");\n+    }\n+\n+    @Override\n+    public byte[] getDestinationTable(){\n+        throw new UnsupportedOperationException();\n+    }\n+\n+\n+    public static class NullFunction<V> implements org.apache.spark.api.java.function.Function<V, Object>{", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzE3Mw=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYxNTkyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjowNzozMVrOGOwOSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoxOTozM1rOGPDEZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzMzNw==", "bodyText": "year", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418123337", "createdAt": "2020-04-30T16:07:31Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjEwMQ==", "bodyText": "Fixed in new commit 8d9468c.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432101", "createdAt": "2020-05-01T06:19:33Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzMzNw=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYxNjI5OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/vti/KafkaVTI.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjowNzozOFrOGOwOiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoyMTowOFrOGPDFxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzQwMw==", "bodyText": "year", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418123403", "createdAt": "2020-04-30T16:07:38Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/vti/KafkaVTI.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjQ1NA==", "bodyText": "Fixed in new commit 05cba38.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432454", "createdAt": "2020-05-01T06:21:08Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/vti/KafkaVTI.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzQwMw=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYzMTkwOnYy", "diffSide": "RIGHT", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoxMToyOFrOGOwYkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoyMToxMVrOGPDF0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNTk2OQ==", "bodyText": "year", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418125969", "createdAt": "2020-04-30T16:11:28Z", "author": {"login": "dgomezferro"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjQ2Nw==", "bodyText": "Fixed in new commit 05cba38.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432467", "createdAt": "2020-05-01T06:21:11Z", "author": {"login": "jpanko1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNTk2OQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYzNjA0OnYy", "diffSide": "LEFT", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoxMjozMlrOGOwbRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNTo1NDoxN1rOGPkLkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjY2MA==", "bodyText": "Shouldn't be removed", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418126660", "createdAt": "2020-04-30T16:12:32Z", "author": {"login": "dgomezferro"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -52,546 +52,542 @@\n      * compile system.\n      */\n     @Override\n-    public boolean canSupport(Properties startParams) {\n-        return Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n-    }\n+\tpublic boolean canSupport(Properties startParams) {\n+\t\treturn Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t\t@exception StandardException Ooops\n+\t  */\n+\n+\tpublic void boot(boolean create, Properties startParams) throws StandardException {\n+\t\t/*\n+\t\t** This system property determines whether to optimize join order\n+\t\t** by default.  It is used mainly for testing - there are many tests\n+\t\t** that assume the join order is fixed.\n+\t\t*/\n+\t\tString opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n+\t\tif (opt != null) {\n+\t\t\tjoinOrderOptimization = Boolean.valueOf(opt);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t  */\n+\n+\tpublic void stop() {\n+\t}\n+\n+\t/**\n+\t  Every Module needs a public niladic constructor. It just does.\n+\t  */\n+    public\tSpliceNodeFactoryImpl() {}\n+\n+\t/** @see NodeFactory#doJoinOrderOptimization */\n+\tpublic Boolean doJoinOrderOptimization() {\n+\t\treturn joinOrderOptimization;\n+\t}\n+\n+\t/**\n+\t * @see NodeFactory#getNode\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tpublic Node getNode(int nodeType, ContextManager cm) throws StandardException {\n+\n+\t\tClassInfo ci = nodeCi[nodeType];\n+\t\tClass nodeClass = null;\n+\t\tif (ci == null) {\n+\t\t\tString nodeName = nodeName(nodeType);\n+\t\t\ttry {\n+\t\t\t\tnodeClass = Class.forName(nodeName);\n+\t\t\t}\n+\t\t\tcatch (ClassNotFoundException cnfe) {\n+\t\t\t\tif (SanityManager.DEBUG) {\n+\t\t\t\t\tSanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcnfe);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tci = new ClassInfo(nodeClass);\n+\t\t\tnodeCi[nodeType] = ci;\n+\t\t}\n+\n+\t\tQueryTreeNode retval;\n+\n+\t\ttry {\n+\t\t\tretval = (QueryTreeNode) ci.getNewInstance();\n+\t\t\t//retval = (QueryTreeNode) nodeClass.newInstance();\n+\t\t} catch (Exception iae) {\n+\t\t\tthrow new RuntimeException(iae);\n+\t\t}\n+\n+\t\tretval.setContextManager(cm);\n+\t\tretval.setNodeType(nodeType);\n+\n+\t\treturn retval;\n+\t}\n+\n+\t/**\n+\t * Translate a node type from C_NodeTypes to a class name\n+\t *\n+\t * @param nodeType\tA node type identifier from C_NodeTypes\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tprotected String nodeName(int nodeType)\n+\t\t\tthrows StandardException\n+\t{\n+\t\tswitch (nodeType)\n+\t\t{\n+\t\t  // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n+\t\t  // THEM TO tools/jar/DBMSnode.properties\n+\t\t\t// xxxRESOLVE: why not make this a giant array and simply index into\n+\t\t\t// it? manish Thu Feb 22 14:49:41 PST 2001\n+\t\t  case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.GROUP_BY_LIST:\n+\t\t  \treturn C_NodeNames.GROUP_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ORDER_BY_LIST:\n+\t\t  \treturn C_NodeNames.ORDER_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.PREDICATE_LIST:\n+\t\t  \treturn C_NodeNames.PREDICATE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.RESULT_COLUMN_LIST:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.SUBQUERY_LIST:\n+\t\t  \treturn C_NodeNames.SUBQUERY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_LIST:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_NODE:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n \n-    /**\n-     @see Monitor\n-     @exception StandardException Ooops\n-     */\n-\n-    public void boot(boolean create, Properties startParams) throws StandardException {\n-        /*\n-         ** This system property determines whether to optimize join order\n-         ** by default.  It is used mainly for testing - there are many tests\n-         ** that assume the join order is fixed.\n-         */\n-        String opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n-        if (opt != null) {\n-            joinOrderOptimization = Boolean.valueOf(opt);\n-        }\n-    }\n-\n-    /**\n-     @see Monitor\n-     */\n-\n-    public void stop() {\n-    }\n-\n-    /**\n-     Every Module needs a public niladic constructor. It just does.\n-     */\n-    public    SpliceNodeFactoryImpl() {}\n-\n-    /** @see NodeFactory#doJoinOrderOptimization */\n-    public Boolean doJoinOrderOptimization() {\n-        return joinOrderOptimization;\n-    }\n-\n-    /**\n-     * @see NodeFactory#getNode\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    public Node getNode(int nodeType, ContextManager cm) throws StandardException {\n-\n-        ClassInfo ci = nodeCi[nodeType];\n-        Class nodeClass = null;\n-        if (ci == null) {\n-            String nodeName = nodeName(nodeType);\n-            try {\n-                nodeClass = Class.forName(nodeName);\n-            }\n-            catch (ClassNotFoundException cnfe) {\n-                if (SanityManager.DEBUG) {\n-                    SanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n-                            cnfe);\n-                }\n-            }\n-\n-            ci = new ClassInfo(nodeClass);\n-            nodeCi[nodeType] = ci;\n-        }\n-\n-        QueryTreeNode retval;\n-\n-        try {\n-            retval = (QueryTreeNode) ci.getNewInstance();\n-            //retval = (QueryTreeNode) nodeClass.newInstance();\n-        } catch (Exception iae) {\n-            throw new RuntimeException(iae);\n-        }\n-\n-        retval.setContextManager(cm);\n-        retval.setNodeType(nodeType);\n-\n-        return retval;\n-    }\n-\n-    /**\n-     * Translate a node type from C_NodeTypes to a class name\n-     *\n-     * @param nodeType    A node type identifier from C_NodeTypes\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    protected String nodeName(int nodeType)\n-            throws StandardException\n-    {\n-        switch (nodeType)\n-        {\n-            // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n-            // THEM TO tools/jar/DBMSnode.properties\n-            // xxxRESOLVE: why not make this a giant array and simply index into\n-            // it? manish Thu Feb 22 14:49:41 PST 2001\n-            case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n-                return C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n-\n-            case C_NodeTypes.GROUP_BY_LIST:\n-                return C_NodeNames.GROUP_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_NODE_LIST:\n+\t\t  \treturn C_NodeNames.VALUE_NODE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ALL_RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\n+\t\t  case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n+\t\t  \treturn C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.NOP_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.NOP_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_LIST:\n-                return C_NodeNames.ORDER_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n+\t\t  \treturn C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE_LIST:\n-                return C_NodeNames.PREDICATE_LIST_NAME;\n+\t\t  case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n+\t\t\treturn C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN_LIST:\n-                return C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\t\t  // ISNOTNULL compressed into ISNULL\n+\t\t  case C_NodeTypes.IS_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.IS_NULL_NODE:\n+\t\t  \treturn C_NodeNames.IS_NULL_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_LIST:\n-                return C_NodeNames.SUBQUERY_LIST_NAME;\n+\t\t  case C_NodeTypes.NOT_NODE:\n+\t\t  \treturn C_NodeNames.NOT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_LIST:\n-                return C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\t\t  case C_NodeTypes.TABLE_NAME:\n+\t\t  \treturn C_NodeNames.TABLE_NAME_NAME;\n \n-            case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n-                return C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.GROUP_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.GROUP_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_NODE:\n-                return C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.VALUE_TUPLE_NODE:\n-                return C_NodeNames.VALUE_TUPLE_NODE_NAME;\n+\t\t  case C_NodeTypes.FROM_LIST:\n+\t\t  \treturn C_NodeNames.FROM_LIST_NAME;\n \n-            case C_NodeTypes.VALUE_NODE_LIST:\n-                return C_NodeNames.VALUE_NODE_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_TUPLE_NODE:\n+\t\t\treturn C_NodeNames.VALUE_TUPLE_NODE_NAME;\n \n-            case C_NodeTypes.ALL_RESULT_COLUMN:\n-                return C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n-                return C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\t\t  case C_NodeTypes.LIST_VALUE_NODE:\n+\t\t    return C_NodeNames.LIST_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.NOP_STATEMENT_NODE:\n-                return C_NodeNames.NOP_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.AND_NODE:\n+\t\t  \treturn C_NodeNames.AND_NODE_NAME;\n \n-            case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n-                return C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n+\t\t\t  return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n-                return C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.MOD_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n \n-            // ISNOTNULL compressed into ISNULL\n-            case C_NodeTypes.IS_NOT_NULL_NODE:\n-            case C_NodeTypes.IS_NULL_NODE:\n-                return C_NodeNames.IS_NULL_NODE_NAME;\n+\t\t  case C_NodeTypes.COALESCE_FUNCTION_NODE:\n+\t\t  \treturn C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.NOT_NODE:\n-                return C_NodeNames.NOT_NODE_NAME;\n+\t\t  case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n-                return C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LIKE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_NAME:\n-                return C_NodeNames.TABLE_NAME_NAME;\n+\t\t  case C_NodeTypes.OR_NODE:\n+\t\t  \treturn C_NodeNames.OR_NODE_NAME;\n \n-            case C_NodeTypes.GROUP_BY_COLUMN:\n-                return C_NodeNames.GROUP_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n-                return C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONDITIONAL_NODE:\n+\t\t  \treturn C_NodeNames.CONDITIONAL_NODE_NAME;\n \n-            case C_NodeTypes.FROM_LIST:\n-                return C_NodeNames.FROM_LIST_NAME;\n+\t\t  case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n-                return C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.BIT_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.LIST_VALUE_NODE:\n-                return C_NodeNames.LIST_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARBIT_CONSTANT_NODE:\n+          case C_NodeTypes.BLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.AND_NODE:\n-                return C_NodeNames.AND_NODE_NAME;\n+\t\t  case C_NodeTypes.CAST_NODE:\n+\t\t  \treturn C_NodeNames.CAST_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n+          case C_NodeTypes.CLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n-            case C_NodeTypes.MOD_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n+          case C_NodeTypes.XML_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.XML_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.COALESCE_FUNCTION_NODE:\n-                return C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_REFERENCE:\n+\t\t  \treturn C_NodeNames.COLUMN_REFERENCE_NAME;\n \n-            case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n-                return C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_INDEX_NODE:\n+\t\t  \treturn C_NodeNames.DROP_INDEX_NODE_NAME;\n \n-            case C_NodeTypes.LIKE_OPERATOR_NODE:\n-                return C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_TRIGGER_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TRIGGER_NODE_NAME;\n \n-            case C_NodeTypes.OR_NODE:\n-                return C_NodeNames.OR_NODE_NAME;\n+\t\t  case C_NodeTypes.TINYINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.INT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.FLOAT_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n-                return C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.CONDITIONAL_NODE:\n-                return C_NodeNames.CONDITIONAL_NODE_NAME;\n+\t\t  case C_NodeTypes.PREDICATE:\n+\t\t  \treturn C_NodeNames.PREDICATE_NAME;\n \n-            case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n-                return C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_NAME;\n \n-            case C_NodeTypes.BIT_CONSTANT_NODE:\n-                return C_NodeNames.BIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.SET_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.VARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.BLOB_CONSTANT_NODE:\n-                return C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.SET_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.CAST_NODE:\n-                return C_NodeNames.CAST_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_CONSTANT_NODE:\n-            case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.CLOB_CONSTANT_NODE:\n-                return C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.XML_CONSTANT_NODE:\n-                return C_NodeNames.XML_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.COLUMN_REFERENCE:\n-                return C_NodeNames.COLUMN_REFERENCE_NAME;\n+\t\t  case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.DROP_INDEX_NODE:\n-                return C_NodeNames.DROP_INDEX_NODE_NAME;\n+\t\t  case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.DROP_TRIGGER_NODE:\n-                return C_NodeNames.DROP_TRIGGER_NODE_NAME;\n+\t\t  case C_NodeTypes.PARAMETER_NODE:\n+\t\t  \treturn C_NodeNames.PARAMETER_NODE_NAME;\n \n-            case C_NodeTypes.TINYINT_CONSTANT_NODE:\n-            case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n-            case C_NodeTypes.INT_CONSTANT_NODE:\n-            case C_NodeTypes.LONGINT_CONSTANT_NODE:\n-            case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n-            case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n-            case C_NodeTypes.FLOAT_CONSTANT_NODE:\n-                return C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.DROP_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n-                return C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE:\n-                return C_NodeNames.PREDICATE_NAME;\n+\t\t  case C_NodeTypes.DROP_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN:\n-                return C_NodeNames.RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.DROP_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.DROP_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.SET_ROLE_NODE:\n-                return C_NodeNames.SET_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SUBQUERY_NODE:\n+\t\t  \treturn C_NodeNames.SUBQUERY_NODE_NAME;\n \n-            case C_NodeTypes.SET_SCHEMA_NODE:\n-                return C_NodeNames.SET_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.BASE_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.BASE_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CALL_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.CALL_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n+          case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.DROP_COLUMN_NODE:\n+\t\t\treturn C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n-                return C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n+\t\t  case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_OF_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_OF_NODE_NAME;\n \n-            case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n-                return C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DEFAULT_NODE:\n+\t\t  \treturn C_NodeNames.DEFAULT_NODE_NAME;\n \n-            case C_NodeTypes.PARAMETER_NODE:\n-                return C_NodeNames.PARAMETER_NODE_NAME;\n+\t\t  case C_NodeTypes.DELETE_NODE:\n+\t\t  \treturn C_NodeNames.DELETE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SCHEMA_NODE:\n-                return C_NodeNames.DROP_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.UPDATE_NODE:\n+\t\t  \treturn C_NodeNames.UPDATE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_ROLE_NODE:\n-                return C_NodeNames.DROP_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.ORDER_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.DROP_TABLE_NODE:\n-                return C_NodeNames.DROP_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ROW_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.DROP_VIEW_NODE:\n-                return C_NodeNames.DROP_VIEW_NODE_NAME;\n+\t\t  case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_NODE:\n-                return C_NodeNames.SUBQUERY_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BASE_COLUMN_NODE:\n-                return C_NodeNames.BASE_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_USER_NODE:\n+\t\t  case C_NodeTypes.SESSION_USER_NODE:\n+\t\t  case C_NodeTypes.SYSTEM_USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_ISOLATION_NODE:\n+\t\t  case C_NodeTypes.IDENTITY_VAL_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SCHEMA_NODE:\n+          case C_NodeTypes.CURRENT_ROLE_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n+\t\t  case C_NodeTypes.GROUP_USER_NODE:\n+\t\t  \treturn C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CALL_STATEMENT_NODE:\n-                return C_NodeNames.CALL_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.IS_NODE:\n+\t\t  \treturn C_NodeNames.IS_NODE_NAME;\n \n-            case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n-            case C_NodeTypes.DROP_COLUMN_NODE:\n-                return C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.LOCK_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.LOCK_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.ALTER_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.ALTER_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_OF_NODE:\n-                return C_NodeNames.CURRENT_OF_NODE_NAME;\n+\t\t  case C_NodeTypes.AGGREGATE_NODE:\n+\t\t  \treturn C_NodeNames.AGGREGATE_NODE_NAME;\n \n-            case C_NodeTypes.DEFAULT_NODE:\n-                return C_NodeNames.DEFAULT_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.COLUMN_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.DELETE_NODE:\n-                return C_NodeNames.DELETE_NODE_NAME;\n+\t\t  case C_NodeTypes.EXEC_SPS_NODE:\n+\t\t  \treturn C_NodeNames.EXEC_SPS_NODE_NAME;\n \n-            case C_NodeTypes.UPDATE_NODE:\n-                return C_NodeNames.UPDATE_NODE_NAME;\n+\t\t  case C_NodeTypes.FK_CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.FK_CONSTRAINT_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_COLUMN:\n-                return C_NodeNames.ORDER_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.FROM_VTI:\n+\t\t  \treturn C_NodeNames.FROM_VTI_NAME;\n \n-            case C_NodeTypes.ROW_RESULT_SET_NODE:\n-                return C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n+\t\t  case C_NodeTypes.MATERIALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.MATERIALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n-                return C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.NORMALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.NORMALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n-                return C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.SCROLL_INSENSITIVE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.SCROLL_INSENSITIVE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.USER_NODE:\n-            case C_NodeTypes.CURRENT_USER_NODE:\n-            case C_NodeTypes.SESSION_USER_NODE:\n-            case C_NodeTypes.SYSTEM_USER_NODE:\n-            case C_NodeTypes.CURRENT_ISOLATION_NODE:\n-            case C_NodeTypes.IDENTITY_VAL_NODE:\n-            case C_NodeTypes.CURRENT_SCHEMA_NODE:\n-            case C_NodeTypes.CURRENT_ROLE_NODE:\n-            case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n-            case C_NodeTypes.GROUP_USER_NODE:\n-            case C_NodeTypes.CURRENT_SERVER_NODE:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 677}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3NDYxMA==", "bodyText": "Fixed in new commit 96dd889.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418974610", "createdAt": "2020-05-02T15:54:17Z", "author": {"login": "jpanko1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -52,546 +52,542 @@\n      * compile system.\n      */\n     @Override\n-    public boolean canSupport(Properties startParams) {\n-        return Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n-    }\n+\tpublic boolean canSupport(Properties startParams) {\n+\t\treturn Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t\t@exception StandardException Ooops\n+\t  */\n+\n+\tpublic void boot(boolean create, Properties startParams) throws StandardException {\n+\t\t/*\n+\t\t** This system property determines whether to optimize join order\n+\t\t** by default.  It is used mainly for testing - there are many tests\n+\t\t** that assume the join order is fixed.\n+\t\t*/\n+\t\tString opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n+\t\tif (opt != null) {\n+\t\t\tjoinOrderOptimization = Boolean.valueOf(opt);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t  */\n+\n+\tpublic void stop() {\n+\t}\n+\n+\t/**\n+\t  Every Module needs a public niladic constructor. It just does.\n+\t  */\n+    public\tSpliceNodeFactoryImpl() {}\n+\n+\t/** @see NodeFactory#doJoinOrderOptimization */\n+\tpublic Boolean doJoinOrderOptimization() {\n+\t\treturn joinOrderOptimization;\n+\t}\n+\n+\t/**\n+\t * @see NodeFactory#getNode\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tpublic Node getNode(int nodeType, ContextManager cm) throws StandardException {\n+\n+\t\tClassInfo ci = nodeCi[nodeType];\n+\t\tClass nodeClass = null;\n+\t\tif (ci == null) {\n+\t\t\tString nodeName = nodeName(nodeType);\n+\t\t\ttry {\n+\t\t\t\tnodeClass = Class.forName(nodeName);\n+\t\t\t}\n+\t\t\tcatch (ClassNotFoundException cnfe) {\n+\t\t\t\tif (SanityManager.DEBUG) {\n+\t\t\t\t\tSanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcnfe);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tci = new ClassInfo(nodeClass);\n+\t\t\tnodeCi[nodeType] = ci;\n+\t\t}\n+\n+\t\tQueryTreeNode retval;\n+\n+\t\ttry {\n+\t\t\tretval = (QueryTreeNode) ci.getNewInstance();\n+\t\t\t//retval = (QueryTreeNode) nodeClass.newInstance();\n+\t\t} catch (Exception iae) {\n+\t\t\tthrow new RuntimeException(iae);\n+\t\t}\n+\n+\t\tretval.setContextManager(cm);\n+\t\tretval.setNodeType(nodeType);\n+\n+\t\treturn retval;\n+\t}\n+\n+\t/**\n+\t * Translate a node type from C_NodeTypes to a class name\n+\t *\n+\t * @param nodeType\tA node type identifier from C_NodeTypes\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tprotected String nodeName(int nodeType)\n+\t\t\tthrows StandardException\n+\t{\n+\t\tswitch (nodeType)\n+\t\t{\n+\t\t  // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n+\t\t  // THEM TO tools/jar/DBMSnode.properties\n+\t\t\t// xxxRESOLVE: why not make this a giant array and simply index into\n+\t\t\t// it? manish Thu Feb 22 14:49:41 PST 2001\n+\t\t  case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.GROUP_BY_LIST:\n+\t\t  \treturn C_NodeNames.GROUP_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ORDER_BY_LIST:\n+\t\t  \treturn C_NodeNames.ORDER_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.PREDICATE_LIST:\n+\t\t  \treturn C_NodeNames.PREDICATE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.RESULT_COLUMN_LIST:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.SUBQUERY_LIST:\n+\t\t  \treturn C_NodeNames.SUBQUERY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_LIST:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_NODE:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n \n-    /**\n-     @see Monitor\n-     @exception StandardException Ooops\n-     */\n-\n-    public void boot(boolean create, Properties startParams) throws StandardException {\n-        /*\n-         ** This system property determines whether to optimize join order\n-         ** by default.  It is used mainly for testing - there are many tests\n-         ** that assume the join order is fixed.\n-         */\n-        String opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n-        if (opt != null) {\n-            joinOrderOptimization = Boolean.valueOf(opt);\n-        }\n-    }\n-\n-    /**\n-     @see Monitor\n-     */\n-\n-    public void stop() {\n-    }\n-\n-    /**\n-     Every Module needs a public niladic constructor. It just does.\n-     */\n-    public    SpliceNodeFactoryImpl() {}\n-\n-    /** @see NodeFactory#doJoinOrderOptimization */\n-    public Boolean doJoinOrderOptimization() {\n-        return joinOrderOptimization;\n-    }\n-\n-    /**\n-     * @see NodeFactory#getNode\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    public Node getNode(int nodeType, ContextManager cm) throws StandardException {\n-\n-        ClassInfo ci = nodeCi[nodeType];\n-        Class nodeClass = null;\n-        if (ci == null) {\n-            String nodeName = nodeName(nodeType);\n-            try {\n-                nodeClass = Class.forName(nodeName);\n-            }\n-            catch (ClassNotFoundException cnfe) {\n-                if (SanityManager.DEBUG) {\n-                    SanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n-                            cnfe);\n-                }\n-            }\n-\n-            ci = new ClassInfo(nodeClass);\n-            nodeCi[nodeType] = ci;\n-        }\n-\n-        QueryTreeNode retval;\n-\n-        try {\n-            retval = (QueryTreeNode) ci.getNewInstance();\n-            //retval = (QueryTreeNode) nodeClass.newInstance();\n-        } catch (Exception iae) {\n-            throw new RuntimeException(iae);\n-        }\n-\n-        retval.setContextManager(cm);\n-        retval.setNodeType(nodeType);\n-\n-        return retval;\n-    }\n-\n-    /**\n-     * Translate a node type from C_NodeTypes to a class name\n-     *\n-     * @param nodeType    A node type identifier from C_NodeTypes\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    protected String nodeName(int nodeType)\n-            throws StandardException\n-    {\n-        switch (nodeType)\n-        {\n-            // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n-            // THEM TO tools/jar/DBMSnode.properties\n-            // xxxRESOLVE: why not make this a giant array and simply index into\n-            // it? manish Thu Feb 22 14:49:41 PST 2001\n-            case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n-                return C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n-\n-            case C_NodeTypes.GROUP_BY_LIST:\n-                return C_NodeNames.GROUP_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_NODE_LIST:\n+\t\t  \treturn C_NodeNames.VALUE_NODE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ALL_RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\n+\t\t  case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n+\t\t  \treturn C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.NOP_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.NOP_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_LIST:\n-                return C_NodeNames.ORDER_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n+\t\t  \treturn C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE_LIST:\n-                return C_NodeNames.PREDICATE_LIST_NAME;\n+\t\t  case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n+\t\t\treturn C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN_LIST:\n-                return C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\t\t  // ISNOTNULL compressed into ISNULL\n+\t\t  case C_NodeTypes.IS_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.IS_NULL_NODE:\n+\t\t  \treturn C_NodeNames.IS_NULL_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_LIST:\n-                return C_NodeNames.SUBQUERY_LIST_NAME;\n+\t\t  case C_NodeTypes.NOT_NODE:\n+\t\t  \treturn C_NodeNames.NOT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_LIST:\n-                return C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\t\t  case C_NodeTypes.TABLE_NAME:\n+\t\t  \treturn C_NodeNames.TABLE_NAME_NAME;\n \n-            case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n-                return C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.GROUP_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.GROUP_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_NODE:\n-                return C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.VALUE_TUPLE_NODE:\n-                return C_NodeNames.VALUE_TUPLE_NODE_NAME;\n+\t\t  case C_NodeTypes.FROM_LIST:\n+\t\t  \treturn C_NodeNames.FROM_LIST_NAME;\n \n-            case C_NodeTypes.VALUE_NODE_LIST:\n-                return C_NodeNames.VALUE_NODE_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_TUPLE_NODE:\n+\t\t\treturn C_NodeNames.VALUE_TUPLE_NODE_NAME;\n \n-            case C_NodeTypes.ALL_RESULT_COLUMN:\n-                return C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n-                return C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\t\t  case C_NodeTypes.LIST_VALUE_NODE:\n+\t\t    return C_NodeNames.LIST_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.NOP_STATEMENT_NODE:\n-                return C_NodeNames.NOP_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.AND_NODE:\n+\t\t  \treturn C_NodeNames.AND_NODE_NAME;\n \n-            case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n-                return C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n+\t\t\t  return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n-                return C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.MOD_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n \n-            // ISNOTNULL compressed into ISNULL\n-            case C_NodeTypes.IS_NOT_NULL_NODE:\n-            case C_NodeTypes.IS_NULL_NODE:\n-                return C_NodeNames.IS_NULL_NODE_NAME;\n+\t\t  case C_NodeTypes.COALESCE_FUNCTION_NODE:\n+\t\t  \treturn C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.NOT_NODE:\n-                return C_NodeNames.NOT_NODE_NAME;\n+\t\t  case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n-                return C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LIKE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_NAME:\n-                return C_NodeNames.TABLE_NAME_NAME;\n+\t\t  case C_NodeTypes.OR_NODE:\n+\t\t  \treturn C_NodeNames.OR_NODE_NAME;\n \n-            case C_NodeTypes.GROUP_BY_COLUMN:\n-                return C_NodeNames.GROUP_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n-                return C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONDITIONAL_NODE:\n+\t\t  \treturn C_NodeNames.CONDITIONAL_NODE_NAME;\n \n-            case C_NodeTypes.FROM_LIST:\n-                return C_NodeNames.FROM_LIST_NAME;\n+\t\t  case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n-                return C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.BIT_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.LIST_VALUE_NODE:\n-                return C_NodeNames.LIST_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARBIT_CONSTANT_NODE:\n+          case C_NodeTypes.BLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.AND_NODE:\n-                return C_NodeNames.AND_NODE_NAME;\n+\t\t  case C_NodeTypes.CAST_NODE:\n+\t\t  \treturn C_NodeNames.CAST_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n+          case C_NodeTypes.CLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n-            case C_NodeTypes.MOD_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n+          case C_NodeTypes.XML_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.XML_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.COALESCE_FUNCTION_NODE:\n-                return C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_REFERENCE:\n+\t\t  \treturn C_NodeNames.COLUMN_REFERENCE_NAME;\n \n-            case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n-                return C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_INDEX_NODE:\n+\t\t  \treturn C_NodeNames.DROP_INDEX_NODE_NAME;\n \n-            case C_NodeTypes.LIKE_OPERATOR_NODE:\n-                return C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_TRIGGER_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TRIGGER_NODE_NAME;\n \n-            case C_NodeTypes.OR_NODE:\n-                return C_NodeNames.OR_NODE_NAME;\n+\t\t  case C_NodeTypes.TINYINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.INT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.FLOAT_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n-                return C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.CONDITIONAL_NODE:\n-                return C_NodeNames.CONDITIONAL_NODE_NAME;\n+\t\t  case C_NodeTypes.PREDICATE:\n+\t\t  \treturn C_NodeNames.PREDICATE_NAME;\n \n-            case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n-                return C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_NAME;\n \n-            case C_NodeTypes.BIT_CONSTANT_NODE:\n-                return C_NodeNames.BIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.SET_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.VARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.BLOB_CONSTANT_NODE:\n-                return C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.SET_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.CAST_NODE:\n-                return C_NodeNames.CAST_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_CONSTANT_NODE:\n-            case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.CLOB_CONSTANT_NODE:\n-                return C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.XML_CONSTANT_NODE:\n-                return C_NodeNames.XML_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.COLUMN_REFERENCE:\n-                return C_NodeNames.COLUMN_REFERENCE_NAME;\n+\t\t  case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.DROP_INDEX_NODE:\n-                return C_NodeNames.DROP_INDEX_NODE_NAME;\n+\t\t  case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.DROP_TRIGGER_NODE:\n-                return C_NodeNames.DROP_TRIGGER_NODE_NAME;\n+\t\t  case C_NodeTypes.PARAMETER_NODE:\n+\t\t  \treturn C_NodeNames.PARAMETER_NODE_NAME;\n \n-            case C_NodeTypes.TINYINT_CONSTANT_NODE:\n-            case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n-            case C_NodeTypes.INT_CONSTANT_NODE:\n-            case C_NodeTypes.LONGINT_CONSTANT_NODE:\n-            case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n-            case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n-            case C_NodeTypes.FLOAT_CONSTANT_NODE:\n-                return C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.DROP_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n-                return C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE:\n-                return C_NodeNames.PREDICATE_NAME;\n+\t\t  case C_NodeTypes.DROP_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN:\n-                return C_NodeNames.RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.DROP_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.DROP_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.SET_ROLE_NODE:\n-                return C_NodeNames.SET_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SUBQUERY_NODE:\n+\t\t  \treturn C_NodeNames.SUBQUERY_NODE_NAME;\n \n-            case C_NodeTypes.SET_SCHEMA_NODE:\n-                return C_NodeNames.SET_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.BASE_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.BASE_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CALL_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.CALL_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n+          case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.DROP_COLUMN_NODE:\n+\t\t\treturn C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n-                return C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n+\t\t  case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_OF_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_OF_NODE_NAME;\n \n-            case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n-                return C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DEFAULT_NODE:\n+\t\t  \treturn C_NodeNames.DEFAULT_NODE_NAME;\n \n-            case C_NodeTypes.PARAMETER_NODE:\n-                return C_NodeNames.PARAMETER_NODE_NAME;\n+\t\t  case C_NodeTypes.DELETE_NODE:\n+\t\t  \treturn C_NodeNames.DELETE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SCHEMA_NODE:\n-                return C_NodeNames.DROP_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.UPDATE_NODE:\n+\t\t  \treturn C_NodeNames.UPDATE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_ROLE_NODE:\n-                return C_NodeNames.DROP_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.ORDER_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.DROP_TABLE_NODE:\n-                return C_NodeNames.DROP_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ROW_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.DROP_VIEW_NODE:\n-                return C_NodeNames.DROP_VIEW_NODE_NAME;\n+\t\t  case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_NODE:\n-                return C_NodeNames.SUBQUERY_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BASE_COLUMN_NODE:\n-                return C_NodeNames.BASE_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_USER_NODE:\n+\t\t  case C_NodeTypes.SESSION_USER_NODE:\n+\t\t  case C_NodeTypes.SYSTEM_USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_ISOLATION_NODE:\n+\t\t  case C_NodeTypes.IDENTITY_VAL_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SCHEMA_NODE:\n+          case C_NodeTypes.CURRENT_ROLE_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n+\t\t  case C_NodeTypes.GROUP_USER_NODE:\n+\t\t  \treturn C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CALL_STATEMENT_NODE:\n-                return C_NodeNames.CALL_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.IS_NODE:\n+\t\t  \treturn C_NodeNames.IS_NODE_NAME;\n \n-            case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n-            case C_NodeTypes.DROP_COLUMN_NODE:\n-                return C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.LOCK_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.LOCK_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.ALTER_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.ALTER_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_OF_NODE:\n-                return C_NodeNames.CURRENT_OF_NODE_NAME;\n+\t\t  case C_NodeTypes.AGGREGATE_NODE:\n+\t\t  \treturn C_NodeNames.AGGREGATE_NODE_NAME;\n \n-            case C_NodeTypes.DEFAULT_NODE:\n-                return C_NodeNames.DEFAULT_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.COLUMN_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.DELETE_NODE:\n-                return C_NodeNames.DELETE_NODE_NAME;\n+\t\t  case C_NodeTypes.EXEC_SPS_NODE:\n+\t\t  \treturn C_NodeNames.EXEC_SPS_NODE_NAME;\n \n-            case C_NodeTypes.UPDATE_NODE:\n-                return C_NodeNames.UPDATE_NODE_NAME;\n+\t\t  case C_NodeTypes.FK_CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.FK_CONSTRAINT_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_COLUMN:\n-                return C_NodeNames.ORDER_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.FROM_VTI:\n+\t\t  \treturn C_NodeNames.FROM_VTI_NAME;\n \n-            case C_NodeTypes.ROW_RESULT_SET_NODE:\n-                return C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n+\t\t  case C_NodeTypes.MATERIALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.MATERIALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n-                return C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.NORMALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.NORMALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n-                return C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.SCROLL_INSENSITIVE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.SCROLL_INSENSITIVE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.USER_NODE:\n-            case C_NodeTypes.CURRENT_USER_NODE:\n-            case C_NodeTypes.SESSION_USER_NODE:\n-            case C_NodeTypes.SYSTEM_USER_NODE:\n-            case C_NodeTypes.CURRENT_ISOLATION_NODE:\n-            case C_NodeTypes.IDENTITY_VAL_NODE:\n-            case C_NodeTypes.CURRENT_SCHEMA_NODE:\n-            case C_NodeTypes.CURRENT_ROLE_NODE:\n-            case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n-            case C_NodeTypes.GROUP_USER_NODE:\n-            case C_NodeTypes.CURRENT_SERVER_NODE:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjY2MA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 677}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYzNjYxOnYy", "diffSide": "LEFT", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoxMjo0MlrOGOwbrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNTo1NDoyMlrOGPkLnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjc2NA==", "bodyText": "Shouldn't be removed", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418126764", "createdAt": "2020-04-30T16:12:42Z", "author": {"login": "dgomezferro"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -52,546 +52,542 @@\n      * compile system.\n      */\n     @Override\n-    public boolean canSupport(Properties startParams) {\n-        return Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n-    }\n+\tpublic boolean canSupport(Properties startParams) {\n+\t\treturn Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t\t@exception StandardException Ooops\n+\t  */\n+\n+\tpublic void boot(boolean create, Properties startParams) throws StandardException {\n+\t\t/*\n+\t\t** This system property determines whether to optimize join order\n+\t\t** by default.  It is used mainly for testing - there are many tests\n+\t\t** that assume the join order is fixed.\n+\t\t*/\n+\t\tString opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n+\t\tif (opt != null) {\n+\t\t\tjoinOrderOptimization = Boolean.valueOf(opt);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t  */\n+\n+\tpublic void stop() {\n+\t}\n+\n+\t/**\n+\t  Every Module needs a public niladic constructor. It just does.\n+\t  */\n+    public\tSpliceNodeFactoryImpl() {}\n+\n+\t/** @see NodeFactory#doJoinOrderOptimization */\n+\tpublic Boolean doJoinOrderOptimization() {\n+\t\treturn joinOrderOptimization;\n+\t}\n+\n+\t/**\n+\t * @see NodeFactory#getNode\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tpublic Node getNode(int nodeType, ContextManager cm) throws StandardException {\n+\n+\t\tClassInfo ci = nodeCi[nodeType];\n+\t\tClass nodeClass = null;\n+\t\tif (ci == null) {\n+\t\t\tString nodeName = nodeName(nodeType);\n+\t\t\ttry {\n+\t\t\t\tnodeClass = Class.forName(nodeName);\n+\t\t\t}\n+\t\t\tcatch (ClassNotFoundException cnfe) {\n+\t\t\t\tif (SanityManager.DEBUG) {\n+\t\t\t\t\tSanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcnfe);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tci = new ClassInfo(nodeClass);\n+\t\t\tnodeCi[nodeType] = ci;\n+\t\t}\n+\n+\t\tQueryTreeNode retval;\n+\n+\t\ttry {\n+\t\t\tretval = (QueryTreeNode) ci.getNewInstance();\n+\t\t\t//retval = (QueryTreeNode) nodeClass.newInstance();\n+\t\t} catch (Exception iae) {\n+\t\t\tthrow new RuntimeException(iae);\n+\t\t}\n+\n+\t\tretval.setContextManager(cm);\n+\t\tretval.setNodeType(nodeType);\n+\n+\t\treturn retval;\n+\t}\n+\n+\t/**\n+\t * Translate a node type from C_NodeTypes to a class name\n+\t *\n+\t * @param nodeType\tA node type identifier from C_NodeTypes\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tprotected String nodeName(int nodeType)\n+\t\t\tthrows StandardException\n+\t{\n+\t\tswitch (nodeType)\n+\t\t{\n+\t\t  // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n+\t\t  // THEM TO tools/jar/DBMSnode.properties\n+\t\t\t// xxxRESOLVE: why not make this a giant array and simply index into\n+\t\t\t// it? manish Thu Feb 22 14:49:41 PST 2001\n+\t\t  case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.GROUP_BY_LIST:\n+\t\t  \treturn C_NodeNames.GROUP_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ORDER_BY_LIST:\n+\t\t  \treturn C_NodeNames.ORDER_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.PREDICATE_LIST:\n+\t\t  \treturn C_NodeNames.PREDICATE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.RESULT_COLUMN_LIST:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.SUBQUERY_LIST:\n+\t\t  \treturn C_NodeNames.SUBQUERY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_LIST:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_NODE:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n \n-    /**\n-     @see Monitor\n-     @exception StandardException Ooops\n-     */\n-\n-    public void boot(boolean create, Properties startParams) throws StandardException {\n-        /*\n-         ** This system property determines whether to optimize join order\n-         ** by default.  It is used mainly for testing - there are many tests\n-         ** that assume the join order is fixed.\n-         */\n-        String opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n-        if (opt != null) {\n-            joinOrderOptimization = Boolean.valueOf(opt);\n-        }\n-    }\n-\n-    /**\n-     @see Monitor\n-     */\n-\n-    public void stop() {\n-    }\n-\n-    /**\n-     Every Module needs a public niladic constructor. It just does.\n-     */\n-    public    SpliceNodeFactoryImpl() {}\n-\n-    /** @see NodeFactory#doJoinOrderOptimization */\n-    public Boolean doJoinOrderOptimization() {\n-        return joinOrderOptimization;\n-    }\n-\n-    /**\n-     * @see NodeFactory#getNode\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    public Node getNode(int nodeType, ContextManager cm) throws StandardException {\n-\n-        ClassInfo ci = nodeCi[nodeType];\n-        Class nodeClass = null;\n-        if (ci == null) {\n-            String nodeName = nodeName(nodeType);\n-            try {\n-                nodeClass = Class.forName(nodeName);\n-            }\n-            catch (ClassNotFoundException cnfe) {\n-                if (SanityManager.DEBUG) {\n-                    SanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n-                            cnfe);\n-                }\n-            }\n-\n-            ci = new ClassInfo(nodeClass);\n-            nodeCi[nodeType] = ci;\n-        }\n-\n-        QueryTreeNode retval;\n-\n-        try {\n-            retval = (QueryTreeNode) ci.getNewInstance();\n-            //retval = (QueryTreeNode) nodeClass.newInstance();\n-        } catch (Exception iae) {\n-            throw new RuntimeException(iae);\n-        }\n-\n-        retval.setContextManager(cm);\n-        retval.setNodeType(nodeType);\n-\n-        return retval;\n-    }\n-\n-    /**\n-     * Translate a node type from C_NodeTypes to a class name\n-     *\n-     * @param nodeType    A node type identifier from C_NodeTypes\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    protected String nodeName(int nodeType)\n-            throws StandardException\n-    {\n-        switch (nodeType)\n-        {\n-            // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n-            // THEM TO tools/jar/DBMSnode.properties\n-            // xxxRESOLVE: why not make this a giant array and simply index into\n-            // it? manish Thu Feb 22 14:49:41 PST 2001\n-            case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n-                return C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n-\n-            case C_NodeTypes.GROUP_BY_LIST:\n-                return C_NodeNames.GROUP_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_NODE_LIST:\n+\t\t  \treturn C_NodeNames.VALUE_NODE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ALL_RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\n+\t\t  case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n+\t\t  \treturn C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.NOP_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.NOP_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_LIST:\n-                return C_NodeNames.ORDER_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n+\t\t  \treturn C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE_LIST:\n-                return C_NodeNames.PREDICATE_LIST_NAME;\n+\t\t  case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n+\t\t\treturn C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN_LIST:\n-                return C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\t\t  // ISNOTNULL compressed into ISNULL\n+\t\t  case C_NodeTypes.IS_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.IS_NULL_NODE:\n+\t\t  \treturn C_NodeNames.IS_NULL_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_LIST:\n-                return C_NodeNames.SUBQUERY_LIST_NAME;\n+\t\t  case C_NodeTypes.NOT_NODE:\n+\t\t  \treturn C_NodeNames.NOT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_LIST:\n-                return C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\t\t  case C_NodeTypes.TABLE_NAME:\n+\t\t  \treturn C_NodeNames.TABLE_NAME_NAME;\n \n-            case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n-                return C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.GROUP_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.GROUP_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_NODE:\n-                return C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.VALUE_TUPLE_NODE:\n-                return C_NodeNames.VALUE_TUPLE_NODE_NAME;\n+\t\t  case C_NodeTypes.FROM_LIST:\n+\t\t  \treturn C_NodeNames.FROM_LIST_NAME;\n \n-            case C_NodeTypes.VALUE_NODE_LIST:\n-                return C_NodeNames.VALUE_NODE_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_TUPLE_NODE:\n+\t\t\treturn C_NodeNames.VALUE_TUPLE_NODE_NAME;\n \n-            case C_NodeTypes.ALL_RESULT_COLUMN:\n-                return C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n-                return C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\t\t  case C_NodeTypes.LIST_VALUE_NODE:\n+\t\t    return C_NodeNames.LIST_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.NOP_STATEMENT_NODE:\n-                return C_NodeNames.NOP_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.AND_NODE:\n+\t\t  \treturn C_NodeNames.AND_NODE_NAME;\n \n-            case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n-                return C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n+\t\t\t  return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n-                return C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.MOD_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n \n-            // ISNOTNULL compressed into ISNULL\n-            case C_NodeTypes.IS_NOT_NULL_NODE:\n-            case C_NodeTypes.IS_NULL_NODE:\n-                return C_NodeNames.IS_NULL_NODE_NAME;\n+\t\t  case C_NodeTypes.COALESCE_FUNCTION_NODE:\n+\t\t  \treturn C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.NOT_NODE:\n-                return C_NodeNames.NOT_NODE_NAME;\n+\t\t  case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n-                return C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LIKE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_NAME:\n-                return C_NodeNames.TABLE_NAME_NAME;\n+\t\t  case C_NodeTypes.OR_NODE:\n+\t\t  \treturn C_NodeNames.OR_NODE_NAME;\n \n-            case C_NodeTypes.GROUP_BY_COLUMN:\n-                return C_NodeNames.GROUP_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n-                return C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONDITIONAL_NODE:\n+\t\t  \treturn C_NodeNames.CONDITIONAL_NODE_NAME;\n \n-            case C_NodeTypes.FROM_LIST:\n-                return C_NodeNames.FROM_LIST_NAME;\n+\t\t  case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n-                return C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.BIT_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.LIST_VALUE_NODE:\n-                return C_NodeNames.LIST_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARBIT_CONSTANT_NODE:\n+          case C_NodeTypes.BLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.AND_NODE:\n-                return C_NodeNames.AND_NODE_NAME;\n+\t\t  case C_NodeTypes.CAST_NODE:\n+\t\t  \treturn C_NodeNames.CAST_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n+          case C_NodeTypes.CLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n-            case C_NodeTypes.MOD_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n+          case C_NodeTypes.XML_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.XML_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.COALESCE_FUNCTION_NODE:\n-                return C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_REFERENCE:\n+\t\t  \treturn C_NodeNames.COLUMN_REFERENCE_NAME;\n \n-            case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n-                return C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_INDEX_NODE:\n+\t\t  \treturn C_NodeNames.DROP_INDEX_NODE_NAME;\n \n-            case C_NodeTypes.LIKE_OPERATOR_NODE:\n-                return C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_TRIGGER_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TRIGGER_NODE_NAME;\n \n-            case C_NodeTypes.OR_NODE:\n-                return C_NodeNames.OR_NODE_NAME;\n+\t\t  case C_NodeTypes.TINYINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.INT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.FLOAT_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n-                return C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.CONDITIONAL_NODE:\n-                return C_NodeNames.CONDITIONAL_NODE_NAME;\n+\t\t  case C_NodeTypes.PREDICATE:\n+\t\t  \treturn C_NodeNames.PREDICATE_NAME;\n \n-            case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n-                return C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_NAME;\n \n-            case C_NodeTypes.BIT_CONSTANT_NODE:\n-                return C_NodeNames.BIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.SET_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.VARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.BLOB_CONSTANT_NODE:\n-                return C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.SET_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.CAST_NODE:\n-                return C_NodeNames.CAST_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_CONSTANT_NODE:\n-            case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.CLOB_CONSTANT_NODE:\n-                return C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.XML_CONSTANT_NODE:\n-                return C_NodeNames.XML_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.COLUMN_REFERENCE:\n-                return C_NodeNames.COLUMN_REFERENCE_NAME;\n+\t\t  case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.DROP_INDEX_NODE:\n-                return C_NodeNames.DROP_INDEX_NODE_NAME;\n+\t\t  case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.DROP_TRIGGER_NODE:\n-                return C_NodeNames.DROP_TRIGGER_NODE_NAME;\n+\t\t  case C_NodeTypes.PARAMETER_NODE:\n+\t\t  \treturn C_NodeNames.PARAMETER_NODE_NAME;\n \n-            case C_NodeTypes.TINYINT_CONSTANT_NODE:\n-            case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n-            case C_NodeTypes.INT_CONSTANT_NODE:\n-            case C_NodeTypes.LONGINT_CONSTANT_NODE:\n-            case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n-            case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n-            case C_NodeTypes.FLOAT_CONSTANT_NODE:\n-                return C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.DROP_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n-                return C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE:\n-                return C_NodeNames.PREDICATE_NAME;\n+\t\t  case C_NodeTypes.DROP_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN:\n-                return C_NodeNames.RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.DROP_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.DROP_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.SET_ROLE_NODE:\n-                return C_NodeNames.SET_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SUBQUERY_NODE:\n+\t\t  \treturn C_NodeNames.SUBQUERY_NODE_NAME;\n \n-            case C_NodeTypes.SET_SCHEMA_NODE:\n-                return C_NodeNames.SET_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.BASE_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.BASE_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CALL_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.CALL_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n+          case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.DROP_COLUMN_NODE:\n+\t\t\treturn C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n-                return C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n+\t\t  case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_OF_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_OF_NODE_NAME;\n \n-            case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n-                return C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DEFAULT_NODE:\n+\t\t  \treturn C_NodeNames.DEFAULT_NODE_NAME;\n \n-            case C_NodeTypes.PARAMETER_NODE:\n-                return C_NodeNames.PARAMETER_NODE_NAME;\n+\t\t  case C_NodeTypes.DELETE_NODE:\n+\t\t  \treturn C_NodeNames.DELETE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SCHEMA_NODE:\n-                return C_NodeNames.DROP_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.UPDATE_NODE:\n+\t\t  \treturn C_NodeNames.UPDATE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_ROLE_NODE:\n-                return C_NodeNames.DROP_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.ORDER_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.DROP_TABLE_NODE:\n-                return C_NodeNames.DROP_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ROW_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.DROP_VIEW_NODE:\n-                return C_NodeNames.DROP_VIEW_NODE_NAME;\n+\t\t  case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_NODE:\n-                return C_NodeNames.SUBQUERY_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BASE_COLUMN_NODE:\n-                return C_NodeNames.BASE_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_USER_NODE:\n+\t\t  case C_NodeTypes.SESSION_USER_NODE:\n+\t\t  case C_NodeTypes.SYSTEM_USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_ISOLATION_NODE:\n+\t\t  case C_NodeTypes.IDENTITY_VAL_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SCHEMA_NODE:\n+          case C_NodeTypes.CURRENT_ROLE_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n+\t\t  case C_NodeTypes.GROUP_USER_NODE:\n+\t\t  \treturn C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CALL_STATEMENT_NODE:\n-                return C_NodeNames.CALL_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.IS_NODE:\n+\t\t  \treturn C_NodeNames.IS_NODE_NAME;\n \n-            case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n-            case C_NodeTypes.DROP_COLUMN_NODE:\n-                return C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.LOCK_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.LOCK_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.ALTER_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.ALTER_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_OF_NODE:\n-                return C_NodeNames.CURRENT_OF_NODE_NAME;\n+\t\t  case C_NodeTypes.AGGREGATE_NODE:\n+\t\t  \treturn C_NodeNames.AGGREGATE_NODE_NAME;\n \n-            case C_NodeTypes.DEFAULT_NODE:\n-                return C_NodeNames.DEFAULT_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.COLUMN_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.DELETE_NODE:\n-                return C_NodeNames.DELETE_NODE_NAME;\n+\t\t  case C_NodeTypes.EXEC_SPS_NODE:\n+\t\t  \treturn C_NodeNames.EXEC_SPS_NODE_NAME;\n \n-            case C_NodeTypes.UPDATE_NODE:\n-                return C_NodeNames.UPDATE_NODE_NAME;\n+\t\t  case C_NodeTypes.FK_CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.FK_CONSTRAINT_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_COLUMN:\n-                return C_NodeNames.ORDER_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.FROM_VTI:\n+\t\t  \treturn C_NodeNames.FROM_VTI_NAME;\n \n-            case C_NodeTypes.ROW_RESULT_SET_NODE:\n-                return C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n+\t\t  case C_NodeTypes.MATERIALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.MATERIALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n-                return C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.NORMALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.NORMALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n-                return C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.SCROLL_INSENSITIVE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.SCROLL_INSENSITIVE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.USER_NODE:\n-            case C_NodeTypes.CURRENT_USER_NODE:\n-            case C_NodeTypes.SESSION_USER_NODE:\n-            case C_NodeTypes.SYSTEM_USER_NODE:\n-            case C_NodeTypes.CURRENT_ISOLATION_NODE:\n-            case C_NodeTypes.IDENTITY_VAL_NODE:\n-            case C_NodeTypes.CURRENT_SCHEMA_NODE:\n-            case C_NodeTypes.CURRENT_ROLE_NODE:\n-            case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n-            case C_NodeTypes.GROUP_USER_NODE:\n-            case C_NodeTypes.CURRENT_SERVER_NODE:\n-                return C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_NODE:\n+              return C_NodeNames.ORDER_BY_NODE_NAME;\n \n-            case C_NodeTypes.IS_NODE:\n-                return C_NodeNames.IS_NODE_NAME;\n+\t\t  case C_NodeTypes.DISTINCT_NODE:\n+\t\t  \treturn C_NodeNames.DISTINCT_NODE_NAME;\n \n-            case C_NodeTypes.LOCK_TABLE_NODE:\n-                return C_NodeNames.LOCK_TABLE_NODE_NAME;\n+          case C_NodeTypes.LOCATE_FUNCTION_NODE:\n+\t\t  case C_NodeTypes.SUBSTRING_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.RIGHT_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.TRIM_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.LEFT_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.TIMESTAMP_ADD_FN_NODE:\n+\t\t  case C_NodeTypes.TIMESTAMP_DIFF_FN_NODE:\n+\t\t  case C_NodeTypes.REPLACE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.TERNARY_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.ALTER_TABLE_NODE:\n-                return C_NodeNames.ALTER_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SELECT_NODE:\n+\t\t  \treturn C_NodeNames.SELECT_NODE_NAME;\n \n-            case C_NodeTypes.AGGREGATE_NODE:\n-                return C_NodeNames.AGGREGATE_NODE_NAME;\n+\t\t  case C_NodeTypes.CREATE_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.CREATE_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.STRING_AGGREGATE_NODE:\n-                return C_NodeNames.STRING_AGGREGATE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.CONSTRAINT_DEFINITION_NODE_NAME;\n ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 713}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3NDYyMA==", "bodyText": "Fixed in new commit 96dd889.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418974620", "createdAt": "2020-05-02T15:54:22Z", "author": {"login": "jpanko1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -52,546 +52,542 @@\n      * compile system.\n      */\n     @Override\n-    public boolean canSupport(Properties startParams) {\n-        return Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n-    }\n+\tpublic boolean canSupport(Properties startParams) {\n+\t\treturn Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t\t@exception StandardException Ooops\n+\t  */\n+\n+\tpublic void boot(boolean create, Properties startParams) throws StandardException {\n+\t\t/*\n+\t\t** This system property determines whether to optimize join order\n+\t\t** by default.  It is used mainly for testing - there are many tests\n+\t\t** that assume the join order is fixed.\n+\t\t*/\n+\t\tString opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n+\t\tif (opt != null) {\n+\t\t\tjoinOrderOptimization = Boolean.valueOf(opt);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t  */\n+\n+\tpublic void stop() {\n+\t}\n+\n+\t/**\n+\t  Every Module needs a public niladic constructor. It just does.\n+\t  */\n+    public\tSpliceNodeFactoryImpl() {}\n+\n+\t/** @see NodeFactory#doJoinOrderOptimization */\n+\tpublic Boolean doJoinOrderOptimization() {\n+\t\treturn joinOrderOptimization;\n+\t}\n+\n+\t/**\n+\t * @see NodeFactory#getNode\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tpublic Node getNode(int nodeType, ContextManager cm) throws StandardException {\n+\n+\t\tClassInfo ci = nodeCi[nodeType];\n+\t\tClass nodeClass = null;\n+\t\tif (ci == null) {\n+\t\t\tString nodeName = nodeName(nodeType);\n+\t\t\ttry {\n+\t\t\t\tnodeClass = Class.forName(nodeName);\n+\t\t\t}\n+\t\t\tcatch (ClassNotFoundException cnfe) {\n+\t\t\t\tif (SanityManager.DEBUG) {\n+\t\t\t\t\tSanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcnfe);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tci = new ClassInfo(nodeClass);\n+\t\t\tnodeCi[nodeType] = ci;\n+\t\t}\n+\n+\t\tQueryTreeNode retval;\n+\n+\t\ttry {\n+\t\t\tretval = (QueryTreeNode) ci.getNewInstance();\n+\t\t\t//retval = (QueryTreeNode) nodeClass.newInstance();\n+\t\t} catch (Exception iae) {\n+\t\t\tthrow new RuntimeException(iae);\n+\t\t}\n+\n+\t\tretval.setContextManager(cm);\n+\t\tretval.setNodeType(nodeType);\n+\n+\t\treturn retval;\n+\t}\n+\n+\t/**\n+\t * Translate a node type from C_NodeTypes to a class name\n+\t *\n+\t * @param nodeType\tA node type identifier from C_NodeTypes\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tprotected String nodeName(int nodeType)\n+\t\t\tthrows StandardException\n+\t{\n+\t\tswitch (nodeType)\n+\t\t{\n+\t\t  // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n+\t\t  // THEM TO tools/jar/DBMSnode.properties\n+\t\t\t// xxxRESOLVE: why not make this a giant array and simply index into\n+\t\t\t// it? manish Thu Feb 22 14:49:41 PST 2001\n+\t\t  case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.GROUP_BY_LIST:\n+\t\t  \treturn C_NodeNames.GROUP_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ORDER_BY_LIST:\n+\t\t  \treturn C_NodeNames.ORDER_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.PREDICATE_LIST:\n+\t\t  \treturn C_NodeNames.PREDICATE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.RESULT_COLUMN_LIST:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.SUBQUERY_LIST:\n+\t\t  \treturn C_NodeNames.SUBQUERY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_LIST:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_NODE:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n \n-    /**\n-     @see Monitor\n-     @exception StandardException Ooops\n-     */\n-\n-    public void boot(boolean create, Properties startParams) throws StandardException {\n-        /*\n-         ** This system property determines whether to optimize join order\n-         ** by default.  It is used mainly for testing - there are many tests\n-         ** that assume the join order is fixed.\n-         */\n-        String opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n-        if (opt != null) {\n-            joinOrderOptimization = Boolean.valueOf(opt);\n-        }\n-    }\n-\n-    /**\n-     @see Monitor\n-     */\n-\n-    public void stop() {\n-    }\n-\n-    /**\n-     Every Module needs a public niladic constructor. It just does.\n-     */\n-    public    SpliceNodeFactoryImpl() {}\n-\n-    /** @see NodeFactory#doJoinOrderOptimization */\n-    public Boolean doJoinOrderOptimization() {\n-        return joinOrderOptimization;\n-    }\n-\n-    /**\n-     * @see NodeFactory#getNode\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    public Node getNode(int nodeType, ContextManager cm) throws StandardException {\n-\n-        ClassInfo ci = nodeCi[nodeType];\n-        Class nodeClass = null;\n-        if (ci == null) {\n-            String nodeName = nodeName(nodeType);\n-            try {\n-                nodeClass = Class.forName(nodeName);\n-            }\n-            catch (ClassNotFoundException cnfe) {\n-                if (SanityManager.DEBUG) {\n-                    SanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n-                            cnfe);\n-                }\n-            }\n-\n-            ci = new ClassInfo(nodeClass);\n-            nodeCi[nodeType] = ci;\n-        }\n-\n-        QueryTreeNode retval;\n-\n-        try {\n-            retval = (QueryTreeNode) ci.getNewInstance();\n-            //retval = (QueryTreeNode) nodeClass.newInstance();\n-        } catch (Exception iae) {\n-            throw new RuntimeException(iae);\n-        }\n-\n-        retval.setContextManager(cm);\n-        retval.setNodeType(nodeType);\n-\n-        return retval;\n-    }\n-\n-    /**\n-     * Translate a node type from C_NodeTypes to a class name\n-     *\n-     * @param nodeType    A node type identifier from C_NodeTypes\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    protected String nodeName(int nodeType)\n-            throws StandardException\n-    {\n-        switch (nodeType)\n-        {\n-            // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n-            // THEM TO tools/jar/DBMSnode.properties\n-            // xxxRESOLVE: why not make this a giant array and simply index into\n-            // it? manish Thu Feb 22 14:49:41 PST 2001\n-            case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n-                return C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n-\n-            case C_NodeTypes.GROUP_BY_LIST:\n-                return C_NodeNames.GROUP_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_NODE_LIST:\n+\t\t  \treturn C_NodeNames.VALUE_NODE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ALL_RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\n+\t\t  case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n+\t\t  \treturn C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.NOP_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.NOP_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_LIST:\n-                return C_NodeNames.ORDER_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n+\t\t  \treturn C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE_LIST:\n-                return C_NodeNames.PREDICATE_LIST_NAME;\n+\t\t  case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n+\t\t\treturn C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN_LIST:\n-                return C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\t\t  // ISNOTNULL compressed into ISNULL\n+\t\t  case C_NodeTypes.IS_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.IS_NULL_NODE:\n+\t\t  \treturn C_NodeNames.IS_NULL_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_LIST:\n-                return C_NodeNames.SUBQUERY_LIST_NAME;\n+\t\t  case C_NodeTypes.NOT_NODE:\n+\t\t  \treturn C_NodeNames.NOT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_LIST:\n-                return C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\t\t  case C_NodeTypes.TABLE_NAME:\n+\t\t  \treturn C_NodeNames.TABLE_NAME_NAME;\n \n-            case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n-                return C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.GROUP_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.GROUP_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_NODE:\n-                return C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.VALUE_TUPLE_NODE:\n-                return C_NodeNames.VALUE_TUPLE_NODE_NAME;\n+\t\t  case C_NodeTypes.FROM_LIST:\n+\t\t  \treturn C_NodeNames.FROM_LIST_NAME;\n \n-            case C_NodeTypes.VALUE_NODE_LIST:\n-                return C_NodeNames.VALUE_NODE_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_TUPLE_NODE:\n+\t\t\treturn C_NodeNames.VALUE_TUPLE_NODE_NAME;\n \n-            case C_NodeTypes.ALL_RESULT_COLUMN:\n-                return C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n-                return C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\t\t  case C_NodeTypes.LIST_VALUE_NODE:\n+\t\t    return C_NodeNames.LIST_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.NOP_STATEMENT_NODE:\n-                return C_NodeNames.NOP_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.AND_NODE:\n+\t\t  \treturn C_NodeNames.AND_NODE_NAME;\n \n-            case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n-                return C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n+\t\t\t  return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n-                return C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.MOD_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n \n-            // ISNOTNULL compressed into ISNULL\n-            case C_NodeTypes.IS_NOT_NULL_NODE:\n-            case C_NodeTypes.IS_NULL_NODE:\n-                return C_NodeNames.IS_NULL_NODE_NAME;\n+\t\t  case C_NodeTypes.COALESCE_FUNCTION_NODE:\n+\t\t  \treturn C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.NOT_NODE:\n-                return C_NodeNames.NOT_NODE_NAME;\n+\t\t  case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n-                return C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LIKE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_NAME:\n-                return C_NodeNames.TABLE_NAME_NAME;\n+\t\t  case C_NodeTypes.OR_NODE:\n+\t\t  \treturn C_NodeNames.OR_NODE_NAME;\n \n-            case C_NodeTypes.GROUP_BY_COLUMN:\n-                return C_NodeNames.GROUP_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n-                return C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONDITIONAL_NODE:\n+\t\t  \treturn C_NodeNames.CONDITIONAL_NODE_NAME;\n \n-            case C_NodeTypes.FROM_LIST:\n-                return C_NodeNames.FROM_LIST_NAME;\n+\t\t  case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n-                return C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.BIT_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.LIST_VALUE_NODE:\n-                return C_NodeNames.LIST_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARBIT_CONSTANT_NODE:\n+          case C_NodeTypes.BLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.AND_NODE:\n-                return C_NodeNames.AND_NODE_NAME;\n+\t\t  case C_NodeTypes.CAST_NODE:\n+\t\t  \treturn C_NodeNames.CAST_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n+          case C_NodeTypes.CLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n-            case C_NodeTypes.MOD_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n+          case C_NodeTypes.XML_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.XML_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.COALESCE_FUNCTION_NODE:\n-                return C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_REFERENCE:\n+\t\t  \treturn C_NodeNames.COLUMN_REFERENCE_NAME;\n \n-            case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n-                return C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_INDEX_NODE:\n+\t\t  \treturn C_NodeNames.DROP_INDEX_NODE_NAME;\n \n-            case C_NodeTypes.LIKE_OPERATOR_NODE:\n-                return C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_TRIGGER_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TRIGGER_NODE_NAME;\n \n-            case C_NodeTypes.OR_NODE:\n-                return C_NodeNames.OR_NODE_NAME;\n+\t\t  case C_NodeTypes.TINYINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.INT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.FLOAT_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n-                return C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.CONDITIONAL_NODE:\n-                return C_NodeNames.CONDITIONAL_NODE_NAME;\n+\t\t  case C_NodeTypes.PREDICATE:\n+\t\t  \treturn C_NodeNames.PREDICATE_NAME;\n \n-            case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n-                return C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_NAME;\n \n-            case C_NodeTypes.BIT_CONSTANT_NODE:\n-                return C_NodeNames.BIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.SET_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.VARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.BLOB_CONSTANT_NODE:\n-                return C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.SET_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.CAST_NODE:\n-                return C_NodeNames.CAST_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_CONSTANT_NODE:\n-            case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.CLOB_CONSTANT_NODE:\n-                return C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.XML_CONSTANT_NODE:\n-                return C_NodeNames.XML_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.COLUMN_REFERENCE:\n-                return C_NodeNames.COLUMN_REFERENCE_NAME;\n+\t\t  case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.DROP_INDEX_NODE:\n-                return C_NodeNames.DROP_INDEX_NODE_NAME;\n+\t\t  case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.DROP_TRIGGER_NODE:\n-                return C_NodeNames.DROP_TRIGGER_NODE_NAME;\n+\t\t  case C_NodeTypes.PARAMETER_NODE:\n+\t\t  \treturn C_NodeNames.PARAMETER_NODE_NAME;\n \n-            case C_NodeTypes.TINYINT_CONSTANT_NODE:\n-            case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n-            case C_NodeTypes.INT_CONSTANT_NODE:\n-            case C_NodeTypes.LONGINT_CONSTANT_NODE:\n-            case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n-            case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n-            case C_NodeTypes.FLOAT_CONSTANT_NODE:\n-                return C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.DROP_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n-                return C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE:\n-                return C_NodeNames.PREDICATE_NAME;\n+\t\t  case C_NodeTypes.DROP_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN:\n-                return C_NodeNames.RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.DROP_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.DROP_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.SET_ROLE_NODE:\n-                return C_NodeNames.SET_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SUBQUERY_NODE:\n+\t\t  \treturn C_NodeNames.SUBQUERY_NODE_NAME;\n \n-            case C_NodeTypes.SET_SCHEMA_NODE:\n-                return C_NodeNames.SET_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.BASE_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.BASE_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CALL_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.CALL_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n+          case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.DROP_COLUMN_NODE:\n+\t\t\treturn C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n-                return C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n+\t\t  case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_OF_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_OF_NODE_NAME;\n \n-            case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n-                return C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DEFAULT_NODE:\n+\t\t  \treturn C_NodeNames.DEFAULT_NODE_NAME;\n \n-            case C_NodeTypes.PARAMETER_NODE:\n-                return C_NodeNames.PARAMETER_NODE_NAME;\n+\t\t  case C_NodeTypes.DELETE_NODE:\n+\t\t  \treturn C_NodeNames.DELETE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SCHEMA_NODE:\n-                return C_NodeNames.DROP_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.UPDATE_NODE:\n+\t\t  \treturn C_NodeNames.UPDATE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_ROLE_NODE:\n-                return C_NodeNames.DROP_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.ORDER_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.DROP_TABLE_NODE:\n-                return C_NodeNames.DROP_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ROW_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.DROP_VIEW_NODE:\n-                return C_NodeNames.DROP_VIEW_NODE_NAME;\n+\t\t  case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_NODE:\n-                return C_NodeNames.SUBQUERY_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BASE_COLUMN_NODE:\n-                return C_NodeNames.BASE_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_USER_NODE:\n+\t\t  case C_NodeTypes.SESSION_USER_NODE:\n+\t\t  case C_NodeTypes.SYSTEM_USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_ISOLATION_NODE:\n+\t\t  case C_NodeTypes.IDENTITY_VAL_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SCHEMA_NODE:\n+          case C_NodeTypes.CURRENT_ROLE_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n+\t\t  case C_NodeTypes.GROUP_USER_NODE:\n+\t\t  \treturn C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CALL_STATEMENT_NODE:\n-                return C_NodeNames.CALL_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.IS_NODE:\n+\t\t  \treturn C_NodeNames.IS_NODE_NAME;\n \n-            case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n-            case C_NodeTypes.DROP_COLUMN_NODE:\n-                return C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.LOCK_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.LOCK_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.ALTER_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.ALTER_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_OF_NODE:\n-                return C_NodeNames.CURRENT_OF_NODE_NAME;\n+\t\t  case C_NodeTypes.AGGREGATE_NODE:\n+\t\t  \treturn C_NodeNames.AGGREGATE_NODE_NAME;\n \n-            case C_NodeTypes.DEFAULT_NODE:\n-                return C_NodeNames.DEFAULT_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.COLUMN_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.DELETE_NODE:\n-                return C_NodeNames.DELETE_NODE_NAME;\n+\t\t  case C_NodeTypes.EXEC_SPS_NODE:\n+\t\t  \treturn C_NodeNames.EXEC_SPS_NODE_NAME;\n \n-            case C_NodeTypes.UPDATE_NODE:\n-                return C_NodeNames.UPDATE_NODE_NAME;\n+\t\t  case C_NodeTypes.FK_CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.FK_CONSTRAINT_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_COLUMN:\n-                return C_NodeNames.ORDER_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.FROM_VTI:\n+\t\t  \treturn C_NodeNames.FROM_VTI_NAME;\n \n-            case C_NodeTypes.ROW_RESULT_SET_NODE:\n-                return C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n+\t\t  case C_NodeTypes.MATERIALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.MATERIALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n-                return C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.NORMALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.NORMALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n-                return C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.SCROLL_INSENSITIVE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.SCROLL_INSENSITIVE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.USER_NODE:\n-            case C_NodeTypes.CURRENT_USER_NODE:\n-            case C_NodeTypes.SESSION_USER_NODE:\n-            case C_NodeTypes.SYSTEM_USER_NODE:\n-            case C_NodeTypes.CURRENT_ISOLATION_NODE:\n-            case C_NodeTypes.IDENTITY_VAL_NODE:\n-            case C_NodeTypes.CURRENT_SCHEMA_NODE:\n-            case C_NodeTypes.CURRENT_ROLE_NODE:\n-            case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n-            case C_NodeTypes.GROUP_USER_NODE:\n-            case C_NodeTypes.CURRENT_SERVER_NODE:\n-                return C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_NODE:\n+              return C_NodeNames.ORDER_BY_NODE_NAME;\n \n-            case C_NodeTypes.IS_NODE:\n-                return C_NodeNames.IS_NODE_NAME;\n+\t\t  case C_NodeTypes.DISTINCT_NODE:\n+\t\t  \treturn C_NodeNames.DISTINCT_NODE_NAME;\n \n-            case C_NodeTypes.LOCK_TABLE_NODE:\n-                return C_NodeNames.LOCK_TABLE_NODE_NAME;\n+          case C_NodeTypes.LOCATE_FUNCTION_NODE:\n+\t\t  case C_NodeTypes.SUBSTRING_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.RIGHT_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.TRIM_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.LEFT_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.TIMESTAMP_ADD_FN_NODE:\n+\t\t  case C_NodeTypes.TIMESTAMP_DIFF_FN_NODE:\n+\t\t  case C_NodeTypes.REPLACE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.TERNARY_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.ALTER_TABLE_NODE:\n-                return C_NodeNames.ALTER_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SELECT_NODE:\n+\t\t  \treturn C_NodeNames.SELECT_NODE_NAME;\n \n-            case C_NodeTypes.AGGREGATE_NODE:\n-                return C_NodeNames.AGGREGATE_NODE_NAME;\n+\t\t  case C_NodeTypes.CREATE_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.CREATE_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.STRING_AGGREGATE_NODE:\n-                return C_NodeNames.STRING_AGGREGATE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.CONSTRAINT_DEFINITION_NODE_NAME;\n ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjc2NA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 713}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTYzNzk2OnYy", "diffSide": "LEFT", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoxMzowN1rOGOwcoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNTo1NDoyOVrOGPkLqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNzAwOQ==", "bodyText": "This looks like a bad merge, these nodes shouldn't be removed", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418127009", "createdAt": "2020-04-30T16:13:07Z", "author": {"login": "dgomezferro"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -608,71 +604,62 @@ protected String nodeName(int nodeType)\n             case C_NodeTypes.RANK_FUNCTION_NODE:\n                 return C_NodeNames.RANK_FUNCTION_NAME;\n \n-            case C_NodeTypes.GROUPING_FUNCTION_NODE:\n-                return C_NodeNames.GROUPING_FUNCTION_NODE_NAME;\n+\t\t\tcase C_NodeTypes.GROUPING_FUNCTION_NODE:\n+\t\t\t\treturn C_NodeNames.GROUPING_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CREATE_SEQUENCE_NODE:\n-                return C_NodeNames.CREATE_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.CREATE_SEQUENCE_NODE:\n+            return C_NodeNames.CREATE_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SEQUENCE_NODE:\n-                return C_NodeNames.DROP_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.DROP_SEQUENCE_NODE:\n+            return C_NodeNames.DROP_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.NEXT_SEQUENCE_NODE:\n-                return C_NodeNames.NEXT_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.NEXT_SEQUENCE_NODE:\n+            return C_NodeNames.NEXT_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.EXPLAIN_NODE:\n+          case C_NodeTypes.EXPLAIN_NODE:\n                 return C_NodeNames.EXPLAIN_NODE_NAME;\n \n-            case C_NodeTypes.EXPORT_NODE:\n+          case C_NodeTypes.EXPORT_NODE:\n                 return C_NodeNames.EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EXPORT_NODE:\n-                return C_NodeNames.BINARY_EXPORT_NODE_NAME;\n+          case C_NodeTypes.BINARY_EXPORT_NODE:\n+\t\t\t\treturn C_NodeNames.BINARY_EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.TRUNC_NODE:\n-                return C_NodeNames.TRUNC_NODE_NAME;\n+          case C_NodeTypes.KAFKA_EXPORT_NODE:\n+\t\t\t\treturn C_NodeNames.KAFKA_EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.CREATE_PIN_NODE:\n-                return C_NodeNames.CREATE_PIN_NODE_NAME;\n-\n-            case C_NodeTypes.DROP_PIN_NODE:\n-                return C_NodeNames.DROP_PIN_NODE_NAME;\n-\n-            case C_NodeTypes.ARRAY_OPERATOR_NODE:\n-                return C_NodeNames.ARRAY_OPERATOR_NODE_NAME;\n-\n-            case C_NodeTypes.ARRAY_CONSTANT_NODE:\n-                return C_NodeNames.ARRAY_CONSTANT_NODE_NAME;\n+          case C_NodeTypes.TRUNC_NODE:\n+                return C_NodeNames.TRUNC_NODE_NAME;\n \n-            case C_NodeTypes.SET_SESSION_PROPERTY_NODE:\n-                return C_NodeNames.SET_SESSION_PROPERTY_NAME;\n+\t\t\tcase C_NodeTypes.CREATE_PIN_NODE:\n+\t\t\t\treturn C_NodeNames.CREATE_PIN_NODE_NAME;\n \n-            case C_NodeTypes.SELF_REFERENCE_NODE:\n-                return C_NodeNames.SELF_REFERENCE_NODE_NAME;\n+\t\t\tcase C_NodeTypes.DROP_PIN_NODE:\n+\t\t\t\treturn C_NodeNames.DROP_PIN_NODE_NAME;\n \n-            case C_NodeTypes.DIGITS_OPERATOR_NODE:\n-                return C_NodeNames.UNARY_OPERATOR_NODE_NAME;\n+\t\t\tcase C_NodeTypes.ARRAY_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.ARRAY_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SIGNAL_NODE:\n-                return C_NodeNames.SIGNAL_NAME;\n+\t\t\tcase C_NodeTypes.ARRAY_CONSTANT_NODE:\n+\t\t\t\treturn C_NodeNames.ARRAY_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.SET_NODE:\n-                return C_NodeNames.SET_NAME;\n+\t\t\tcase C_NodeTypes.SET_SESSION_PROPERTY_NODE:\n+\t\t\t\treturn C_NodeNames.SET_SESSION_PROPERTY_NAME;\n \n-            case C_NodeTypes.FULL_OUTER_JOIN_NODE:\n-                return C_NodeNames.FULL_OUTER_JOIN_NODE_NAME;\n+\t\t\tcase C_NodeTypes.SELF_REFERENCE_NODE:\n+\t\t\t\treturn C_NodeNames.SELF_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.EMPTY_DEFAULT_CONSTANT_NODE:\n-                return C_NodeNames.EMPTY_DEFAULT_CONSTANT_NODE;\n+\t\t\tcase C_NodeTypes.DIGITS_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.UNARY_OPERATOR_NODE_NAME;\n ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 1073}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3NDYzNA==", "bodyText": "Fixed in new commit 96dd889.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418974634", "createdAt": "2020-05-02T15:54:29Z", "author": {"login": "jpanko1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -608,71 +604,62 @@ protected String nodeName(int nodeType)\n             case C_NodeTypes.RANK_FUNCTION_NODE:\n                 return C_NodeNames.RANK_FUNCTION_NAME;\n \n-            case C_NodeTypes.GROUPING_FUNCTION_NODE:\n-                return C_NodeNames.GROUPING_FUNCTION_NODE_NAME;\n+\t\t\tcase C_NodeTypes.GROUPING_FUNCTION_NODE:\n+\t\t\t\treturn C_NodeNames.GROUPING_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CREATE_SEQUENCE_NODE:\n-                return C_NodeNames.CREATE_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.CREATE_SEQUENCE_NODE:\n+            return C_NodeNames.CREATE_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SEQUENCE_NODE:\n-                return C_NodeNames.DROP_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.DROP_SEQUENCE_NODE:\n+            return C_NodeNames.DROP_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.NEXT_SEQUENCE_NODE:\n-                return C_NodeNames.NEXT_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.NEXT_SEQUENCE_NODE:\n+            return C_NodeNames.NEXT_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.EXPLAIN_NODE:\n+          case C_NodeTypes.EXPLAIN_NODE:\n                 return C_NodeNames.EXPLAIN_NODE_NAME;\n \n-            case C_NodeTypes.EXPORT_NODE:\n+          case C_NodeTypes.EXPORT_NODE:\n                 return C_NodeNames.EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EXPORT_NODE:\n-                return C_NodeNames.BINARY_EXPORT_NODE_NAME;\n+          case C_NodeTypes.BINARY_EXPORT_NODE:\n+\t\t\t\treturn C_NodeNames.BINARY_EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.TRUNC_NODE:\n-                return C_NodeNames.TRUNC_NODE_NAME;\n+          case C_NodeTypes.KAFKA_EXPORT_NODE:\n+\t\t\t\treturn C_NodeNames.KAFKA_EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.CREATE_PIN_NODE:\n-                return C_NodeNames.CREATE_PIN_NODE_NAME;\n-\n-            case C_NodeTypes.DROP_PIN_NODE:\n-                return C_NodeNames.DROP_PIN_NODE_NAME;\n-\n-            case C_NodeTypes.ARRAY_OPERATOR_NODE:\n-                return C_NodeNames.ARRAY_OPERATOR_NODE_NAME;\n-\n-            case C_NodeTypes.ARRAY_CONSTANT_NODE:\n-                return C_NodeNames.ARRAY_CONSTANT_NODE_NAME;\n+          case C_NodeTypes.TRUNC_NODE:\n+                return C_NodeNames.TRUNC_NODE_NAME;\n \n-            case C_NodeTypes.SET_SESSION_PROPERTY_NODE:\n-                return C_NodeNames.SET_SESSION_PROPERTY_NAME;\n+\t\t\tcase C_NodeTypes.CREATE_PIN_NODE:\n+\t\t\t\treturn C_NodeNames.CREATE_PIN_NODE_NAME;\n \n-            case C_NodeTypes.SELF_REFERENCE_NODE:\n-                return C_NodeNames.SELF_REFERENCE_NODE_NAME;\n+\t\t\tcase C_NodeTypes.DROP_PIN_NODE:\n+\t\t\t\treturn C_NodeNames.DROP_PIN_NODE_NAME;\n \n-            case C_NodeTypes.DIGITS_OPERATOR_NODE:\n-                return C_NodeNames.UNARY_OPERATOR_NODE_NAME;\n+\t\t\tcase C_NodeTypes.ARRAY_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.ARRAY_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SIGNAL_NODE:\n-                return C_NodeNames.SIGNAL_NAME;\n+\t\t\tcase C_NodeTypes.ARRAY_CONSTANT_NODE:\n+\t\t\t\treturn C_NodeNames.ARRAY_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.SET_NODE:\n-                return C_NodeNames.SET_NAME;\n+\t\t\tcase C_NodeTypes.SET_SESSION_PROPERTY_NODE:\n+\t\t\t\treturn C_NodeNames.SET_SESSION_PROPERTY_NAME;\n \n-            case C_NodeTypes.FULL_OUTER_JOIN_NODE:\n-                return C_NodeNames.FULL_OUTER_JOIN_NODE_NAME;\n+\t\t\tcase C_NodeTypes.SELF_REFERENCE_NODE:\n+\t\t\t\treturn C_NodeNames.SELF_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.EMPTY_DEFAULT_CONSTANT_NODE:\n-                return C_NodeNames.EMPTY_DEFAULT_CONSTANT_NODE;\n+\t\t\tcase C_NodeTypes.DIGITS_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.UNARY_OPERATOR_NODE_NAME;\n ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNzAwOQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 1073}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTY3NDU4OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SpliceJDBCUtil.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoyMjo0NlrOGOw0jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1NDoxM1rOGPoVmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzMzEzNQ==", "bodyText": "I don't think these methods get used, we can remove them", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418133135", "createdAt": "2020-04-30T16:22:46Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SpliceJDBCUtil.scala", "diffHunk": "@@ -0,0 +1,192 @@\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.sql.{Connection,SQLException,ResultSet,Timestamp,Date}\n+\n+import org.apache.commons.lang3.StringUtils\n+import org.apache.spark.sql.execution.datasources.jdbc.{JdbcUtils, JDBCOptions, JDBCRDD}\n+import org.apache.spark.sql.jdbc.{JdbcType, JdbcDialect, JdbcDialects}\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.types._\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+  * Created by jleach on 4/10/17.\n+  */\n+object SpliceJDBCUtil {\n+\n+  /**\n+    * `columns`, but as a String suitable for injection into a SQL query.\n+    */\n+  def listColumns(columns: Array[String]): String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.isEmpty) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+    * Prune all but the specified columns from the specified Catalyst schema.\n+    *\n+    * @param schema - The Catalyst schema of the master table\n+    * @param columns - The list of desired columns\n+    * @return A Catalyst schema corresponding to columns in the given order.\n+    */\n+  def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields.map(x => x.metadata.getString(\"name\") -> x): _*)\n+    new StructType(columns.map(name => fieldMap(name)))\n+  }\n+\n+  /**\n+    * Create Where Clause Filter\n+    */\n+  def filterWhereClause(url: String, filters: Array[Filter]): String = {\n+    filters\n+      .flatMap(JDBCRDD.compileFilter(_, JdbcDialects.get(url)))\n+      .map(p => s\"($p)\").mkString(\" AND \")\n+  }\n+\n+\n+  /**\n+    * Compute the schema string for this RDD.\n+    */\n+  def schemaWithoutNullableString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name =\n+        if (field.metadata.contains(\"name\"))\n+          dialect.quoteIdentifier(field.metadata.getString(\"name\"))\n+      else\n+          dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      sb.append(s\", $name $typ\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    * Takes a (schema, table) specification and returns the table's Catalyst\n+    * schema.\n+    *\n+    * @param options - JDBC options that contains url, table and other information.\n+    * @return A StructType giving the table's Catalyst schema.\n+    * @throws SQLException if the table specification is garbage.\n+    * @throws SQLException if the table contains an unsupported type.\n+    */\n+\n+  def retrievePrimaryKeys(options: JDBCOptions): Array[String] = {\n+    val url = options.url\n+    val table = options.table\n+    val dialect = JdbcDialects.get(url)\n+    val conn: Connection = JdbcUtils.createConnectionFactory(options)()\n+    try {\n+      val meta = table.split(\"\\\\.\")\n+      val rs: ResultSet =\n+        if (table.contains(\".\"))\n+          conn.getMetaData.getPrimaryKeys(null, meta(0), meta(1))\n+        else\n+          conn.getMetaData.getPrimaryKeys(null, null, table)\n+      var keys = ArrayBuffer[String]()\n+      while (rs.next()) {\n+        keys+=rs.getString(4)\n+      }\n+      keys.toArray\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  private def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  def getCommonJDBCType(dt: DataType): Option[JdbcType] = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+    * Turns a single Filter into a String representing a SQL expression.\n+    * Returns None for an unhandled filter.\n+    */\n+  def compileFilter(f: Filter, dialect: JdbcDialect): Option[String] = {\n+    def quote(colName: String): String = dialect.quoteIdentifier(colName)\n+\n+    Option(f match {\n+      case EqualTo(attr, value) => s\"${quote(attr)} = ${compileValue(value)}\"\n+      case EqualNullSafe(attr, value) =>\n+        val col = quote(attr)\n+        s\"(NOT ($col != ${compileValue(value)} OR $col IS NULL OR \" +\n+          s\"${compileValue(value)} IS NULL) OR ($col IS NULL AND ${compileValue(value)} IS NULL))\"\n+      case LessThan(attr, value) => s\"${quote(attr)} < ${compileValue(value)}\"\n+      case GreaterThan(attr, value) => s\"${quote(attr)} > ${compileValue(value)}\"\n+      case LessThanOrEqual(attr, value) => s\"${quote(attr)} <= ${compileValue(value)}\"\n+      case GreaterThanOrEqual(attr, value) => s\"${quote(attr)} >= ${compileValue(value)}\"\n+      case IsNull(attr) => s\"${quote(attr)} IS NULL\"\n+      case IsNotNull(attr) => s\"${quote(attr)} IS NOT NULL\"\n+      case StringStartsWith(attr, value) => s\"${quote(attr)} LIKE '${value}%'\"\n+      case StringEndsWith(attr, value) => s\"${quote(attr)} LIKE '%${value}'\"\n+      case StringContains(attr, value) => s\"${quote(attr)} LIKE '%${value}%'\"\n+      case In(attr, value) if value.isEmpty =>\n+        s\"CASE WHEN ${quote(attr)} IS NULL THEN NULL ELSE FALSE END\"\n+      case In(attr, value) => s\"${quote(attr)} IN (${compileValue(value)})\"\n+      case Not(f) => compileFilter(f, dialect).map(p => s\"(NOT ($p))\").getOrElse(null)\n+      case Or(f1, f2) =>\n+        // We can't compile Or filter unless both sub-filters are compiled successfully.\n+        // It applies too for the following And filter.\n+        // If we can make sure compileFilter supports all filters, we can remove this check.\n+        val or = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (or.size == 2) {\n+          or.map(p => s\"($p)\").mkString(\" OR \")\n+        } else {\n+          null\n+        }\n+      case And(f1, f2) =>\n+        val and = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (and.size == 2) {\n+          and.map(p => s\"($p)\").mkString(\" AND \")\n+        } else {\n+          null\n+        }\n+      case _ => null\n+    })\n+  }\n+\n+  /**\n+    * Converts value to SQL expression.\n+    */\n+  private def compileValue(value: Any): Any = value match {\n+    case stringValue: String => s\"'${escapeSql(stringValue)}'\"\n+    case timestampValue: Timestamp => \"'\" + timestampValue + \"'\"\n+    case dateValue: Date => \"'\" + dateValue + \"'\"\n+    case arrayValue: Array[Any] => arrayValue.map(compileValue).mkString(\", \")\n+    case _ => value\n+  }\n+\n+  private def escapeSql(value: String): String =\n+    if (value == null) null else StringUtils.replace(value, \"'\", \"''\")\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjcxMw==", "bodyText": "Fixed in new commit 4606a8b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419042713", "createdAt": "2020-05-03T03:54:13Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SpliceJDBCUtil.scala", "diffHunk": "@@ -0,0 +1,192 @@\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.sql.{Connection,SQLException,ResultSet,Timestamp,Date}\n+\n+import org.apache.commons.lang3.StringUtils\n+import org.apache.spark.sql.execution.datasources.jdbc.{JdbcUtils, JDBCOptions, JDBCRDD}\n+import org.apache.spark.sql.jdbc.{JdbcType, JdbcDialect, JdbcDialects}\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.types._\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+  * Created by jleach on 4/10/17.\n+  */\n+object SpliceJDBCUtil {\n+\n+  /**\n+    * `columns`, but as a String suitable for injection into a SQL query.\n+    */\n+  def listColumns(columns: Array[String]): String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.isEmpty) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+    * Prune all but the specified columns from the specified Catalyst schema.\n+    *\n+    * @param schema - The Catalyst schema of the master table\n+    * @param columns - The list of desired columns\n+    * @return A Catalyst schema corresponding to columns in the given order.\n+    */\n+  def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields.map(x => x.metadata.getString(\"name\") -> x): _*)\n+    new StructType(columns.map(name => fieldMap(name)))\n+  }\n+\n+  /**\n+    * Create Where Clause Filter\n+    */\n+  def filterWhereClause(url: String, filters: Array[Filter]): String = {\n+    filters\n+      .flatMap(JDBCRDD.compileFilter(_, JdbcDialects.get(url)))\n+      .map(p => s\"($p)\").mkString(\" AND \")\n+  }\n+\n+\n+  /**\n+    * Compute the schema string for this RDD.\n+    */\n+  def schemaWithoutNullableString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name =\n+        if (field.metadata.contains(\"name\"))\n+          dialect.quoteIdentifier(field.metadata.getString(\"name\"))\n+      else\n+          dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      sb.append(s\", $name $typ\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    * Takes a (schema, table) specification and returns the table's Catalyst\n+    * schema.\n+    *\n+    * @param options - JDBC options that contains url, table and other information.\n+    * @return A StructType giving the table's Catalyst schema.\n+    * @throws SQLException if the table specification is garbage.\n+    * @throws SQLException if the table contains an unsupported type.\n+    */\n+\n+  def retrievePrimaryKeys(options: JDBCOptions): Array[String] = {\n+    val url = options.url\n+    val table = options.table\n+    val dialect = JdbcDialects.get(url)\n+    val conn: Connection = JdbcUtils.createConnectionFactory(options)()\n+    try {\n+      val meta = table.split(\"\\\\.\")\n+      val rs: ResultSet =\n+        if (table.contains(\".\"))\n+          conn.getMetaData.getPrimaryKeys(null, meta(0), meta(1))\n+        else\n+          conn.getMetaData.getPrimaryKeys(null, null, table)\n+      var keys = ArrayBuffer[String]()\n+      while (rs.next()) {\n+        keys+=rs.getString(4)\n+      }\n+      keys.toArray\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  private def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  def getCommonJDBCType(dt: DataType): Option[JdbcType] = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+    * Turns a single Filter into a String representing a SQL expression.\n+    * Returns None for an unhandled filter.\n+    */\n+  def compileFilter(f: Filter, dialect: JdbcDialect): Option[String] = {\n+    def quote(colName: String): String = dialect.quoteIdentifier(colName)\n+\n+    Option(f match {\n+      case EqualTo(attr, value) => s\"${quote(attr)} = ${compileValue(value)}\"\n+      case EqualNullSafe(attr, value) =>\n+        val col = quote(attr)\n+        s\"(NOT ($col != ${compileValue(value)} OR $col IS NULL OR \" +\n+          s\"${compileValue(value)} IS NULL) OR ($col IS NULL AND ${compileValue(value)} IS NULL))\"\n+      case LessThan(attr, value) => s\"${quote(attr)} < ${compileValue(value)}\"\n+      case GreaterThan(attr, value) => s\"${quote(attr)} > ${compileValue(value)}\"\n+      case LessThanOrEqual(attr, value) => s\"${quote(attr)} <= ${compileValue(value)}\"\n+      case GreaterThanOrEqual(attr, value) => s\"${quote(attr)} >= ${compileValue(value)}\"\n+      case IsNull(attr) => s\"${quote(attr)} IS NULL\"\n+      case IsNotNull(attr) => s\"${quote(attr)} IS NOT NULL\"\n+      case StringStartsWith(attr, value) => s\"${quote(attr)} LIKE '${value}%'\"\n+      case StringEndsWith(attr, value) => s\"${quote(attr)} LIKE '%${value}'\"\n+      case StringContains(attr, value) => s\"${quote(attr)} LIKE '%${value}%'\"\n+      case In(attr, value) if value.isEmpty =>\n+        s\"CASE WHEN ${quote(attr)} IS NULL THEN NULL ELSE FALSE END\"\n+      case In(attr, value) => s\"${quote(attr)} IN (${compileValue(value)})\"\n+      case Not(f) => compileFilter(f, dialect).map(p => s\"(NOT ($p))\").getOrElse(null)\n+      case Or(f1, f2) =>\n+        // We can't compile Or filter unless both sub-filters are compiled successfully.\n+        // It applies too for the following And filter.\n+        // If we can make sure compileFilter supports all filters, we can remove this check.\n+        val or = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (or.size == 2) {\n+          or.map(p => s\"($p)\").mkString(\" OR \")\n+        } else {\n+          null\n+        }\n+      case And(f1, f2) =>\n+        val and = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (and.size == 2) {\n+          and.map(p => s\"($p)\").mkString(\" AND \")\n+        } else {\n+          null\n+        }\n+      case _ => null\n+    })\n+  }\n+\n+  /**\n+    * Converts value to SQL expression.\n+    */\n+  private def compileValue(value: Any): Any = value match {\n+    case stringValue: String => s\"'${escapeSql(stringValue)}'\"\n+    case timestampValue: Timestamp => \"'\" + timestampValue + \"'\"\n+    case dateValue: Date => \"'\" + dateValue + \"'\"\n+    case arrayValue: Array[Any] => arrayValue.map(compileValue).mkString(\", \")\n+    case _ => value\n+  }\n+\n+  private def escapeSql(value: String): String =\n+    if (value == null) null else StringUtils.replace(value, \"'\", \"''\")\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzMzEzNQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTY3NzYzOnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoyMzozN1rOGOw2hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzo1NDoyMlrOGPoVog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzMzYzOQ==", "bodyText": "I don't think this gets used", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418133639", "createdAt": "2020-04-30T16:23:37Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,596 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.security.{PrivilegedExceptionAction, SecureRandom}\n+import java.sql.Connection\n+\n+import com.splicemachine.EngineDriver\n+import com.splicemachine.client.SpliceClient\n+import com.splicemachine.db.impl.jdbc.EmbedConnection\n+import com.splicemachine.derby.impl.SpliceSpark\n+import com.splicemachine.derby.stream.spark.SparkUtils\n+import com.splicemachine.derby.vti.SpliceDatasetVTI\n+import com.splicemachine.derby.vti.SpliceRDDVTI\n+import com.splicemachine.tools.EmbedConnectionMaker\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.JdbcDialects\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+import java.util.Properties\n+\n+import com.splicemachine.access.HConfiguration\n+import com.splicemachine.primitives.Bytes\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}\n+\n+object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  val url = options.get(JDBCOptions.JDBC_URL).get\n+\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MjcyMg==", "bodyText": "Fixed in new commit 4606a8b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419042722", "createdAt": "2020-05-03T03:54:22Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,596 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.security.{PrivilegedExceptionAction, SecureRandom}\n+import java.sql.Connection\n+\n+import com.splicemachine.EngineDriver\n+import com.splicemachine.client.SpliceClient\n+import com.splicemachine.db.impl.jdbc.EmbedConnection\n+import com.splicemachine.derby.impl.SpliceSpark\n+import com.splicemachine.derby.stream.spark.SparkUtils\n+import com.splicemachine.derby.vti.SpliceDatasetVTI\n+import com.splicemachine.derby.vti.SpliceRDDVTI\n+import com.splicemachine.tools.EmbedConnectionMaker\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.JdbcDialects\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+import java.util.Properties\n+\n+import com.splicemachine.access.HConfiguration\n+import com.splicemachine.primitives.Bytes\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.fs.permission.FsPermission\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}\n+\n+object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  val url = options.get(JDBCOptions.JDBC_URL).get\n+\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzMzYzOQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTY4NTE0OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoyNTozM1rOGOw7aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwODoxMTo1MFrOGP2cXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNDg5MQ==", "bodyText": "spark2.1 doesn't seem to be supported, we should remove it in that case", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418134891", "createdAt": "2020-04-30T16:25:33Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,596 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTI3MzgyMQ==", "bodyText": "Fixed in new commit f299177.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419273821", "createdAt": "2020-05-04T08:11:50Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.1/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,596 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNDg5MQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTY4NzI1OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/KafkaToDF.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoyNjowNFrOGOw8xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDowNjoxOVrOGPoZVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNTIzOA==", "bodyText": "remove comments", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418135238", "createdAt": "2020-04-30T16:26:04Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/KafkaToDF.scala", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.util\n+import java.util.{Properties, UUID}\n+\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableDeserializer\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.serialization.IntegerDeserializer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.{Dataset, Row, SparkSession}\n+\n+import scala.collection.JavaConverters._\n+\n+class KafkaToDF(kafkaServers: String, pollTimeout: Long) {\n+  private[this] val destServers = kafkaServers\n+  private[this] val timeout = pollTimeout\n+\n+  def spark(): SparkSession = SparkSession.builder.getOrCreate\n+\n+  def df(topicName: String): Dataset[Row] = {\n+    val (rdd, schema) = rdd_schema(topicName)\n+    spark.createDataFrame( rdd , schema )\n+  }\n+\n+  def rdd(topicName: String): RDD[Row] = rdd_schema(topicName)._1\n+\n+  def rdd_schema(topicName: String): (RDD[Row], StructType) = {\n+    val props = new Properties()\n+    val consumerId = UUID.randomUUID()\n+    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, destServers)\n+    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"spark-consumer-group-\"+consumerId)\n+    props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"spark-consumer-\"+consumerId)\n+    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[IntegerDeserializer].getName)\n+    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[ExternalizableDeserializer].getName)\n+    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n+\n+    val consumer = new KafkaConsumer[Integer, Externalizable](props)\n+    consumer.subscribe(util.Arrays.asList(topicName))\n+\n+    val records = consumer.poll(timeout).asScala  // records: Iterable[ConsumerRecords[Integer, Externalizable]]\n+    consumer.close\n+\n+    if (records.isEmpty) { throw new Exception(s\"Call timed out after ${timeout/1000.0} seconds.\") }\n+    else if(records.size == 1) {\n+      val row = records.head.value.asInstanceOf[Row]\n+      ( spark.sparkContext.parallelize( if(row.size > 0) { Seq(row) } else { Seq[Row]() } ),\n+        row.schema\n+      )\n+    } else {\n+      val seqBuilder = Seq.newBuilder[Row]\n+      for (record <- records.iterator) {\n+//        println(s\"${record.value.getClass.getName}\")\n+//        println(s\"offset = ${record.offset}, key = ${record.key}, value = ${record.value}\")\n+//        println(s\"valuerow = ${record.value.asInstanceOf[ValueRow]}, valuerow.key = ${record.value.asInstanceOf[ValueRow].getKey}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzY2OQ==", "bodyText": "Fixed in new commit 4606a8b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419043669", "createdAt": "2020-05-03T04:06:19Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/KafkaToDF.scala", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.util\n+import java.util.{Properties, UUID}\n+\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableDeserializer\n+import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\n+import org.apache.kafka.common.serialization.IntegerDeserializer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.{Dataset, Row, SparkSession}\n+\n+import scala.collection.JavaConverters._\n+\n+class KafkaToDF(kafkaServers: String, pollTimeout: Long) {\n+  private[this] val destServers = kafkaServers\n+  private[this] val timeout = pollTimeout\n+\n+  def spark(): SparkSession = SparkSession.builder.getOrCreate\n+\n+  def df(topicName: String): Dataset[Row] = {\n+    val (rdd, schema) = rdd_schema(topicName)\n+    spark.createDataFrame( rdd , schema )\n+  }\n+\n+  def rdd(topicName: String): RDD[Row] = rdd_schema(topicName)._1\n+\n+  def rdd_schema(topicName: String): (RDD[Row], StructType) = {\n+    val props = new Properties()\n+    val consumerId = UUID.randomUUID()\n+    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, destServers)\n+    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"spark-consumer-group-\"+consumerId)\n+    props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"spark-consumer-\"+consumerId)\n+    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, classOf[IntegerDeserializer].getName)\n+    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, classOf[ExternalizableDeserializer].getName)\n+    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n+\n+    val consumer = new KafkaConsumer[Integer, Externalizable](props)\n+    consumer.subscribe(util.Arrays.asList(topicName))\n+\n+    val records = consumer.poll(timeout).asScala  // records: Iterable[ConsumerRecords[Integer, Externalizable]]\n+    consumer.close\n+\n+    if (records.isEmpty) { throw new Exception(s\"Call timed out after ${timeout/1000.0} seconds.\") }\n+    else if(records.size == 1) {\n+      val row = records.head.value.asInstanceOf[Row]\n+      ( spark.sparkContext.parallelize( if(row.size > 0) { Seq(row) } else { Seq[Row]() } ),\n+        row.schema\n+      )\n+    } else {\n+      val seqBuilder = Seq.newBuilder[Row]\n+      for (record <- records.iterator) {\n+//        println(s\"${record.value.getClass.getName}\")\n+//        println(s\"offset = ${record.offset}, key = ${record.key}, value = ${record.value}\")\n+//        println(s\"valuerow = ${record.value.asInstanceOf[ValueRow]}, valuerow.key = ${record.value.asInstanceOf[ValueRow].getKey}\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNTIzOA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTY5NTE4OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoyODowN1rOGOxB4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDowNjo0M1rOGPoZcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNjU0NQ==", "bodyText": "remove comments", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418136545", "createdAt": "2020-04-30T16:28:07Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,838 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()\n+  JdbcDialects.registerDialect(new SplicemachineDialect2)\n+\n+  /**\n+    *\n+    * Generate the schema string for create table.\n+    *\n+    * @param schema\n+    * @param url\n+    * @return\n+    */\n+  def schemaString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name = dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      val nullable = if (field.nullable) \"\" else \"NOT NULL\"\n+      sb.append(s\", $name $typ $nullable\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    *\n+    * Retrieve the JDBC type based on the Spark DataType and JDBC Dialect.\n+    *\n+    * @param dt\n+    * @param dialect\n+    * @return\n+    */\n+  private[this]def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  private[this] def getCommonJDBCType(dt: DataType) = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Determine whether a table exists (uses JDBC).\n+    *\n+    * @param schemaTableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaTableName: String): Boolean = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.tableExists(conn, jdbcOptions)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    * Determine whether a table exists given the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaName: String, tableName: String): Boolean = {\n+    tableExists(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    */\n+  def dropTable(schemaName: String, tableName: String): Unit = {\n+    dropTable(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schemaTableName (schema.table)\n+    *\n+    * @param schemaTableName\n+    */\n+  def dropTable(schemaTableName: String): Unit = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.dropTable(conn, jdbcOptions.table)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Create Table based on the table name, the struct (data types), key sequence and createTableOptions.\n+    *\n+    * We currently do not have any custom table options.  These could be added if needed.\n+    *\n+    * @param tableName\n+    * @param structType\n+    * @param keys\n+    * @param createTableOptions\n+    */\n+  def createTable(tableName: String,\n+                  structType: StructType,\n+                  keys: Seq[String],  // not being used\n+                  createTableOptions: String): Unit = {  // createTableOptions not being used\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> tableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      val actSchemaString = schemaString(structType, jdbcOptions.url)\n+      val keyArray = SpliceJDBCUtil.retrievePrimaryKeys(jdbcOptions)\n+      val primaryKeyString = new StringBuilder()\n+      val dialect = JdbcDialects.get(jdbcOptions.url)\n+      keyArray foreach { field =>\n+        val name = dialect.quoteIdentifier(field)\n+        primaryKeyString.append(s\", $name\")\n+      }\n+      val sql = s\"CREATE TABLE $tableName ($actSchemaString) $primaryKeyString\"\n+      val statement = conn.createStatement\n+      statement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  def getConnection(): Connection = {\n+    val jdbcOptions = new JDBCOptions( Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> \"placeholder\"\n+    ))\n+    JdbcUtils.createConnectionFactory( jdbcOptions )()\n+  }\n+\n+  /**\n+    *\n+    * Execute an update statement via JDBC against Splice Machine\n+    *\n+    * @param sql\n+    */\n+  def executeUpdate(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Execute SQL against Splice Machine via JDBC.\n+    *\n+    * @param sql\n+    */\n+  def execute(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.execute(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Truncate the table supplied.  The tableName should be in the form schema.table\n+    *\n+    * @param tableName\n+    */\n+  def truncateTable(tableName: String): Unit = {\n+    executeUpdate(s\"TRUNCATE TABLE $tableName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze the scheme supplied via JDBC\n+    *\n+    * @param schemaName\n+    */\n+  def analyzeSchema(schemaName: String): Unit = {\n+    execute(s\"ANALYZE SCHEMA $schemaName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze table provided.  Will use estimate statistics if provided.\n+    *\n+    * @param tableName\n+    * @param estimateStatistics\n+    * @param samplePercent\n+    */\n+  def analyzeTable(tableName: String, estimateStatistics: Boolean = false, samplePercent: Double = 10.0 ): Unit = {\n+    if (!estimateStatistics)\n+      execute(s\"ANALYZE TABLE $tableName\")\n+    else\n+      execute(s\"ANALYZE TABLE $tableName ESTIMATE STATISTICS SAMPLE $samplePercent PERCENT\")\n+  }\n+\n+  /**\n+    * SQL to Dataset translation.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app through Kafka\n+    *\n+    * @param sql SQL query\n+    * @return Dataset[Row] with the result of the query\n+    */\n+  def df(sql: String): Dataset[Row] = new KafkaToDF( kafkaServers , kafkaPollTimeout ).df( sendSql(sql) )\n+\n+  def internalDf(sql: String): Dataset[Row] = df(sql)\n+\n+  private[this] def sendSql(sql: String): String = {\n+    if( sql.toUpperCase.replace(\" \",\"\").contains(\"USESPARK=FALSE\") ) {\n+      throw new IllegalArgumentException(s\"Property useSpark=false is not supported by ${this.getClass.getName}\")\n+    }\n+\n+    val topicName = getRandomName() // Kafka topic\n+//    println( s\"SMC.sendSql topic $topicName\" )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 355}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0MzY5Nw==", "bodyText": "Fixed in new commit 4606a8b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419043697", "createdAt": "2020-05-03T04:06:43Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,838 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()\n+  JdbcDialects.registerDialect(new SplicemachineDialect2)\n+\n+  /**\n+    *\n+    * Generate the schema string for create table.\n+    *\n+    * @param schema\n+    * @param url\n+    * @return\n+    */\n+  def schemaString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name = dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      val nullable = if (field.nullable) \"\" else \"NOT NULL\"\n+      sb.append(s\", $name $typ $nullable\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    *\n+    * Retrieve the JDBC type based on the Spark DataType and JDBC Dialect.\n+    *\n+    * @param dt\n+    * @param dialect\n+    * @return\n+    */\n+  private[this]def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  private[this] def getCommonJDBCType(dt: DataType) = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Determine whether a table exists (uses JDBC).\n+    *\n+    * @param schemaTableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaTableName: String): Boolean = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.tableExists(conn, jdbcOptions)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    * Determine whether a table exists given the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaName: String, tableName: String): Boolean = {\n+    tableExists(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    */\n+  def dropTable(schemaName: String, tableName: String): Unit = {\n+    dropTable(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schemaTableName (schema.table)\n+    *\n+    * @param schemaTableName\n+    */\n+  def dropTable(schemaTableName: String): Unit = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.dropTable(conn, jdbcOptions.table)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Create Table based on the table name, the struct (data types), key sequence and createTableOptions.\n+    *\n+    * We currently do not have any custom table options.  These could be added if needed.\n+    *\n+    * @param tableName\n+    * @param structType\n+    * @param keys\n+    * @param createTableOptions\n+    */\n+  def createTable(tableName: String,\n+                  structType: StructType,\n+                  keys: Seq[String],  // not being used\n+                  createTableOptions: String): Unit = {  // createTableOptions not being used\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> tableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      val actSchemaString = schemaString(structType, jdbcOptions.url)\n+      val keyArray = SpliceJDBCUtil.retrievePrimaryKeys(jdbcOptions)\n+      val primaryKeyString = new StringBuilder()\n+      val dialect = JdbcDialects.get(jdbcOptions.url)\n+      keyArray foreach { field =>\n+        val name = dialect.quoteIdentifier(field)\n+        primaryKeyString.append(s\", $name\")\n+      }\n+      val sql = s\"CREATE TABLE $tableName ($actSchemaString) $primaryKeyString\"\n+      val statement = conn.createStatement\n+      statement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  def getConnection(): Connection = {\n+    val jdbcOptions = new JDBCOptions( Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> \"placeholder\"\n+    ))\n+    JdbcUtils.createConnectionFactory( jdbcOptions )()\n+  }\n+\n+  /**\n+    *\n+    * Execute an update statement via JDBC against Splice Machine\n+    *\n+    * @param sql\n+    */\n+  def executeUpdate(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Execute SQL against Splice Machine via JDBC.\n+    *\n+    * @param sql\n+    */\n+  def execute(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.execute(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Truncate the table supplied.  The tableName should be in the form schema.table\n+    *\n+    * @param tableName\n+    */\n+  def truncateTable(tableName: String): Unit = {\n+    executeUpdate(s\"TRUNCATE TABLE $tableName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze the scheme supplied via JDBC\n+    *\n+    * @param schemaName\n+    */\n+  def analyzeSchema(schemaName: String): Unit = {\n+    execute(s\"ANALYZE SCHEMA $schemaName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze table provided.  Will use estimate statistics if provided.\n+    *\n+    * @param tableName\n+    * @param estimateStatistics\n+    * @param samplePercent\n+    */\n+  def analyzeTable(tableName: String, estimateStatistics: Boolean = false, samplePercent: Double = 10.0 ): Unit = {\n+    if (!estimateStatistics)\n+      execute(s\"ANALYZE TABLE $tableName\")\n+    else\n+      execute(s\"ANALYZE TABLE $tableName ESTIMATE STATISTICS SAMPLE $samplePercent PERCENT\")\n+  }\n+\n+  /**\n+    * SQL to Dataset translation.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app through Kafka\n+    *\n+    * @param sql SQL query\n+    * @return Dataset[Row] with the result of the query\n+    */\n+  def df(sql: String): Dataset[Row] = new KafkaToDF( kafkaServers , kafkaPollTimeout ).df( sendSql(sql) )\n+\n+  def internalDf(sql: String): Dataset[Row] = df(sql)\n+\n+  private[this] def sendSql(sql: String): String = {\n+    if( sql.toUpperCase.replace(\" \",\"\").contains(\"USESPARK=FALSE\") ) {\n+      throw new IllegalArgumentException(s\"Property useSpark=false is not supported by ${this.getClass.getName}\")\n+    }\n+\n+    val topicName = getRandomName() // Kafka topic\n+//    println( s\"SMC.sendSql topic $topicName\" )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNjU0NQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 355}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTcwMTM5OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjoyOTozOVrOGOxFuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDowNzoyNVrOGPoZwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNzUyOA==", "bodyText": "I don't think this is needed", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418137528", "createdAt": "2020-04-30T16:29:39Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,838 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0Mzc3Ng==", "bodyText": "Fixed in new commit 4606a8b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419043776", "createdAt": "2020-05-03T04:07:25Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,838 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzNzUyOA=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTcxMDA4OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNjozMTo1M1rOGOxLLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMzoxODozMFrOGO9jZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzODkyNQ==", "bodyText": "Where's the Kafka topic being created?", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418138925", "createdAt": "2020-04-30T16:31:53Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,838 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()\n+  JdbcDialects.registerDialect(new SplicemachineDialect2)\n+\n+  /**\n+    *\n+    * Generate the schema string for create table.\n+    *\n+    * @param schema\n+    * @param url\n+    * @return\n+    */\n+  def schemaString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name = dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      val nullable = if (field.nullable) \"\" else \"NOT NULL\"\n+      sb.append(s\", $name $typ $nullable\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    *\n+    * Retrieve the JDBC type based on the Spark DataType and JDBC Dialect.\n+    *\n+    * @param dt\n+    * @param dialect\n+    * @return\n+    */\n+  private[this]def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  private[this] def getCommonJDBCType(dt: DataType) = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Determine whether a table exists (uses JDBC).\n+    *\n+    * @param schemaTableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaTableName: String): Boolean = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.tableExists(conn, jdbcOptions)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    * Determine whether a table exists given the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaName: String, tableName: String): Boolean = {\n+    tableExists(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    */\n+  def dropTable(schemaName: String, tableName: String): Unit = {\n+    dropTable(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schemaTableName (schema.table)\n+    *\n+    * @param schemaTableName\n+    */\n+  def dropTable(schemaTableName: String): Unit = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.dropTable(conn, jdbcOptions.table)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Create Table based on the table name, the struct (data types), key sequence and createTableOptions.\n+    *\n+    * We currently do not have any custom table options.  These could be added if needed.\n+    *\n+    * @param tableName\n+    * @param structType\n+    * @param keys\n+    * @param createTableOptions\n+    */\n+  def createTable(tableName: String,\n+                  structType: StructType,\n+                  keys: Seq[String],  // not being used\n+                  createTableOptions: String): Unit = {  // createTableOptions not being used\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> tableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      val actSchemaString = schemaString(structType, jdbcOptions.url)\n+      val keyArray = SpliceJDBCUtil.retrievePrimaryKeys(jdbcOptions)\n+      val primaryKeyString = new StringBuilder()\n+      val dialect = JdbcDialects.get(jdbcOptions.url)\n+      keyArray foreach { field =>\n+        val name = dialect.quoteIdentifier(field)\n+        primaryKeyString.append(s\", $name\")\n+      }\n+      val sql = s\"CREATE TABLE $tableName ($actSchemaString) $primaryKeyString\"\n+      val statement = conn.createStatement\n+      statement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  def getConnection(): Connection = {\n+    val jdbcOptions = new JDBCOptions( Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> \"placeholder\"\n+    ))\n+    JdbcUtils.createConnectionFactory( jdbcOptions )()\n+  }\n+\n+  /**\n+    *\n+    * Execute an update statement via JDBC against Splice Machine\n+    *\n+    * @param sql\n+    */\n+  def executeUpdate(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Execute SQL against Splice Machine via JDBC.\n+    *\n+    * @param sql\n+    */\n+  def execute(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.execute(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Truncate the table supplied.  The tableName should be in the form schema.table\n+    *\n+    * @param tableName\n+    */\n+  def truncateTable(tableName: String): Unit = {\n+    executeUpdate(s\"TRUNCATE TABLE $tableName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze the scheme supplied via JDBC\n+    *\n+    * @param schemaName\n+    */\n+  def analyzeSchema(schemaName: String): Unit = {\n+    execute(s\"ANALYZE SCHEMA $schemaName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze table provided.  Will use estimate statistics if provided.\n+    *\n+    * @param tableName\n+    * @param estimateStatistics\n+    * @param samplePercent\n+    */\n+  def analyzeTable(tableName: String, estimateStatistics: Boolean = false, samplePercent: Double = 10.0 ): Unit = {\n+    if (!estimateStatistics)\n+      execute(s\"ANALYZE TABLE $tableName\")\n+    else\n+      execute(s\"ANALYZE TABLE $tableName ESTIMATE STATISTICS SAMPLE $samplePercent PERCENT\")\n+  }\n+\n+  /**\n+    * SQL to Dataset translation.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app through Kafka\n+    *\n+    * @param sql SQL query\n+    * @return Dataset[Row] with the result of the query\n+    */\n+  def df(sql: String): Dataset[Row] = new KafkaToDF( kafkaServers , kafkaPollTimeout ).df( sendSql(sql) )\n+\n+  def internalDf(sql: String): Dataset[Row] = df(sql)\n+\n+  private[this] def sendSql(sql: String): String = {\n+    if( sql.toUpperCase.replace(\" \",\"\").contains(\"USESPARK=FALSE\") ) {\n+      throw new IllegalArgumentException(s\"Property useSpark=false is not supported by ${this.getClass.getName}\")\n+    }\n+\n+    val topicName = getRandomName() // Kafka topic", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM0MTczNA==", "bodyText": "getRandomName() creates the topic name, but we\u2019re not explicitly creating the topic itself.  It\u2019s created automatically by Kafka the first time we need it.  Kafka has a config property for supporting automatic topic creation or not.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418341734", "createdAt": "2020-04-30T23:18:30Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,838 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()\n+  JdbcDialects.registerDialect(new SplicemachineDialect2)\n+\n+  /**\n+    *\n+    * Generate the schema string for create table.\n+    *\n+    * @param schema\n+    * @param url\n+    * @return\n+    */\n+  def schemaString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name = dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      val nullable = if (field.nullable) \"\" else \"NOT NULL\"\n+      sb.append(s\", $name $typ $nullable\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    *\n+    * Retrieve the JDBC type based on the Spark DataType and JDBC Dialect.\n+    *\n+    * @param dt\n+    * @param dialect\n+    * @return\n+    */\n+  private[this]def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  private[this] def getCommonJDBCType(dt: DataType) = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Determine whether a table exists (uses JDBC).\n+    *\n+    * @param schemaTableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaTableName: String): Boolean = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.tableExists(conn, jdbcOptions)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    * Determine whether a table exists given the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaName: String, tableName: String): Boolean = {\n+    tableExists(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    */\n+  def dropTable(schemaName: String, tableName: String): Unit = {\n+    dropTable(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schemaTableName (schema.table)\n+    *\n+    * @param schemaTableName\n+    */\n+  def dropTable(schemaTableName: String): Unit = {\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      JdbcUtils.dropTable(conn, jdbcOptions.table)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Create Table based on the table name, the struct (data types), key sequence and createTableOptions.\n+    *\n+    * We currently do not have any custom table options.  These could be added if needed.\n+    *\n+    * @param tableName\n+    * @param structType\n+    * @param keys\n+    * @param createTableOptions\n+    */\n+  def createTable(tableName: String,\n+                  structType: StructType,\n+                  keys: Seq[String],  // not being used\n+                  createTableOptions: String): Unit = {  // createTableOptions not being used\n+    val spliceOptions = Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> tableName)\n+    val jdbcOptions = new JDBCOptions(spliceOptions)\n+    val conn = JdbcUtils.createConnectionFactory(jdbcOptions)()\n+    try {\n+      val actSchemaString = schemaString(structType, jdbcOptions.url)\n+      val keyArray = SpliceJDBCUtil.retrievePrimaryKeys(jdbcOptions)\n+      val primaryKeyString = new StringBuilder()\n+      val dialect = JdbcDialects.get(jdbcOptions.url)\n+      keyArray foreach { field =>\n+        val name = dialect.quoteIdentifier(field)\n+        primaryKeyString.append(s\", $name\")\n+      }\n+      val sql = s\"CREATE TABLE $tableName ($actSchemaString) $primaryKeyString\"\n+      val statement = conn.createStatement\n+      statement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  def getConnection(): Connection = {\n+    val jdbcOptions = new JDBCOptions( Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> \"placeholder\"\n+    ))\n+    JdbcUtils.createConnectionFactory( jdbcOptions )()\n+  }\n+\n+  /**\n+    *\n+    * Execute an update statement via JDBC against Splice Machine\n+    *\n+    * @param sql\n+    */\n+  def executeUpdate(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Execute SQL against Splice Machine via JDBC.\n+    *\n+    * @param sql\n+    */\n+  def execute(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.execute(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Truncate the table supplied.  The tableName should be in the form schema.table\n+    *\n+    * @param tableName\n+    */\n+  def truncateTable(tableName: String): Unit = {\n+    executeUpdate(s\"TRUNCATE TABLE $tableName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze the scheme supplied via JDBC\n+    *\n+    * @param schemaName\n+    */\n+  def analyzeSchema(schemaName: String): Unit = {\n+    execute(s\"ANALYZE SCHEMA $schemaName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze table provided.  Will use estimate statistics if provided.\n+    *\n+    * @param tableName\n+    * @param estimateStatistics\n+    * @param samplePercent\n+    */\n+  def analyzeTable(tableName: String, estimateStatistics: Boolean = false, samplePercent: Double = 10.0 ): Unit = {\n+    if (!estimateStatistics)\n+      execute(s\"ANALYZE TABLE $tableName\")\n+    else\n+      execute(s\"ANALYZE TABLE $tableName ESTIMATE STATISTICS SAMPLE $samplePercent PERCENT\")\n+  }\n+\n+  /**\n+    * SQL to Dataset translation.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app through Kafka\n+    *\n+    * @param sql SQL query\n+    * @return Dataset[Row] with the result of the query\n+    */\n+  def df(sql: String): Dataset[Row] = new KafkaToDF( kafkaServers , kafkaPollTimeout ).df( sendSql(sql) )\n+\n+  def internalDf(sql: String): Dataset[Row] = df(sql)\n+\n+  private[this] def sendSql(sql: String): String = {\n+    if( sql.toUpperCase.replace(\" \",\"\").contains(\"USESPARK=FALSE\") ) {\n+      throw new IllegalArgumentException(s\"Property useSpark=false is not supported by ${this.getClass.getName}\")\n+    }\n+\n+    val topicName = getRandomName() // Kafka topic", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEzODkyNQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 354}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTgzODU0OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzowNzoyOVrOGOydkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QyMzowNzo0MlrOGPwRVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MDAxNw==", "bodyText": "This is not handling task retries", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418160017", "createdAt": "2020-04-30T17:07:29Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,832 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()\n+  JdbcDialects.registerDialect(new SplicemachineDialect2)\n+\n+  /**\n+    *\n+    * Generate the schema string for create table.\n+    *\n+    * @param schema\n+    * @param url\n+    * @return\n+    */\n+  def schemaString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name = dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      val nullable = if (field.nullable) \"\" else \"NOT NULL\"\n+      sb.append(s\", $name $typ $nullable\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    *\n+    * Retrieve the JDBC type based on the Spark DataType and JDBC Dialect.\n+    *\n+    * @param dt\n+    * @param dialect\n+    * @return\n+    */\n+  private[this]def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  private[this] def getCommonJDBCType(dt: DataType) = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Determine whether a table exists (uses JDBC).\n+    *\n+    * @param schemaTableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaTableName: String): Boolean = {\n+    val (conn, jdbcOptionsInWrite) = getConnection(schemaTableName)\n+    try {\n+      JdbcUtils.tableExists(conn, jdbcOptionsInWrite)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    * Determine whether a table exists given the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaName: String, tableName: String): Boolean = {\n+    tableExists(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    */\n+  def dropTable(schemaName: String, tableName: String): Unit = {\n+    dropTable(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schemaTableName (schema.table)\n+    *\n+    * @param schemaTableName\n+    */\n+  def dropTable(schemaTableName: String): Unit = {\n+    val (conn, jdbcOptionsInWrite) = getConnection(schemaTableName)\n+    try {\n+      JdbcUtils.dropTable(conn, jdbcOptionsInWrite.table, jdbcOptionsInWrite)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Create Table based on the table name, the struct (data types), key sequence and createTableOptions.\n+    *\n+    * We currently do not have any custom table options.  These could be added if needed.\n+    *\n+    * @param schemaTableName\n+    * @param structType\n+    * @param keys\n+    * @param createTableOptions\n+    */\n+  def createTable(schemaTableName: String,\n+                  structType: StructType,\n+                  keys: Seq[String],  // not being used\n+                  createTableOptions: String): Unit = {  // createTableOptions not being used\n+    val (conn, jdbcOptionsInWrite) = getConnection(schemaTableName)\n+    try {\n+      val actSchemaString = schemaString(structType, jdbcOptionsInWrite.url)\n+      val keyArray = SpliceJDBCUtil.retrievePrimaryKeys(jdbcOptionsInWrite)\n+      val primaryKeyString = new StringBuilder()\n+      val dialect = JdbcDialects.get(jdbcOptionsInWrite.url)\n+      keyArray foreach { field =>\n+        val name = dialect.quoteIdentifier(field)\n+        primaryKeyString.append(s\", $name\")\n+      }\n+      val sql = s\"CREATE TABLE $schemaTableName ($actSchemaString) $primaryKeyString\"\n+      val statement = conn.createStatement\n+      statement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  def getConnection(): Connection = getConnection(\"placeholder\")._1\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  private[this] def getConnection(schemaTableName: String): (Connection, JdbcOptionsInWrite) = {\n+    val jdbcOptionsInWrite = new JdbcOptionsInWrite( Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName\n+    ))\n+    val conn = JdbcUtils.createConnectionFactory( jdbcOptionsInWrite )()\n+    (conn, jdbcOptionsInWrite)\n+  }\n+\n+  /**\n+    *\n+    * Execute an update statement via JDBC against Splice Machine\n+    *\n+    * @param sql\n+    */\n+  def executeUpdate(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Execute SQL against Splice Machine via JDBC.\n+    *\n+    * @param sql\n+    */\n+  def execute(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.execute(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Truncate the table supplied.  The tableName should be in the form schema.table\n+    *\n+    * @param tableName\n+    */\n+  def truncateTable(tableName: String): Unit = {\n+    executeUpdate(s\"TRUNCATE TABLE $tableName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze the scheme supplied via JDBC\n+    *\n+    * @param schemaName\n+    */\n+  def analyzeSchema(schemaName: String): Unit = {\n+    execute(s\"ANALYZE SCHEMA $schemaName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze table provided.  Will use estimate statistics if provided.\n+    *\n+    * @param tableName\n+    * @param estimateStatistics\n+    * @param samplePercent\n+    */\n+  def analyzeTable(tableName: String, estimateStatistics: Boolean = false, samplePercent: Double = 10.0 ): Unit = {\n+    if (!estimateStatistics)\n+      execute(s\"ANALYZE TABLE $tableName\")\n+    else\n+      execute(s\"ANALYZE TABLE $tableName ESTIMATE STATISTICS SAMPLE $samplePercent PERCENT\")\n+  }\n+\n+  /**\n+    * SQL to Dataset translation.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app through Kafka\n+    *\n+    * @param sql SQL query\n+    * @return Dataset[Row] with the result of the query\n+    */\n+  def df(sql: String): Dataset[Row] = new KafkaToDF( kafkaServers , kafkaPollTimeout ).df( sendSql(sql) )\n+\n+  def internalDf(sql: String): Dataset[Row] = df(sql)\n+\n+  private[this] def sendSql(sql: String): String = {\n+    if( sql.toUpperCase.replace(\" \",\"\").contains(\"USESPARK=FALSE\") ) {\n+      throw new IllegalArgumentException(s\"Property useSpark=false is not supported by ${this.getClass.getName}\")\n+    }\n+\n+    val topicName = getRandomName() // Kafka topic\n+//    println( s\"SMC.sendSql topic $topicName\" )\n+\n+    // hbase user has read/write permission on the topic\n+\n+    val conn = getConnection()\n+    try {\n+//      println( s\"SMC.sendSql sql $sql\" )\n+      conn.prepareStatement(s\"EXPORT_KAFKA('$topicName') \" + sql).execute()\n+    } finally {\n+      conn.close()\n+    }\n+\n+    topicName\n+  }\n+\n+  /**\n+    *\n+    * Table with projections in Splice mapped to an RDD.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app\n+    *\n+    * @param schemaTableName Accessed table\n+    * @param columnProjection Selected columns\n+    * @return RDD[Row] with the result of the projection\n+    */\n+  def rdd(schemaTableName: String,\n+                  columnProjection: Seq[String] = Nil): RDD[Row] = {\n+    val columnList = SpliceJDBCUtil.listColumns(columnProjection.toArray)\n+    val sqlText = s\"SELECT $columnList FROM ${schemaTableName}\"\n+    new KafkaToDF( kafkaServers , kafkaPollTimeout ).rdd( sendSql(sqlText) )\n+  }\n+\n+  def getRandomName(): String = {\n+    val name = new Array[Byte](32)\n+    new SecureRandom().nextBytes(name)\n+    Bytes.toHex(name)+\"-\"+System.nanoTime()\n+  }\n+\n+  /**\n+   *\n+   * Table with projections in Splice mapped to an RDD.\n+   * Runs the query inside Splice Machine and sends the results to the Spark Adapter app\n+   *\n+   * @param schemaTableName Accessed table\n+   * @param columnProjection Selected columns\n+   * @return RDD[Row] with the result of the projection\n+   */\n+  def internalRdd(schemaTableName: String,\n+                  columnProjection: Seq[String] = Nil): RDD[Row] = rdd(schemaTableName, columnProjection)\n+\n+  /**\n+   *\n+   * Insert a dataFrame into a table (schema.table).  This corresponds to an\n+   *\n+   * insert into from select statement\n+   *\n+   * The status directory and number of badRecordsAllowed allows for duplicate primary keys to be\n+   * written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n+   * to the status directory.\n+   *\n+   * @param dataFrame input data\n+   * @param schemaTableName\n+   * @param statusDirectory status directory where bad records file will be created\n+   * @param badRecordsAllowed how many bad records are allowed. -1 for unlimited\n+   */\n+  def insert(dataFrame: DataFrame, schemaTableName: String, statusDirectory: String, badRecordsAllowed: Integer): Unit =\n+    insert(dataFrame.rdd, dataFrame.schema, schemaTableName, statusDirectory, badRecordsAllowed)\n+\n+  /**\n+   * Insert a RDD into a table (schema.table).  The schema is required since RDD's do not have schema.\n+   *\n+   * The status directory and number of badRecordsAllowed allows for duplicate primary keys to be\n+   * written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n+   * to the status directory.\n+   *\n+   * @param rdd input data\n+   * @param schema\n+   * @param schemaTableName\n+   * @param statusDirectory status directory where bad records file will be created\n+   * @param badRecordsAllowed how many bad records are allowed. -1 for unlimited\n+   *\n+   */\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, statusDirectory: String, badRecordsAllowed: Integer): Unit =\n+    insert(rdd, schema, schemaTableName, Map(\"insertMode\"->\"INSERT\",\"statusDirectory\"->statusDirectory,\"badRecordsAllowed\"->badRecordsAllowed.toString) )\n+\n+  /**\n+    *\n+    * Insert a dataFrame into a table (schema.table).  This corresponds to an\n+    *\n+    * insert into from select statement\n+    *\n+    * @param dataFrame input data\n+    * @param schemaTableName output table\n+    */\n+  def insert(dataFrame: DataFrame, schemaTableName: String): Unit = insert(dataFrame.rdd, dataFrame.schema, schemaTableName)\n+\n+  /**\n+   * Insert a RDD into a table (schema.table).  The schema is required since RDD's do not have schema.\n+   *\n+   * @param rdd input data\n+   * @param schema\n+   * @param schemaTableName\n+   */\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+\n+  private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n+  private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n+\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n+    val topicName = getRandomName() //Kafka topic\n+\n+//    println( s\"SMC.insert topic $topicName\" )\n+\n+    // hbase user has read/write permission on the topic\n+\n+    sendData(topicName, rdd, schema)\n+\n+    val colList = columnList(schema)\n+    val sProps = spliceProperties.map({case (k,v) => k+\"=\"+v}).fold(\"--splice-properties useSpark=true\")(_+\", \"+_)\n+    val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \"+sProps+\"\\nselect \" + colList + \" from \" +\n+      \"new com.splicemachine.derby.vti.KafkaVTI('\"+topicName+\"') \" +\n+      \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+\n+//    println( s\"SMC.insert sql $sqlText\" )\n+    executeUpdate(sqlText)\n+  }\n+\n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+    rdd.rdd.mapPartitions(\n+      itrRow => {\n+        //println( s\"SMC.sendData $topicName\" )\n+        //println( s\"SMC.sendData mapPartitions\" )\n+        val props = new Properties\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-\"+java.util.UUID.randomUUID() )\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[IntegerSerializer].getName)\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ExternalizableSerializer].getName)\n+\n+        val producer = new KafkaProducer[Integer, Externalizable](props)\n+\n+        var count = 0\n+        itrRow.foreach( row => {\n+          //println( s\"SMC.sendData record $count\" )\n+          producer.send( new ProducerRecord(topicName, count, externalizable(row, schema)) )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 491}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE3MjY5NQ==", "bodyText": "Fixed in new commit 0bd8e29.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419172695", "createdAt": "2020-05-03T23:07:42Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -0,0 +1,832 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.io.Externalizable\n+import java.security.SecureRandom\n+import java.sql.Connection\n+import java.util.Properties\n+\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.execution.datasources.jdbc._\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+\n+import org.apache.kafka.clients.producer.KafkaProducer\n+import org.apache.kafka.clients.producer.ProducerConfig\n+import org.apache.kafka.clients.producer.ProducerRecord\n+import org.apache.kafka.common.serialization.IntegerSerializer\n+\n+import com.splicemachine.db.iapi.types.SQLInteger\n+import com.splicemachine.db.iapi.types.SQLLongint\n+import com.splicemachine.db.iapi.types.SQLDouble\n+import com.splicemachine.db.iapi.types.SQLReal\n+import com.splicemachine.db.iapi.types.SQLSmallint\n+import com.splicemachine.db.iapi.types.SQLTinyint\n+import com.splicemachine.db.iapi.types.SQLBoolean\n+import com.splicemachine.db.iapi.types.SQLClob\n+import com.splicemachine.db.iapi.types.SQLBlob\n+import com.splicemachine.db.iapi.types.SQLTimestamp\n+import com.splicemachine.db.iapi.types.SQLDate\n+import com.splicemachine.db.iapi.types.SQLDecimal\n+import com.splicemachine.db.impl.sql.execute.ValueRow\n+import com.splicemachine.derby.stream.spark.ExternalizableSerializer\n+import com.splicemachine.primitives.Bytes\n+\n+import org.apache.hadoop.security.UserGroupInformation\n+import org.apache.log4j.Logger\n+import scala.collection.JavaConverters._\n+\n+private object Holder extends Serializable {\n+  @transient lazy val log = Logger.getLogger(getClass.getName)\n+}\n+\n+object KafkaOptions {\n+  val KAFKA_SERVERS = \"KAFKA_SERVERS\"\n+  val KAFKA_POLL_TIMEOUT = \"KAFKA_POLL_TIMEOUT\"\n+}\n+\n+/**\n+  *\n+  * Context for Splice Machine.\n+  *\n+  * @param options Supported options are JDBCOptions.JDBC_URL (required), JDBCOptions.JDBC_INTERNAL_QUERIES,\n+  *                JDBCOptions.JDBC_TEMP_DIRECTORY, KafkaOptions.KAFKA_SERVERS, KafkaOptions.KAFKA_POLL_TIMEOUT\n+  */\n+class SplicemachineContext(options: Map[String, String]) extends Serializable {\n+  private[this] val url = options(JDBCOptions.JDBC_URL) + \";useSpark=true\"\n+\n+  private[this] val kafkaServers = options.getOrElse(KafkaOptions.KAFKA_SERVERS, \"localhost:9092\")\n+  println(s\"Kafka: $kafkaServers\")\n+\n+  private[this] val kafkaPollTimeout = options.getOrElse(KafkaOptions.KAFKA_POLL_TIMEOUT, \"20000\").toLong\n+\n+  /**\n+   *\n+   * Context for Splice Machine, specifying only the JDBC url.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   */\n+  def this(url: String) {\n+    this(Map(JDBCOptions.JDBC_URL -> url));\n+  }\n+\n+  /**\n+   *\n+   * Context for Splice Machine.\n+   *\n+   * @param url JDBC Url with authentication parameters\n+   * @param kafkaServers Comma-separated list of Kafka broker addresses in the form host:port\n+   * @param kafkaPollTimeout Number of milliseconds to wait when polling Kafka\n+   */\n+  def this(url: String, kafkaServers: String = \"localhost:9092\", kafkaPollTimeout: Long = 20000) {\n+    this(Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      KafkaOptions.KAFKA_SERVERS -> kafkaServers,\n+      KafkaOptions.KAFKA_POLL_TIMEOUT -> kafkaPollTimeout.toString\n+    ))\n+  }\n+\n+  @transient var credentials = UserGroupInformation.getCurrentUser().getCredentials()\n+  JdbcDialects.registerDialect(new SplicemachineDialect2)\n+\n+  /**\n+    *\n+    * Generate the schema string for create table.\n+    *\n+    * @param schema\n+    * @param url\n+    * @return\n+    */\n+  def schemaString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name = dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      val nullable = if (field.nullable) \"\" else \"NOT NULL\"\n+      sb.append(s\", $name $typ $nullable\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    *\n+    * Retrieve the JDBC type based on the Spark DataType and JDBC Dialect.\n+    *\n+    * @param dt\n+    * @param dialect\n+    * @return\n+    */\n+  private[this]def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  private[this] def getCommonJDBCType(dt: DataType) = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Determine whether a table exists (uses JDBC).\n+    *\n+    * @param schemaTableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaTableName: String): Boolean = {\n+    val (conn, jdbcOptionsInWrite) = getConnection(schemaTableName)\n+    try {\n+      JdbcUtils.tableExists(conn, jdbcOptionsInWrite)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    * Determine whether a table exists given the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    * @return true if the table exists, false otherwise\n+    */\n+  def tableExists(schemaName: String, tableName: String): Boolean = {\n+    tableExists(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schema name and table name.\n+    *\n+    * @param schemaName\n+    * @param tableName\n+    */\n+  def dropTable(schemaName: String, tableName: String): Unit = {\n+    dropTable(schemaName + \".\" + tableName)\n+  }\n+\n+  /**\n+    *\n+    * Drop a table based on the schemaTableName (schema.table)\n+    *\n+    * @param schemaTableName\n+    */\n+  def dropTable(schemaTableName: String): Unit = {\n+    val (conn, jdbcOptionsInWrite) = getConnection(schemaTableName)\n+    try {\n+      JdbcUtils.dropTable(conn, jdbcOptionsInWrite.table, jdbcOptionsInWrite)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Create Table based on the table name, the struct (data types), key sequence and createTableOptions.\n+    *\n+    * We currently do not have any custom table options.  These could be added if needed.\n+    *\n+    * @param schemaTableName\n+    * @param structType\n+    * @param keys\n+    * @param createTableOptions\n+    */\n+  def createTable(schemaTableName: String,\n+                  structType: StructType,\n+                  keys: Seq[String],  // not being used\n+                  createTableOptions: String): Unit = {  // createTableOptions not being used\n+    val (conn, jdbcOptionsInWrite) = getConnection(schemaTableName)\n+    try {\n+      val actSchemaString = schemaString(structType, jdbcOptionsInWrite.url)\n+      val keyArray = SpliceJDBCUtil.retrievePrimaryKeys(jdbcOptionsInWrite)\n+      val primaryKeyString = new StringBuilder()\n+      val dialect = JdbcDialects.get(jdbcOptionsInWrite.url)\n+      keyArray foreach { field =>\n+        val name = dialect.quoteIdentifier(field)\n+        primaryKeyString.append(s\", $name\")\n+      }\n+      val sql = s\"CREATE TABLE $schemaTableName ($actSchemaString) $primaryKeyString\"\n+      val statement = conn.createStatement\n+      statement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  def getConnection(): Connection = getConnection(\"placeholder\")._1\n+\n+  /**\n+   * Get JDBC connection\n+   */\n+  private[this] def getConnection(schemaTableName: String): (Connection, JdbcOptionsInWrite) = {\n+    val jdbcOptionsInWrite = new JdbcOptionsInWrite( Map(\n+      JDBCOptions.JDBC_URL -> url,\n+      JDBCOptions.JDBC_TABLE_NAME -> schemaTableName\n+    ))\n+    val conn = JdbcUtils.createConnectionFactory( jdbcOptionsInWrite )()\n+    (conn, jdbcOptionsInWrite)\n+  }\n+\n+  /**\n+    *\n+    * Execute an update statement via JDBC against Splice Machine\n+    *\n+    * @param sql\n+    */\n+  def executeUpdate(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.executeUpdate(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Execute SQL against Splice Machine via JDBC.\n+    *\n+    * @param sql\n+    */\n+  def execute(sql: String): Unit = {\n+    val conn = getConnection()\n+    try {\n+      conn.createStatement.execute(sql)\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  /**\n+    *\n+    * Truncate the table supplied.  The tableName should be in the form schema.table\n+    *\n+    * @param tableName\n+    */\n+  def truncateTable(tableName: String): Unit = {\n+    executeUpdate(s\"TRUNCATE TABLE $tableName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze the scheme supplied via JDBC\n+    *\n+    * @param schemaName\n+    */\n+  def analyzeSchema(schemaName: String): Unit = {\n+    execute(s\"ANALYZE SCHEMA $schemaName\")\n+  }\n+\n+  /**\n+    *\n+    * Analyze table provided.  Will use estimate statistics if provided.\n+    *\n+    * @param tableName\n+    * @param estimateStatistics\n+    * @param samplePercent\n+    */\n+  def analyzeTable(tableName: String, estimateStatistics: Boolean = false, samplePercent: Double = 10.0 ): Unit = {\n+    if (!estimateStatistics)\n+      execute(s\"ANALYZE TABLE $tableName\")\n+    else\n+      execute(s\"ANALYZE TABLE $tableName ESTIMATE STATISTICS SAMPLE $samplePercent PERCENT\")\n+  }\n+\n+  /**\n+    * SQL to Dataset translation.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app through Kafka\n+    *\n+    * @param sql SQL query\n+    * @return Dataset[Row] with the result of the query\n+    */\n+  def df(sql: String): Dataset[Row] = new KafkaToDF( kafkaServers , kafkaPollTimeout ).df( sendSql(sql) )\n+\n+  def internalDf(sql: String): Dataset[Row] = df(sql)\n+\n+  private[this] def sendSql(sql: String): String = {\n+    if( sql.toUpperCase.replace(\" \",\"\").contains(\"USESPARK=FALSE\") ) {\n+      throw new IllegalArgumentException(s\"Property useSpark=false is not supported by ${this.getClass.getName}\")\n+    }\n+\n+    val topicName = getRandomName() // Kafka topic\n+//    println( s\"SMC.sendSql topic $topicName\" )\n+\n+    // hbase user has read/write permission on the topic\n+\n+    val conn = getConnection()\n+    try {\n+//      println( s\"SMC.sendSql sql $sql\" )\n+      conn.prepareStatement(s\"EXPORT_KAFKA('$topicName') \" + sql).execute()\n+    } finally {\n+      conn.close()\n+    }\n+\n+    topicName\n+  }\n+\n+  /**\n+    *\n+    * Table with projections in Splice mapped to an RDD.\n+    * Runs the query inside Splice Machine and sends the results to the Spark Adapter app\n+    *\n+    * @param schemaTableName Accessed table\n+    * @param columnProjection Selected columns\n+    * @return RDD[Row] with the result of the projection\n+    */\n+  def rdd(schemaTableName: String,\n+                  columnProjection: Seq[String] = Nil): RDD[Row] = {\n+    val columnList = SpliceJDBCUtil.listColumns(columnProjection.toArray)\n+    val sqlText = s\"SELECT $columnList FROM ${schemaTableName}\"\n+    new KafkaToDF( kafkaServers , kafkaPollTimeout ).rdd( sendSql(sqlText) )\n+  }\n+\n+  def getRandomName(): String = {\n+    val name = new Array[Byte](32)\n+    new SecureRandom().nextBytes(name)\n+    Bytes.toHex(name)+\"-\"+System.nanoTime()\n+  }\n+\n+  /**\n+   *\n+   * Table with projections in Splice mapped to an RDD.\n+   * Runs the query inside Splice Machine and sends the results to the Spark Adapter app\n+   *\n+   * @param schemaTableName Accessed table\n+   * @param columnProjection Selected columns\n+   * @return RDD[Row] with the result of the projection\n+   */\n+  def internalRdd(schemaTableName: String,\n+                  columnProjection: Seq[String] = Nil): RDD[Row] = rdd(schemaTableName, columnProjection)\n+\n+  /**\n+   *\n+   * Insert a dataFrame into a table (schema.table).  This corresponds to an\n+   *\n+   * insert into from select statement\n+   *\n+   * The status directory and number of badRecordsAllowed allows for duplicate primary keys to be\n+   * written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n+   * to the status directory.\n+   *\n+   * @param dataFrame input data\n+   * @param schemaTableName\n+   * @param statusDirectory status directory where bad records file will be created\n+   * @param badRecordsAllowed how many bad records are allowed. -1 for unlimited\n+   */\n+  def insert(dataFrame: DataFrame, schemaTableName: String, statusDirectory: String, badRecordsAllowed: Integer): Unit =\n+    insert(dataFrame.rdd, dataFrame.schema, schemaTableName, statusDirectory, badRecordsAllowed)\n+\n+  /**\n+   * Insert a RDD into a table (schema.table).  The schema is required since RDD's do not have schema.\n+   *\n+   * The status directory and number of badRecordsAllowed allows for duplicate primary keys to be\n+   * written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n+   * to the status directory.\n+   *\n+   * @param rdd input data\n+   * @param schema\n+   * @param schemaTableName\n+   * @param statusDirectory status directory where bad records file will be created\n+   * @param badRecordsAllowed how many bad records are allowed. -1 for unlimited\n+   *\n+   */\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, statusDirectory: String, badRecordsAllowed: Integer): Unit =\n+    insert(rdd, schema, schemaTableName, Map(\"insertMode\"->\"INSERT\",\"statusDirectory\"->statusDirectory,\"badRecordsAllowed\"->badRecordsAllowed.toString) )\n+\n+  /**\n+    *\n+    * Insert a dataFrame into a table (schema.table).  This corresponds to an\n+    *\n+    * insert into from select statement\n+    *\n+    * @param dataFrame input data\n+    * @param schemaTableName output table\n+    */\n+  def insert(dataFrame: DataFrame, schemaTableName: String): Unit = insert(dataFrame.rdd, dataFrame.schema, schemaTableName)\n+\n+  /**\n+   * Insert a RDD into a table (schema.table).  The schema is required since RDD's do not have schema.\n+   *\n+   * @param rdd input data\n+   * @param schema\n+   * @param schemaTableName\n+   */\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+\n+  private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n+  private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n+\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n+    val topicName = getRandomName() //Kafka topic\n+\n+//    println( s\"SMC.insert topic $topicName\" )\n+\n+    // hbase user has read/write permission on the topic\n+\n+    sendData(topicName, rdd, schema)\n+\n+    val colList = columnList(schema)\n+    val sProps = spliceProperties.map({case (k,v) => k+\"=\"+v}).fold(\"--splice-properties useSpark=true\")(_+\", \"+_)\n+    val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \"+sProps+\"\\nselect \" + colList + \" from \" +\n+      \"new com.splicemachine.derby.vti.KafkaVTI('\"+topicName+\"') \" +\n+      \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+\n+//    println( s\"SMC.insert sql $sqlText\" )\n+    executeUpdate(sqlText)\n+  }\n+\n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+    rdd.rdd.mapPartitions(\n+      itrRow => {\n+        //println( s\"SMC.sendData $topicName\" )\n+        //println( s\"SMC.sendData mapPartitions\" )\n+        val props = new Properties\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-\"+java.util.UUID.randomUUID() )\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[IntegerSerializer].getName)\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ExternalizableSerializer].getName)\n+\n+        val producer = new KafkaProducer[Integer, Externalizable](props)\n+\n+        var count = 0\n+        itrRow.foreach( row => {\n+          //println( s\"SMC.sendData record $count\" )\n+          producer.send( new ProducerRecord(topicName, count, externalizable(row, schema)) )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MDAxNw=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 491}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTg1NTU0OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SpliceJDBCUtil.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzoxMjowMlrOGOyn8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwNDoxMjowOFrOGPobIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MjY3Mg==", "bodyText": "not used", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418162672", "createdAt": "2020-04-30T17:12:02Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SpliceJDBCUtil.scala", "diffHunk": "@@ -0,0 +1,217 @@\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.sql.{Connection, Date, ResultSet, SQLException, Timestamp}\n+\n+import org.apache.commons.lang3.StringUtils\n+import org.apache.spark.sql.execution.datasources.jdbc.{JDBCRDD, JDBCOptions, JdbcUtils}\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.types._\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+  * Created by jleach on 4/10/17.\n+  */\n+object SpliceJDBCUtil {\n+\n+  /**\n+    * `columns`, but as a String suitable for injection into a SQL query.\n+    */\n+  def listColumns(columns: Array[String]): String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.isEmpty) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+    * Prune all but the specified columns from the specified Catalyst schema.\n+    *\n+    * @param schema - The Catalyst schema of the master table\n+    * @param columns - The list of desired columns\n+    * @return A Catalyst schema corresponding to columns in the given order.\n+    */\n+  def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields.map(x => x.metadata.getString(\"name\") -> x): _*)\n+    new StructType(columns.map(name => fieldMap(name)))\n+  }\n+\n+  /**\n+    * Create Where Clause Filter\n+    */\n+  def filterWhereClause(url: String, filters: Array[Filter]): String = {\n+    filters\n+      .flatMap(JDBCRDD.compileFilter(_, JdbcDialects.get(url)))\n+      .map(p => s\"($p)\").mkString(\" AND \")\n+  }\n+\n+\n+  /**\n+    * Compute the schema string for this RDD.\n+    */\n+  def schemaWithoutNullableString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name =\n+        if (field.metadata.contains(\"name\"))\n+          dialect.quoteIdentifier(field.metadata.getString(\"name\"))\n+      else\n+          dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      sb.append(s\", $name $typ\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    * Takes a (schema, table) specification and returns the table's Catalyst\n+    * schema.\n+    *\n+    * @param options - JDBC options that contains url, table and other information.\n+    * @return A StructType giving the table's Catalyst schema.\n+    * @throws SQLException if the table specification is garbage.\n+    * @throws SQLException if the table contains an unsupported type.\n+    */\n+\n+  def retrievePrimaryKeys(options: JDBCOptions): Array[String] =\n+    retrieveMetaData(\n+      options,\n+      (conn,schema,tablename) => conn.getMetaData.getPrimaryKeys(null, schema, tablename),\n+      (conn,tablename) => conn.getMetaData.getPrimaryKeys(null, null, tablename),\n+      rs => Seq(rs.getString(\"COLUMN_NAME\"))\n+    ).map(_(0))\n+\n+  def retrieveColumnInfo(options: JDBCOptions): Array[Seq[String]] =\n+    retrieveMetaData(\n+      options,\n+      (conn,schema,tablename) => conn.getMetaData.getColumns(null, schema.toUpperCase, tablename.toUpperCase, null),\n+      (conn,tablename) => conn.getMetaData.getColumns(null, null, tablename.toUpperCase, null),\n+      rs => Seq(\n+        rs.getString(\"COLUMN_NAME\"),\n+        rs.getString(\"TYPE_NAME\"),\n+        rs.getString(\"COLUMN_SIZE\")\n+      )\n+    )\n+\n+  private def retrieveMetaData(\n+    options: JDBCOptions,\n+    getWithSchemaTablename: (Connection,String,String) => ResultSet,\n+    getWithTablename: (Connection,String) => ResultSet,\n+    getData: ResultSet => Seq[String]\n+  ): Array[Seq[String]] = {\n+    val table = options.table\n+    val conn: Connection = JdbcUtils.createConnectionFactory(options)()\n+    try {\n+      val rs: ResultSet =\n+        if (table.contains(\".\")) {\n+          val meta = table.split(\"\\\\.\")\n+          getWithSchemaTablename(conn, meta(0), meta(1))\n+        }\n+        else {\n+          getWithTablename(conn, table)\n+        }\n+      val buffer = ArrayBuffer[Seq[String]]()\n+      while (rs.next()) {\n+        buffer += getData(rs)\n+      }\n+      buffer.toArray\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  private def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  def getCommonJDBCType(dt: DataType): Option[JdbcType] = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+    * Turns a single Filter into a String representing a SQL expression.\n+    * Returns None for an unhandled filter.\n+    */\n+  def compileFilter(f: Filter, dialect: JdbcDialect): Option[String] = {\n+    def quote(colName: String): String = dialect.quoteIdentifier(colName)\n+\n+    Option(f match {\n+      case EqualTo(attr, value) => s\"${quote(attr)} = ${compileValue(value)}\"\n+      case EqualNullSafe(attr, value) =>\n+        val col = quote(attr)\n+        s\"(NOT ($col != ${compileValue(value)} OR $col IS NULL OR \" +\n+          s\"${compileValue(value)} IS NULL) OR ($col IS NULL AND ${compileValue(value)} IS NULL))\"\n+      case LessThan(attr, value) => s\"${quote(attr)} < ${compileValue(value)}\"\n+      case GreaterThan(attr, value) => s\"${quote(attr)} > ${compileValue(value)}\"\n+      case LessThanOrEqual(attr, value) => s\"${quote(attr)} <= ${compileValue(value)}\"\n+      case GreaterThanOrEqual(attr, value) => s\"${quote(attr)} >= ${compileValue(value)}\"\n+      case IsNull(attr) => s\"${quote(attr)} IS NULL\"\n+      case IsNotNull(attr) => s\"${quote(attr)} IS NOT NULL\"\n+      case StringStartsWith(attr, value) => s\"${quote(attr)} LIKE '${value}%'\"\n+      case StringEndsWith(attr, value) => s\"${quote(attr)} LIKE '%${value}'\"\n+      case StringContains(attr, value) => s\"${quote(attr)} LIKE '%${value}%'\"\n+      case In(attr, value) if value.isEmpty =>\n+        s\"CASE WHEN ${quote(attr)} IS NULL THEN NULL ELSE FALSE END\"\n+      case In(attr, value) => s\"${quote(attr)} IN (${compileValue(value)})\"\n+      case Not(f) => compileFilter(f, dialect).map(p => s\"(NOT ($p))\").getOrElse(null)\n+      case Or(f1, f2) =>\n+        // We can't compile Or filter unless both sub-filters are compiled successfully.\n+        // It applies too for the following And filter.\n+        // If we can make sure compileFilter supports all filters, we can remove this check.\n+        val or = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (or.size == 2) {\n+          or.map(p => s\"($p)\").mkString(\" OR \")\n+        } else {\n+          null\n+        }\n+      case And(f1, f2) =>\n+        val and = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (and.size == 2) {\n+          and.map(p => s\"($p)\").mkString(\" AND \")\n+        } else {\n+          null\n+        }\n+      case _ => null\n+    })\n+  }\n+\n+  /**\n+    * Converts value to SQL expression.\n+    */\n+  private def compileValue(value: Any): Any = value match {\n+    case stringValue: String => s\"'${escapeSql(stringValue)}'\"\n+    case timestampValue: Timestamp => \"'\" + timestampValue + \"'\"\n+    case dateValue: Date => \"'\" + dateValue + \"'\"\n+    case arrayValue: Array[Any] => arrayValue.map(compileValue).mkString(\", \")\n+    case _ => value\n+  }\n+\n+  private def escapeSql(value: String): String =\n+    if (value == null) null else StringUtils.replace(value, \"'\", \"''\")\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTA0NDEzMQ==", "bodyText": "Fixed in new commit 4606a8b.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419044131", "createdAt": "2020-05-03T04:12:08Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SpliceJDBCUtil.scala", "diffHunk": "@@ -0,0 +1,217 @@\n+package com.splicemachine.spark2.splicemachine\n+\n+import java.sql.{Connection, Date, ResultSet, SQLException, Timestamp}\n+\n+import org.apache.commons.lang3.StringUtils\n+import org.apache.spark.sql.execution.datasources.jdbc.{JDBCRDD, JDBCOptions, JdbcUtils}\n+import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}\n+import org.apache.spark.sql.sources._\n+import org.apache.spark.sql.types._\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+/**\n+  * Created by jleach on 4/10/17.\n+  */\n+object SpliceJDBCUtil {\n+\n+  /**\n+    * `columns`, but as a String suitable for injection into a SQL query.\n+    */\n+  def listColumns(columns: Array[String]): String = {\n+    val sb = new StringBuilder()\n+    columns.foreach(x => sb.append(\",\").append(x))\n+    if (sb.isEmpty) \"1\" else sb.substring(1)\n+  }\n+\n+  /**\n+    * Prune all but the specified columns from the specified Catalyst schema.\n+    *\n+    * @param schema - The Catalyst schema of the master table\n+    * @param columns - The list of desired columns\n+    * @return A Catalyst schema corresponding to columns in the given order.\n+    */\n+  def pruneSchema(schema: StructType, columns: Array[String]): StructType = {\n+    val fieldMap = Map(schema.fields.map(x => x.metadata.getString(\"name\") -> x): _*)\n+    new StructType(columns.map(name => fieldMap(name)))\n+  }\n+\n+  /**\n+    * Create Where Clause Filter\n+    */\n+  def filterWhereClause(url: String, filters: Array[Filter]): String = {\n+    filters\n+      .flatMap(JDBCRDD.compileFilter(_, JdbcDialects.get(url)))\n+      .map(p => s\"($p)\").mkString(\" AND \")\n+  }\n+\n+\n+  /**\n+    * Compute the schema string for this RDD.\n+    */\n+  def schemaWithoutNullableString(schema: StructType, url: String): String = {\n+    val sb = new StringBuilder()\n+    val dialect = JdbcDialects.get(url)\n+    schema.fields foreach { field =>\n+      val name =\n+        if (field.metadata.contains(\"name\"))\n+          dialect.quoteIdentifier(field.metadata.getString(\"name\"))\n+      else\n+          dialect.quoteIdentifier(field.name)\n+      val typ: String = getJdbcType(field.dataType, dialect).databaseTypeDefinition\n+      sb.append(s\", $name $typ\")\n+    }\n+    if (sb.length < 2) \"\" else sb.substring(2)\n+  }\n+\n+  /**\n+    * Takes a (schema, table) specification and returns the table's Catalyst\n+    * schema.\n+    *\n+    * @param options - JDBC options that contains url, table and other information.\n+    * @return A StructType giving the table's Catalyst schema.\n+    * @throws SQLException if the table specification is garbage.\n+    * @throws SQLException if the table contains an unsupported type.\n+    */\n+\n+  def retrievePrimaryKeys(options: JDBCOptions): Array[String] =\n+    retrieveMetaData(\n+      options,\n+      (conn,schema,tablename) => conn.getMetaData.getPrimaryKeys(null, schema, tablename),\n+      (conn,tablename) => conn.getMetaData.getPrimaryKeys(null, null, tablename),\n+      rs => Seq(rs.getString(\"COLUMN_NAME\"))\n+    ).map(_(0))\n+\n+  def retrieveColumnInfo(options: JDBCOptions): Array[Seq[String]] =\n+    retrieveMetaData(\n+      options,\n+      (conn,schema,tablename) => conn.getMetaData.getColumns(null, schema.toUpperCase, tablename.toUpperCase, null),\n+      (conn,tablename) => conn.getMetaData.getColumns(null, null, tablename.toUpperCase, null),\n+      rs => Seq(\n+        rs.getString(\"COLUMN_NAME\"),\n+        rs.getString(\"TYPE_NAME\"),\n+        rs.getString(\"COLUMN_SIZE\")\n+      )\n+    )\n+\n+  private def retrieveMetaData(\n+    options: JDBCOptions,\n+    getWithSchemaTablename: (Connection,String,String) => ResultSet,\n+    getWithTablename: (Connection,String) => ResultSet,\n+    getData: ResultSet => Seq[String]\n+  ): Array[Seq[String]] = {\n+    val table = options.table\n+    val conn: Connection = JdbcUtils.createConnectionFactory(options)()\n+    try {\n+      val rs: ResultSet =\n+        if (table.contains(\".\")) {\n+          val meta = table.split(\"\\\\.\")\n+          getWithSchemaTablename(conn, meta(0), meta(1))\n+        }\n+        else {\n+          getWithTablename(conn, table)\n+        }\n+      val buffer = ArrayBuffer[Seq[String]]()\n+      while (rs.next()) {\n+        buffer += getData(rs)\n+      }\n+      buffer.toArray\n+    } finally {\n+      conn.close()\n+    }\n+  }\n+\n+  private def getJdbcType(dt: DataType, dialect: JdbcDialect): JdbcType = {\n+    dialect.getJDBCType(dt).orElse(getCommonJDBCType(dt)).getOrElse(\n+      throw new IllegalArgumentException(s\"Can't get JDBC type for ${dt.simpleString}\"))\n+  }\n+\n+  /**\n+    * Retrieve standard jdbc types.\n+    *\n+    * @param dt The datatype (e.g. [[org.apache.spark.sql.types.StringType]])\n+    * @return The default JdbcType for this DataType\n+    */\n+  def getCommonJDBCType(dt: DataType): Option[JdbcType] = {\n+    dt match {\n+      case IntegerType => Option(JdbcType(\"INTEGER\", java.sql.Types.INTEGER))\n+      case LongType => Option(JdbcType(\"BIGINT\", java.sql.Types.BIGINT))\n+      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n+      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n+      case ShortType => Option(JdbcType(\"INTEGER\", java.sql.Types.SMALLINT))\n+      case ByteType => Option(JdbcType(\"BYTE\", java.sql.Types.TINYINT))\n+      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))\n+      case StringType => Option(JdbcType(\"TEXT\", java.sql.Types.CLOB))\n+      case BinaryType => Option(JdbcType(\"BLOB\", java.sql.Types.BLOB))\n+      case TimestampType => Option(JdbcType(\"TIMESTAMP\", java.sql.Types.TIMESTAMP))\n+      case DateType => Option(JdbcType(\"DATE\", java.sql.Types.DATE))\n+      case t: DecimalType => Option(\n+        JdbcType(s\"DECIMAL(${t.precision},${t.scale})\", java.sql.Types.DECIMAL))\n+      case _ => None\n+    }\n+  }\n+\n+\n+  /**\n+    * Turns a single Filter into a String representing a SQL expression.\n+    * Returns None for an unhandled filter.\n+    */\n+  def compileFilter(f: Filter, dialect: JdbcDialect): Option[String] = {\n+    def quote(colName: String): String = dialect.quoteIdentifier(colName)\n+\n+    Option(f match {\n+      case EqualTo(attr, value) => s\"${quote(attr)} = ${compileValue(value)}\"\n+      case EqualNullSafe(attr, value) =>\n+        val col = quote(attr)\n+        s\"(NOT ($col != ${compileValue(value)} OR $col IS NULL OR \" +\n+          s\"${compileValue(value)} IS NULL) OR ($col IS NULL AND ${compileValue(value)} IS NULL))\"\n+      case LessThan(attr, value) => s\"${quote(attr)} < ${compileValue(value)}\"\n+      case GreaterThan(attr, value) => s\"${quote(attr)} > ${compileValue(value)}\"\n+      case LessThanOrEqual(attr, value) => s\"${quote(attr)} <= ${compileValue(value)}\"\n+      case GreaterThanOrEqual(attr, value) => s\"${quote(attr)} >= ${compileValue(value)}\"\n+      case IsNull(attr) => s\"${quote(attr)} IS NULL\"\n+      case IsNotNull(attr) => s\"${quote(attr)} IS NOT NULL\"\n+      case StringStartsWith(attr, value) => s\"${quote(attr)} LIKE '${value}%'\"\n+      case StringEndsWith(attr, value) => s\"${quote(attr)} LIKE '%${value}'\"\n+      case StringContains(attr, value) => s\"${quote(attr)} LIKE '%${value}%'\"\n+      case In(attr, value) if value.isEmpty =>\n+        s\"CASE WHEN ${quote(attr)} IS NULL THEN NULL ELSE FALSE END\"\n+      case In(attr, value) => s\"${quote(attr)} IN (${compileValue(value)})\"\n+      case Not(f) => compileFilter(f, dialect).map(p => s\"(NOT ($p))\").getOrElse(null)\n+      case Or(f1, f2) =>\n+        // We can't compile Or filter unless both sub-filters are compiled successfully.\n+        // It applies too for the following And filter.\n+        // If we can make sure compileFilter supports all filters, we can remove this check.\n+        val or = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (or.size == 2) {\n+          or.map(p => s\"($p)\").mkString(\" OR \")\n+        } else {\n+          null\n+        }\n+      case And(f1, f2) =>\n+        val and = Seq(f1, f2).flatMap(compileFilter(_, dialect))\n+        if (and.size == 2) {\n+          and.map(p => s\"($p)\").mkString(\" AND \")\n+        } else {\n+          null\n+        }\n+      case _ => null\n+    })\n+  }\n+\n+  /**\n+    * Converts value to SQL expression.\n+    */\n+  private def compileValue(value: Any): Any = value match {\n+    case stringValue: String => s\"'${escapeSql(stringValue)}'\"\n+    case timestampValue: Timestamp => \"'\" + timestampValue + \"'\"\n+    case dateValue: Date => \"'\" + dateValue + \"'\"\n+    case arrayValue: Array[Any] => arrayValue.map(compileValue).mkString(\", \")\n+    case _ => value\n+  }\n+\n+  private def escapeSql(value: String): String =\n+    if (value == null) null else StringUtils.replace(value, \"'\", \"''\")\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MjY3Mg=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMTg2MTE4OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/test/scala/com/splicemachine/spark2/splicemachine/NativeTransformationsIT.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzoxMzo0MlrOGOyrgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjowNjoyNVrOGPC6tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MzU4NQ==", "bodyText": "We can remove this IT since it doesn't really make much sense for NSDS", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418163585", "createdAt": "2020-04-30T17:13:42Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/test/scala/com/splicemachine/spark2/splicemachine/NativeTransformationsIT.scala", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import org.apache.spark.sql._\n+import org.junit.runner.RunWith\n+import org.scalatest.junit.JUnitRunner\n+import org.scalatest.{BeforeAndAfter, FunSuite, Matchers}\n+\n+import scala.collection.immutable.IndexedSeq\n+\n+@RunWith(classOf[JUnitRunner])\n+class NativeTransformationsIT extends FunSuite with TestContext with BeforeAndAfter with Matchers {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQyOTYyMw==", "bodyText": "Removed in new commit 2c35ac1.", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418429623", "createdAt": "2020-05-01T06:06:25Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/test/scala/com/splicemachine/spark2/splicemachine/NativeTransformationsIT.scala", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+package com.splicemachine.spark2.splicemachine\n+\n+import org.apache.spark.sql._\n+import org.junit.runner.RunWith\n+import org.scalatest.junit.JUnitRunner\n+import org.scalatest.{BeforeAndAfter, FunSuite, Matchers}\n+\n+import scala.collection.immutable.IndexedSeq\n+\n+@RunWith(classOf[JUnitRunner])\n+class NativeTransformationsIT extends FunSuite with TestContext with BeforeAndAfter with Matchers {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE2MzU4NQ=="}, "originalCommit": {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3316, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}