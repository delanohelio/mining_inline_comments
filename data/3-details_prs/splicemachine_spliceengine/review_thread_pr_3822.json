{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4ODI4ODMw", "number": 3822, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODo1NTozMFrOEOl34A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxNjo1MlrOE1MulA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzM2MDMyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODo1NTozMFrOGx0xyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QyMDozMzo0NlrOHs_ewA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5ODEyMg==", "bodyText": "I think this is doing an explicit cast to Date when we expect that because the inferred type might not be it.", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454898122", "createdAt": "2020-07-15T08:55:30Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +381,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzOTQ1Ng==", "bodyText": "thanks, comment removed!", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r516939456", "createdAt": "2020-11-03T20:33:46Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +381,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5ODEyMg=="}, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzM2OTkyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODo1Nzo0NlrOGx03cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODo1Nzo0NlrOGx03cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5OTU3MA==", "bodyText": "We borrowed Presto's ORC reader because it was faster than Spark's reader at some point in the past. We need to revisit that decision, and even if we stick with Presto's we'd need to bring all changes from the last few months/years", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454899570", "createdAt": "2020-07-15T08:57:46Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -707,6 +698,10 @@ public Boolean isCached(long conglomerateId) throws StandardException {\n         assert baseColumnMap != null:\"baseColumnMap Null\";\n         assert partitionColumnMap != null:\"partitionColumnMap Null\";\n         try {\n+            // todo: check why we're using our own reader here.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 236}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzM4NDgyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTowMTowOFrOGx1AQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMzowMjo0MlrOGx9B1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTgyNg==", "bodyText": "Since you made the effort to document the class (thanks!) please use Javadoc comments to visibilize them", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454901826", "createdAt": "2020-07-15T09:01:08Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzMzMwMQ==", "bodyText": "fixed in #3811", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r455033301", "createdAt": "2020-07-15T13:02:42Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTgyNg=="}, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzM4NTgzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTowMToyNlrOGx1A3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMzowMjo0OFrOGx9CGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTk4MA==", "bodyText": "Ditto here and all other methods", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454901980", "createdAt": "2020-07-15T09:01:26Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them\n+/// to make writing tests for all column types easier.\n+\n+public class CreateTableTypeHelper {\n+    /// @param types: an array of Types that should be used", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzMzM3MQ==", "bodyText": "fixed in #3811", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r455033371", "createdAt": "2020-07-15T13:02:48Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them\n+/// to make writing tests for all column types easier.\n+\n+public class CreateTableTypeHelper {\n+    /// @param types: an array of Types that should be used", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTk4MA=="}, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjIyODc1OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToxNDoxMlrOHmm1FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToxNDoxMlrOHmm1FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0NDExNw==", "bodyText": "note: this small function is added to remove a bigger Avro write function in SparkDataSet", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r510244117", "createdAt": "2020-10-22T15:14:12Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1136,11 +1026,29 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n-                                          String compression, OperationContext context) throws StandardException {\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12"}, "originalPosition": 251}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNDc0MjA2OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjo0MTozNlrOHpTprA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxMDoxNzoxNVrOHpjAbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3NTYyOA==", "bodyText": "Remove context because it is not used", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r513075628", "createdAt": "2020-10-27T22:41:36Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1207,9 +1078,8 @@ public void close() {\n         }\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n-\n+    private DataFrameWriter getDataFrameWriter(StructType tableSchema, int[] partitionBy,\n+                                               OperationContext context) throws StandardException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6b959e5bc583d0e2729866297cb3c7cfe061394"}, "originalPosition": 325}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzMyNzIxMw==", "bodyText": "thanks, done!", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r513327213", "createdAt": "2020-10-28T10:17:15Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1207,9 +1078,8 @@ public void close() {\n         }\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n-\n+    private DataFrameWriter getDataFrameWriter(StructType tableSchema, int[] partitionBy,\n+                                               OperationContext context) throws StandardException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3NTYyOA=="}, "originalCommit": {"oid": "a6b959e5bc583d0e2729866297cb3c7cfe061394"}, "originalPosition": 325}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MDQ1NjMyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDo1NDozOVrOHtFk6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDo1NDozOVrOHtFk6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzOTMzOQ==", "bodyText": "Good documentation.", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517039339", "createdAt": "2020-11-04T00:54:39Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -487,41 +458,65 @@ public StructType getExternalFileSchema(String storedAs, String location, boolea\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n                                 .option(\"mergeSchema\", mergeSchemaOption)\n-                                .parquet(location);\n+                                .parquet(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"a\")) {\n                         // spark does not support schema merging for avro\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n                                 .option(\"ignoreExtension\", false)\n                                 .format(\"com.databricks.spark.avro\")\n-                                .load(location);\n+                                .load(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"o\")) {\n                         // spark does not support schema merging for orc\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n-                                .orc(location);\n+                                .orc(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"t\")) {\n                         // spark-2.2.0: commons-lang3-3.3.2 does not support 'XXX' timezone, specify 'ZZ' instead\n-                        dataset = SpliceSpark.getSession().read().options(getCsvOptions(csvOptions)).csv(location);\n+                        dataset = SpliceSpark.getSession().read().options(getCsvOptions(csvOptions)).csv(rootPath);\n                     } else {\n                         throw new UnsupportedOperationException(\"Unsupported storedAs \" + storedAs);\n                     }\n-                    dataset.printSchema();\n+                    //dataset.printSchema();\n                     schema = dataset.schema();\n                 }\n             } catch (Exception e) {\n-                handleExceptionInferSchema(e, location);\n-            } finally {\n-                if (!mergeSchema && fs != null && temp!= null && fs.exists(temp)){\n-                    fs.delete(temp, true);\n-                    SpliceLogUtils.info(LOG, \"deleted temporary directory %s\", temp);\n-                }\n+                handleExceptionInferSchema(e, rootPath);\n             }\n         }catch (Exception e) {\n             throw StandardException.newException(SQLState.EXTERNAL_TABLES_READ_FAILURE, e, e.getMessage());\n         }\n \n-        return schema;\n+        return new GetSchemaExternalResult(partition_schema, schema);\n+    }\n+\n+    /**\n+     * get the directory partitioning\n+     * @param rootPath root path of the dataset e.g. hdfs://cluster:123/path/to/directory\n+     * @param fileName file name below that path (including directories), e.g. firstCol=34/column2=HELLO/file.parquet\n+     * @param givenPartitionColumns the types as specified in CREATE TABLE ... PARTITIONED BY ( X ).\n+     *                              this is important as you can have e.g. firstCol = VARCHAR, and without this\n+     *                              we would infere firstCol=34 to be INTEGER.\n+     * @return if successful, the StructType for the partitioned columns compatible with partitionColumns.\n+     *         if we can't apply partitionColumns (e.g. have column2=HELLO, but we want column2 to be INT),\n+     *         we infere the schema without the given information, which is then used for suggest schema.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3"}, "originalPosition": 226}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MjE3MzYxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSet.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxMzowNlrOHtVgog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNToxMzozM1rOHyyNcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng==", "bodyText": "Do we need the EmptyFunction here?", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517300386", "createdAt": "2020-11-04T12:13:06Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSet.java", "diffHunk": "@@ -789,43 +773,17 @@ public static StructType generateTableSchema(OperationContext context) throws St\n                                           String compression,\n                                           OperationContext context) throws StandardException\n     {\n-        compression = SparkDataSet.getAvroCompression(compression);\n \n-        StructType dataSchema = null;\n         StructType tableSchema = generateTableSchema(context);\n-\n-        // what is this? why is this so different from parquet/orc ?\n-        // actually very close to NativeSparkDataSet.writeFile\n-        dataSchema = ExternalTableUtils.getDataSchema(dsp, tableSchema, partitionBy, location, \"a\");\n-\n+        StructType dataSchema = SparkExternalTableUtil.getDataSchemaAvro(dsp, tableSchema, partitionBy, location);\n         if (dataSchema == null)\n             dataSchema = tableSchema;\n-\n         Dataset<Row> insertDF = SpliceSpark.getSession().createDataFrame(\n-                rdd.map(new SparkSpliceFunctionWrapper<>(new CountWriteFunction(context))).map(new LocatedRowToRowAvroFunction()),\n+                rdd.map(new SparkSpliceFunctionWrapper<>(new EmptyFunction())).map(new LocatedRowToRowAvroFunction()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAxMzI2MQ==", "bodyText": "i'm not exactly sure how to remove it", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r523013261", "createdAt": "2020-11-13T15:13:14Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSet.java", "diffHunk": "@@ -789,43 +773,17 @@ public static StructType generateTableSchema(OperationContext context) throws St\n                                           String compression,\n                                           OperationContext context) throws StandardException\n     {\n-        compression = SparkDataSet.getAvroCompression(compression);\n \n-        StructType dataSchema = null;\n         StructType tableSchema = generateTableSchema(context);\n-\n-        // what is this? why is this so different from parquet/orc ?\n-        // actually very close to NativeSparkDataSet.writeFile\n-        dataSchema = ExternalTableUtils.getDataSchema(dsp, tableSchema, partitionBy, location, \"a\");\n-\n+        StructType dataSchema = SparkExternalTableUtil.getDataSchemaAvro(dsp, tableSchema, partitionBy, location);\n         if (dataSchema == null)\n             dataSchema = tableSchema;\n-\n         Dataset<Row> insertDF = SpliceSpark.getSession().createDataFrame(\n-                rdd.map(new SparkSpliceFunctionWrapper<>(new CountWriteFunction(context))).map(new LocatedRowToRowAvroFunction()),\n+                rdd.map(new SparkSpliceFunctionWrapper<>(new EmptyFunction())).map(new LocatedRowToRowAvroFunction()),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng=="}, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAxMzQ5MQ==", "bodyText": "if i remove it, it doesn't compile anymore", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r523013491", "createdAt": "2020-11-13T15:13:33Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSet.java", "diffHunk": "@@ -789,43 +773,17 @@ public static StructType generateTableSchema(OperationContext context) throws St\n                                           String compression,\n                                           OperationContext context) throws StandardException\n     {\n-        compression = SparkDataSet.getAvroCompression(compression);\n \n-        StructType dataSchema = null;\n         StructType tableSchema = generateTableSchema(context);\n-\n-        // what is this? why is this so different from parquet/orc ?\n-        // actually very close to NativeSparkDataSet.writeFile\n-        dataSchema = ExternalTableUtils.getDataSchema(dsp, tableSchema, partitionBy, location, \"a\");\n-\n+        StructType dataSchema = SparkExternalTableUtil.getDataSchemaAvro(dsp, tableSchema, partitionBy, location);\n         if (dataSchema == null)\n             dataSchema = tableSchema;\n-\n         Dataset<Row> insertDF = SpliceSpark.getSession().createDataFrame(\n-                rdd.map(new SparkSpliceFunctionWrapper<>(new CountWriteFunction(context))).map(new LocatedRowToRowAvroFunction()),\n+                rdd.map(new SparkSpliceFunctionWrapper<>(new EmptyFunction())).map(new LocatedRowToRowAvroFunction()),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng=="}, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MjE4MTIyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableUnitTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxNTozNFrOHtVlTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxNTozNFrOHtVlTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMTU4Mg==", "bodyText": "It'd be nice to check for a specific Exception / Error code here", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517301582", "createdAt": "2020-11-04T12:15:34Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableUnitTests.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.derby.stream.spark.SparkExternalTableUtil;\n+import com.splicemachine.derby.stream.utils.ExternalTableUtils;\n+import com.splicemachine.system.CsvOptions;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class ExternalTableUnitTests {\n+\n+    @Test\n+    public void testParsePartitionsFromFiles() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \".DS_Store\",\n+                root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__/part-00042.c000.snappy.parquet\",\n+                root + \"_SUCCESS\",\n+                root + \"c=3.14/ws_sold_date_sk=2450817/part-01434.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450816/part-00026.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c001.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( root );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                files, true, basePaths, null, null );\n+        Assert.assertEquals(\"StructType(StructField(c,DoubleType,true), StructField(ws_sold_date_sk,IntegerType,true))\",\n+                ps.partitionColumns().toString());\n+        Assert.assertEquals(\"List(\" +\n+                \"PartitionPath([3.14,null],\" + root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__), \" +\n+                \"PartitionPath([3.14,2450817],\" + root + \"c=3.14/ws_sold_date_sk=2450817), \" +\n+                \"PartitionPath([3.14,2450816],\" + root + \"c=3.14/ws_sold_date_sk=2450816), \" +\n+                \"PartitionPath([3.14,2450818],\" + root + \"c=3.14/ws_sold_date_sk=2450818))\",\n+                ps.partitions().toString());\n+    }\n+\n+    @Test\n+    public void testParsePartitionsFromFiles_one_userDefined() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( root );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        StructType s = new StructType();\n+        s = s.add(\"c\", DataTypes.StringType);\n+        s = s.add(\"ws_sold_date_sk\", DataTypes.DoubleType);\n+\n+        com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                files, true, basePaths, s, null );\n+        Assert.assertEquals(\"StructType(StructField(c,StringType,true), StructField(ws_sold_date_sk,DoubleType,true))\",\n+                ps.partitionColumns().toString());\n+        Assert.assertEquals(\"List(PartitionPath([3.14,2450818.0],\" + root + \"c=3.14/ws_sold_date_sk=2450818))\",\n+                ps.partitions().toString());\n+    }\n+\n+    @Test\n+    public void testParsePartitionsFromFiles_wrong_type() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__/part-00780.c002.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=HELLO/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( \"hdfs://host:123/partition_test/web_sales5/\" );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        StructType s = new StructType();\n+        s = s.add(\"ws_sold_date_sk\", DataTypes.DoubleType);\n+\n+        try {\n+            com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                    files, true, basePaths, s,null);\n+            Assert.fail(\"no exception\");\n+        }\n+        catch(Exception e)\n+        {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MjE4NTE2OnYy", "diffSide": "RIGHT", "path": "scala_util/src/main/scala/com/splicemachine/spark/splicemachine/SplicePartitioningUtils.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxNjo1MlrOHtVnwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjowNjo1N1rOHuKjcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMjIwOA==", "bodyText": "The license goes typically at the very first line", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517302208", "createdAt": "2020-11-04T12:16:52Z", "author": {"login": "dgomezferro"}, "path": "scala_util/src/main/scala/com/splicemachine/spark/splicemachine/SplicePartitioningUtils.scala", "diffHunk": "@@ -0,0 +1,495 @@\n+package com.splicemachine.spark.splicemachine\n+\n+// note: this is marked as private in package org.apache.spark.sql.execution.datasources,\n+// so we had to copy this out.\n+// see\n+// https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala\n+// this file is marked as excluded from spotbugs, see splice_protocol/findbugs-exclude.xml\n+// to be able to detect wrong types for directory partitioning, we modified some part of the code, marked with\n+// modified splicemachine { ... }\n+\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE2OTQ1Ng==", "bodyText": "done", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r518169456", "createdAt": "2020-11-05T16:06:57Z", "author": {"login": "martinrupp"}, "path": "scala_util/src/main/scala/com/splicemachine/spark/splicemachine/SplicePartitioningUtils.scala", "diffHunk": "@@ -0,0 +1,495 @@\n+package com.splicemachine.spark.splicemachine\n+\n+// note: this is marked as private in package org.apache.spark.sql.execution.datasources,\n+// so we had to copy this out.\n+// see\n+// https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala\n+// this file is marked as excluded from spotbugs, see splice_protocol/findbugs-exclude.xml\n+// to be able to detect wrong types for directory partitioning, we modified some part of the code, marked with\n+// modified splicemachine { ... }\n+\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMjIwOA=="}, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3097, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}