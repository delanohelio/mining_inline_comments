{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMyMzY5NDMw", "number": 3671, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxMjozOTowMlrOEERoGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxMjozOTowMlrOEERoGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyOTE4NTU1OnYy", "diffSide": "RIGHT", "path": "scala_util/src/main/scala/org/apache/spark/sql/WithColumns.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxMjozOTowMlrOGhyzLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNDoyMToyMFrOGjF1gA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODA4ODQ5NQ==", "bodyText": "col could be replaced with _ maybe?", "url": "https://github.com/splicemachine/spliceengine/pull/3671#discussion_r438088495", "createdAt": "2020-06-10T12:39:02Z", "author": {"login": "hatyo"}, "path": "scala_util/src/main/scala/org/apache/spark/sql/WithColumns.scala", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package org.apache.spark.sql\n+\n+object WithColumns {\n+\n+  def withColumns(colNames: Seq[String], cols: Seq[Column], ds: DataFrame): DataFrame = {\n+    val resolver = ds.sparkSession.sessionState.analyzer.resolver\n+    val output = ds.queryExecution.analyzed.output\n+\n+    val columnMap = colNames.zip(cols).toMap\n+\n+    val replacedAndExistingColumns = output.map { field =>\n+      columnMap.find { case (colName, _) =>\n+        resolver(field.name, colName)\n+      } match {\n+        case Some((colName: String, col: Column)) => col.as(colName)\n+        case _ => Column(field)\n+      }\n+    }\n+\n+    val newColumns = columnMap.filter { case (colName, col) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7701184fb8474af19a290d5abd68e024abd6239e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ0ODk2MA==", "bodyText": "I'd rather leave it like that because I copied this directly from Spark code: Dataset.withColumns(), which is only available in Spark 2.3+\nI added a clarifying comment in case we ever need to update this to match Spark", "url": "https://github.com/splicemachine/spliceengine/pull/3671#discussion_r439448960", "createdAt": "2020-06-12T14:21:20Z", "author": {"login": "dgomezferro"}, "path": "scala_util/src/main/scala/org/apache/spark/sql/WithColumns.scala", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package org.apache.spark.sql\n+\n+object WithColumns {\n+\n+  def withColumns(colNames: Seq[String], cols: Seq[Column], ds: DataFrame): DataFrame = {\n+    val resolver = ds.sparkSession.sessionState.analyzer.resolver\n+    val output = ds.queryExecution.analyzed.output\n+\n+    val columnMap = colNames.zip(cols).toMap\n+\n+    val replacedAndExistingColumns = output.map { field =>\n+      columnMap.find { case (colName, _) =>\n+        resolver(field.name, colName)\n+      } match {\n+        case Some((colName: String, col: Column)) => col.as(colName)\n+        case _ => Column(field)\n+      }\n+    }\n+\n+    val newColumns = columnMap.filter { case (colName, col) =>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODA4ODQ5NQ=="}, "originalCommit": {"oid": "7701184fb8474af19a290d5abd68e024abd6239e"}, "originalPosition": 35}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3196, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}