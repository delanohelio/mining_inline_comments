{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkwNzYwNjMw", "number": 4171, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0ODowMVrOEl_AwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0OTo1N1rOEl_EAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4MjY1MTUzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0ODowMVrOHVwLcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1NDozNFrOHVwbSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTUwNw==", "bodyText": "Can you make it a Set? Since we are going to call lots of .contains() on it I think that makes sense.", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492571507", "createdAt": "2020-09-22T08:48:01Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1114,64 +1116,98 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n         return (JavaRDD<V>) dataSet.javaRDD().map(new RowToLocatedRowFunction(context));\n     }\n \n+    private DataSet<ExecRow> getRowsWritten(OperationContext context) {\n+        ValueRow valueRow = new ValueRow(1);\n+        valueRow.setColumn(1, new SQLLongint(context.getRecordsWritten()));\n+        return new SparkDataSet<>(SpliceSpark.getContext()\n+                .parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n     @Override\n-    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp,\n-                                             int[] partitionBy,\n-                                             String location,\n-                                             String compression,\n-                                             OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n+    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                             String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .parquet(location);\n+        }\n \n-        // construct a DF using schema of data\n-        Dataset<Row> insertDF = SpliceSpark.getSession()\n-                .createDataFrame(dataset.rdd(), tableSchema);\n+        return getRowsWritten(context);\n+    }\n \n-        List<String> partitionByCols = new ArrayList();\n-        for (int i = 0; i < partitionBy.length; i++) {\n-            partitionByCols.add(tableSchema.fields()[partitionBy[i]].name());\n-        }\n-        if (partitionBy.length > 0) {\n-            List<Column> repartitionCols = new ArrayList();\n-            for (int i = 0; i < partitionBy.length; i++) {\n-                repartitionCols.add(new Column(tableSchema.fields()[partitionBy[i]].name()));\n-            }\n-            insertDF = insertDF.repartition(scala.collection.JavaConversions.asScalaBuffer(repartitionCols).toList());\n+    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                          String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            compression = SparkDataSet.getAvroCompression(compression);\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .format(\"com.databricks.spark.avro\").save(location);\n         }\n-        insertDF.write().option(SPARK_COMPRESSION_OPTION,compression)\n-                .partitionBy(partitionByCols.toArray(new String[partitionByCols.size()]))\n-                .mode(SaveMode.Append).parquet(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        return getRowsWritten(context);\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,\n-                                          int[] partitionBy,\n-                                          String location,\n-                                          String compression,\n-                                          OperationContext context) throws StandardException\n-    {\n-        compression = SparkDataSet.getAvroCompression(compression);\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).format(\"com.databricks.spark.avro\").save(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    public DataSet<ExecRow> writeTextFile(int[] partitionBy, String location, CsvOptions csvOptions,\n+                                          OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .options(getCsvOptions(csvOptions))\n+                    .csv(location);\n+        }\n+        return getRowsWritten(context);\n     }\n-\n-\n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n     public DataSet<ExecRow> writeORCFile(int[] baseColumnMap, int[] partitionBy, String location,  String compression,\n                                                     OperationContext context) throws StandardException {\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).orc(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .orc(location);\n+        }\n+        return getRowsWritten(context);\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, String compression, OperationContext context) throws StandardException {\n+    class CountingListener extends SparkListener implements AutoCloseable\n+    {\n+        OperationContext context;\n+        SparkContext sc;\n+        String uuid;\n+        List<Integer> stageIdsToWatch;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3NTU2MQ==", "bodyText": "yes will do", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492575561", "createdAt": "2020-09-22T08:54:34Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1114,64 +1116,98 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n         return (JavaRDD<V>) dataSet.javaRDD().map(new RowToLocatedRowFunction(context));\n     }\n \n+    private DataSet<ExecRow> getRowsWritten(OperationContext context) {\n+        ValueRow valueRow = new ValueRow(1);\n+        valueRow.setColumn(1, new SQLLongint(context.getRecordsWritten()));\n+        return new SparkDataSet<>(SpliceSpark.getContext()\n+                .parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n     @Override\n-    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp,\n-                                             int[] partitionBy,\n-                                             String location,\n-                                             String compression,\n-                                             OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n+    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                             String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .parquet(location);\n+        }\n \n-        // construct a DF using schema of data\n-        Dataset<Row> insertDF = SpliceSpark.getSession()\n-                .createDataFrame(dataset.rdd(), tableSchema);\n+        return getRowsWritten(context);\n+    }\n \n-        List<String> partitionByCols = new ArrayList();\n-        for (int i = 0; i < partitionBy.length; i++) {\n-            partitionByCols.add(tableSchema.fields()[partitionBy[i]].name());\n-        }\n-        if (partitionBy.length > 0) {\n-            List<Column> repartitionCols = new ArrayList();\n-            for (int i = 0; i < partitionBy.length; i++) {\n-                repartitionCols.add(new Column(tableSchema.fields()[partitionBy[i]].name()));\n-            }\n-            insertDF = insertDF.repartition(scala.collection.JavaConversions.asScalaBuffer(repartitionCols).toList());\n+    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                          String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            compression = SparkDataSet.getAvroCompression(compression);\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .format(\"com.databricks.spark.avro\").save(location);\n         }\n-        insertDF.write().option(SPARK_COMPRESSION_OPTION,compression)\n-                .partitionBy(partitionByCols.toArray(new String[partitionByCols.size()]))\n-                .mode(SaveMode.Append).parquet(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        return getRowsWritten(context);\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,\n-                                          int[] partitionBy,\n-                                          String location,\n-                                          String compression,\n-                                          OperationContext context) throws StandardException\n-    {\n-        compression = SparkDataSet.getAvroCompression(compression);\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).format(\"com.databricks.spark.avro\").save(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    public DataSet<ExecRow> writeTextFile(int[] partitionBy, String location, CsvOptions csvOptions,\n+                                          OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .options(getCsvOptions(csvOptions))\n+                    .csv(location);\n+        }\n+        return getRowsWritten(context);\n     }\n-\n-\n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n     public DataSet<ExecRow> writeORCFile(int[] baseColumnMap, int[] partitionBy, String location,  String compression,\n                                                     OperationContext context) throws StandardException {\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).orc(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .orc(location);\n+        }\n+        return getRowsWritten(context);\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, String compression, OperationContext context) throws StandardException {\n+    class CountingListener extends SparkListener implements AutoCloseable\n+    {\n+        OperationContext context;\n+        SparkContext sc;\n+        String uuid;\n+        List<Integer> stageIdsToWatch;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTUwNw=="}, "originalCommit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4MjY1OTg0OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0OTo1N1rOHVwQQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo1NDoyMVrOHVwasg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MjczNw==", "bodyText": "Why the change? If this is deliberate we'd need to change the test name too", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492572737", "createdAt": "2020-09-22T08:49:57Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -2409,15 +2446,15 @@ public void testParquetPartitionColumnName() throws Exception {\n     public void testOrcColumnName() throws Exception {\n         String tablePath = getExternalResourceDirectory()+\"orc_colname\";\n         methodWatcher.execute(String.format(\"create external table t_orc (col1 int, col2 varchar(5))\" +\n-                \" STORED AS ORC LOCATION '%s'\", tablePath));\n+                \" STORED AS PARQUET LOCATION '%s'\", tablePath));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3NTQxMA==", "bodyText": "good catch, that wasn't intentional.", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492575410", "createdAt": "2020-09-22T08:54:21Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -2409,15 +2446,15 @@ public void testParquetPartitionColumnName() throws Exception {\n     public void testOrcColumnName() throws Exception {\n         String tablePath = getExternalResourceDirectory()+\"orc_colname\";\n         methodWatcher.execute(String.format(\"create external table t_orc (col1 int, col2 varchar(5))\" +\n-                \" STORED AS ORC LOCATION '%s'\", tablePath));\n+                \" STORED AS PARQUET LOCATION '%s'\", tablePath));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MjczNw=="}, "originalCommit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "originalPosition": 96}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2912, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}