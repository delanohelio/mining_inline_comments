{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkwNzYwNjMw", "number": 4171, "title": "DB-9770/DB-10267 Fix External Table Row Counts after cross-join insert", "bodyText": "consisting of changes DB-9770 (fix) and DB-10267 (speed improvement)", "createdAt": "2020-09-22T07:57:42Z", "url": "https://github.com/splicemachine/spliceengine/pull/4171", "merged": true, "mergeCommit": {"oid": "e30f4e225667690765548dfc26cebb1f5aced1a5"}, "closed": true, "closedAt": "2020-09-30T16:40:14Z", "author": {"login": "martinrupp"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLUBmVgFqTQ5MzI0NzU4MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdN4PyrgBqjM4MjMyMTU0NDQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjQ3NTgx", "url": "https://github.com/splicemachine/spliceengine/pull/4171#pullrequestreview-493247581", "createdAt": "2020-09-22T08:48:01Z", "commit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0ODowMVrOHVwLcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwODo0OTo1N1rOHVwQQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MTUwNw==", "bodyText": "Can you make it a Set? Since we are going to call lots of .contains() on it I think that makes sense.", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492571507", "createdAt": "2020-09-22T08:48:01Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1114,64 +1116,98 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n         return (JavaRDD<V>) dataSet.javaRDD().map(new RowToLocatedRowFunction(context));\n     }\n \n+    private DataSet<ExecRow> getRowsWritten(OperationContext context) {\n+        ValueRow valueRow = new ValueRow(1);\n+        valueRow.setColumn(1, new SQLLongint(context.getRecordsWritten()));\n+        return new SparkDataSet<>(SpliceSpark.getContext()\n+                .parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n     @Override\n-    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp,\n-                                             int[] partitionBy,\n-                                             String location,\n-                                             String compression,\n-                                             OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n+    public DataSet<ExecRow> writeParquetFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                             String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .parquet(location);\n+        }\n \n-        // construct a DF using schema of data\n-        Dataset<Row> insertDF = SpliceSpark.getSession()\n-                .createDataFrame(dataset.rdd(), tableSchema);\n+        return getRowsWritten(context);\n+    }\n \n-        List<String> partitionByCols = new ArrayList();\n-        for (int i = 0; i < partitionBy.length; i++) {\n-            partitionByCols.add(tableSchema.fields()[partitionBy[i]].name());\n-        }\n-        if (partitionBy.length > 0) {\n-            List<Column> repartitionCols = new ArrayList();\n-            for (int i = 0; i < partitionBy.length; i++) {\n-                repartitionCols.add(new Column(tableSchema.fields()[partitionBy[i]].name()));\n-            }\n-            insertDF = insertDF.repartition(scala.collection.JavaConversions.asScalaBuffer(repartitionCols).toList());\n+    @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n+                                          String compression, OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            compression = SparkDataSet.getAvroCompression(compression);\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .format(\"com.databricks.spark.avro\").save(location);\n         }\n-        insertDF.write().option(SPARK_COMPRESSION_OPTION,compression)\n-                .partitionBy(partitionByCols.toArray(new String[partitionByCols.size()]))\n-                .mode(SaveMode.Append).parquet(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        return getRowsWritten(context);\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,\n-                                          int[] partitionBy,\n-                                          String location,\n-                                          String compression,\n-                                          OperationContext context) throws StandardException\n-    {\n-        compression = SparkDataSet.getAvroCompression(compression);\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).format(\"com.databricks.spark.avro\").save(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    public DataSet<ExecRow> writeTextFile(int[] partitionBy, String location, CsvOptions csvOptions,\n+                                          OperationContext context) throws StandardException {\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .options(getCsvOptions(csvOptions))\n+                    .csv(location);\n+        }\n+        return getRowsWritten(context);\n     }\n-\n-\n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n     public DataSet<ExecRow> writeORCFile(int[] baseColumnMap, int[] partitionBy, String location,  String compression,\n                                                     OperationContext context) throws StandardException {\n-        DataFrameWriter writer = getDataFrameWriter(partitionBy, compression, context);\n-        writer.mode(SaveMode.Append).orc(location);\n-        ValueRow valueRow=new ValueRow(1);\n-        valueRow.setColumn(1,new SQLLongint(context.getRecordsWritten()));\n-        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+        try( CountingListener counter = new CountingListener(context) ) {\n+            getDataFrameWriter(partitionBy, context)\n+                    .option(SPARK_COMPRESSION_OPTION, compression)\n+                    .orc(location);\n+        }\n+        return getRowsWritten(context);\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, String compression, OperationContext context) throws StandardException {\n+    class CountingListener extends SparkListener implements AutoCloseable\n+    {\n+        OperationContext context;\n+        SparkContext sc;\n+        String uuid;\n+        List<Integer> stageIdsToWatch;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU3MjczNw==", "bodyText": "Why the change? If this is deliberate we'd need to change the test name too", "url": "https://github.com/splicemachine/spliceengine/pull/4171#discussion_r492572737", "createdAt": "2020-09-22T08:49:57Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -2409,15 +2446,15 @@ public void testParquetPartitionColumnName() throws Exception {\n     public void testOrcColumnName() throws Exception {\n         String tablePath = getExternalResourceDirectory()+\"orc_colname\";\n         methodWatcher.execute(String.format(\"create external table t_orc (col1 int, col2 varchar(5))\" +\n-                \" STORED AS ORC LOCATION '%s'\", tablePath));\n+                \" STORED AS PARQUET LOCATION '%s'\", tablePath));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMjUzMDY2", "url": "https://github.com/splicemachine/spliceengine/pull/4171#pullrequestreview-493253066", "createdAt": "2020-09-22T08:54:51Z", "commit": {"oid": "2b3d3a2adbe99b963bad460690729f70683af015"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzMzQ5MjQ0", "url": "https://github.com/splicemachine/spliceengine/pull/4171#pullrequestreview-493349244", "createdAt": "2020-09-22T11:07:20Z", "commit": {"oid": "5c22489633fa4e90ecb8e8951662184da60e57ad"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5c22489633fa4e90ecb8e8951662184da60e57ad", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/5c22489633fa4e90ecb8e8951662184da60e57ad", "committedDate": "2020-09-22T09:06:16Z", "message": "DB-10267 address code review"}, "afterCommit": {"oid": "a34fd0c451848475f9022e89c281c2dc17d1b57c", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/a34fd0c451848475f9022e89c281c2dc17d1b57c", "committedDate": "2020-09-23T09:28:18Z", "message": "DB-10267 address code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a34fd0c451848475f9022e89c281c2dc17d1b57c", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/a34fd0c451848475f9022e89c281c2dc17d1b57c", "committedDate": "2020-09-23T09:28:18Z", "message": "DB-10267 address code review"}, "afterCommit": {"oid": "b39d843594c0bb8455186245a96f6ffd11de0351", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/b39d843594c0bb8455186245a96f6ffd11de0351", "committedDate": "2020-09-24T10:55:46Z", "message": "DB-10267 address code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b39d843594c0bb8455186245a96f6ffd11de0351", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/b39d843594c0bb8455186245a96f6ffd11de0351", "committedDate": "2020-09-24T10:55:46Z", "message": "DB-10267 address code review"}, "afterCommit": {"oid": "924330eb77553233fd9690af6b7d6be92e311aed", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/924330eb77553233fd9690af6b7d6be92e311aed", "committedDate": "2020-09-24T10:56:29Z", "message": "DB-10267 address code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "924330eb77553233fd9690af6b7d6be92e311aed", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/924330eb77553233fd9690af6b7d6be92e311aed", "committedDate": "2020-09-24T10:56:29Z", "message": "DB-10267 address code review"}, "afterCommit": {"oid": "03e5534b370f170e73e93f9bcb118f8824b3fa83", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/03e5534b370f170e73e93f9bcb118f8824b3fa83", "committedDate": "2020-09-28T21:32:59Z", "message": "DB-10267 address code review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5MDI4ODY5", "url": "https://github.com/splicemachine/spliceengine/pull/4171#pullrequestreview-499028869", "createdAt": "2020-09-30T01:29:45Z", "commit": {"oid": "53cfb3ca4ccf01a773b92e2a624611f1c3926f0c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e72b07f3ade56909e7d9a8ce8017574339c64682", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/e72b07f3ade56909e7d9a8ce8017574339c64682", "committedDate": "2020-09-30T08:09:49Z", "message": "DB-9770 Fix External Table Row Counts after cross-join insert"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9e2d21b7f4feb6b4b8b333ffa86793f20fb30c4", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/b9e2d21b7f4feb6b4b8b333ffa86793f20fb30c4", "committedDate": "2020-09-30T08:09:49Z", "message": "DB-9770 refactoring"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fa12bf99601a26961577e8d878b29810accdf3e", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/8fa12bf99601a26961577e8d878b29810accdf3e", "committedDate": "2020-09-30T08:09:49Z", "message": "DB-10267 improving write count speed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "000f754b0152cc03fe24b7ddb39c43f0abe5c990", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/000f754b0152cc03fe24b7ddb39c43f0abe5c990", "committedDate": "2020-09-30T08:09:49Z", "message": "DB-10267 address code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ed5d801e7318e343187c14f8322508a4c9a73fa", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/4ed5d801e7318e343187c14f8322508a4c9a73fa", "committedDate": "2020-09-30T08:09:49Z", "message": "DB-10267 fix test failure for CSV"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "53cfb3ca4ccf01a773b92e2a624611f1c3926f0c", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/53cfb3ca4ccf01a773b92e2a624611f1c3926f0c", "committedDate": "2020-09-29T11:21:37Z", "message": "DB-10267 fix test failure for CSV"}, "afterCommit": {"oid": "4ed5d801e7318e343187c14f8322508a4c9a73fa", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/4ed5d801e7318e343187c14f8322508a4c9a73fa", "committedDate": "2020-09-30T08:09:49Z", "message": "DB-10267 fix test failure for CSV"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1122, "cost": 1, "resetAt": "2021-11-02T10:47:05Z"}}}