{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM1NzMxMTkw", "number": 4822, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMDoyNjoxMlrOFFgyrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMzo1Nzo0MlrOFHMaaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzI0NDYyOnYy", "diffSide": "RIGHT", "path": "hbase_storage/pom.xml", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMDoyNjoxMlrOIGDufw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMjoyMzo0OFrOIGpMaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIyMzQyMw==", "bodyText": "I'd rather not add this dependency, since we are only using .deepEquals() it should be easy to remove.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r543223423", "createdAt": "2020-12-15T10:26:12Z", "author": {"login": "dgomezferro"}, "path": "hbase_storage/pom.xml", "diffHunk": "@@ -136,6 +136,11 @@\n             <version>${project.version}</version>\n             <scope>test</scope>\n         </dependency>\n+        <dependency>\n+          <groupId>com.cedarsoftware</groupId>\n+          <artifactId>java-util</artifactId>\n+          <version>1.61.0</version>\n+        </dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61e1f19c1795edf66de01e5a7b4cd50b1a00a758"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNTE1MA==", "bodyText": "@dgomezferro OK, is it because this is not a well-known library that it's not a good idea to include?  Just trying to know the reason in case I want to include other libraries in the future.  They use the Apache license, so I'm assuming it's not a license issue.  https://github.com/jdereg/java-util\nAlso, if we don't use YARN in the cloud, can you point me to the code that determines the memory available to start spark executors?  I only have one YARN config setting I'm using that's related to how much memory we can use, which determines the number of spark executors we use.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r543525150", "createdAt": "2020-12-15T17:06:15Z", "author": {"login": "msirek"}, "path": "hbase_storage/pom.xml", "diffHunk": "@@ -136,6 +136,11 @@\n             <version>${project.version}</version>\n             <scope>test</scope>\n         </dependency>\n+        <dependency>\n+          <groupId>com.cedarsoftware</groupId>\n+          <artifactId>java-util</artifactId>\n+          <version>1.61.0</version>\n+        </dependency>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIyMzQyMw=="}, "originalCommit": {"oid": "61e1f19c1795edf66de01e5a7b4cd50b1a00a758"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzgzNzI5MQ==", "bodyText": "@dgomezferro OK, I've removed the usage and reference to DeepEquals.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r543837291", "createdAt": "2020-12-16T02:23:48Z", "author": {"login": "msirek"}, "path": "hbase_storage/pom.xml", "diffHunk": "@@ -136,6 +136,11 @@\n             <version>${project.version}</version>\n             <scope>test</scope>\n         </dependency>\n+        <dependency>\n+          <groupId>com.cedarsoftware</groupId>\n+          <artifactId>java-util</artifactId>\n+          <version>1.61.0</version>\n+        </dependency>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIyMzQyMw=="}, "originalCommit": {"oid": "61e1f19c1795edf66de01e5a7b4cd50b1a00a758"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMDg2NjI0OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMzo1NDo1MFrOIIjulA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNjozNTo0NlrOIIqIcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA==", "bodyText": "I think this is not used, remove if so", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545844884", "createdAt": "2020-12-18T13:54:50Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -68,6 +71,8 @@\n     // numNodes is written to zookeeper by OlapServerMaster, so we have\n     // to wait until the Olap Server comes up before getting numNodes from zookeeper.\n     private static int MAX_EXECUTOR_CORES = -1;\n+    // A value to use until the Olap Server comes up.\n+    private static int DEFAULT_MAX_EXECUTOR_CORES = 8;\n     private static int numSparkNodes = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg5ODg5OQ==", "bodyText": "This is needed.  I ran into problems if I made getMaxExecutorCores non-retryable.  If we call it now before ZK data is there we can use the default.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545898899", "createdAt": "2020-12-18T15:12:14Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -68,6 +71,8 @@\n     // numNodes is written to zookeeper by OlapServerMaster, so we have\n     // to wait until the Olap Server comes up before getting numNodes from zookeeper.\n     private static int MAX_EXECUTOR_CORES = -1;\n+    // A value to use until the Olap Server comes up.\n+    private static int DEFAULT_MAX_EXECUTOR_CORES = 8;\n     private static int numSparkNodes = -1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwNzcyNw==", "bodyText": "I meant the numSparkNodes field, I believe it's only written to but never used. Sorry for not being clear, I can see how my message was confusing!", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545907727", "createdAt": "2020-12-18T15:26:05Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -68,6 +71,8 @@\n     // numNodes is written to zookeeper by OlapServerMaster, so we have\n     // to wait until the Olap Server comes up before getting numNodes from zookeeper.\n     private static int MAX_EXECUTOR_CORES = -1;\n+    // A value to use until the Olap Server comes up.\n+    private static int DEFAULT_MAX_EXECUTOR_CORES = 8;\n     private static int numSparkNodes = -1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkzMTQ5MA==", "bodyText": "I see.  It might actually help for debugging to have a static field that tells us how many spark nodes there are (or how many we think there are), so if you don't mind I'd like to leave it in for now.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545931490", "createdAt": "2020-12-18T16:05:04Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -68,6 +71,8 @@\n     // numNodes is written to zookeeper by OlapServerMaster, so we have\n     // to wait until the Olap Server comes up before getting numNodes from zookeeper.\n     private static int MAX_EXECUTOR_CORES = -1;\n+    // A value to use until the Olap Server comes up.\n+    private static int DEFAULT_MAX_EXECUTOR_CORES = 8;\n     private static int numSparkNodes = -1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk0OTgxMA==", "bodyText": "Ok", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545949810", "createdAt": "2020-12-18T16:35:46Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -68,6 +71,8 @@\n     // numNodes is written to zookeeper by OlapServerMaster, so we have\n     // to wait until the Olap Server comes up before getting numNodes from zookeeper.\n     private static int MAX_EXECUTOR_CORES = -1;\n+    // A value to use until the Olap Server comes up.\n+    private static int DEFAULT_MAX_EXECUTOR_CORES = 8;\n     private static int numSparkNodes = -1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMDg3MjA1OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMzo1NjoyMVrOIIjyCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToxMjo0MFrOIInCyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NTc3MA==", "bodyText": "I'd rather have this function in OlapServerMaster or a class logically close to the OlapServer, I think this class shouldn't have that much knowledge about YARN. Ideally we should just read the number of executor cores from Zookeeper.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545845770", "createdAt": "2020-12-18T13:56:21Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -265,21 +270,38 @@ public int getMaxExecutorCores() {\n         if (MAX_EXECUTOR_CORES != -1)\n             return MAX_EXECUTOR_CORES;\n         synchronized (HEngineSqlEnv.class) {\n-            int sparkNodes = getNumSparkNodes();\n+            byte [] sparkYarnConfigBytes = getSparkYarnConfigBytes();\n+            if (sparkYarnConfigBytes == null)\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+\n+            SparkYarnConfiguration conf = null;\n+            try {\n+                ByteArrayInputStream bis = new ByteArrayInputStream(sparkYarnConfigBytes);\n+                ObjectInput in = new ObjectInputStream(bis);\n+                conf = (SparkYarnConfiguration) in.readObject();\n+            }\n+            catch (Exception e) {\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+            }\n+            if (conf == null)\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+\n             int maxExecutorCores =\n-              calculateMaxExecutorCores(HConfiguration.unwrapDelegate().get(\"yarn.nodemanager.resource.memory-mb\"),\n-                                        getProperty(\"splice.spark.dynamicAllocation.enabled\"),\n-                                        getProperty(\"splice.spark.executor.instances\"),\n-                                        getProperty(\"splice.spark.executor.cores\"),\n-                                        getProperty(\"splice.spark.executor.memory\"),\n-                                        getProperty(\"splice.spark.dynamicAllocation.maxExecutors\"),\n-                                        getProperty(\"splice.spark.executor.memoryOverhead\"),\n-                                        getProperty(\"splice.spark.yarn.executor.memoryOverhead\"),\n-                                        sparkNodes > 0 ? sparkNodes : 1);\n-            if (sparkNodes > 0) {\n-                numSparkNodes = sparkNodes;\n+              calculateMaxExecutorCores(conf.getYarnNodemanagerResourceMemoryMB(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg5OTIwOQ==", "bodyText": "OK, I moved the calculation to OlapServerMaster.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545899209", "createdAt": "2020-12-18T15:12:40Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -265,21 +270,38 @@ public int getMaxExecutorCores() {\n         if (MAX_EXECUTOR_CORES != -1)\n             return MAX_EXECUTOR_CORES;\n         synchronized (HEngineSqlEnv.class) {\n-            int sparkNodes = getNumSparkNodes();\n+            byte [] sparkYarnConfigBytes = getSparkYarnConfigBytes();\n+            if (sparkYarnConfigBytes == null)\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+\n+            SparkYarnConfiguration conf = null;\n+            try {\n+                ByteArrayInputStream bis = new ByteArrayInputStream(sparkYarnConfigBytes);\n+                ObjectInput in = new ObjectInputStream(bis);\n+                conf = (SparkYarnConfiguration) in.readObject();\n+            }\n+            catch (Exception e) {\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+            }\n+            if (conf == null)\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+\n             int maxExecutorCores =\n-              calculateMaxExecutorCores(HConfiguration.unwrapDelegate().get(\"yarn.nodemanager.resource.memory-mb\"),\n-                                        getProperty(\"splice.spark.dynamicAllocation.enabled\"),\n-                                        getProperty(\"splice.spark.executor.instances\"),\n-                                        getProperty(\"splice.spark.executor.cores\"),\n-                                        getProperty(\"splice.spark.executor.memory\"),\n-                                        getProperty(\"splice.spark.dynamicAllocation.maxExecutors\"),\n-                                        getProperty(\"splice.spark.executor.memoryOverhead\"),\n-                                        getProperty(\"splice.spark.yarn.executor.memoryOverhead\"),\n-                                        sparkNodes > 0 ? sparkNodes : 1);\n-            if (sparkNodes > 0) {\n-                numSparkNodes = sparkNodes;\n+              calculateMaxExecutorCores(conf.getYarnNodemanagerResourceMemoryMB(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NTc3MA=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMDg3Mzc0OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMzo1Njo1MFrOIIjzAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMzozMzowNVrOII2GNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw==", "bodyText": "If it's not a lot of work make this a Protobuf message for future extendability.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545846017", "createdAt": "2020-12-18T13:56:50Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -78,22 +83,22 @@\n     private OlapClient olapClient;\n     private OperationManager operationManager;\n     private ZkServiceDiscovery serviceDiscovery;\n-    private static final String sparkNumNodesZkPath =\n-            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_NUM_NODES_PATH;\n+    private static final String sparkYarnConfigZkPath =\n+            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_YARN_CONFIG_PATH;\n \n \n-    // Find the number of nodes on which Spark executors can be run.\n-    private static int getNumSparkNodes() {\n-        int numNodes = -1;\n+    // Find the Spark and YARN configuration from Zookeeper.\n+    private static byte [] getSparkYarnConfigBytes() {\n+        byte [] returnData = null;\n         try {\n-            byte [] data = ZkUtils.getData(sparkNumNodesZkPath);\n+            byte [] data = ZkUtils.getData(sparkYarnConfigZkPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwMDcyNA==", "bodyText": "This is an ephemeral ZK field, so we don't need to be backwards compatible.  It goes away every time zookeeper reboots.  Would it matter to use protobuf?", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545900724", "createdAt": "2020-12-18T15:15:09Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -78,22 +83,22 @@\n     private OlapClient olapClient;\n     private OperationManager operationManager;\n     private ZkServiceDiscovery serviceDiscovery;\n-    private static final String sparkNumNodesZkPath =\n-            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_NUM_NODES_PATH;\n+    private static final String sparkYarnConfigZkPath =\n+            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_YARN_CONFIG_PATH;\n \n \n-    // Find the number of nodes on which Spark executors can be run.\n-    private static int getNumSparkNodes() {\n-        int numNodes = -1;\n+    // Find the Spark and YARN configuration from Zookeeper.\n+    private static byte [] getSparkYarnConfigBytes() {\n+        byte [] returnData = null;\n         try {\n-            byte [] data = ZkUtils.getData(sparkNumNodesZkPath);\n+            byte [] data = ZkUtils.getData(sparkYarnConfigZkPath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkxMDU1NA==", "bodyText": "I created https://splicemachine.atlassian.net/browse/DB-11068 to change all messages in OlapServerMaster. It could matter for online upgrade.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545910554", "createdAt": "2020-12-18T15:30:36Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -78,22 +83,22 @@\n     private OlapClient olapClient;\n     private OperationManager operationManager;\n     private ZkServiceDiscovery serviceDiscovery;\n-    private static final String sparkNumNodesZkPath =\n-            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_NUM_NODES_PATH;\n+    private static final String sparkYarnConfigZkPath =\n+            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_YARN_CONFIG_PATH;\n \n \n-    // Find the number of nodes on which Spark executors can be run.\n-    private static int getNumSparkNodes() {\n-        int numNodes = -1;\n+    // Find the Spark and YARN configuration from Zookeeper.\n+    private static byte [] getSparkYarnConfigBytes() {\n+        byte [] returnData = null;\n         try {\n-            byte [] data = ZkUtils.getData(sparkNumNodesZkPath);\n+            byte [] data = ZkUtils.getData(sparkYarnConfigZkPath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE0NTg0NQ==", "bodyText": "OK, we can handle it through the Jira.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r546145845", "createdAt": "2020-12-18T23:33:05Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -78,22 +83,22 @@\n     private OlapClient olapClient;\n     private OperationManager operationManager;\n     private ZkServiceDiscovery serviceDiscovery;\n-    private static final String sparkNumNodesZkPath =\n-            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_NUM_NODES_PATH;\n+    private static final String sparkYarnConfigZkPath =\n+            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_YARN_CONFIG_PATH;\n \n \n-    // Find the number of nodes on which Spark executors can be run.\n-    private static int getNumSparkNodes() {\n-        int numNodes = -1;\n+    // Find the Spark and YARN configuration from Zookeeper.\n+    private static byte [] getSparkYarnConfigBytes() {\n+        byte [] returnData = null;\n         try {\n-            byte [] data = ZkUtils.getData(sparkNumNodesZkPath);\n+            byte [] data = ZkUtils.getData(sparkYarnConfigZkPath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMDg3NzIxOnYy", "diffSide": "RIGHT", "path": "hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxMzo1Nzo0MlrOIIj08g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToxNToyMlrOIInJJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjUxNA==", "bodyText": "If we compute the number of cores in OlapServerMaster (or close by) I don't think we need this class anymore", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545846514", "createdAt": "2020-12-18T13:57:42Z", "author": {"login": "dgomezferro"}, "path": "hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+\n+package com.splicemachine.access;\n+\n+import java.io.*;\n+\n+/**\n+ * A serializable class for storing YARN and Spark properties in Zookeeper.\n+ *\n+ */\n+public class SparkYarnConfiguration implements Externalizable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwMDgzOA==", "bodyText": "Yes, it is removed.", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545900838", "createdAt": "2020-12-18T15:15:22Z", "author": {"login": "msirek"}, "path": "hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+\n+package com.splicemachine.access;\n+\n+import java.io.*;\n+\n+/**\n+ * A serializable class for storing YARN and Spark properties in Zookeeper.\n+ *\n+ */\n+public class SparkYarnConfiguration implements Externalizable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjUxNA=="}, "originalCommit": {"oid": "94546d92d1d082d0b604bf430476c1f0955863db"}, "originalPosition": 23}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2723, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}