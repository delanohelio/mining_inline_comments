{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk5NjYyNDgy", "number": 4243, "title": "SPLICE-2289 MultiRowRangeFilter for MultiProbeScan and MergeJoin.", "bodyText": "Improve performance of merge join involving large tables.\nImprove performance of MultiProbeScan on Spark.\nFix issues related to OLAP join costing.\n\nA detailed description of the changes and performance test results are in SPLICE-2289.\nCode Review Kick-Off Notes\nMultiProbeScan Performance Test Results\nMerge Join Performance Test Results", "createdAt": "2020-10-08T05:15:38Z", "url": "https://github.com/splicemachine/spliceengine/pull/4243", "merged": true, "mergeCommit": {"oid": "cfbb04cf428bc4118e4863610d7db5ae0cc99c2a"}, "closed": true, "closedAt": "2020-10-27T16:42:44Z", "author": {"login": "msirek"}, "timelineItems": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdQqBlZABqjM4NTc3Mjk4MzA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdWhQWtAFqTUxNzM0MDE4Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8783ab501e57a89819121481d9b404a3b352f03e", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/8783ab501e57a89819121481d9b404a3b352f03e", "committedDate": "2020-10-08T20:23:04Z", "message": "SPLICE-2289 Fix Spotbugs."}, "afterCommit": {"oid": "9ba8c4f8dd47eaa49c2296a11ba8f2fd5a43f6e6", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/9ba8c4f8dd47eaa49c2296a11ba8f2fd5a43f6e6", "committedDate": "2020-10-08T23:15:35Z", "message": "SPLICE-2289 Fix Spotbugs."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "25c2eb4768cc501de63ce61f12a5caa23b89aa05", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/25c2eb4768cc501de63ce61f12a5caa23b89aa05", "committedDate": "2020-10-13T05:53:12Z", "message": "SPLICE-2289 Favor old merge join if probing isn't expected\n            to reduce the number of right rows sent to join\n\t    with the left rows."}, "afterCommit": {"oid": "dce4fd907dffc5d829c838732532314ed730ea1b", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/dce4fd907dffc5d829c838732532314ed730ea1b", "committedDate": "2020-10-13T16:17:39Z", "message": "SPLICE-2289 Favor old merge join if probing isn't expected\n            to reduce the number of right rows sent to join\n\t    with the left rows."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA5MDgwODk0", "url": "https://github.com/splicemachine/spliceengine/pull/4243#pullrequestreview-509080894", "createdAt": "2020-10-15T07:14:37Z", "commit": {"oid": "6cca67bb5d7d90db00dae368ebbd90f8894e9281"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwNzoxNDozN1rOHh15kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQwNzoyMToyMFrOHh2XVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI0ODE0Nw==", "bodyText": "else throw?", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r505248147", "createdAt": "2020-10-15T07:14:37Z", "author": {"login": "martinrupp"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/JoinNode.java", "diffHunk": "@@ -1296,19 +1304,34 @@ protected void generateCore(ActivationClassBuilder acb,\n          */\n         String joinResultSetString;\n \n+        AccessPath ap = ((Optimizable)rightResultSet).getTrulyTheBestAccessPath();\n         if (joinType==FULLOUTERJOIN) {\n-            joinResultSetString=((Optimizable)rightResultSet).getTrulyTheBestAccessPath().\n-                    getJoinStrategy().fullOuterJoinResultSetMethodName();\n+            joinResultSetString=ap.getJoinStrategy().fullOuterJoinResultSetMethodName();\n         } else if(joinType==LEFTOUTERJOIN){\n-            joinResultSetString=((Optimizable)rightResultSet).getTrulyTheBestAccessPath().\n-                    getJoinStrategy().halfOuterJoinResultSetMethodName();\n+            joinResultSetString=ap.getJoinStrategy().halfOuterJoinResultSetMethodName();\n         }else{\n-            joinResultSetString=((Optimizable)rightResultSet).getTrulyTheBestAccessPath().\n-                    getJoinStrategy().joinResultSetMethodName();\n+            joinResultSetString=ap.getJoinStrategy().joinResultSetMethodName();\n         }\n \n         acb.pushGetResultSetFactoryExpression(mb);\n         int nargs=getJoinArguments(acb,mb,joinClause);\n+        if (RSUtils.isMJ(ap)) {\n+            nargs++;\n+\n+            CompilerContext.NewMergeJoinExecutionType\n+                newMergeJoin = getCompilerContext().getNewMergeJoin();\n+\n+            // Favor Old Merge Join if we estimate probing won't reduce the scanned row count much.\n+            boolean chooseOldMergeJoin = favorOldMergeJoin(ap.getCostEstimate());\n+            if (chooseOldMergeJoin) {\n+                if (newMergeJoin == SYSTEM)\n+                    newMergeJoin = SYSTEM_OFF;\n+                else if (newMergeJoin == ON)\n+                    newMergeJoin = OFF;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6cca67bb5d7d90db00dae368ebbd90f8894e9281"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1MjkyMg==", "bodyText": "i know we don't do a lot of unit tests, but this function would be very easy to unit test, please add a test.", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r505252922", "createdAt": "2020-10-15T07:19:04Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -169,4 +179,177 @@ public ServiceDiscovery serviceDiscovery() {\n     }\n \n \n+    /**\n+     * Parse a Spark or Hadoop size parameter value, that may use k, m, g or t\n+     * to represent kilobytes, megabytes, gigabytes or terabytes, respectively,\n+     * and return back the corresponding number of bytes.\n+     * @param sizeString the parameter value string to parse\n+     * @param defaultValue the default value of the parameter if an invalid\n+     *                     <code>sizeString</code> was passed.\n+     * @return The value in bytes of <code>sizeString</code>, or <code>defaultValue</code>\n+     *         if a <code>sizeString</code> was passed that could not be parsed.\n+     */\n+    private static long parseSizeString(String sizeString, long defaultValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6cca67bb5d7d90db00dae368ebbd90f8894e9281"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1NTc2Nw==", "bodyText": "if converted to\nprivate static int calculateMaxExecutorCores(String memorySize, String sparkDynamicAllocationString, String executorInstancesString,  String executorCoresString, String sparkExecutorMemory)\nwe can also unit test this function whose logic isn't trivial", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r505255767", "createdAt": "2020-10-15T07:21:20Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -169,4 +179,177 @@ public ServiceDiscovery serviceDiscovery() {\n     }\n \n \n+    /**\n+     * Parse a Spark or Hadoop size parameter value, that may use k, m, g or t\n+     * to represent kilobytes, megabytes, gigabytes or terabytes, respectively,\n+     * and return back the corresponding number of bytes.\n+     * @param sizeString the parameter value string to parse\n+     * @param defaultValue the default value of the parameter if an invalid\n+     *                     <code>sizeString</code> was passed.\n+     * @return The value in bytes of <code>sizeString</code>, or <code>defaultValue</code>\n+     *         if a <code>sizeString</code> was passed that could not be parsed.\n+     */\n+    private static long parseSizeString(String sizeString, long defaultValue) {\n+        long retVal = defaultValue;\n+        Pattern sizePattern = Pattern.compile(\"([\\\\d.]+)([kmgt])\", Pattern.CASE_INSENSITIVE);\n+        Matcher matcher = sizePattern.matcher(sizeString);\n+        Map<String, Integer> suffixes = new HashMap<>();\n+        suffixes.put(\"k\", 1);\n+        suffixes.put(\"m\", 2);\n+        suffixes.put(\"g\", 3);\n+        suffixes.put(\"t\", 4);\n+        if (matcher.find()) {\n+            BigInteger value;\n+            String digits = matcher.group(1);\n+            try {\n+              value = new BigInteger(digits);\n+            }\n+            catch (NumberFormatException e) {\n+              return defaultValue;\n+            }\n+            int power = suffixes.get(matcher.group(2).toLowerCase());\n+            BigInteger multiplicand = BigInteger.valueOf(1024).pow(power);\n+            value = value.multiply(multiplicand);\n+            if (value.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0)\n+              return Long.MAX_VALUE;\n+            if (value.compareTo(BigInteger.valueOf(1)) < 0)\n+              return defaultValue;\n+\n+            retVal = value.longValue();\n+        }\n+        else {\n+            try {\n+                retVal = Long.parseLong(sizeString);\n+            }\n+            catch (NumberFormatException e) {\n+                return defaultValue;\n+            }\n+            if (retVal < 1)\n+                retVal = defaultValue;\n+        }\n+        return retVal;\n+    }\n+\n+    @Override\n+    public int getMaxExecutorCores() {\n+        return MAX_EXECUTOR_CORES;\n+    }\n+\n+    /**\n+     * Estimate the maximum number of Spark executor cores that could be simultaneously\n+     * be running given the current YARN and splice.spark settings.\n+     *\n+     * @return The maximum number of Spark executor cores.\n+     */\n+    private static int calculateMaxExecutorCores() {\n+        String memorySize = HConfiguration.unwrapDelegate().get(\"yarn.nodemanager.resource.memory-mb\");\n+        String sparkDynamicAllocationString = getProperty(\"splice.spark.dynamicAllocation.enabled\");\n+        String executorInstancesString = getProperty(\"splice.spark.executor.instances\");\n+        String executorCoresString = getProperty(\"splice.spark.executor.cores\");\n+        String sparkExecutorMemory = getProperty(\"splice.spark.executor.memory\");\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6cca67bb5d7d90db00dae368ebbd90f8894e9281"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef0023ed7e5bf6837efe112f31f51bb7868f6c24", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/ef0023ed7e5bf6837efe112f31f51bb7868f6c24", "committedDate": "2020-10-18T05:31:10Z", "message": "SPLICE-2289 MultiRowRangeFilter for MultiProbeScan and MergeJoin."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d6e058c44595d7524623a461a4bf272daf2cd71", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/8d6e058c44595d7524623a461a4bf272daf2cd71", "committedDate": "2020-10-18T05:31:14Z", "message": "SPLICE-2289 Fixes for HBase 2.0."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7910f393b5db1664a3d47694ef2eff2915814725", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/7910f393b5db1664a3d47694ef2eff2915814725", "committedDate": "2020-10-18T05:31:14Z", "message": "SPLICE-2289 Fix Spotbugs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5be38629407d0452cf924f55ffcb1f24da5397e", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/f5be38629407d0452cf924f55ffcb1f24da5397e", "committedDate": "2020-10-18T05:55:27Z", "message": "SPLICE-2289 Fix issue in DB-8896 trying to access lcc from wrong thread."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f68598e6aca1d7977161032958bf93d504fbb06", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/0f68598e6aca1d7977161032958bf93d504fbb06", "committedDate": "2020-10-18T05:55:30Z", "message": "SPLICE-2289 Fix Spotbugs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "386d0b2797191b68fb0ff889faa8fcc6cd0530a2", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/386d0b2797191b68fb0ff889faa8fcc6cd0530a2", "committedDate": "2020-10-18T05:55:30Z", "message": "SPLICE-2289 Fix KillOperationIT."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5054e44dd613d1802174feadaed38464f0214cc5", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/5054e44dd613d1802174feadaed38464f0214cc5", "committedDate": "2020-10-18T05:55:30Z", "message": "SPLICE-2289 Fix KillOperation_IT, take 2."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61986bc97ead8d7b0320009e8d694ebd9d5c6589", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/61986bc97ead8d7b0320009e8d694ebd9d5c6589", "committedDate": "2020-10-18T06:03:55Z", "message": "SPLICE-2289 Allow fallback to old merge join algorithm for\n            small table cases, and add a property to force fallback\n\t    to old merge join."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44739d885356c8680a6d32f1a411c51ce5359630", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/44739d885356c8680a6d32f1a411c51ce5359630", "committedDate": "2020-10-18T06:30:11Z", "message": "SPLICE-2289 splice.execution.newMergeJoin database and system property."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0374a40cd5ac41436ebab75222816ff9989e74d", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/e0374a40cd5ac41436ebab75222816ff9989e74d", "committedDate": "2020-10-18T06:30:14Z", "message": "SPLICE-2289 Favor old merge join if probing isn't expected\n            to reduce the number of right rows sent to join\n\t    with the left rows."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8dac331e4f50a369a5fe8a2dc8f11802f4b3113", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/f8dac331e4f50a369a5fe8a2dc8f11802f4b3113", "committedDate": "2020-10-18T06:46:08Z", "message": "SPLICE-2289 Handle upgrade scenarios."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "236d2dde74247b7947da8b554c5cf092f9c49615", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/236d2dde74247b7947da8b554c5cf092f9c49615", "committedDate": "2020-10-18T06:46:11Z", "message": "SPLICE-2289 Fix some formulas."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4197e164e6da6a900291985f47eea363acd0c7c9", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/4197e164e6da6a900291985f47eea363acd0c7c9", "committedDate": "2020-10-18T06:46:11Z", "message": "SPLICE-2289 Fix HEngineSqlEnv methods and add unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/20892e371d6ac13494b2c8dc7c3b543dde010c8a", "committedDate": "2020-10-18T06:47:15Z", "message": "SPLICE-2289 Store the number of Spark nodes in Zookeeper."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "762cb60e06367c07ec571c1452f41e5d74b0cab9", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/762cb60e06367c07ec571c1452f41e5d74b0cab9", "committedDate": "2020-10-18T04:43:58Z", "message": "SPLICE-2289 Store the number of Spark nodes in Zookeeper."}, "afterCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/20892e371d6ac13494b2c8dc7c3b543dde010c8a", "committedDate": "2020-10-18T06:47:15Z", "message": "SPLICE-2289 Store the number of Spark nodes in Zookeeper."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyNTE4NjU5", "url": "https://github.com/splicemachine/spliceengine/pull/4243#pullrequestreview-512518659", "createdAt": "2020-10-20T09:38:11Z", "commit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzMTY1NzY0", "url": "https://github.com/splicemachine/spliceengine/pull/4243#pullrequestreview-513165764", "createdAt": "2020-10-20T21:58:04Z", "commit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMTo1ODowNFrOHlSrHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzoyNTowMlrOHldw0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg2NTMwOQ==", "bodyText": "Please remove the extra semi-column.", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508865309", "createdAt": "2020-10-20T21:58:04Z", "author": {"login": "yxia92"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/function/merge/AbstractMergeJoinFlatMapFunction.java", "diffHunk": "@@ -32,98 +33,281 @@\n import splice.com.google.common.base.Function;\n import splice.com.google.common.collect.Iterators;\n import splice.com.google.common.collect.PeekingIterator;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.utils.Pair;\n+import splice.com.google.common.base.Preconditions;\n \n import javax.annotation.Nullable;\n-import java.io.Closeable;\n-import java.io.IOException;\n+import java.io.*;\n+import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Iterator;\n+import java.util.List;\n+\n+import static com.splicemachine.EngineDriver.isMemPlatform;\n+import static com.splicemachine.db.shared.common.reference.SQLState.LANG_INTERNAL_ERROR;\n \n /**\n  * Created by jleach on 6/9/15.\n  */\n-public abstract class AbstractMergeJoinFlatMapFunction extends SpliceFlatMapFunction<JoinOperation,Iterator<ExecRow>,ExecRow> {\n+public abstract class AbstractMergeJoinFlatMapFunction extends SpliceFlatMapFunction<JoinOperation,Iterator<ExecRow>, ExecRow> {\n     boolean initialized;\n     protected JoinOperation joinOperation;\n+    protected SpliceOperation leftSide;\n+    protected SpliceOperation rightSide;\n+    private PeekingIterator<ExecRow> leftPeekingIterator;\n+    private Iterator<ExecRow> mergeJoinIterator;\n+    private static final boolean IS_MEM_PLATFORM = isMemPlatform();\n+    private final SIDriver driver = SIDriver.driver();\n+    private boolean useOldMergeJoin = false;\n \n     public AbstractMergeJoinFlatMapFunction() {\n         super();\n     }\n \n-    public AbstractMergeJoinFlatMapFunction(OperationContext<JoinOperation> operationContext) {\n+    public AbstractMergeJoinFlatMapFunction(OperationContext<JoinOperation> operationContext, boolean useOldMergeJoin) {\n         super(operationContext);\n+        this.useOldMergeJoin = useOldMergeJoin || isMemPlatform();\n     }\n \n-    @Override\n-    public Iterator<ExecRow> call(Iterator<ExecRow> locatedRows) throws Exception {\n-        PeekingIterator<ExecRow> leftPeekingIterator = Iterators.peekingIterator(locatedRows);\n-        if (!initialized) {\n-            joinOperation = getOperation();\n-            initialized = true;\n-            if (!leftPeekingIterator.hasNext())\n-                return Collections.EMPTY_LIST.iterator();\n-            initRightScan(leftPeekingIterator);\n-        }\n-        final SpliceOperation rightSide = joinOperation.getRightOperation();\n-        rightSide.reset();\n-        DataSetProcessor dsp =EngineDriver.driver().processorFactory().bulkProcessor(getOperation().getActivation(), rightSide);\n-        final Iterator<ExecRow> rightIterator = Iterators.transform(rightSide.getDataSet(dsp).toLocalIterator(), new Function<ExecRow, ExecRow>() {\n-            @Override\n-            public ExecRow apply(@Nullable ExecRow locatedRow) {\n-                operationContext.recordJoinedRight();\n-                return locatedRow;\n+    protected class BufferedMergeJoinIterator implements PeekingIterator<ExecRow> {\n+        protected static final int BUFFERSIZE=400;\n+        private static final int INITIALCAPACITY=50;\n+        private ArrayList<ExecRow> bufferedRowList = new ArrayList<>(INITIALCAPACITY);\n+        private PeekingIterator<ExecRow> sourceIterator;\n+        private boolean hasPeeked;\n+        private boolean firstTime = true;\n+\n+        // A pointer to the next row to return when next() is called.\n+        private int bufferPosition;\n+\n+        private void fillBuffer() throws StandardException {\n+            bufferPosition = 0;\n+\n+            if (firstTime) {\n+                bufferedRowList.clear();\n+                for (int i = 0; i < BUFFERSIZE && sourceIterator.hasNext(); i++) {\n+                    bufferedRowList.add((ExecRow) sourceIterator.next().getClone());\n+                }\n             }\n-        });\n-        ((BaseActivation)joinOperation.getActivation()).setScanStartOverride(null); // reset to null to avoid any side effects\n-        ((BaseActivation)joinOperation.getActivation()).setScanKeys(null);\n-        ((BaseActivation)joinOperation.getActivation()).setScanStopOverride(null);\n-        AbstractMergeJoinIterator iterator = createMergeJoinIterator(leftPeekingIterator,\n-                Iterators.peekingIterator(rightIterator),\n-                joinOperation.getLeftHashKeys(), joinOperation.getRightHashKeys(),\n-                joinOperation, operationContext);\n-        iterator.registerCloseable(new Closeable() {\n-            @Override\n-            public void close() throws IOException {\n-                try {\n-                    rightSide.close();\n-                } catch (StandardException e) {\n-                    throw new RuntimeException(e);\n+            else {\n+                // Re-use the buffer rows on subsequent filling of the buffer\n+                // to reduce memory usage and GC pressure.\n+                int i;\n+                for (i = 0; i < BUFFERSIZE && sourceIterator.hasNext(); i++) {\n+                    bufferedRowList.get(i).transfer(sourceIterator.next());\n                 }\n+                if (i < bufferedRowList.size())\n+                    for (int j = bufferedRowList.size()-1; j >= i; j--)\n+                        bufferedRowList.remove(j);\n             }\n-        });\n-        return iterator;\n-    }\n+            firstTime = false;\n+            if (!bufferedRowList.isEmpty())\n+                startNewRightSideScan(this);\n+        }\n \n-    private int[] getColumnOrdering(SpliceOperation op) throws StandardException {\n-        SpliceOperation operation = op;\n-        while (operation != null && !(operation instanceof ScanOperation)) {\n-            operation = operation.getLeftOperation();\n+        public List<ExecRow> getBufferList() {\n+            return bufferedRowList;\n         }\n-        assert operation != null;\n \n-        return ((ScanOperation)operation).getColumnOrdering();\n-    }\n+        protected BufferedMergeJoinIterator(Iterator<ExecRow> sourceIterator) throws StandardException {\n+            this.sourceIterator = Iterators.peekingIterator(sourceIterator);;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg5ODQ5MQ==", "bodyText": "The reset() will reset modifedRowCount, badRecords. Since here we may reset the right operation for every batch of left rows, would we have a side effect of only track the info of the last batch?", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508898491", "createdAt": "2020-10-20T23:28:10Z", "author": {"login": "yxia92"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/function/merge/AbstractMergeJoinFlatMapFunction.java", "diffHunk": "@@ -232,10 +416,137 @@ else if (!leftKeyIsNull) {\n             if (scanInfo != null && scanInfo.getSameStartStopPosition())\n                 ((BaseActivation)joinOperation.getActivation()).setScanStopOverride(startPosition);\n         }\n+        return true;\n+    }\n+\n+    private void startNewRightSideScan(PeekingIterator<ExecRow> leftRows) throws StandardException {\n+        Iterator<ExecRow> rightIterator;\n+\n+        ArrayList<Pair<ExecRow, ExecRow>> keyRows = null;\n+\n+        boolean skipRightSideRead = false;\n+\n+        // The mem platform doesn't support the HBase MultiRangeRowFilter.\n+        if (!IS_MEM_PLATFORM && leftRows instanceof BufferedMergeJoinIterator) {\n+            BufferedMergeJoinIterator mjIter = (BufferedMergeJoinIterator)leftRows;\n+            keyRows = mjIter.getKeyRows();\n+            ((BaseActivation) joinOperation.getActivation()).setKeyRows(keyRows);\n+            skipRightSideRead = (keyRows == null);\n+        }\n+        else\n+            skipRightSideRead = !initRightScanWithStartStopKeys(leftRows);\n+\n+        // If there are no join keys to look up in the right table,\n+        // don't even read the right table.\n+        if (skipRightSideRead)\n+            rightIterator = Collections.emptyIterator();\n+        else {\n+            rightSide = joinOperation.getRightOperation();\n+            rightSide.reset();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 382}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk1MzU1OQ==", "bodyText": "I'm wondering how the MultiRowRangerFilter works with the start and stop key in the following example:\ncreate table t2 (a2 int, b2 int, c2 int, primary key (a2,b2,c2));\ninsert into t1 values (1,6,6), (5,5,5);\ninsert into t2 values (1,6,6), (5,5,5);\ncall syscs_util.syscs_set_global_database_property('splice.execution.newMergeJoin', 'forced');\nselect * from t1 left join t2 --splice-properties useSpark=true\non a1=a2 and b1=b2 and c1=c2 and a2>=5;\n\nIn this example, the right table t2 has a start key of (5), and from the left table, we also get two ranges [(1,6,6), (1,6,6)) and [(5,5,5),(5,5,5)). The above code seems to update the start row to the start point of the first range, that is (1,6,6). Is this expected? The result returned looks correct, so somewhere we must have logic to honor the original start key of (5) which is more tight than (1,6,6). Could you point me to where the logic is?", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508953559", "createdAt": "2020-10-21T02:37:55Z", "author": {"login": "yxia92"}, "path": "hbase_storage/src/main/java/com/splicemachine/storage/HScan.java", "diffHunk": "@@ -43,6 +48,57 @@ public boolean isDescendingScan(){\n         return scan.isReversed();\n     }\n \n+    /**\n+     * Take a list of [startKey, stopKey) rowkey pairs, where the stopKey is excluded,\n+     * convert it to the corresponding {@link MultiRowRangeFilter} and attach\n+     * it to the {@link Scan} held in this {@link HScan} as a {@link Filter}.\n+     *\n+     * @param rowkeyPairs the list of [startKey, stopKey) pairs to convert.\n+     *\n+     * @return if this {@link HScan} currently has no filter, build a new\n+     * {@link MultiRowRangeFilter} out of {@code rowkeyPairs} and attach it\n+     * as a filter.  If this {@link HScan} already has a {@link MultiRowRangeFilter},\n+     * replace it with a new MultiRowRangeFilter built from {@code rowkeyPairs}.\n+     * If {@code rowkeyPairs} has no elements, do not build a filter.\n+     *\n+     * @throws IOException\n+     *\n+     * @notes A possible future enhancement is, instead of replacing an old\n+     * MultiRowRangeFilter with a new one, build the intersection of the\n+     * old filter and the new one.\n+     *\n+     * @see     Scan\n+     */\n+    @Override\n+    public void addRowkeyRangesFilter(List<Pair<byte[],byte[]>> rowkeyPairs) throws IOException {\n+        if (rowkeyPairs == null || rowkeyPairs.size() < 1) {\n+            return;\n+        }\n+        Filter currentFilter = scan.getFilter();\n+        if (currentFilter != null && !(currentFilter instanceof MultiRowRangeFilter))\n+            return;\n+\n+        List<MultiRowRangeFilter.RowRange> ranges = new ArrayList<>(rowkeyPairs.size());\n+        byte[] startKey;\n+        byte[] stopKey;\n+        for (Pair<byte[],byte[]> startStopKey: rowkeyPairs) {\n+            startKey=startStopKey.getFirst();\n+            stopKey=startStopKey.getSecond();\n+            MultiRowRangeFilter.RowRange rr =\n+            new MultiRowRangeFilter.RowRange(startKey, true,\n+                                             stopKey, false);\n+            ranges.add(rr);\n+        }\n+\n+        if (ranges.size() > 0) {\n+            MultiRowRangeFilter filter = new MultiRowRangeFilter(ranges);\n+            scan.setFilter(filter);\n+            List<MultiRowRangeFilter.RowRange> sortedRanges = filter.getRowRanges();\n+            scan.withStartRow(sortedRanges.get(0).getStartRow());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4MjE2OA==", "bodyText": "Why stopKey should be exclusive? If it is a point range, shouldn't the stopKey also be inclusive?", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508982168", "createdAt": "2020-10-21T04:28:14Z", "author": {"login": "yxia92"}, "path": "hbase_storage/src/main/java/com/splicemachine/storage/HScan.java", "diffHunk": "@@ -43,6 +48,57 @@ public boolean isDescendingScan(){\n         return scan.isReversed();\n     }\n \n+    /**\n+     * Take a list of [startKey, stopKey) rowkey pairs, where the stopKey is excluded,\n+     * convert it to the corresponding {@link MultiRowRangeFilter} and attach\n+     * it to the {@link Scan} held in this {@link HScan} as a {@link Filter}.\n+     *\n+     * @param rowkeyPairs the list of [startKey, stopKey) pairs to convert.\n+     *\n+     * @return if this {@link HScan} currently has no filter, build a new\n+     * {@link MultiRowRangeFilter} out of {@code rowkeyPairs} and attach it\n+     * as a filter.  If this {@link HScan} already has a {@link MultiRowRangeFilter},\n+     * replace it with a new MultiRowRangeFilter built from {@code rowkeyPairs}.\n+     * If {@code rowkeyPairs} has no elements, do not build a filter.\n+     *\n+     * @throws IOException\n+     *\n+     * @notes A possible future enhancement is, instead of replacing an old\n+     * MultiRowRangeFilter with a new one, build the intersection of the\n+     * old filter and the new one.\n+     *\n+     * @see     Scan\n+     */\n+    @Override\n+    public void addRowkeyRangesFilter(List<Pair<byte[],byte[]>> rowkeyPairs) throws IOException {\n+        if (rowkeyPairs == null || rowkeyPairs.size() < 1) {\n+            return;\n+        }\n+        Filter currentFilter = scan.getFilter();\n+        if (currentFilter != null && !(currentFilter instanceof MultiRowRangeFilter))\n+            return;\n+\n+        List<MultiRowRangeFilter.RowRange> ranges = new ArrayList<>(rowkeyPairs.size());\n+        byte[] startKey;\n+        byte[] stopKey;\n+        for (Pair<byte[],byte[]> startStopKey: rowkeyPairs) {\n+            startKey=startStopKey.getFirst();\n+            stopKey=startStopKey.getSecond();\n+            MultiRowRangeFilter.RowRange rr =\n+            new MultiRowRangeFilter.RowRange(startKey, true,\n+                                             stopKey, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk5MzU1OA==", "bodyText": "It seems that MultiRowRangeFilter.RowRange.isAscendingOrder() always returns true, so I assume the range is always in ascending order. Would it be a problem if the rows in the conglomerate are actually in descending order, for example, index with key in descending order?", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508993558", "createdAt": "2020-10-21T05:11:39Z", "author": {"login": "yxia92"}, "path": "hbase_sql/src/main/java/com/splicemachine/mrio/api/core/AbstractSMInputFormat.java", "diffHunk": "@@ -107,6 +108,100 @@\n         return null;\n     }\n \n+    /**\n+     * Given a Scan with a MultiRangeRowFilter, and a list of InputSplits,\n+     * generate a new list of InputSplits that only contain the splits\n+     * required to read the rowkey ranges included in the MultiRangeRowFilter.\n+     *\n+     * @param scan describes the attributes of a read from an HBase table.\n+     * @param splits the list of {@link InputSplit InputSplits} to prune.\n+     * @return if {@code scan} contains a filter of class {@link MultiRowRangeFilter},\n+     * a pruned list of {@link org.apache.hadoop.mapreduce.InputSplit InputSplits}\n+     * which have rowkey range overlap with at least one rowkey range in the\n+     * {@link MultiRowRangeFilter}.  If no {@link MultiRowRangeFilter} exists,\n+     * the original list of splits is returned.\n+     *\n+     * @see     Scan\n+     */\n+     private List<InputSplit> pruneFilteredInputSplits(Scan scan, List<InputSplit> splits) {\n+        // If no row range filter, we need to read all InputSplits.\n+        if (! (scan.getFilter() instanceof MultiRowRangeFilter))\n+            return splits;\n+\n+        List<InputSplit> newList = new ArrayList<>();\n+\n+        MultiRowRangeFilter rangeFilter = (MultiRowRangeFilter)scan.getFilter();\n+\n+        List<MultiRowRangeFilter.RowRange> ranges = rangeFilter.getRowRanges();\n+\n+        for(InputSplit split:splits) {\n+            if (!(split instanceof SMSplit))\n+                return splits;\n+\n+            TableSplit tableSplit = ((SMSplit)split).getSplit();\n+\n+            byte[] regionStartKey = tableSplit.getStartRow();\n+            byte[] regionStopKey = tableSplit.getEndRow();\n+\n+            MultiRowRangeFilter.RowRange startRange =\n+              new MultiRowRangeFilter.RowRange(regionStartKey, true,\n+                                               regionStartKey, true);\n+            MultiRowRangeFilter.RowRange stopRange =\n+              new MultiRowRangeFilter.RowRange(regionStopKey, true,\n+                                               regionStopKey, true);\n+\n+            int startKeyIndex = Collections.binarySearch(ranges, startRange);\n+            // If we find the exact key, we know it's included.\n+            if (startKeyIndex >= 0) {\n+                newList.add(split);\n+                continue;\n+            }\n+            else if (regionStartKey.length == 0) {\n+                // This region has no begin limit, so it is considered to\n+                // be larger than all values.\n+\n+                // If there is no end limit, the region covers all ranges.\n+                // If the first range's start value is less than or equal to\n+                // the region's stop key, there is overlap.\n+                // The region's stop key is not inclusive, but we'll treat\n+                // it as such just to be safe.\n+                if (regionStopKey.length == 0 || ranges.get(0).compareTo(stopRange) <= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAwMDM1OA==", "bodyText": "Could you elaborate the relational behind it? For the example that you gave, if we have one row in the outer (left) table, and a million rows in the inner table, and a join condition of outer.col1 = inner.col1, I think that the inner table's scan selectivity would be min((the left table's cardinality*right table's rows per value)/rightRowCount, 1), that is, the left row has a match for  right table rows with the same value.", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509000358", "createdAt": "2020-10-21T05:34:52Z", "author": {"login": "yxia92"}, "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/BinaryRelationalOperatorNode.java", "diffHunk": "@@ -1628,6 +1629,18 @@ public double scanSelectivity(Optimizable innerTable) throws StandardException {\n             selectivity *= innerTableCostController.getSelectivity(innerColumn.getSource().getColumnPosition(),\n                     startKey, true, endKey, true, false);\n         }\n+        else if (this.operatorType == EQUALS_RELOP) {\n+            // Use a more realistic selectivity that takes the\n+            // inner table RPV into account instead of defaulting\n+            // to a selectivity of 1.\n+            double innerCardinality =\n+                        innerTableCostController.cardinality(innerColumnPos);\n+            double outerRowCount = outerTableCostController.getEstimatedRowCount();\n+\n+            double tempSelectivity = outerRowCount / innerCardinality;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAxMjQ4MA==", "bodyText": "The size of columnOrdering array is different for unique and non-unique index(See DB-10351). For non-unique index, the size is number of index columns + 1(which represents the rowid column), for unique index, the size is the number of index columns, should we keep it consistent here too? ascDescInfo's size is equal to the number of index columns.", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509012480", "createdAt": "2020-10-21T06:10:47Z", "author": {"login": "yxia92"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/store/access/btree/IndexConglomerate.java", "diffHunk": "@@ -525,6 +525,16 @@ private void localReadExternal(ObjectInput in) throws IOException, ClassNotFound\n         }\n         int len=in.readInt();\n         columnOrdering=ConglomerateUtil.readFormatIdArray(len,in);\n+\n+        // DataDictionaryImpl.bootstrapOneIndex creates system indexes with a null\n+        // column ordering, making the IndexConglomerate inconsistent, which may\n+        // lead to broken logic in places that call ScanOperation.getColumnOrdering.\n+        // Fill in the missing information here so the index may be properly used.\n+        if (columnOrdering == null || columnOrdering.length == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA0Njk5Mg==", "bodyText": "I'm concerned about the impact of the call for getNumSplits() for OLTP queries. Now that we've separated the costing for OLTP and OLAP in two passes, I think we can let the OLTP also goes through the IF path here, that is, parallelism =1. We can trigger the computation of the number of splits only in the pass of OLAP costing.", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509046992", "createdAt": "2020-10-21T07:25:02Z", "author": {"login": "yxia92"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/stats/StoreCostControllerImpl.java", "diffHunk": "@@ -183,6 +191,18 @@ public StoreCostControllerImpl(TableDescriptor td, ConglomerateDescriptor conglo\n             tableStatistics = new TableStatisticsImpl(tableId, partitionStats,fallbackNullFraction,extraQualifierMultiplier);\n             useRealTableStatistics = true;\n         }\n+\n+        long tableSize = tableStatistics.rowCount() * tableStatistics.avgRowWidth();\n+        if (isMemPlatform())\n+            parallelism = 1;\n+        else {\n+            if (requestedSplits > 0)\n+                parallelism = requestedSplits;\n+            else\n+                parallelism = EngineDriver.getNumSplits(tableSize, getNumPartitions());\n+            if (parallelism > getMaxExecutorCores())\n+                parallelism = getMaxExecutorCores();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a"}, "originalPosition": 42}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ac753d8d057b30a9a4e0fc6ba29aa75f3a220d9", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/1ac753d8d057b30a9a4e0fc6ba29aa75f3a220d9", "committedDate": "2020-10-22T04:10:35Z", "message": "SPLICE-2289 Address review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "883194ccd6bed04bd154f85976e38170adbf39cd", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/883194ccd6bed04bd154f85976e38170adbf39cd", "committedDate": "2020-10-22T04:21:15Z", "message": "Merge branch 'master' into SPLICE-2289_main"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd16b72c945cda2ff4bb7d09fed9f7664e5f0add", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/dd16b72c945cda2ff4bb7d09fed9f7664e5f0add", "committedDate": "2020-10-22T15:15:57Z", "message": "SPLICE-2289 Fix setting of startKey in addRowkeyRangesFilter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d71e116b7a29f5009441b76ed2ae3a5d7c0cecb", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/8d71e116b7a29f5009441b76ed2ae3a5d7c0cecb", "committedDate": "2020-10-23T05:00:59Z", "message": "SPLICE-2289 Fix MultiProbeScan error on cross join or broadcast join."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1MzY1ODk2", "url": "https://github.com/splicemachine/spliceengine/pull/4243#pullrequestreview-515365896", "createdAt": "2020-10-23T06:00:55Z", "commit": {"oid": "8d71e116b7a29f5009441b76ed2ae3a5d7c0cecb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67b4278adf3cd7ec477b9a0c12f37e28a294c4fa", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/67b4278adf3cd7ec477b9a0c12f37e28a294c4fa", "committedDate": "2020-10-23T06:20:49Z", "message": "SPLICE-2289 Fix scannedBaseTableRows estimation (use OLTP execution when appropriate)."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "09a3b5177605ce0160936e40da401d7d9e9c5cde", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/09a3b5177605ce0160936e40da401d7d9e9c5cde", "committedDate": "2020-10-23T08:12:12Z", "message": "SPLICE-2289 Fix assertion error getting numSparkNodes from Zookeeper."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "096dbcdf051a8f04ed2955c3ad015c9f865c6b82", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/096dbcdf051a8f04ed2955c3ad015c9f865c6b82", "committedDate": "2020-10-24T21:09:08Z", "message": "SPLICE-2289 Add splice.optimizer.disablePerParallelTaskJoinCosting database property."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "238e65157c3c1e97a8bd19bd00c82c171378b64a", "author": {"user": {"login": "msirek", "name": "Mark Sirek"}}, "url": "https://github.com/splicemachine/spliceengine/commit/238e65157c3c1e97a8bd19bd00c82c171378b64a", "committedDate": "2020-10-27T00:14:17Z", "message": "SPLICE-2289 Fix DB-10571 test case."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3MzQwMTgz", "url": "https://github.com/splicemachine/spliceengine/pull/4243#pullrequestreview-517340183", "createdAt": "2020-10-27T04:28:18Z", "commit": {"oid": "238e65157c3c1e97a8bd19bd00c82c171378b64a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1149, "cost": 1, "resetAt": "2021-11-02T10:47:05Z"}}}