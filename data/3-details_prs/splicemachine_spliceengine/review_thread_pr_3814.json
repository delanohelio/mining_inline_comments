{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4Mjc3NDQx", "number": 3814, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwOToyODo1M1rOEPdw6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMDoxMTowN1rOERxR0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NjUxNzUzOnYy", "diffSide": "LEFT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwOToyODo1NFrOGzMKrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxMzoyNzowOVrOGzS9CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMyOTkwMg==", "bodyText": "So if I understand correctly, you removed the redundant check for directory existence and you move it to exception handling logic during the actual read, right?", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456329902", "createdAt": "2020-07-17T09:28:54Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -343,23 +343,17 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                           double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n+            ExternalTableUtils.preSortColumns(tableSchema.fields(), partitionColumnMap);\n \n             try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a7837e608f6d4085b8a9735fff2bd287bf698f1"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzMzU2OA==", "bodyText": "yes. spark is anyhow checking first if the directory exists. so if the directory would exist, the previous version would do this check twice, which is costing us some performance on EVERY read (depending on ping ~300ms). of course we want to be good with error messages, so in the new version, if the query fails, we can do the check again to have a good error output.", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456333568", "createdAt": "2020-07-17T09:36:01Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -343,23 +343,17 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                           double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n+            ExternalTableUtils.preSortColumns(tableSchema.fields(), partitionColumnMap);\n \n             try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMyOTkwMg=="}, "originalCommit": {"oid": "6a7837e608f6d4085b8a9735fff2bd287bf698f1"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQ0MTA5Ng==", "bodyText": "That's awesome!", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456441096", "createdAt": "2020-07-17T13:27:09Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -343,23 +343,17 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                           double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n+            ExternalTableUtils.preSortColumns(tableSchema.fields(), partitionColumnMap);\n \n             try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMyOTkwMg=="}, "originalCommit": {"oid": "6a7837e608f6d4085b8a9735fff2bd287bf698f1"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0NjUyMDgzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwOToyOTo0OFrOGzMMtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwOTo0MTowNlrOGzMidw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzMDQyMw==", "bodyText": "uhm ... do you want to keep this comment? :)", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456330423", "createdAt": "2020-07-17T09:29:48Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +377,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a7837e608f6d4085b8a9735fff2bd287bf698f1"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzNTk5MQ==", "bodyText": "It seems like Dates are not correctly handled by the AVRO Dataframe and returned as something else. We need to document this behavior, have a function doing this.", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456335991", "createdAt": "2020-07-17T09:41:06Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +377,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzMDQyMw=="}, "originalCommit": {"oid": "6a7837e608f6d4085b8a9735fff2bd287bf698f1"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MDY4NjI1OnYy", "diffSide": "RIGHT", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMDoxMTowN1rOG2qIUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMDoyMTo0MlrOG2qYhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2NjU0Nw==", "bodyText": "Is supportAvroDateTypeColumns() function effectively deleted? And, it seems castDateTypeInAvroDataSet() is used only once. Is it expected to be used in other places, too?", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r459966547", "createdAt": "2020-07-24T10:11:07Z", "author": {"login": "ascend1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java", "diffHunk": "@@ -64,15 +66,18 @@ public static StructType supportAvroDateType(StructType schema, String storedAs)\n         return schema;\n     }\n \n-    public static void supportAvroDateTypeColumns(ExecRow execRow) throws StandardException {\n-        for(int i=0; i < execRow.size(); i++){\n-            if (execRow.getColumn(i + 1).getTypeName().equals(\"DATE\")) {\n-                execRow.setColumn(i + 1, new SQLVarchar());\n+    public static Dataset<Row> castDateTypeInAvroDataSet(Dataset<Row> dataset, StructType tableSchema) {\n+        int i = 0;\n+        for (StructField sf : tableSchema.fields()) {\n+            if (sf.dataType().sameType(DataTypes.DateType)) {\n+                String colName = dataset.schema().fields()[i].name();\n+                dataset = dataset.withColumn(colName, dataset.col(colName).cast(DataTypes.DateType));\n             }\n+            i++;\n         }\n+        return dataset;\n     }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93aa90215de7944604ae5795cda632d14cdd6f97"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2OTY3Nw==", "bodyText": "yes supportAvroDateTypeColumns wasn't used anywhere, so i deleted it. yes castDateTypeInAvroDataSet is only used in one place, but i put it next to supportAvroDateType, because these two functions are related. That's why it's in ExternalTableUtils.", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r459969677", "createdAt": "2020-07-24T10:19:07Z", "author": {"login": "martinrupp"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java", "diffHunk": "@@ -64,15 +66,18 @@ public static StructType supportAvroDateType(StructType schema, String storedAs)\n         return schema;\n     }\n \n-    public static void supportAvroDateTypeColumns(ExecRow execRow) throws StandardException {\n-        for(int i=0; i < execRow.size(); i++){\n-            if (execRow.getColumn(i + 1).getTypeName().equals(\"DATE\")) {\n-                execRow.setColumn(i + 1, new SQLVarchar());\n+    public static Dataset<Row> castDateTypeInAvroDataSet(Dataset<Row> dataset, StructType tableSchema) {\n+        int i = 0;\n+        for (StructField sf : tableSchema.fields()) {\n+            if (sf.dataType().sameType(DataTypes.DateType)) {\n+                String colName = dataset.schema().fields()[i].name();\n+                dataset = dataset.withColumn(colName, dataset.col(colName).cast(DataTypes.DateType));\n             }\n+            i++;\n         }\n+        return dataset;\n     }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2NjU0Nw=="}, "originalCommit": {"oid": "93aa90215de7944604ae5795cda632d14cdd6f97"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk3MDY5Mw==", "bodyText": "OK, that makes sense. Thanks!", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r459970693", "createdAt": "2020-07-24T10:21:42Z", "author": {"login": "ascend1"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java", "diffHunk": "@@ -64,15 +66,18 @@ public static StructType supportAvroDateType(StructType schema, String storedAs)\n         return schema;\n     }\n \n-    public static void supportAvroDateTypeColumns(ExecRow execRow) throws StandardException {\n-        for(int i=0; i < execRow.size(); i++){\n-            if (execRow.getColumn(i + 1).getTypeName().equals(\"DATE\")) {\n-                execRow.setColumn(i + 1, new SQLVarchar());\n+    public static Dataset<Row> castDateTypeInAvroDataSet(Dataset<Row> dataset, StructType tableSchema) {\n+        int i = 0;\n+        for (StructField sf : tableSchema.fields()) {\n+            if (sf.dataType().sameType(DataTypes.DateType)) {\n+                String colName = dataset.schema().fields()[i].name();\n+                dataset = dataset.withColumn(colName, dataset.col(colName).cast(DataTypes.DateType));\n             }\n+            i++;\n         }\n+        return dataset;\n     }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2NjU0Nw=="}, "originalCommit": {"oid": "93aa90215de7944604ae5795cda632d14cdd6f97"}, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3093, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}