{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2OTU1ODk5", "number": 3463, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDowNjozOVrOD6Q-7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzozNjowNFrOD8EaZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNDIyMjU0OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDowNjozOVrOGSAQCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDowNjozOVrOGSAQCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzMTY1OQ==", "bodyText": "re-add SpliceSpark.popScope()\n(this function does SpliceSpark.pushScope , but SpliceSpark.popScope(); was moved (unintentionally i guess) to line 204))", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421531659", "createdAt": "2020-05-07T14:06:39Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNDIyNDAzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDowNjo1NlrOGSAQ-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyMTowM1rOGURDXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzMTg5Ng==", "bodyText": "remove popScope", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421531896", "createdAt": "2020-05-07T14:06:56Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;\n+            }\n+            count++;\n+        }\n         messages.add(0, String.format(\"The following %d indexes are duplicates:\", count));\n         SpliceSpark.popScope();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwNDA5NQ==", "bodyText": "Good catch!", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r423904095", "createdAt": "2020-05-12T17:21:03Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;\n+            }\n+            count++;\n+        }\n         messages.add(0, String.format(\"The following %d indexes are duplicates:\", count));\n         SpliceSpark.popScope();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzMTg5Ng=="}, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNDI0MzI2OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDoxMTowN1rOGSAdRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzoyMjo0NFrOGURHcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNTA0Ng==", "bodyText": "the only difference i see between these two functions is\nif(count >= maxCheckTableError) . I don't see how fixDuplicateIndexes fixes anything.\nIn general, we could do\nprivate List<String> reportDuplicateIndexes(Iterator it, int _maxCheckTableError)\nthen have\nif (_maxCheckTableError >= 0 || count >= _maxCheckTableError)\nand then do\nreturn reportDuplicateIndexes(it, fix ? -1 : maxCheckTableError);", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421535046", "createdAt": "2020-05-07T14:11:07Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNzI5MA==", "bodyText": "i think fixDuplicateIndexes is missing the lines https://github.com/splicemachine/spliceengine/pull/3463/files#diff-49eff1a8a3acfe993feb4e304ff615ceL161-L167 , no?", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421537290", "createdAt": "2020-05-07T14:14:08Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNTA0Ng=="}, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkwNTEzNg==", "bodyText": "Depending on whether fix is true\nJavaRDD duplicateIndex = joinedRdd\n.mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\nDeleteDuplicateIndexFunction deletes (or ignores)  extra indexes", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r423905136", "createdAt": "2020-05-12T17:22:44Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNTA0Ng=="}, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MTI3MjQxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QwMjo1MTowOFrOGUfrkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNToyMToyNlrOGU23mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0Mzc2MA==", "bodyText": "By aborting here, we won't be able to know the actual number of duplicate indexes, is this intended? Before the change, we can get the the actual number of duplicate indexes.", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424143760", "createdAt": "2020-05-13T02:51:08Z", "author": {"login": "yxia92"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUyMzY3Mg==", "bodyText": "This was intentional to avoid counting exact number. However, my second thought is that it is rarely a very expensive operations, so I will revert back", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424523672", "createdAt": "2020-05-13T15:21:26Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0Mzc2MA=="}, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MTMzMTYzOnYy", "diffSide": "RIGHT", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlTableChecker.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QwMzozMTo1NlrOGUgP_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNToxNjo1NlrOGU2qdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE1MzA4NQ==", "bodyText": "We seem to assume that if there are duplicate index rows, the extra ones must have different values than the base row. Is it possible that the extra index row also has the same values as the base row?", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424153085", "createdAt": "2020-05-13T03:31:56Z", "author": {"login": "yxia92"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlTableChecker.java", "diffHunk": "@@ -134,41 +136,100 @@ private void populateData(PairDataSet table, PairDataSet index) {\n \n         indexData = ArrayListMultimap.create();\n         indexCount = 0;\n-        Iterator<Tuple2<String, byte[]>> indexSource = ((ControlPairDataSet)index).source;\n+        Iterator<Tuple2<String, Tuple2<byte[], ExecRow>>> indexSource = ((ControlPairDataSet)index).source;\n         while(indexSource.hasNext()) {\n             indexCount++;\n-            Tuple2<String, byte[]> t = indexSource.next();\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = indexSource.next();\n             String baseRowId = t._1;\n-            byte[] indexKey = t._2;\n-            indexData.put(baseRowId, indexKey);\n+            Tuple2<byte[], ExecRow> row = t._2;\n+            indexData.put(baseRowId, row);\n         }\n     }\n \n     private List<String> checkDuplicateIndexes() throws StandardException {\n-        List<String> messages = new LinkedList<>();\n-        ArrayListMultimap<String, byte[]> result = ArrayListMultimap.create();\n+        ArrayListMultimap<String, Tuple2<byte[], ExecRow>> result = ArrayListMultimap.create();\n         long duplicateIndexCount = 0;\n         for (String baseRowId : indexData.keySet()) {\n-            List<byte[]> rows = indexData.get(baseRowId);\n+            List<Tuple2<byte[], ExecRow>> rows = indexData.get(baseRowId);\n             if (rows.size() > 1) {\n-                duplicateIndexCount += rows.size();\n+                duplicateIndexCount += rows.size()-1;\n                 result.putAll(baseRowId, rows);\n             }\n         }\n-        int i = 0;\n         if (duplicateIndexCount > 0) {\n-            messages.add(String.format(\"The following %d indexes are duplicates:\", duplicateIndexCount));\n+            if (fix) {\n+                fixDuplicateIndexes(result);\n+            }\n+            return reportDuplicateIndexes(result, duplicateIndexCount);\n+        }\n+        return new LinkedList<>();\n+    }\n+\n+    private void fixDuplicateIndexes(ArrayListMultimap<String, Tuple2<byte[], ExecRow>> result) throws StandardException {\n+        try {\n+            WriteCoordinator writeCoordinator = PipelineDriver.driver().writeCoordinator();\n+            WriteConfiguration writeConfiguration = writeCoordinator.defaultWriteConfiguration();\n+            Partition indexPartition = SIDriver.driver().getTableFactory().getTable(Long.toString(conglomerate));\n+            RecordingCallBuffer<KVPair> writeBuffer = writeCoordinator.writeBuffer(indexPartition, txn, null, writeConfiguration);\n+\n+            List<Integer> indexToMain = tentativeIndex.getIndex().getIndexColsToMainColMapList();\n             for (String baseRowId : result.keySet()) {\n-                List<byte[]> indexKeys = result.get(baseRowId);\n-                for (byte[] indexKey : indexKeys) {\n-                    if (i >= maxCheckTableErrors) {\n+                ExecRow baseRow = tableData.get(baseRowId);\n+                List<Tuple2<byte[], ExecRow>> indexRows = result.get(baseRowId);\n+                for (Tuple2<byte[], ExecRow> indexRow : indexRows) {\n+                    boolean duplicate = false;\n+                    DataValueDescriptor[] dvds = indexRow._2.getRowArray();\n+                    for (int i = 0; i < dvds.length - 1; ++i) {\n+                        int col = baseColumnMap[indexToMain.get(i) - 1];\n+                        if (!dvds[i].equals(baseRow.getColumn(col + 1))) {\n+                            duplicate = true;\n+                            break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUyMDMwOQ==", "bodyText": "The index that has some column values as base row is the correct one and should be kept.", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424520309", "createdAt": "2020-05-13T15:16:56Z", "author": {"login": "jyuanca"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlTableChecker.java", "diffHunk": "@@ -134,41 +136,100 @@ private void populateData(PairDataSet table, PairDataSet index) {\n \n         indexData = ArrayListMultimap.create();\n         indexCount = 0;\n-        Iterator<Tuple2<String, byte[]>> indexSource = ((ControlPairDataSet)index).source;\n+        Iterator<Tuple2<String, Tuple2<byte[], ExecRow>>> indexSource = ((ControlPairDataSet)index).source;\n         while(indexSource.hasNext()) {\n             indexCount++;\n-            Tuple2<String, byte[]> t = indexSource.next();\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = indexSource.next();\n             String baseRowId = t._1;\n-            byte[] indexKey = t._2;\n-            indexData.put(baseRowId, indexKey);\n+            Tuple2<byte[], ExecRow> row = t._2;\n+            indexData.put(baseRowId, row);\n         }\n     }\n \n     private List<String> checkDuplicateIndexes() throws StandardException {\n-        List<String> messages = new LinkedList<>();\n-        ArrayListMultimap<String, byte[]> result = ArrayListMultimap.create();\n+        ArrayListMultimap<String, Tuple2<byte[], ExecRow>> result = ArrayListMultimap.create();\n         long duplicateIndexCount = 0;\n         for (String baseRowId : indexData.keySet()) {\n-            List<byte[]> rows = indexData.get(baseRowId);\n+            List<Tuple2<byte[], ExecRow>> rows = indexData.get(baseRowId);\n             if (rows.size() > 1) {\n-                duplicateIndexCount += rows.size();\n+                duplicateIndexCount += rows.size()-1;\n                 result.putAll(baseRowId, rows);\n             }\n         }\n-        int i = 0;\n         if (duplicateIndexCount > 0) {\n-            messages.add(String.format(\"The following %d indexes are duplicates:\", duplicateIndexCount));\n+            if (fix) {\n+                fixDuplicateIndexes(result);\n+            }\n+            return reportDuplicateIndexes(result, duplicateIndexCount);\n+        }\n+        return new LinkedList<>();\n+    }\n+\n+    private void fixDuplicateIndexes(ArrayListMultimap<String, Tuple2<byte[], ExecRow>> result) throws StandardException {\n+        try {\n+            WriteCoordinator writeCoordinator = PipelineDriver.driver().writeCoordinator();\n+            WriteConfiguration writeConfiguration = writeCoordinator.defaultWriteConfiguration();\n+            Partition indexPartition = SIDriver.driver().getTableFactory().getTable(Long.toString(conglomerate));\n+            RecordingCallBuffer<KVPair> writeBuffer = writeCoordinator.writeBuffer(indexPartition, txn, null, writeConfiguration);\n+\n+            List<Integer> indexToMain = tentativeIndex.getIndex().getIndexColsToMainColMapList();\n             for (String baseRowId : result.keySet()) {\n-                List<byte[]> indexKeys = result.get(baseRowId);\n-                for (byte[] indexKey : indexKeys) {\n-                    if (i >= maxCheckTableErrors) {\n+                ExecRow baseRow = tableData.get(baseRowId);\n+                List<Tuple2<byte[], ExecRow>> indexRows = result.get(baseRowId);\n+                for (Tuple2<byte[], ExecRow> indexRow : indexRows) {\n+                    boolean duplicate = false;\n+                    DataValueDescriptor[] dvds = indexRow._2.getRowArray();\n+                    for (int i = 0; i < dvds.length - 1; ++i) {\n+                        int col = baseColumnMap[indexToMain.get(i) - 1];\n+                        if (!dvds[i].equals(baseRow.getColumn(col + 1))) {\n+                            duplicate = true;\n+                            break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE1MzA4NQ=="}, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MzEzNDQ1OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzozNjowNFrOGUx60Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNjowNDo1NlrOGU426A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ0MjU3Nw==", "bodyText": "i still don't understand how this function is fixing any duplicate indices.\nit does pretty much the same as reportDuplicateIndexes when maxCheckTableError = infinity (well, except for the \"Deleted the following %d duplicate indexes\" / \"The following %d indexes are duplicates\" ).", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424442577", "createdAt": "2020-05-13T13:36:04Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUyNDg5NQ==", "bodyText": "The index is fixed by DeleteDuplicateIndexFunction. This function reports fixed indexes.", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424524895", "createdAt": "2020-05-13T15:23:02Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ0MjU3Nw=="}, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU1NjI2NA==", "bodyText": "Let me restructure the code around here so not to cause confusion", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424556264", "createdAt": "2020-05-13T16:04:56Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ0MjU3Nw=="}, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 132}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3267, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}