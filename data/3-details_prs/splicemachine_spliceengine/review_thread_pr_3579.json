{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5MDEyNjg3", "number": 3579, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMTowMjo1MVrOD9-LZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToxMTozMFrOD9-Vkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MzA4NDU1OnYy", "diffSide": "RIGHT", "path": "splice_spark/src/main/spark2.1/com/splicemachine/spark/splicemachine/SplicemachineContext.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMTowMjo1MVrOGXyhng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDoxMzoyNlrOGX6oNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU5ODIzOA==", "bodyText": "What fields was it complaining about?", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427598238", "createdAt": "2020-05-19T21:02:51Z", "author": {"login": "dgomezferro"}, "path": "splice_spark/src/main/spark2.1/com/splicemachine/spark/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -39,10 +39,16 @@ import org.apache.hadoop.hbase.util.Bytes\n import org.apache.log4j.Logger\n import org.apache.spark.api.java.JavaRDD\n import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings\n \n+@SerialVersionUID(20200513211L)\n object Holder extends Serializable {\n   @transient lazy val log = Logger.getLogger(getClass.getName)\n }\n+@SerialVersionUID(20200513212L)\n+@SuppressFBWarnings(value = Array(\"ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD\"), justification = \"Need to set SpliceClient.connectionString\")\n+@SuppressFBWarnings(value = Array(\"NP_ALWAYS_NULL\"), justification = \"These fields usually are not null\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12bad63aaa86c6286c8b5bee849482c4807d4904"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczMDk5Nw==", "bodyText": "Wrote https://splicemachine.atlassian.net/browse/DB-9580 and listed the issues in the jira.", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427730997", "createdAt": "2020-05-20T04:13:26Z", "author": {"login": "jpanko1"}, "path": "splice_spark/src/main/spark2.1/com/splicemachine/spark/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -39,10 +39,16 @@ import org.apache.hadoop.hbase.util.Bytes\n import org.apache.log4j.Logger\n import org.apache.spark.api.java.JavaRDD\n import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings\n \n+@SerialVersionUID(20200513211L)\n object Holder extends Serializable {\n   @transient lazy val log = Logger.getLogger(getClass.getName)\n }\n+@SerialVersionUID(20200513212L)\n+@SuppressFBWarnings(value = Array(\"ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD\"), justification = \"Need to set SpliceClient.connectionString\")\n+@SuppressFBWarnings(value = Array(\"NP_ALWAYS_NULL\"), justification = \"These fields usually are not null\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU5ODIzOA=="}, "originalCommit": {"oid": "12bad63aaa86c6286c8b5bee849482c4807d4904"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MzEwNzEyOnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToxMDozNlrOGXywQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNDoxNDoxNFrOGX6pAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwMTk4Ng==", "bodyText": "Does this work for quoted columns?\ncreate table a (\"notEQUAL\" int, \"NOTequal\" int);", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427601986", "createdAt": "2020-05-19T21:10:36Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -595,11 +595,13 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n     )\n   \n   /** Schema string built from JDBC metadata. */\n-  def schemaString(schemaTableName: String): String =\n+  def schemaString(schemaTableName: String, schema: StructType = new StructType()): String =\n     SpliceJDBCUtil.retrieveColumnInfo(\n       new JDBCOptions(Map(\n         JDBCOptions.JDBC_URL -> url,\n         JDBCOptions.JDBC_TABLE_NAME -> schemaTableName))\n+    ).filter(\n+      col => schema.isEmpty || schema.exists( field => field.name.toUpperCase.equals( col(0).toUpperCase ) )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bb04d980f4655e8b09171a73699d5323c7538cb"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYyNzEzNQ==", "bodyText": "Verified now that it works for unquoted columns but not quoted columns.  I\u2019ll file a jira.", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427627135", "createdAt": "2020-05-19T22:04:27Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -595,11 +595,13 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n     )\n   \n   /** Schema string built from JDBC metadata. */\n-  def schemaString(schemaTableName: String): String =\n+  def schemaString(schemaTableName: String, schema: StructType = new StructType()): String =\n     SpliceJDBCUtil.retrieveColumnInfo(\n       new JDBCOptions(Map(\n         JDBCOptions.JDBC_URL -> url,\n         JDBCOptions.JDBC_TABLE_NAME -> schemaTableName))\n+    ).filter(\n+      col => schema.isEmpty || schema.exists( field => field.name.toUpperCase.equals( col(0).toUpperCase ) )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwMTk4Ng=="}, "originalCommit": {"oid": "5bb04d980f4655e8b09171a73699d5323c7538cb"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzczMTIwMg==", "bodyText": "Wrote https://splicemachine.atlassian.net/browse/DB-9581", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427731202", "createdAt": "2020-05-20T04:14:14Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -595,11 +595,13 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n     )\n   \n   /** Schema string built from JDBC metadata. */\n-  def schemaString(schemaTableName: String): String =\n+  def schemaString(schemaTableName: String, schema: StructType = new StructType()): String =\n     SpliceJDBCUtil.retrieveColumnInfo(\n       new JDBCOptions(Map(\n         JDBCOptions.JDBC_URL -> url,\n         JDBCOptions.JDBC_TABLE_NAME -> schemaTableName))\n+    ).filter(\n+      col => schema.isEmpty || schema.exists( field => field.name.toUpperCase.equals( col(0).toUpperCase ) )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwMTk4Ng=="}, "originalCommit": {"oid": "5bb04d980f4655e8b09171a73699d5323c7538cb"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MzExMDU4OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToxMTozMFrOGXyyUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMzoxODozNFrOGX14xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwMjUxNQ==", "bodyText": "This could force a computation of the RDD, ideally we'd want to make sure the RDD is only computed once.\nWhat's the error if the RDD is empty?", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427602515", "createdAt": "2020-05-19T21:11:30Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -577,7 +577,7 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName table to delete from\n    */\n-  def delete(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = {\n+  def delete(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = if( !rdd.isEmpty ) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5bb04d980f4655e8b09171a73699d5323c7538cb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzY1MzMxNg==", "bodyText": "The delete function (and it's the same for update) calls modifyOnKeys() which passes the rdd to sendData() which would put the data into Kafka, if there was data.  But in this case sendData() returns after having no data to pass, and then the sql is created and it hangs at\nstatement.executeUpdate(sql).  The code in the DB is probably polling Kafka when no data was put on the topic.  It looks like KafkaReadFunction would go into an infinite loop polling Kafka:\n                    ConsumerRecords<Integer, Externalizable> records = null;\n                    while (records == null || records.isEmpty()) {\n                        records = consumer.poll( java.time.Duration.ofMillis(1000) );\n                        if (TaskContext.get().isInterrupted()) {\n                            consumer.close();\n                            throw new TaskKilledException();\n                        }\n                    }", "url": "https://github.com/splicemachine/spliceengine/pull/3579#discussion_r427653316", "createdAt": "2020-05-19T23:18:34Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -577,7 +577,7 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName table to delete from\n    */\n-  def delete(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = {\n+  def delete(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = if( !rdd.isEmpty ) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwMjUxNQ=="}, "originalCommit": {"oid": "5bb04d980f4655e8b09171a73699d5323c7538cb"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3236, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}