{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU2Mjg2NzE4", "number": 3889, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDozODo1M1rOESavKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMjozMzo0MVrOESyiAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzQ3ODgyOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDozODo1M1rOG3lhGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMjozMTowOVrOG5MioQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkzOTU0Ng==", "bodyText": "I am almost sure you don't need col parameter here. If you think you still need it, maybe you could consider splitting this function again such that you don't have as in/out parameter.", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r460939546", "createdAt": "2020-07-27T14:38:53Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -672,6 +671,37 @@ public Boolean isCached(long conglomerateId) throws StandardException {\n \n     }\n \n+    /**\n+     * since spark has only strings, not CHAR/VARCHAR,\n+     * we need to \"create\" CHAR/VARCHAR column out of string columns\n+     * - for CHAR, we need to right-pad strings\n+     * - for CHAR/VARCHAR, we need to make sure we're considering the maximum string length\n+     *   note that we will use Java/Scala String length, which is measured in\n+     *   UTF-16 characters, which is NOT the byte length and also NOT necessarily the character length.\n+     *   e.g. the single character U+1F602 (https://www.fileformat.info/info/unicode/char/1f602/index.htm)\n+     *   is encoded as 0xD83D 0xDE02 and therefore has length 2.\n+     */\n+    private Column convertSparkStringColToCharVarchar(Column col, DataValueDescriptor dvd, String name) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzQ4OQ==", "bodyText": "the column is needed as you create the new column from the old column by transforming it with a function", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r462627489", "createdAt": "2020-07-29T22:31:09Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -672,6 +671,37 @@ public Boolean isCached(long conglomerateId) throws StandardException {\n \n     }\n \n+    /**\n+     * since spark has only strings, not CHAR/VARCHAR,\n+     * we need to \"create\" CHAR/VARCHAR column out of string columns\n+     * - for CHAR, we need to right-pad strings\n+     * - for CHAR/VARCHAR, we need to make sure we're considering the maximum string length\n+     *   note that we will use Java/Scala String length, which is measured in\n+     *   UTF-16 characters, which is NOT the byte length and also NOT necessarily the character length.\n+     *   e.g. the single character U+1F602 (https://www.fileformat.info/info/unicode/char/1f602/index.htm)\n+     *   is encoded as 0xD83D 0xDE02 and therefore has length 2.\n+     */\n+    private Column convertSparkStringColToCharVarchar(Column col, DataValueDescriptor dvd, String name) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDkzOTU0Ng=="}, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzUwNDUzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo0NDoyN1rOG3lw2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMjozMToyOVrOG5MjJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk0MzU3OQ==", "bodyText": "nice! \ud83d\udc4d could be made static I guess?", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r460943579", "createdAt": "2020-07-27T14:44:27Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -810,7 +842,17 @@ private String intArrayToString(int[] ints) {\n         }\n     }\n \n-    private static Column createFilterCondition(Dataset dataset,String[] allColIdInSpark, Qualifier[][] qual_list, int[] baseColumnMap, DataValueDescriptor probeValue) throws StandardException {\n+    /// check that we don't access a column that's not there with baseColumnMap\n+    private void checkNumColumns(String location, int[] baseColumnMap, Dataset<Row> table) throws StandardException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNzYyMg==", "bodyText": "done", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r462627622", "createdAt": "2020-07-29T22:31:29Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -810,7 +842,17 @@ private String intArrayToString(int[] ints) {\n         }\n     }\n \n-    private static Column createFilterCondition(Dataset dataset,String[] allColIdInSpark, Qualifier[][] qual_list, int[] baseColumnMap, DataValueDescriptor probeValue) throws StandardException {\n+    /// check that we don't access a column that's not there with baseColumnMap\n+    private void checkNumColumns(String location, int[] baseColumnMap, Dataset<Row> table) throws StandardException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk0MzU3OQ=="}, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzUxODU5OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo0NzoyNlrOG3l5hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo0NzoyNlrOG3l5hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk0NTc5Nw==", "bodyText": "could be removed since it is not adding any info :)", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r460945797", "createdAt": "2020-07-27T14:47:26Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -151,6 +146,15 @@ public void checkResultSetSelectAll(ResultSet rs) throws SQLException {\n             results.add(sb.toString());\n         }\n         results.sort(String::compareTo);\n+        return results;\n+    }\n+    /**\n+     * compare that result in ResultSet rs is the same as from the generated insert values.\n+     * @param rs ResultSet from a select *.\n+     * @throws SQLException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzUyMTM3OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo0Nzo1N1rOG3l7OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMjozNToyMVrOG5MpRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk0NjIzMg==", "bodyText": "List instead of ArrayList maybe?", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r460946232", "createdAt": "2020-07-27T14:47:57Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -151,6 +146,15 @@ public void checkResultSetSelectAll(ResultSet rs) throws SQLException {\n             results.add(sb.toString());\n         }\n         results.sort(String::compareTo);\n+        return results;\n+    }\n+    /**\n+     * compare that result in ResultSet rs is the same as from the generated insert values.\n+     * @param rs ResultSet from a select *.\n+     * @throws SQLException\n+     */\n+    public void checkResultSetSelectAll(ResultSet rs) throws SQLException {\n+        ArrayList<String> results = getArrayListResult(rs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyOTE4OA==", "bodyText": "done", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r462629188", "createdAt": "2020-07-29T22:35:21Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -151,6 +146,15 @@ public void checkResultSetSelectAll(ResultSet rs) throws SQLException {\n             results.add(sb.toString());\n         }\n         results.sort(String::compareTo);\n+        return results;\n+    }\n+    /**\n+     * compare that result in ResultSet rs is the same as from the generated insert values.\n+     * @param rs ResultSet from a select *.\n+     * @throws SQLException\n+     */\n+    public void checkResultSetSelectAll(ResultSet rs) throws SQLException {\n+        ArrayList<String> results = getArrayListResult(rs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk0NjIzMg=="}, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzU1MTQzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo1NDoxOFrOG3mN4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMjozNTozMFrOG5Mpcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk1MTAxMA==", "bodyText": "ditto", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r460951010", "createdAt": "2020-07-27T14:54:18Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -485,6 +486,45 @@ public void testWriteReadFromSimpleExternalTable() throws Exception {\n \n     }\n \n+    @Test\n+    public void testWriteReadCharVarcharTruncation() throws Exception {\n+        for( String fileFormat : fileFormats ){\n+            // we're using our internal ORC reader which doesn't support CHAR padding or CHAR/VARCHAR truncation\n+            // see DB-9911 for reevalutation of this\n+            if( fileFormat.equals(\"ORC\")) {\n+                continue;\n+            }\n+            String name = \"char_varchar_\" + fileFormat;\n+\n+            String file = getExternalResourceDirectory() + name;\n+            methodWatcher.executeUpdate(\"create external table \" + name +\n+                    \" ( col1 varchar(100), col2 varchar(100), col3 varchar(100) ) \" +\n+                    \"STORED AS \" + fileFormat + \" LOCATION '\" + file + \"'\");\n+\n+            int insertCount = methodWatcher.executeUpdate(\"insert into \" + name + \" values \" +\n+                    \"( '123456789', '123456789', '12345')\");\n+            Assert.assertEquals(\"insertCount is wrong\", 1, insertCount);\n+\n+            ResultSet rs1 = methodWatcher.executeQuery(\"select * from \" + name);\n+            ArrayList<String> res1 = CreateTableTypeHelper.getArrayListResult(rs1);\n+            Assert.assertEquals(1, res1.size());\n+            Assert.assertEquals(\"'123456789', '123456789', '12345'\", res1.get(0));\n+\n+            methodWatcher.execute(\"drop table \" + name );\n+\n+            // create table in same location, but with shorter strings\n+            methodWatcher.executeUpdate(\"create external table \" + name + \" ( col1 VARCHAR(5), col2 CHAR(5), col3 CHAR(10) ) \" +\n+                    \"STORED AS \" + fileFormat + \" LOCATION '\" + file + \"'\");\n+\n+            ResultSet rs2 = methodWatcher.executeQuery(\"select * from \" + name);\n+            ArrayList<String> res2 = CreateTableTypeHelper.getArrayListResult(rs2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyOTIzNQ==", "bodyText": "done", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r462629235", "createdAt": "2020-07-29T22:35:30Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -485,6 +486,45 @@ public void testWriteReadFromSimpleExternalTable() throws Exception {\n \n     }\n \n+    @Test\n+    public void testWriteReadCharVarcharTruncation() throws Exception {\n+        for( String fileFormat : fileFormats ){\n+            // we're using our internal ORC reader which doesn't support CHAR padding or CHAR/VARCHAR truncation\n+            // see DB-9911 for reevalutation of this\n+            if( fileFormat.equals(\"ORC\")) {\n+                continue;\n+            }\n+            String name = \"char_varchar_\" + fileFormat;\n+\n+            String file = getExternalResourceDirectory() + name;\n+            methodWatcher.executeUpdate(\"create external table \" + name +\n+                    \" ( col1 varchar(100), col2 varchar(100), col3 varchar(100) ) \" +\n+                    \"STORED AS \" + fileFormat + \" LOCATION '\" + file + \"'\");\n+\n+            int insertCount = methodWatcher.executeUpdate(\"insert into \" + name + \" values \" +\n+                    \"( '123456789', '123456789', '12345')\");\n+            Assert.assertEquals(\"insertCount is wrong\", 1, insertCount);\n+\n+            ResultSet rs1 = methodWatcher.executeQuery(\"select * from \" + name);\n+            ArrayList<String> res1 = CreateTableTypeHelper.getArrayListResult(rs1);\n+            Assert.assertEquals(1, res1.size());\n+            Assert.assertEquals(\"'123456789', '123456789', '12345'\", res1.get(0));\n+\n+            methodWatcher.execute(\"drop table \" + name );\n+\n+            // create table in same location, but with shorter strings\n+            methodWatcher.executeUpdate(\"create external table \" + name + \" ( col1 VARCHAR(5), col2 CHAR(5), col3 CHAR(10) ) \" +\n+                    \"STORED AS \" + fileFormat + \" LOCATION '\" + file + \"'\");\n+\n+            ResultSet rs2 = methodWatcher.executeQuery(\"select * from \" + name);\n+            ArrayList<String> res2 = CreateTableTypeHelper.getArrayListResult(rs2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk1MTAxMA=="}, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NzU2MjYzOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo1NjozOVrOG3mUtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNDo1NjozOVrOG3mUtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk1Mjc1Ng==", "bodyText": "awesome", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r460952756", "createdAt": "2020-07-27T14:56:39Z", "author": {"login": "hatyo"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableIT.java", "diffHunk": "@@ -941,6 +981,24 @@ public void testReadTextExternalTableDelimiter() throws Exception {\n                 \"  2  |MACHINE |2015-09-02 |\" ,TestUtils.FormattedResult.ResultFactory.toString(rs));\n     }\n \n+    @Test // DB-9682\n+    public void testReadTextMismatchSchema() throws Exception {\n+        String file = getResourceDirectory() + \"test_external_text\";\n+        methodWatcher.executeUpdate(String.format(\"create external table external_t2 (col1 int, col2 varchar(20), col3 date, col4 date)\" +\n+                \"ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' ESCAPED BY '\\\\\\\\' LINES TERMINATED BY '\\\\n'\" +\n+                \" STORED AS TEXTFILE LOCATION '%s'\", file));\n+        try {\n+            methodWatcher.executeQuery(\"select * from external_t2\");\n+            Assert.fail(\"Exception not thrown\");\n+        } catch (SQLException e) {\n+            Assert.assertEquals(\"Wrong Exception (\" + e.getMessage() + \")\", EXTERNAL_TABLES_READ_FAILURE, e.getSQLState());\n+            Assert.assertEquals( \"wrong exception message\",\n+                    \"External Table read failed with exception '4 attribute(s) defined but 3 present \" +\n+                            \"in the external file : '\" + file + \"'. Suggested Schema is 'CREATE EXTERNAL TABLE T \" +\n+                            \"(_c0 CHAR/VARCHAR(x), _c1 CHAR/VARCHAR(x), _c2 CHAR/VARCHAR(x));'.'\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTM3NzI4OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMjozMzo0MVrOG4KYKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxMzoyNzozMVrOHK2PGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU0MzQ2NA==", "bodyText": "is only \">\" condition is error? Should not it be \"!=\" here?", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r461543464", "createdAt": "2020-07-28T12:33:41Z", "author": {"login": "ipraznik-splice"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -810,7 +842,17 @@ private String intArrayToString(int[] ints) {\n         }\n     }\n \n-    private static Column createFilterCondition(Dataset dataset,String[] allColIdInSpark, Qualifier[][] qual_list, int[] baseColumnMap, DataValueDescriptor probeValue) throws StandardException {\n+    /// check that we don't access a column that's not there with baseColumnMap\n+    private void checkNumColumns(String location, int[] baseColumnMap, Dataset<Row> table) throws StandardException {\n+        if( baseColumnMap.length > table.schema().fields().length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyNDg2OQ==", "bodyText": "is enough. That means your File eg CSV has more columns than you specified, which is a case we can handle", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r462624869", "createdAt": "2020-07-29T22:24:27Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -810,7 +842,17 @@ private String intArrayToString(int[] ints) {\n         }\n     }\n \n-    private static Column createFilterCondition(Dataset dataset,String[] allColIdInSpark, Qualifier[][] qual_list, int[] baseColumnMap, DataValueDescriptor probeValue) throws StandardException {\n+    /// check that we don't access a column that's not there with baseColumnMap\n+    private void checkNumColumns(String location, int[] baseColumnMap, Dataset<Row> table) throws StandardException {\n+        if( baseColumnMap.length > table.schema().fields().length) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU0MzQ2NA=="}, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTEzNjQwOA==", "bodyText": "actually, you were right", "url": "https://github.com/splicemachine/spliceengine/pull/3889#discussion_r481136408", "createdAt": "2020-09-01T13:27:31Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -810,7 +842,17 @@ private String intArrayToString(int[] ints) {\n         }\n     }\n \n-    private static Column createFilterCondition(Dataset dataset,String[] allColIdInSpark, Qualifier[][] qual_list, int[] baseColumnMap, DataValueDescriptor probeValue) throws StandardException {\n+    /// check that we don't access a column that's not there with baseColumnMap\n+    private void checkNumColumns(String location, int[] baseColumnMap, Dataset<Row> table) throws StandardException {\n+        if( baseColumnMap.length > table.schema().fields().length) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU0MzQ2NA=="}, "originalCommit": {"oid": "be1772e1bc366cfa52d35433ce007ec14bdfeee1"}, "originalPosition": 90}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3128, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}