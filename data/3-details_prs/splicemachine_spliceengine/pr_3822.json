{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4ODI4ODMw", "number": 3822, "title": "DB-9843 External Tables: Own directory partition parsing", "bodyText": "fixing issues:\n\nDB-9843: Don't copy files for schema infer, but also don't check all files for schema -> overall faster\nDB-10568 Partitioned by using VARCHAR but value is 1 will be infered as INT and can't be read\n\ncurrently we copy one file out of the directory to another directory, keeping the subdirectories like path/col1=22/col2=ABC/file.parquet . The idea behind this was that Spark should only have to look at ONE file, not ALL. However, we would still need directory partitioning information, so we had to COPY the file (couldn't just point at one file).\nNow we can do directory partitioning ourselfs, we can skip the copy part.", "createdAt": "2020-07-14T11:44:35Z", "url": "https://github.com/splicemachine/spliceengine/pull/3822", "merged": true, "mergeCommit": {"oid": "4f49e5695d3b28db780c07f2c1bc4089fe0e676d"}, "closed": true, "closedAt": "2020-11-23T18:43:48Z", "author": {"login": "martinrupp"}, "timelineItems": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc00kjaABqjM1NDM3OTE5MTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdefr4YgBqjQwMjMwOTk4NDM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "04858e89f979b0bb42d02185151bd3b085ab35e9", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/04858e89f979b0bb42d02185151bd3b085ab35e9", "committedDate": "2020-07-14T11:38:28Z", "message": "DB-8269 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "333538dd654315baad564dde0e27deab2bfbf5a6", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/333538dd654315baad564dde0e27deab2bfbf5a6", "committedDate": "2020-07-14T11:44:53Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "333538dd654315baad564dde0e27deab2bfbf5a6", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/333538dd654315baad564dde0e27deab2bfbf5a6", "committedDate": "2020-07-14T11:44:53Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/049d627b5c410f8da02e4104df334b888f7d75e2", "committedDate": "2020-07-14T13:05:13Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4NzU4Mzg5", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-448758389", "createdAt": "2020-07-15T08:55:30Z", "commit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwODo1NTozMFrOGx0xyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTowMToyNlrOGx1A3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5ODEyMg==", "bodyText": "I think this is doing an explicit cast to Date when we expect that because the inferred type might not be it.", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454898122", "createdAt": "2020-07-15T08:55:30Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +381,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5OTU3MA==", "bodyText": "We borrowed Presto's ORC reader because it was faster than Spark's reader at some point in the past. We need to revisit that decision, and even if we stick with Presto's we'd need to bring all changes from the last few months/years", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454899570", "createdAt": "2020-07-15T08:57:46Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -707,6 +698,10 @@ public Boolean isCached(long conglomerateId) throws StandardException {\n         assert baseColumnMap != null:\"baseColumnMap Null\";\n         assert partitionColumnMap != null:\"partitionColumnMap Null\";\n         try {\n+            // todo: check why we're using our own reader here.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTgyNg==", "bodyText": "Since you made the effort to document the class (thanks!) please use Javadoc comments to visibilize them", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454901826", "createdAt": "2020-07-15T09:01:08Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTk4MA==", "bodyText": "Ditto here and all other methods", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454901980", "createdAt": "2020-07-15T09:01:26Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them\n+/// to make writing tests for all column types easier.\n+\n+public class CreateTableTypeHelper {\n+    /// @param types: an array of Types that should be used", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2"}, "originalPosition": 17}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/049d627b5c410f8da02e4104df334b888f7d75e2", "committedDate": "2020-07-14T13:05:13Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "d6428f4548be6134177362c7c70588125c893a79", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/d6428f4548be6134177362c7c70588125c893a79", "committedDate": "2020-07-24T08:52:02Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d6428f4548be6134177362c7c70588125c893a79", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/d6428f4548be6134177362c7c70588125c893a79", "committedDate": "2020-07-24T08:52:02Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "56e4cd65dc3f5d9801671994e987dcf688601fc5", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/56e4cd65dc3f5d9801671994e987dcf688601fc5", "committedDate": "2020-07-30T11:13:25Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "56e4cd65dc3f5d9801671994e987dcf688601fc5", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/56e4cd65dc3f5d9801671994e987dcf688601fc5", "committedDate": "2020-07-30T11:13:25Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "ee19754c5ff8befec111463d28b5ceaa2b34c050", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/ee19754c5ff8befec111463d28b5ceaa2b34c050", "committedDate": "2020-07-31T07:47:58Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ee19754c5ff8befec111463d28b5ceaa2b34c050", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/ee19754c5ff8befec111463d28b5ceaa2b34c050", "committedDate": "2020-07-31T07:47:58Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "f6927d24471239535d2ba8f983689260487a0d17", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/f6927d24471239535d2ba8f983689260487a0d17", "committedDate": "2020-08-03T10:21:11Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f6927d24471239535d2ba8f983689260487a0d17", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/f6927d24471239535d2ba8f983689260487a0d17", "committedDate": "2020-08-03T10:21:11Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "1900c94cc1048c83443414ebf2312b8403554a90", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/1900c94cc1048c83443414ebf2312b8403554a90", "committedDate": "2020-10-01T16:28:01Z", "message": "DB-9843 fix avro"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1900c94cc1048c83443414ebf2312b8403554a90", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/1900c94cc1048c83443414ebf2312b8403554a90", "committedDate": "2020-10-01T16:28:01Z", "message": "DB-9843 fix avro"}, "afterCommit": {"oid": "1c363f4678850596720d5e61ecc693bf9146b408", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/1c363f4678850596720d5e61ecc693bf9146b408", "committedDate": "2020-10-02T09:40:42Z", "message": "DB-9843 extract CountingListener"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1c363f4678850596720d5e61ecc693bf9146b408", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/1c363f4678850596720d5e61ecc693bf9146b408", "committedDate": "2020-10-02T09:40:42Z", "message": "DB-9843 extract CountingListener"}, "afterCommit": {"oid": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "committedDate": "2020-10-22T15:03:39Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0ODM5OTg2", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-514839986", "createdAt": "2020-10-22T15:14:12Z", "commit": {"oid": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToxNDoxMlrOHmm1FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToxNDoxMlrOHmm1FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0NDExNw==", "bodyText": "note: this small function is added to remove a bigger Avro write function in SparkDataSet", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r510244117", "createdAt": "2020-10-22T15:14:12Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1136,11 +1026,29 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n-                                          String compression, OperationContext context) throws StandardException {\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12"}, "originalPosition": 251}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "committedDate": "2020-10-22T15:03:39Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "fe64535ef9e6750853fbedca1a181f1ca211f5ae", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/fe64535ef9e6750853fbedca1a181f1ca211f5ae", "committedDate": "2020-10-22T15:39:56Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fe64535ef9e6750853fbedca1a181f1ca211f5ae", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/fe64535ef9e6750853fbedca1a181f1ca211f5ae", "committedDate": "2020-10-22T15:39:56Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "f020859648f047e4b42c540360f7824cdf189f01", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/f020859648f047e4b42c540360f7824cdf189f01", "committedDate": "2020-10-25T21:18:43Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f020859648f047e4b42c540360f7824cdf189f01", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/f020859648f047e4b42c540360f7824cdf189f01", "committedDate": "2020-10-25T21:18:43Z", "message": "DB-9843 External Tables: Don't copy files for schema infer"}, "afterCommit": {"oid": "ac41c923c40e9707832645e8e0345e378d46ceb8", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/ac41c923c40e9707832645e8e0345e378d46ceb8", "committedDate": "2020-10-27T11:10:45Z", "message": "DB-9843 External Tables: Own directory partition parsing\n\nfixing issues:\n- DB-9843: Don't copy files for schema infer, but also don't check all files for schema -> overall faster\n- DB-10568 Partitioned by using VARCHAR but value is 1 will be infered as INT and can't be read"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MjA0NjMw", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-518204630", "createdAt": "2020-10-27T22:41:36Z", "commit": {"oid": "a6b959e5bc583d0e2729866297cb3c7cfe061394"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjo0MTozNlrOHpTprA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjo0MTozNlrOHpTprA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3NTYyOA==", "bodyText": "Remove context because it is not used", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r513075628", "createdAt": "2020-10-27T22:41:36Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1207,9 +1078,8 @@ public void close() {\n         }\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n-\n+    private DataFrameWriter getDataFrameWriter(StructType tableSchema, int[] partitionBy,\n+                                               OperationContext context) throws StandardException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6b959e5bc583d0e2729866297cb3c7cfe061394"}, "originalPosition": 325}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4ede31162ae0ac28e9ab981888eb673bd9d3724c", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/4ede31162ae0ac28e9ab981888eb673bd9d3724c", "committedDate": "2020-10-28T10:38:02Z", "message": "DB-9843 fix mem"}, "afterCommit": {"oid": "354bef78388c19c336f0b653777c517f1a23e002", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/354bef78388c19c336f0b653777c517f1a23e002", "committedDate": "2020-10-29T08:46:49Z", "message": "DB-9843 fix mem"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "354bef78388c19c336f0b653777c517f1a23e002", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/354bef78388c19c336f0b653777c517f1a23e002", "committedDate": "2020-10-29T08:46:49Z", "message": "DB-9843 fix mem"}, "afterCommit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "committedDate": "2020-11-03T10:48:29Z", "message": "DB-9843 fix rebase issues, add unit test getCsvOptions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjUyOTY1", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-522652965", "createdAt": "2020-11-03T16:09:37Z", "commit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyOTc1NDMw", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-522975430", "createdAt": "2020-11-04T00:54:39Z", "commit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDo1NDozOVrOHtFk6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDo1NDozOVrOHtFk6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzOTMzOQ==", "bodyText": "Good documentation.", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517039339", "createdAt": "2020-11-04T00:54:39Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -487,41 +458,65 @@ public StructType getExternalFileSchema(String storedAs, String location, boolea\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n                                 .option(\"mergeSchema\", mergeSchemaOption)\n-                                .parquet(location);\n+                                .parquet(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"a\")) {\n                         // spark does not support schema merging for avro\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n                                 .option(\"ignoreExtension\", false)\n                                 .format(\"com.databricks.spark.avro\")\n-                                .load(location);\n+                                .load(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"o\")) {\n                         // spark does not support schema merging for orc\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n-                                .orc(location);\n+                                .orc(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"t\")) {\n                         // spark-2.2.0: commons-lang3-3.3.2 does not support 'XXX' timezone, specify 'ZZ' instead\n-                        dataset = SpliceSpark.getSession().read().options(getCsvOptions(csvOptions)).csv(location);\n+                        dataset = SpliceSpark.getSession().read().options(getCsvOptions(csvOptions)).csv(rootPath);\n                     } else {\n                         throw new UnsupportedOperationException(\"Unsupported storedAs \" + storedAs);\n                     }\n-                    dataset.printSchema();\n+                    //dataset.printSchema();\n                     schema = dataset.schema();\n                 }\n             } catch (Exception e) {\n-                handleExceptionInferSchema(e, location);\n-            } finally {\n-                if (!mergeSchema && fs != null && temp!= null && fs.exists(temp)){\n-                    fs.delete(temp, true);\n-                    SpliceLogUtils.info(LOG, \"deleted temporary directory %s\", temp);\n-                }\n+                handleExceptionInferSchema(e, rootPath);\n             }\n         }catch (Exception e) {\n             throw StandardException.newException(SQLState.EXTERNAL_TABLES_READ_FAILURE, e, e.getMessage());\n         }\n \n-        return schema;\n+        return new GetSchemaExternalResult(partition_schema, schema);\n+    }\n+\n+    /**\n+     * get the directory partitioning\n+     * @param rootPath root path of the dataset e.g. hdfs://cluster:123/path/to/directory\n+     * @param fileName file name below that path (including directories), e.g. firstCol=34/column2=HELLO/file.parquet\n+     * @param givenPartitionColumns the types as specified in CREATE TABLE ... PARTITIONED BY ( X ).\n+     *                              this is important as you can have e.g. firstCol = VARCHAR, and without this\n+     *                              we would infere firstCol=34 to be INTEGER.\n+     * @return if successful, the StructType for the partitioned columns compatible with partitionColumns.\n+     *         if we can't apply partitionColumns (e.g. have column2=HELLO, but we want column2 to be INT),\n+     *         we infere the schema without the given information, which is then used for suggest schema.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3"}, "originalPosition": 226}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMDYyMDMz", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-523062033", "createdAt": "2020-11-04T05:59:36Z", "commit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "committedDate": "2020-11-03T10:48:29Z", "message": "DB-9843 fix rebase issues, add unit test getCsvOptions"}, "afterCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/2db7687aa88892b701c7bb012d3f48de7886b7fd", "committedDate": "2020-11-04T11:14:09Z", "message": "DB-9843 fix spotbugs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMzA0MzUy", "url": "https://github.com/splicemachine/spliceengine/pull/3822#pullrequestreview-523304352", "createdAt": "2020-11-04T12:13:06Z", "commit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxMzowNlrOHtVgog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMjoxNjo1MlrOHtVnwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng==", "bodyText": "Do we need the EmptyFunction here?", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517300386", "createdAt": "2020-11-04T12:13:06Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSet.java", "diffHunk": "@@ -789,43 +773,17 @@ public static StructType generateTableSchema(OperationContext context) throws St\n                                           String compression,\n                                           OperationContext context) throws StandardException\n     {\n-        compression = SparkDataSet.getAvroCompression(compression);\n \n-        StructType dataSchema = null;\n         StructType tableSchema = generateTableSchema(context);\n-\n-        // what is this? why is this so different from parquet/orc ?\n-        // actually very close to NativeSparkDataSet.writeFile\n-        dataSchema = ExternalTableUtils.getDataSchema(dsp, tableSchema, partitionBy, location, \"a\");\n-\n+        StructType dataSchema = SparkExternalTableUtil.getDataSchemaAvro(dsp, tableSchema, partitionBy, location);\n         if (dataSchema == null)\n             dataSchema = tableSchema;\n-\n         Dataset<Row> insertDF = SpliceSpark.getSession().createDataFrame(\n-                rdd.map(new SparkSpliceFunctionWrapper<>(new CountWriteFunction(context))).map(new LocatedRowToRowAvroFunction()),\n+                rdd.map(new SparkSpliceFunctionWrapper<>(new EmptyFunction())).map(new LocatedRowToRowAvroFunction()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMTU4Mg==", "bodyText": "It'd be nice to check for a specific Exception / Error code here", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517301582", "createdAt": "2020-11-04T12:15:34Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableUnitTests.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.derby.stream.spark.SparkExternalTableUtil;\n+import com.splicemachine.derby.stream.utils.ExternalTableUtils;\n+import com.splicemachine.system.CsvOptions;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class ExternalTableUnitTests {\n+\n+    @Test\n+    public void testParsePartitionsFromFiles() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \".DS_Store\",\n+                root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__/part-00042.c000.snappy.parquet\",\n+                root + \"_SUCCESS\",\n+                root + \"c=3.14/ws_sold_date_sk=2450817/part-01434.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450816/part-00026.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c001.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( root );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                files, true, basePaths, null, null );\n+        Assert.assertEquals(\"StructType(StructField(c,DoubleType,true), StructField(ws_sold_date_sk,IntegerType,true))\",\n+                ps.partitionColumns().toString());\n+        Assert.assertEquals(\"List(\" +\n+                \"PartitionPath([3.14,null],\" + root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__), \" +\n+                \"PartitionPath([3.14,2450817],\" + root + \"c=3.14/ws_sold_date_sk=2450817), \" +\n+                \"PartitionPath([3.14,2450816],\" + root + \"c=3.14/ws_sold_date_sk=2450816), \" +\n+                \"PartitionPath([3.14,2450818],\" + root + \"c=3.14/ws_sold_date_sk=2450818))\",\n+                ps.partitions().toString());\n+    }\n+\n+    @Test\n+    public void testParsePartitionsFromFiles_one_userDefined() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( root );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        StructType s = new StructType();\n+        s = s.add(\"c\", DataTypes.StringType);\n+        s = s.add(\"ws_sold_date_sk\", DataTypes.DoubleType);\n+\n+        com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                files, true, basePaths, s, null );\n+        Assert.assertEquals(\"StructType(StructField(c,StringType,true), StructField(ws_sold_date_sk,DoubleType,true))\",\n+                ps.partitionColumns().toString());\n+        Assert.assertEquals(\"List(PartitionPath([3.14,2450818.0],\" + root + \"c=3.14/ws_sold_date_sk=2450818))\",\n+                ps.partitions().toString());\n+    }\n+\n+    @Test\n+    public void testParsePartitionsFromFiles_wrong_type() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__/part-00780.c002.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=HELLO/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( \"hdfs://host:123/partition_test/web_sales5/\" );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        StructType s = new StructType();\n+        s = s.add(\"ws_sold_date_sk\", DataTypes.DoubleType);\n+\n+        try {\n+            com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                    files, true, basePaths, s,null);\n+            Assert.fail(\"no exception\");\n+        }\n+        catch(Exception e)\n+        {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMjIwOA==", "bodyText": "The license goes typically at the very first line", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517302208", "createdAt": "2020-11-04T12:16:52Z", "author": {"login": "dgomezferro"}, "path": "scala_util/src/main/scala/com/splicemachine/spark/splicemachine/SplicePartitioningUtils.scala", "diffHunk": "@@ -0,0 +1,495 @@\n+package com.splicemachine.spark.splicemachine\n+\n+// note: this is marked as private in package org.apache.spark.sql.execution.datasources,\n+// so we had to copy this out.\n+// see\n+// https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala\n+// this file is marked as excluded from spotbugs, see splice_protocol/findbugs-exclude.xml\n+// to be able to detect wrong types for directory partitioning, we modified some part of the code, marked with\n+// modified splicemachine { ... }\n+\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd"}, "originalPosition": 11}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "347ddb07608dccca079a4bd33d77c315741bf4b1", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/347ddb07608dccca079a4bd33d77c315741bf4b1", "committedDate": "2020-11-04T13:50:18Z", "message": "DB-9843 fix hdp"}, "afterCommit": {"oid": "aec060af0cf56ccefa1eb1a102e7e0c95a040691", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/aec060af0cf56ccefa1eb1a102e7e0c95a040691", "committedDate": "2020-11-05T09:23:04Z", "message": "DB-9843 parameter DateFormat to avoid hdp/cdp differences"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "415128dcdad6c5f49dfcd79ed6b8c9c34881c91a", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/415128dcdad6c5f49dfcd79ed6b8c9c34881c91a", "committedDate": "2020-11-05T16:00:28Z", "message": "DB-9843 fix testStringIntPartitionParsing"}, "afterCommit": {"oid": "94dbdfba6d788d45fb0bb0f0a9a44c5528a0d763", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/94dbdfba6d788d45fb0bb0f0a9a44c5528a0d763", "committedDate": "2020-11-05T16:06:39Z", "message": "DB-9843 fix testStringIntPartitionParsing, move license"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8588b9c7a74062726bfaf57ea4b3a964d1b698c5", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/8588b9c7a74062726bfaf57ea4b3a964d1b698c5", "committedDate": "2020-11-20T12:18:00Z", "message": "DB-9843 External Tables: Own directory partition parsing\n\nfixing issues:\n- DB-9843: Don't copy files for schema infer, but also don't check all files for schema -> overall faster\n- DB-10568 Partitioned by using VARCHAR but value is 1 will be infered as INT and can't be read"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e3f8fc9fd61ee40cc5de550a616afcd631d12ef", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/9e3f8fc9fd61ee40cc5de550a616afcd631d12ef", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "391cccccd118ab319ba8ca790372245dfa56d4a0", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/391cccccd118ab319ba8ca790372245dfa56d4a0", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 address review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "affa79b9740702ac4a193cbb3de45296a6b66dc7", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/affa79b9740702ac4a193cbb3de45296a6b66dc7", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 fix mem"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "feb1f430dff7c421eb27281d19e5e4e70cb0401a", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/feb1f430dff7c421eb27281d19e5e4e70cb0401a", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 fix rebase issues, add unit test getCsvOptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9e7efdfcc176a4528b4abb639901a28f120e481", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/d9e7efdfcc176a4528b4abb639901a28f120e481", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 fix spotbugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "566da997ba47984d20f6ee60ef4bdafaad07cc06", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/566da997ba47984d20f6ee60ef4bdafaad07cc06", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 align with 2.8 code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4057236790cc6ab814cd400e710dec2b47f3350", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/c4057236790cc6ab814cd400e710dec2b47f3350", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 parameter DateFormat to avoid hdp/cdp differences"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de8f6b25bd13778cf059330b328c130e2d65faf2", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/de8f6b25bd13778cf059330b328c130e2d65faf2", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 fix findbugs-exclude-filter.xml"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fdf0afcba03858d7b4e95aed25688af61b3dd7b5", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/fdf0afcba03858d7b4e95aed25688af61b3dd7b5", "committedDate": "2020-11-20T12:18:01Z", "message": "DB-9843 fix testStringIntPartitionParsing, move license"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5a7bd221f5915274252ebd04771457f2612acc8", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/b5a7bd221f5915274252ebd04771457f2612acc8", "committedDate": "2020-11-20T23:09:44Z", "message": "DB-9843 address code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "77f786c05509f11d8f6d4a75bca24eb1368d0dca", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/77f786c05509f11d8f6d4a75bca24eb1368d0dca", "committedDate": "2020-11-16T20:12:07Z", "message": "DB-9843 address code review"}, "afterCommit": {"oid": "b5a7bd221f5915274252ebd04771457f2612acc8", "author": {"user": {"login": "martinrupp", "name": "Martin Rupp"}}, "url": "https://github.com/splicemachine/spliceengine/commit/b5a7bd221f5915274252ebd04771457f2612acc8", "committedDate": "2020-11-20T23:09:44Z", "message": "DB-9843 address code review"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1244, "cost": 1, "resetAt": "2021-11-02T10:47:05Z"}}}