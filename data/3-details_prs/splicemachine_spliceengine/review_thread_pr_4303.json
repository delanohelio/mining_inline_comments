{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA0MTg3NzA1", "number": 4303, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QwMDowNzoyM1rOEutqJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOTozNTozMlrOEvr_wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3NDE4MDIxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QwMDowNzoyM1rOHjSr0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNTozMjozN1rOHlyqBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc2ODMzOA==", "bodyText": "Is this our naming convention for kafka partition?", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r506768338", "createdAt": "2020-10-17T00:07:23Z", "author": {"login": "jyuanca"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1268,13 +1268,29 @@ public void decrementOpDepth() {\n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n-        List ps = consumer.partitionsFor(topicName);\n-        List<Integer> partitions = new ArrayList<>(ps.size());\n-        for (int i = 0; i < ps.size(); ++i) {\n-            partitions.add(i);\n+        List<Integer> partitions;\n+        String topicName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4OTMxOQ==", "bodyText": "Often the first batch received by SSDS at the beginning of a data run does not have data in every partition.  In this case, to the topic name SSDS appends :: followed by a comma-separated list of the partition numbers that have data, so that the consumer in the DB will not poll empty partitions, which results in long timeouts.\nKafka's convention for a partition name is topic name hyphen partition number.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509389319", "createdAt": "2020-10-21T15:32:37Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1268,13 +1268,29 @@ public void decrementOpDepth() {\n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n-        List ps = consumer.partitionsFor(topicName);\n-        List<Integer> partitions = new ArrayList<>(ps.size());\n-        for (int i = 0; i < ps.size(); ++i) {\n-            partitions.add(i);\n+        List<Integer> partitions;\n+        String topicName;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc2ODMzOA=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NDMwOTc5OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOToxNjowOFrOHkyrOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNDozMTo0OVrOHqeqcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ==", "bodyText": "What happens when we reach 10 retries? It looks like we silently drop data.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508341049", "createdAt": "2020-10-20T09:16:08Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQxMDkyMg==", "bodyText": "Each retry has a 1 minute timeout, so after 10 minutes of retrying, if it hasn't received more data within a batch, it stops polling for data and will LOG.error(id + \" KRF.call Didn't get full batch after \" + retries + \" retries, got \" + totalCount + \" records\");.  I've seen data within a batch delayed for several seconds but not as long as minutes, so normally 10 minutes will allow picking up the rest of the data.  In a failure scenario in which the upstream code is unable to send the end of a batch, without the eventual timeout this code will be in an infinite loop tying up a thread / spark task.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509410922", "createdAt": "2020-10-21T16:01:00Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYwMzYwMA==", "bodyText": "The problem is we are not failing the operation (as far as I can see). So after 10 minutes we complete the write without error. I think we should have a shorter timeout (1 minute?) and raise an exception.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r513603600", "createdAt": "2020-10-28T16:50:11Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMwNDYyNA==", "bodyText": "Added exception and shortened timeout in commit 063e869", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r514304624", "createdAt": "2020-10-29T14:31:49Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NDMzNzQ4OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOToyMjozMlrOHky8WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOToyMjozMlrOHky8WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NTQzMw==", "bodyText": "Serializing ValueRows directly with Kryo still has quite a bit of overhead (we are serializing the schema with each row, for instance). I opened https://splicemachine.atlassian.net/browse/DB-10543 to track this improvement", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508345433", "createdAt": "2020-10-20T09:22:32Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;\n+            int retries = 0;\n+            final Duration shortTimeout = java.time.Duration.ofMillis(500L);\n+            final Duration longTimeout = java.time.Duration.ofMinutes(1L);\n+            \n+            private ConsumerRecords<Integer, byte[]> kafkaRecords(int maxAttempts, Duration timeout) throws TaskKilledException {\n                 int attempt = 1;\n-                ConsumerRecords<Integer, Externalizable> records = null;\n+                ConsumerRecords<Integer, byte[]> records = null;\n                 do {\n-                    records = consumer.poll(java.time.Duration.ofMillis(1000));\n+                    records = consumer.poll(timeout);\n                     if (TaskContext.get().isInterrupted()) {\n-                        consumer.close();\n+                        LOG.warn( id+\" KRF.call kafkaRecords Spark TaskContext Interrupted\");\n+                        //consumer.close();\n                         throw new TaskKilledException();\n                     }\n                 } while( noRecords.test(records) && attempt++ < maxAttempts );\n                 \n                 return records;\n             }\n             \n-            private boolean hasMoreRecords(int maxAttempts) throws TaskKilledException {\n-                ConsumerRecords<Integer, Externalizable> records = kafkaRecords(maxAttempts);\n+            private boolean hasMoreRecords(int maxAttempts, Duration timeout) throws TaskKilledException {\n+                ConsumerRecords<Integer, byte[]> records = kafkaRecords(maxAttempts, timeout);\n                 if( noRecords.test(records) ) {\n-                    consumer.close();\n+                    if( !prevMessage.last() && retries < maxRetries ) {\n+                        retries++;\n+                        Duration retryTimeout = longTimeout;\n+                        LOG.warn( id+\" KRF.call Missed rcds, got \"+totalCount+\" retry \"+retries+\" for up to \"+retryTimeout );\n+                        return hasMoreRecords(\n+                            maxAttempts,\n+                            retryTimeout\n+                        );\n+                    }\n+                    //consumer.close();\n+                    if( !prevMessage.last() ) {\n+                        LOG.error(id + \" KRF.call Didn't get full batch after \" + retries + \" retries, got \" + totalCount + \" records\");\n+                    }\n                     return false;\n                 } else {\n+                    int ct = records.count();\n+                    totalCount += ct;\n+                    LOG.trace( id+\" KRF.call p \"+partition+\" t \"+topicName+\" records \"+ct );\n+                    retries = 0;\n+                    \n                     it = records.iterator();\n                     return it.hasNext();\n                 }\n             }\n-\n+            \n             @Override\n             public boolean hasNext() {\n-                if (it == null) {\n-                    return hasMoreRecords(60);\n-                }\n-                if (it.hasNext()) {\n-                    return true;\n+                boolean more = false;\n+                \n+                if (it != null && it.hasNext()) {\n+                    more = true;\n+                } else if (!prevMessage.last()) {\n+                    more = hasMoreRecords(1, shortTimeout);\n                 }\n-                else {\n-                    return hasMoreRecords(1);\n+\n+                if (!more) {\n+                    consumer.close();\n+                    kryo.close();\n                 }\n+\n+                return more;\n             }\n \n             @Override\n             public ExecRow next() {\n-                return (ExecRow)it.next().value();\n+                Message m = (Message)kryo.deserialize( it.next().value() );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NDM0NDc4OnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOToyNDoxNFrOHkzA6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNDozNDo1NVrOHmQPeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NjYwMA==", "bodyText": "remove", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508346600", "createdAt": "2020-10-20T09:24:14Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala", "diffHunk": "@@ -17,28 +17,37 @@ package com.splicemachine.nsds.kafka\n \n import java.util.Properties\n import java.util.concurrent.TimeUnit\n-import org.apache.kafka.clients.admin.AdminClient\n-import org.apache.kafka.clients.admin.AdminClientConfig\n+\n+import org.apache.kafka.clients.admin.{AdminClient, AdminClientConfig, NewTopic}\n import org.apache.kafka.common.KafkaFuture\n+\n import scala.collection.JavaConverters._\n \n-class KafkaAdmin(kafkaServers: String) {\n+@SerialVersionUID(20200722241L)\n+class KafkaAdmin(kafkaServers: String) extends Serializable {\n   val props = new Properties()\n   props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n   val admin = AdminClient.create( props )\n   \n+  def createTopics(topicNames: collection.mutable.Set[String], numParitions: Int, repFactor: Short): Unit = {\n+    admin.createTopics(\n+      topicNames.map(new NewTopic(_,numParitions,repFactor)).asJava\n+    ).all.get\n+//    Thread.sleep(1000)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxODI0MQ==", "bodyText": "Made change in a temporary branch for review while regression tests are running, and will push to this branch after regression is finished.\n\n  \n    \n      spliceengine/splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala\n    \n    \n        Lines 32 to 36\n      in\n      0263ed8\n    \n    \n    \n    \n\n        \n          \n           def createTopics(topicNames: collection.mutable.Set[String], numParitions: Int, repFactor: Short): Unit = { \n        \n\n        \n          \n             admin.createTopics( \n        \n\n        \n          \n               topicNames.map(new NewTopic(_,numParitions,repFactor)).asJava \n        \n\n        \n          \n             ).all.get \n        \n\n        \n          \n           }", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509618241", "createdAt": "2020-10-21T19:41:01Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala", "diffHunk": "@@ -17,28 +17,37 @@ package com.splicemachine.nsds.kafka\n \n import java.util.Properties\n import java.util.concurrent.TimeUnit\n-import org.apache.kafka.clients.admin.AdminClient\n-import org.apache.kafka.clients.admin.AdminClientConfig\n+\n+import org.apache.kafka.clients.admin.{AdminClient, AdminClientConfig, NewTopic}\n import org.apache.kafka.common.KafkaFuture\n+\n import scala.collection.JavaConverters._\n \n-class KafkaAdmin(kafkaServers: String) {\n+@SerialVersionUID(20200722241L)\n+class KafkaAdmin(kafkaServers: String) extends Serializable {\n   val props = new Properties()\n   props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n   val admin = AdminClient.create( props )\n   \n+  def createTopics(topicNames: collection.mutable.Set[String], numParitions: Int, repFactor: Short): Unit = {\n+    admin.createTopics(\n+      topicNames.map(new NewTopic(_,numParitions,repFactor)).asJava\n+    ).all.get\n+//    Thread.sleep(1000)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NjYwMA=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDA0MA==", "bodyText": "Pushed code to this branch.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509874040", "createdAt": "2020-10-22T04:34:55Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/scala/com/splicemachine/nsds/kafka/KafkaAdmin.scala", "diffHunk": "@@ -17,28 +17,37 @@ package com.splicemachine.nsds.kafka\n \n import java.util.Properties\n import java.util.concurrent.TimeUnit\n-import org.apache.kafka.clients.admin.AdminClient\n-import org.apache.kafka.clients.admin.AdminClientConfig\n+\n+import org.apache.kafka.clients.admin.{AdminClient, AdminClientConfig, NewTopic}\n import org.apache.kafka.common.KafkaFuture\n+\n import scala.collection.JavaConverters._\n \n-class KafkaAdmin(kafkaServers: String) {\n+@SerialVersionUID(20200722241L)\n+class KafkaAdmin(kafkaServers: String) extends Serializable {\n   val props = new Properties()\n   props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n   val admin = AdminClient.create( props )\n   \n+  def createTopics(topicNames: collection.mutable.Set[String], numParitions: Int, repFactor: Short): Unit = {\n+    admin.createTopics(\n+      topicNames.map(new NewTopic(_,numParitions,repFactor)).asJava\n+    ).all.get\n+//    Thread.sleep(1000)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NjYwMA=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NDM3MTkxOnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOTozMDoxOVrOHkzRcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNDozNTowOVrOHmQPuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MDgzMg==", "bodyText": "We shouldn't print to console, use Debugging output instead", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508350832", "createdAt": "2020-10-20T09:30:19Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow\n         }\n \n         val props = new Properties\n         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n         props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-s2s-smc-\"+java.util.UUID.randomUUID() )\n         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[IntegerSerializer].getName)\n-        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ExternalizableSerializer].getName)\n-\n-        val producer = new KafkaProducer[Integer, Externalizable](props)\n-\n-        while( itrRow.hasNext ) {\n-          producer.send( new ProducerRecord(topicName, msgIdx, externalizable(itrRow.next, schema)) )\n-          msgIdx += 1\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ByteArraySerializer].getName)\n+//        // Throughput performance?\n+//        println(s\"SMC.sendData batch 1MB linger 750ms\")\n+//        props.put(ProducerConfig.BATCH_SIZE_CONFIG, (1000*1000).toString )\n+//        props.put(ProducerConfig.LINGER_MS_CONFIG, \"500\")\n+\n+        val producer = new KafkaProducer[Integer, Array[Byte]](props)\n+        \n+        val rowK = new RowForKafka(topicName, partition, schema)\n+        rowK.sparkRow = if( itr.hasNext ) {\n+          msgCount += 1\n+          Some(itr.next)\n+        } else None\n+        rowK.msgCount = msgCount\n+        \n+        val kryo = new KryoSerialization()\n+        kryo.init\n+        \n+        while( itr.hasNext ) {\n+          rowK.valueRow = Some(externalizable(rowK.sparkRow.get, schema, partition))\n+          rowK.send(producer, kryo)\n+          msgCount += 1\n+          rowK.sparkRow = Some(itr.next)\n+          rowK.msgCount = msgCount\n         }\n+        lastRowsToSend.add(rowK)\n+        \n+        kryo.close\n \n+        insAccum.add(msgCount)\n+\n+        println(s\"$id SMC.sendData t $topicName p $partition records $msgCount\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYyNTAyNQ==", "bodyText": "Changed all printlns to logs in a temporary branch for review while regression tests are running, and will push to this branch after regression is finished.\n\n  \n    \n      spliceengine/splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala\n    \n    \n        Lines 799 to 801\n      in\n      0263ed8\n    \n    \n    \n    \n\n        \n          \n           insAccum.add(msgCount) \n        \n\n        \n          \n            \n        \n\n        \n          \n           debug(s\"$id SMC.sendData t $topicName p $partition records $msgCount\") \n        \n    \n  \n\n\nAlso included some other code improvements.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509625025", "createdAt": "2020-10-21T19:50:11Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow\n         }\n \n         val props = new Properties\n         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n         props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-s2s-smc-\"+java.util.UUID.randomUUID() )\n         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[IntegerSerializer].getName)\n-        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ExternalizableSerializer].getName)\n-\n-        val producer = new KafkaProducer[Integer, Externalizable](props)\n-\n-        while( itrRow.hasNext ) {\n-          producer.send( new ProducerRecord(topicName, msgIdx, externalizable(itrRow.next, schema)) )\n-          msgIdx += 1\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ByteArraySerializer].getName)\n+//        // Throughput performance?\n+//        println(s\"SMC.sendData batch 1MB linger 750ms\")\n+//        props.put(ProducerConfig.BATCH_SIZE_CONFIG, (1000*1000).toString )\n+//        props.put(ProducerConfig.LINGER_MS_CONFIG, \"500\")\n+\n+        val producer = new KafkaProducer[Integer, Array[Byte]](props)\n+        \n+        val rowK = new RowForKafka(topicName, partition, schema)\n+        rowK.sparkRow = if( itr.hasNext ) {\n+          msgCount += 1\n+          Some(itr.next)\n+        } else None\n+        rowK.msgCount = msgCount\n+        \n+        val kryo = new KryoSerialization()\n+        kryo.init\n+        \n+        while( itr.hasNext ) {\n+          rowK.valueRow = Some(externalizable(rowK.sparkRow.get, schema, partition))\n+          rowK.send(producer, kryo)\n+          msgCount += 1\n+          rowK.sparkRow = Some(itr.next)\n+          rowK.msgCount = msgCount\n         }\n+        lastRowsToSend.add(rowK)\n+        \n+        kryo.close\n \n+        insAccum.add(msgCount)\n+\n+        println(s\"$id SMC.sendData t $topicName p $partition records $msgCount\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MDgzMg=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDEwNA==", "bodyText": "Pushed code to this branch.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509874104", "createdAt": "2020-10-22T04:35:09Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow\n         }\n \n         val props = new Properties\n         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaServers)\n         props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-s2s-smc-\"+java.util.UUID.randomUUID() )\n         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[IntegerSerializer].getName)\n-        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ExternalizableSerializer].getName)\n-\n-        val producer = new KafkaProducer[Integer, Externalizable](props)\n-\n-        while( itrRow.hasNext ) {\n-          producer.send( new ProducerRecord(topicName, msgIdx, externalizable(itrRow.next, schema)) )\n-          msgIdx += 1\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[ByteArraySerializer].getName)\n+//        // Throughput performance?\n+//        println(s\"SMC.sendData batch 1MB linger 750ms\")\n+//        props.put(ProducerConfig.BATCH_SIZE_CONFIG, (1000*1000).toString )\n+//        props.put(ProducerConfig.LINGER_MS_CONFIG, \"500\")\n+\n+        val producer = new KafkaProducer[Integer, Array[Byte]](props)\n+        \n+        val rowK = new RowForKafka(topicName, partition, schema)\n+        rowK.sparkRow = if( itr.hasNext ) {\n+          msgCount += 1\n+          Some(itr.next)\n+        } else None\n+        rowK.msgCount = msgCount\n+        \n+        val kryo = new KryoSerialization()\n+        kryo.init\n+        \n+        while( itr.hasNext ) {\n+          rowK.valueRow = Some(externalizable(rowK.sparkRow.get, schema, partition))\n+          rowK.send(producer, kryo)\n+          msgCount += 1\n+          rowK.sparkRow = Some(itr.next)\n+          rowK.msgCount = msgCount\n         }\n+        lastRowsToSend.add(rowK)\n+        \n+        kryo.close\n \n+        insAccum.add(msgCount)\n+\n+        println(s\"$id SMC.sendData t $topicName p $partition records $msgCount\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MDgzMg=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 422}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NDM5MzYxOnYy", "diffSide": "RIGHT", "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOTozNTozMlrOHkze-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNDozNToyNFrOHmQQEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1NDI5OA==", "bodyText": "Can you explain a bit what's going on here? The naming of variables is a bit confusing.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508354298", "createdAt": "2020-10-20T09:35:32Z", "author": {"login": "dgomezferro"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 377}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc4MzQ5NQ==", "bodyText": "Added comments to the code in a temporary branch for review while regression tests are running, and will push to this branch after regression is finished.\n\n  \n    \n      spliceengine/splice_spark2/src/main/spark2.4/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala\n    \n    \n        Lines 738 to 768\n      in\n      8e1479e\n    \n    \n    \n    \n\n        \n          \n               // Convert last message in Kafka to a ValueRow (lastVR) \n        \n\n        \n          \n               val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr \n        \n\n        \n          \n               // Get the hash code (lastKHash) of lastVR based on the columns that are in lastVR (hashCols) \n        \n\n        \n          \n               val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) } \n        \n\n        \n          \n               val lastKHash = lastVR.hashCode(hashCols) \n        \n\n        \n          \n               // Define function (hash) for converting a spark row to a ValueRow and getting its hash code \n        \n\n        \n          \n               def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols) \n        \n\n        \n          \n            \n        \n\n        \n          \n               // Get a pair of iterators (itr34) of rdd data to use for different purposes \n        \n\n        \n          \n               //val itr34 = itr12._2.duplicate \n        \n\n        \n          \n               val itr34 = itrRow.duplicate   //itr12.duplicate \n        \n\n        \n          \n                \n        \n\n        \n          \n               // Use span to split itr34._1 into a pair of iterators (inKafka_NotInKafka). \n        \n\n        \n          \n               // inKafka_NotInKafka._1 will contain all of the rows from the beginning of itrRow whose hash != lastKHash. \n        \n\n        \n          \n               // inKafka_NotInKafka._2 will contain all of the rows from the one whose hash == lastKHash to the end of itrRow. \n        \n\n        \n          \n               // So inKafka_NotInKafka._1 will contain rows already in Kafka, and inKafka_NotInKafka._2 will contain the  \n        \n\n        \n          \n               //  last row in Kafka followed by rows that are not in Kafka. \n        \n\n        \n          \n               val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash ) \n        \n\n        \n          \n               if( inKafka_NotInKafka._2.hasNext ) { \n        \n\n        \n          \n                 inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it \n        \n\n        \n          \n                 msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure \n        \n\n        \n          \n                 inKafka_NotInKafka._2 \n        \n\n        \n          \n               } else { \n        \n\n        \n          \n                 // In this case, itrRow didn't contain the last row of Kafka, so inKafka_NotInKafka._2 is empty. \n        \n\n        \n          \n                 // This happens when itrRow starts after the last item added to Kafka. \n        \n\n        \n          \n                 itr34._2 \n        \n\n        \n          \n               } \n        \n\n        \n          \n             } \n        \n\n        \n          \n           } else { \n        \n\n        \n          \n             itrRow \n        \n\n        \n          \n           }", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509783495", "createdAt": "2020-10-21T23:01:02Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1NDI5OA=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 377}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg3NDE5Mg==", "bodyText": "Pushed code to this branch.", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509874192", "createdAt": "2020-10-22T04:35:24Z", "author": {"login": "jpanko1"}, "path": "splice_spark2/src/main/spark2.2/com/splicemachine/spark2/splicemachine/SplicemachineContext.scala", "diffHunk": "@@ -489,68 +552,269 @@ class SplicemachineContext(options: Map[String, String]) extends Serializable {\n    * @param schema\n    * @param schemaTableName\n    */\n-  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Unit = insert(rdd, schema, schemaTableName, Map[String,String]())\n+  def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String): Long = insert(rdd, schema, schemaTableName, Map[String,String]())\n \n   private[this] def columnList(schema: StructType): String = SpliceJDBCUtil.listColumns(schema.fieldNames)\n   private[this] def schemaString(schema: StructType): String = SpliceJDBCUtil.schemaWithoutNullableString(schema, url).replace(\"\\\"\",\"\")\n \n-  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, spliceProperties: scala.collection.immutable.Map[String,String]): Unit = {\n-    val topicName = kafkaTopics.create\n+  private[this] def insert(rdd: JavaRDD[Row], schema: StructType, schemaTableName: String, \n+                           spliceProperties: scala.collection.immutable.Map[String,String]): Long = /*if( ! rdd.isEmpty )*/ {\n+    println(s\"${java.time.Instant.now} SMC.ins get topic name\")\n+    val topicName = kafkaTopics.create()\n //    println( s\"SMC.insert topic $topicName\" )\n \n     // hbase user has read/write permission on the topic\n     try {\n+      insAccum.reset\n+      lastRowsToSend.reset\n+      println(s\"${java.time.Instant.now} SMC.ins sendData\")\n       sendData(topicName, rdd, schema)\n+      sendData(lastRowsToSend.value.asScala, true)\n+\n+      if( ! insAccum.isZero ) {\n+        println(s\"${java.time.Instant.now} SMC.ins prepare sql\")\n+        val colList = columnList(schema) + fmColList\n+        val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n+        val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n+          \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n+          \"as SpliceDatasetVTI (\" + schemaString(schema) + fmSchemaStr + \")\"\n+\n+        println( s\"SMC.insert sql $sqlText\" )\n+        println(s\"${java.time.Instant.now} SMC.ins executeUpdate\")\n+        executeUpdate(sqlText)\n+        println(s\"${java.time.Instant.now} SMC.ins done\")\n+      }\n+      \n+      insAccum.sum\n+    } finally {\n+      kafkaTopics.delete(topicName)\n+    }\n+  }\n+\n+  private[this] val activePartitionAcm =\n+    SparkSession.builder.getOrCreate.sparkContext.collectionAccumulator[String](\"ActivePartitions\")\n \n-      val colList = columnList(schema)\n-      val sProps = spliceProperties.map({ case (k, v) => k + \"=\" + v }).fold(\"--splice-properties useSpark=true\")(_ + \", \" + _)\n-      val sqlText = \"insert into \" + schemaTableName + \" (\" + colList + \") \" + sProps + \"\\nselect \" + colList + \" from \" +\n-        \"new com.splicemachine.derby.vti.KafkaVTI('\" + topicName + \"') \" +\n-        \"as SpliceDatasetVTI (\" + schemaString(schema) + \")\"\n+  def activePartitions(df: DataFrame): Seq[Int] = {\n+    activePartitionAcm.reset\n+    df.rdd.mapPartitionsWithIndex((p, itr) => {\n+      activePartitionAcm.add( s\"$p ${itr.nonEmpty}\" )\n+      Iterator.apply(\"OK\")\n+    }).collect\n+  \n+    activePartitionAcm.value.asScala.filter( _.endsWith(\"true\") ).map( _.split(\" \")(0).toInt )\n+  }\n+\n+  var insertSql: String => String = _\n+  \n+  /* Sets up insertSql to be used by insert_streaming */\n+  def setTable(schemaTableName: String, schema: StructType): Unit = {\n+    val colList = columnList(schema) + fmColList\n+    val schStr = schemaString(schema)\n+    // Line break at the end of the first line and before select is required, other line breaks aren't required\n+    insertSql = (topicName: String) => s\"\"\"insert into $schemaTableName ($colList)\n+                                       select $colList from \n+      new com.splicemachine.derby.vti.KafkaVTI('$topicName') \n+      as SpliceDatasetVTI ($schStr$fmSchemaStr)\"\"\"\n+  }\n+  \n+  private[this] def log(msg: String): Unit = {\n+    Holder.log.info(msg)\n+    println(s\"${java.time.Instant.now} $msg\")\n+  }\n \n-      //println( s\"SMC.insert sql $sqlText\" )\n+  def insert_streaming(topicInfo: String, retries: Int = 0): Unit = {\n+    val topicName = if( topicInfo.contains(\"::\") ) {\n+      topicInfo.split(\"::\")(0)\n+    } else {\n+      topicInfo\n+    }\n+    try {\n+      log(\"SMC.inss prepare sql\")\n+      val sqlText = insertSql(topicInfo)\n+      log(s\"SMC.inss sql $sqlText\")\n+      \n+      //log( s\"SMC.inss topicCount preex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+      \n+      log(\"SMC.inss executeUpdate\")\n       executeUpdate(sqlText)\n+      log(\"SMC.inss done\")\n+\n+      log( s\"SMC.inss topicCount postex ${KafkaUtils.messageCount(kafkaServers, topicName)}\")\n+    } catch {\n+      case e: java.sql.SQLNonTransientConnectionException => \n+        if( retries < 2 ) {\n+          insert_streaming(topicInfo, retries + 1)\n+        }\n     } finally {\n       kafkaTopics.delete(topicName)\n     }\n   }\n \n-  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit =\n+  def newTopic_streaming(): String = {\n+    log(\"SMC.nit get topic name\")\n+    kafkaTopics.create()\n+  }\n+  \n+  def sendData_streaming(dataFrame: DataFrame, topicName: String): (Seq[RowForKafka], Long) = {\n+    insAccum.reset\n+    lastRowsToSend.reset\n+    println(s\"${java.time.Instant.now} SMC.sds sendData\")\n+    sendData(topicName, dataFrame.rdd, dataFrame.schema)\n+\n+    val rows = lastRowsToSend.value.asScala\n+    //println(s\"${java.time.Instant.now} SMC.sds last rows ${rows.mkString(\"\\n\")}\")\n+\n+    (rows, insAccum.sum)\n+  }\n+  \n+  /** checkRecovery was written to help debug an issue and normally won't need to be called. */\n+  private[this] def checkRecovery(\n+     id: String,\n+     topicName: String, \n+     partition: Int, \n+     itr: Iterator[Row],\n+     schema: StructType\n+   ): Unit = {\n+\n+    val lastVR = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+      .asInstanceOf[KafkaReadFunction.Message].vr\n+    val cols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+    val lastKHash = lastVR.hashCode(cols)\n+\n+    val khashcodes = KafkaUtils.messagesFrom(kafkaServers, topicName, partition)\n+      .map( _.asInstanceOf[KafkaReadFunction.Message].vr.hashCode(cols) )\n+\n+    println(s\"$id SMC.checkRecovery 1st Kafka hashcode ${khashcodes.headOption.getOrElse(-1)}\" )\n+    \n+    var i = 0\n+    var res = Seq.empty[String]\n+    while( itr.hasNext ) {\n+      val hashcode = externalizable(itr.next, schema, partition).hashCode(cols)\n+      res = res :+ s\"$i,${khashcodes.indexOf(hashcode)}\\t${hashcode==lastKHash}\"\n+      i += 1\n+    }\n+    \n+    println(s\"$id SMC.checkRecovery res: Kafka count ${khashcodes.size}\\n${res.mkString(\"\\n\")}\")\n+  }\n+  \n+  private[this] var sendDataTimestamp: Long = _\n+  \n+  private[this] def sendData(topicName: String, rdd: JavaRDD[Row], schema: StructType): Unit = {\n+    sendDataTimestamp = System.currentTimeMillis\n     rdd.rdd.mapPartitionsWithIndex(\n       (partition, itrRow) => {\n-        val taskContext = TaskContext.get\n+        val id = topicName.substring(0,5)+\":\"+partition.toString\n+        //println(s\"$id SMC.sendData p== $partition\")\n \n-        var msgIdx = 0\n-        if (taskContext != null && taskContext.attemptNumber > 0) {\n-          val entriesInKafka = KafkaUtils.messageCount(kafkaServers, topicName, partition)\n-          for(i <- (1: Long) to entriesInKafka) {\n-            itrRow.next\n+        var msgCount = 0\n+        val taskContext = TaskContext.get\n+        val itr = if (taskContext != null && taskContext.attemptNumber > 0) {\n+          // Recover from previous task failure\n+          // Be sure the iterator is advanced past the items previously published to Kafka\n+          \n+          println(s\"$id SMC.sendData Retry $partition ${taskContext.attemptNumber} ${insAccum.sum}\")\n+\n+          //val itr12 = itrRow //.duplicate\n+          //checkRecovery(id, topicName, partition, itr12._1, schema)\n+\n+          val lastMsg = KafkaUtils.lastMessageOf(kafkaServers, topicName, partition)\n+          if( lastMsg.isEmpty ) {\n+            itrRow\n+          } else {\n+            val lastVR = lastMsg.get.asInstanceOf[KafkaReadFunction.Message].vr\n+            val hashCols = if( lastVR.length > 0) { Range(0,lastVR.length-1).toArray } else { Array(0) }\n+            val lastKHash = lastVR.hashCode(hashCols)\n+            def hash: Row => Int = row => externalizable(row, schema, partition).hashCode(hashCols)\n+            \n+            //val itr34 = itr12._2.duplicate\n+            val itr34 = itrRow.duplicate   //itr12.duplicate\n+            \n+            val inKafka_NotInKafka = itr34._1.span( hash(_) != lastKHash )\n+            if( inKafka_NotInKafka._2.hasNext ) {\n+              inKafka_NotInKafka._2.next  // matches the last item in Kafka, get past it\n+              //println(s\"$id SMC.sendData Retry skip ${hash(r)} $lastKHash\")\n+              msgCount = inKafka_NotInKafka._1.size + 1  // this message count was lost during previous task failure\n+              inKafka_NotInKafka._2\n+            } else {  // itrRow starts after the last item added to Kafka\n+              itr34._2\n+            }\n           }\n-          msgIdx = entriesInKafka.asInstanceOf[Int]\n+        } else {\n+          itrRow", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1NDI5OA=="}, "originalCommit": {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db"}, "originalPosition": 377}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2884, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}