{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2OTU1ODk5", "number": 3463, "title": "DB-9361: remove duplicate indexes", "bodyText": "", "createdAt": "2020-04-21T23:17:04Z", "url": "https://github.com/splicemachine/spliceengine/pull/3463", "merged": true, "mergeCommit": {"oid": "7469cbeb85de2e7ca51eb778f1dd2ab1313d5fa6"}, "closed": true, "closedAt": "2020-05-19T01:03:46Z", "author": {"login": "jyuanca"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcaQQrugBqjMyNjI3MTYzOTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcheAfKAFqTQxMjQ1ODA5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6e6b41e07d544c94d06e3f95eac03d6661b7cc4c", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/6e6b41e07d544c94d06e3f95eac03d6661b7cc4c", "committedDate": "2020-04-22T22:13:47Z", "message": "clean up"}, "afterCommit": {"oid": "988ba9edfcc2984c9c2cb9a8ec63f88bcdaac9ac", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/988ba9edfcc2984c9c2cb9a8ec63f88bcdaac9ac", "committedDate": "2020-04-22T22:17:11Z", "message": "DB-9361: remove duplicate indexes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "988ba9edfcc2984c9c2cb9a8ec63f88bcdaac9ac", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/988ba9edfcc2984c9c2cb9a8ec63f88bcdaac9ac", "committedDate": "2020-04-22T22:17:11Z", "message": "DB-9361: remove duplicate indexes"}, "afterCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/d0cbc26fb625b5340e458eaa852e54d255643c68", "committedDate": "2020-04-22T22:48:02Z", "message": "DB-9361: remove duplicate indexes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA3NTA2OTM5", "url": "https://github.com/splicemachine/spliceengine/pull/3463#pullrequestreview-407506939", "createdAt": "2020-05-07T14:06:38Z", "commit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDowNjozOVrOGSAQCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxNDoxNDowOFrOGSAmCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzMTY1OQ==", "bodyText": "re-add SpliceSpark.popScope()\n(this function does SpliceSpark.pushScope , but SpliceSpark.popScope(); was moved (unintentionally i guess) to line 204))", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421531659", "createdAt": "2020-05-07T14:06:39Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzMTg5Ng==", "bodyText": "remove popScope", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421531896", "createdAt": "2020-05-07T14:06:56Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;\n+            }\n+            count++;\n+        }\n         messages.add(0, String.format(\"The following %d indexes are duplicates:\", count));\n         SpliceSpark.popScope();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNTA0Ng==", "bodyText": "the only difference i see between these two functions is\nif(count >= maxCheckTableError) . I don't see how fixDuplicateIndexes fixes anything.\nIn general, we could do\nprivate List<String> reportDuplicateIndexes(Iterator it, int _maxCheckTableError)\nthen have\nif (_maxCheckTableError >= 0 || count >= _maxCheckTableError)\nand then do\nreturn reportDuplicateIndexes(it, fix ? -1 : maxCheckTableError);", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421535046", "createdAt": "2020-05-07T14:11:07Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNzI5MA==", "bodyText": "i think fixDuplicateIndexes is missing the lines https://github.com/splicemachine/spliceengine/pull/3463/files#diff-49eff1a8a3acfe993feb4e304ff615ceL161-L167 , no?", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r421537290", "createdAt": "2020-05-07T14:14:08Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,30 +148,58 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTUzNTA0Ng=="}, "originalCommit": {"oid": "d0cbc26fb625b5340e458eaa852e54d255643c68"}, "originalPosition": 142}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a547a3d243cea9d0a591b4b3e9997b4f62d5b658", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/a547a3d243cea9d0a591b4b3e9997b4f62d5b658", "committedDate": "2020-05-12T17:29:37Z", "message": "address review comments"}, "afterCommit": {"oid": "67c91c383ccedb0ab21f5ea6550487c0ae670ec9", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/67c91c383ccedb0ab21f5ea6550487c0ae670ec9", "committedDate": "2020-05-12T17:44:39Z", "message": "address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86bd323035855ccd88ad80c0624ae69b283712d1", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/86bd323035855ccd88ad80c0624ae69b283712d1", "committedDate": "2020-05-12T17:55:26Z", "message": "DB-9361: remove duplicate indexes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/38166536439f08fddfb6730303779d8ba941eca6", "committedDate": "2020-05-12T17:56:00Z", "message": "address review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "67c91c383ccedb0ab21f5ea6550487c0ae670ec9", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/67c91c383ccedb0ab21f5ea6550487c0ae670ec9", "committedDate": "2020-05-12T17:44:39Z", "message": "address review comments"}, "afterCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/38166536439f08fddfb6730303779d8ba941eca6", "committedDate": "2020-05-12T17:56:00Z", "message": "address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwNTUyNjM1", "url": "https://github.com/splicemachine/spliceengine/pull/3463#pullrequestreview-410552635", "createdAt": "2020-05-13T02:51:08Z", "commit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QwMjo1MTowOFrOGUfrkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QwMzozMTo1NlrOGUgP_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0Mzc2MA==", "bodyText": "By aborting here, we won't be able to know the actual number of duplicate indexes, is this intended? Before the change, we can get the the actual number of duplicate indexes.", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424143760", "createdAt": "2020-05-13T02:51:08Z", "author": {"login": "yxia92"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));\n+        return messages;\n+    }\n+\n+    private List<String> reportDuplicateIndexes(Iterator it) throws StandardException, InterruptedException, ExecutionException {\n+        List<String> messages = Lists.newLinkedList();\n \n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            if (count >= maxCheckTableError) {\n+                messages.add(\"...\");\n+                break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE1MzA4NQ==", "bodyText": "We seem to assume that if there are duplicate index rows, the extra ones must have different values than the base row. Is it possible that the extra index row also has the same values as the base row?", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424153085", "createdAt": "2020-05-13T03:31:56Z", "author": {"login": "yxia92"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlTableChecker.java", "diffHunk": "@@ -134,41 +136,100 @@ private void populateData(PairDataSet table, PairDataSet index) {\n \n         indexData = ArrayListMultimap.create();\n         indexCount = 0;\n-        Iterator<Tuple2<String, byte[]>> indexSource = ((ControlPairDataSet)index).source;\n+        Iterator<Tuple2<String, Tuple2<byte[], ExecRow>>> indexSource = ((ControlPairDataSet)index).source;\n         while(indexSource.hasNext()) {\n             indexCount++;\n-            Tuple2<String, byte[]> t = indexSource.next();\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = indexSource.next();\n             String baseRowId = t._1;\n-            byte[] indexKey = t._2;\n-            indexData.put(baseRowId, indexKey);\n+            Tuple2<byte[], ExecRow> row = t._2;\n+            indexData.put(baseRowId, row);\n         }\n     }\n \n     private List<String> checkDuplicateIndexes() throws StandardException {\n-        List<String> messages = new LinkedList<>();\n-        ArrayListMultimap<String, byte[]> result = ArrayListMultimap.create();\n+        ArrayListMultimap<String, Tuple2<byte[], ExecRow>> result = ArrayListMultimap.create();\n         long duplicateIndexCount = 0;\n         for (String baseRowId : indexData.keySet()) {\n-            List<byte[]> rows = indexData.get(baseRowId);\n+            List<Tuple2<byte[], ExecRow>> rows = indexData.get(baseRowId);\n             if (rows.size() > 1) {\n-                duplicateIndexCount += rows.size();\n+                duplicateIndexCount += rows.size()-1;\n                 result.putAll(baseRowId, rows);\n             }\n         }\n-        int i = 0;\n         if (duplicateIndexCount > 0) {\n-            messages.add(String.format(\"The following %d indexes are duplicates:\", duplicateIndexCount));\n+            if (fix) {\n+                fixDuplicateIndexes(result);\n+            }\n+            return reportDuplicateIndexes(result, duplicateIndexCount);\n+        }\n+        return new LinkedList<>();\n+    }\n+\n+    private void fixDuplicateIndexes(ArrayListMultimap<String, Tuple2<byte[], ExecRow>> result) throws StandardException {\n+        try {\n+            WriteCoordinator writeCoordinator = PipelineDriver.driver().writeCoordinator();\n+            WriteConfiguration writeConfiguration = writeCoordinator.defaultWriteConfiguration();\n+            Partition indexPartition = SIDriver.driver().getTableFactory().getTable(Long.toString(conglomerate));\n+            RecordingCallBuffer<KVPair> writeBuffer = writeCoordinator.writeBuffer(indexPartition, txn, null, writeConfiguration);\n+\n+            List<Integer> indexToMain = tentativeIndex.getIndex().getIndexColsToMainColMapList();\n             for (String baseRowId : result.keySet()) {\n-                List<byte[]> indexKeys = result.get(baseRowId);\n-                for (byte[] indexKey : indexKeys) {\n-                    if (i >= maxCheckTableErrors) {\n+                ExecRow baseRow = tableData.get(baseRowId);\n+                List<Tuple2<byte[], ExecRow>> indexRows = result.get(baseRowId);\n+                for (Tuple2<byte[], ExecRow> indexRow : indexRows) {\n+                    boolean duplicate = false;\n+                    DataValueDescriptor[] dvds = indexRow._2.getRowArray();\n+                    for (int i = 0; i < dvds.length - 1; ++i) {\n+                        int col = baseColumnMap[indexToMain.get(i) - 1];\n+                        if (!dvds[i].equals(baseRow.getColumn(col + 1))) {\n+                            duplicate = true;\n+                            break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 140}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwOTMwNTQ3", "url": "https://github.com/splicemachine/spliceengine/pull/3463#pullrequestreview-410930547", "createdAt": "2020-05-13T13:36:04Z", "commit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzozNjowNFrOGUx60Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMzozNjowNFrOGUx60Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ0MjU3Nw==", "bodyText": "i still don't understand how this function is fixing any duplicate indices.\nit does pretty much the same as reportDuplicateIndexes when maxCheckTableError = infinity (well, except for the \"Deleted the following %d duplicate indexes\" / \"The following %d indexes are duplicates\" ).", "url": "https://github.com/splicemachine/spliceengine/pull/3463#discussion_r424442577", "createdAt": "2020-05-13T13:36:04Z", "author": {"login": "martinrupp"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkTableChecker.java", "diffHunk": "@@ -148,32 +148,62 @@ public void setTableDataSet(DataSet tableDataSet) {\n      * @throws InterruptedException\n      * @throws ExecutionException\n      */\n-    private List<String> checkDuplicateIndexes(PairDataSet index) throws StandardException, InterruptedException, ExecutionException {\n-        List<String> messages = Lists.newLinkedList();\n-        SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n-        JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet)index).rdd\n-                .combineByKey(new createCombiner(), new mergeValue(), new mergeCombiners())\n-                .filter(new DuplicateIndexFilter());\n+    private List<String> checkDuplicateIndexes(PairDataSet table, PairDataSet index) throws StandardException {\n+        try {\n+            SpliceSpark.pushScope(String.format(\"Check duplicates in index %s.%s\", schemaName, indexName));\n+            JavaPairRDD duplicateIndexRdd = ((SparkPairDataSet) index).rdd\n+                    .combineByKey(new CreateCombiner(), new MergeValue(), new MergeCombiners())\n+                    .filter(new DuplicateIndexFilter());\n \n-        long count = 0;\n-        JavaFutureAction<List> futureAction = duplicateIndexRdd.takeAsync(maxCheckTableError);\n-        List<Tuple2<String, List<byte[]>>> result = futureAction.get();\n-        for(Tuple2<String, List<byte[]>> tuple : result) {\n-            List<byte[]> keys = tuple._2;\n-            for (byte[] key : keys) {\n-                indexKeyDecoder.set(key, 0, key.length);\n-                indexKeyDecoder.decode(indexKeyTemplate);\n-                messages.add(indexKeyTemplate.getClone().toString() + \"=>\" + tuple._1);\n-                count++;\n+            JavaPairRDD joinedRdd = duplicateIndexRdd\n+                    .join(((SparkPairDataSet) table).rdd);\n+\n+            JavaRDD duplicateIndex = joinedRdd\n+                    .mapPartitions(new SparkFlatMapFunction<>(new DeleteDuplicateIndexFunction<>(conglomerate, txn, tentativeIndex, baseColumnMap, fix)));\n+\n+            Iterator it = duplicateIndex.toLocalIterator();\n+\n+            if (fix) {\n+                return fixDuplicateIndexes(it);\n+            } else {\n+                return reportDuplicateIndexes(it);\n             }\n+        }catch (Exception e) {\n+            throw StandardException.plainWrapException(e);\n+        }\n+        finally {\n+            SpliceSpark.popScope();\n         }\n-        if (count >= maxCheckTableError) {\n-            count = duplicateIndexRdd.count();\n-            messages.add(\"...\");\n+    }\n+\n+\n+    private List<String> fixDuplicateIndexes(Iterator it) {\n+        List<String> messages = Lists.newLinkedList();\n+\n+        long count = 0;\n+        while (it.hasNext()) {\n+            Tuple2<String, Tuple2<byte[], ExecRow>> t = (Tuple2<String, Tuple2<byte[], ExecRow>>)it.next();\n+            messages.add(t._2._2 + \"=>\" + t._1);\n+            count++;\n         }\n+        messages.add(0,String.format(\"Deleted the following %d duplicate indexes\", count));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38166536439f08fddfb6730303779d8ba941eca6"}, "originalPosition": 132}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExMjE1MDAy", "url": "https://github.com/splicemachine/spliceengine/pull/3463#pullrequestreview-411215002", "createdAt": "2020-05-13T19:01:46Z", "commit": {"oid": "6b847f9849a43b6baaab10c0b9681106e8dc787c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f885d37fd0e4c552dd86485df0d4f9a13310b5a", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/0f885d37fd0e4c552dd86485df0d4f9a13310b5a", "committedDate": "2020-05-14T13:06:53Z", "message": "address review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6b847f9849a43b6baaab10c0b9681106e8dc787c", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/6b847f9849a43b6baaab10c0b9681106e8dc787c", "committedDate": "2020-05-13T16:03:30Z", "message": "address review comments"}, "afterCommit": {"oid": "0f885d37fd0e4c552dd86485df0d4f9a13310b5a", "author": {"user": null}, "url": "https://github.com/splicemachine/spliceengine/commit/0f885d37fd0e4c552dd86485df0d4f9a13310b5a", "committedDate": "2020-05-14T13:06:53Z", "message": "address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExOTkwODgx", "url": "https://github.com/splicemachine/spliceengine/pull/3463#pullrequestreview-411990881", "createdAt": "2020-05-14T16:42:08Z", "commit": {"oid": "0f885d37fd0e4c552dd86485df0d4f9a13310b5a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNDU4MDk2", "url": "https://github.com/splicemachine/spliceengine/pull/3463#pullrequestreview-412458096", "createdAt": "2020-05-15T08:43:16Z", "commit": {"oid": "0f885d37fd0e4c552dd86485df0d4f9a13310b5a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1374, "cost": 1, "resetAt": "2021-11-02T10:47:05Z"}}}