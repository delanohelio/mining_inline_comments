{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0ODUxNTgw", "number": 4449, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOTo0MDoxMFrOE09C6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOTo0MDoxMFrOE09C6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTYxNTc5OnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkUtils.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOTo0MDoxMFrOHs9y8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNDo0Njo1N1rOHtI_YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkxMTg1OQ==", "bodyText": "Do we need toUpperCase() here? It does not seem consistent with the subsequent HashSet logic, which honors case sensitivity.", "url": "https://github.com/splicemachine/spliceengine/pull/4449#discussion_r516911859", "createdAt": "2020-11-03T19:40:10Z", "author": {"login": "yxia92"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkUtils.java", "diffHunk": "@@ -224,19 +220,37 @@ public ExecRow call(ExecRow row) throws Exception {\n         SpliceBaseOperation operation = (SpliceBaseOperation) serverSideResultSet;\n         DataSetProcessor dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n         DataSet<ExecRow> spliceDataSet = operation.getResultDataSet(dsp);\n-        if(spliceDataSet instanceof SparkDataSet) {\n-            JavaRDD<ExecRow> rdd = ((SparkDataSet)spliceDataSet).rdd;\n-            final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n-\n-            // Generate the schema based on the ResultColumnDescriptors\n-            List<StructField> fields = new ArrayList<>();\n+        \n+        final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n+        // Generate the schema based on the ResultColumnDescriptors\n+        List<StructField> fields = new ArrayList<>();\n+        for (ResultColumnDescriptor column : columns) {\n+            fields.add(column.getStructField());\n+        }\n+        if( fields.stream().map( f -> f.name().toUpperCase() ).distinct().count() != fields.size() ) {  // has duplicate names", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a16dfbe5453916854c1c837e9fe0adf2c0ba8175"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NzAyMQ==", "bodyText": "I think I could leave that and change the HashSet uses to used.contains(name.toUpperCase()) and used.add(name.toUpperCase()).  I would also leave the case of the name passed to StructField unchanged.", "url": "https://github.com/splicemachine/spliceengine/pull/4449#discussion_r516957021", "createdAt": "2020-11-03T21:09:51Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkUtils.java", "diffHunk": "@@ -224,19 +220,37 @@ public ExecRow call(ExecRow row) throws Exception {\n         SpliceBaseOperation operation = (SpliceBaseOperation) serverSideResultSet;\n         DataSetProcessor dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n         DataSet<ExecRow> spliceDataSet = operation.getResultDataSet(dsp);\n-        if(spliceDataSet instanceof SparkDataSet) {\n-            JavaRDD<ExecRow> rdd = ((SparkDataSet)spliceDataSet).rdd;\n-            final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n-\n-            // Generate the schema based on the ResultColumnDescriptors\n-            List<StructField> fields = new ArrayList<>();\n+        \n+        final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n+        // Generate the schema based on the ResultColumnDescriptors\n+        List<StructField> fields = new ArrayList<>();\n+        for (ResultColumnDescriptor column : columns) {\n+            fields.add(column.getStructField());\n+        }\n+        if( fields.stream().map( f -> f.name().toUpperCase() ).distinct().count() != fields.size() ) {  // has duplicate names", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkxMTg1OQ=="}, "originalCommit": {"oid": "a16dfbe5453916854c1c837e9fe0adf2c0ba8175"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMDg2Mw==", "bodyText": "Thanks @jpanko1 ! We could have case sensitive column names. For example:\nsplice> create table t1 (\"a1\" int, \"A1\" int); \n0 rows inserted/updated/deleted\nELAPSED TIME = 19 milliseconds\nsplice> select * from t1;\na1         |A1         \n-----------------------\n\n0 rows selected\nELAPSED TIME = 28 milliseconds\n\nAfter the binding phase, all the regular column names should have already been in the UPPER case except for the case sensitive column names. With that consideration, should we remove the toUpperCase() call?", "url": "https://github.com/splicemachine/spliceengine/pull/4449#discussion_r517030863", "createdAt": "2020-11-04T00:22:30Z", "author": {"login": "yxia92"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkUtils.java", "diffHunk": "@@ -224,19 +220,37 @@ public ExecRow call(ExecRow row) throws Exception {\n         SpliceBaseOperation operation = (SpliceBaseOperation) serverSideResultSet;\n         DataSetProcessor dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n         DataSet<ExecRow> spliceDataSet = operation.getResultDataSet(dsp);\n-        if(spliceDataSet instanceof SparkDataSet) {\n-            JavaRDD<ExecRow> rdd = ((SparkDataSet)spliceDataSet).rdd;\n-            final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n-\n-            // Generate the schema based on the ResultColumnDescriptors\n-            List<StructField> fields = new ArrayList<>();\n+        \n+        final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n+        // Generate the schema based on the ResultColumnDescriptors\n+        List<StructField> fields = new ArrayList<>();\n+        for (ResultColumnDescriptor column : columns) {\n+            fields.add(column.getStructField());\n+        }\n+        if( fields.stream().map( f -> f.name().toUpperCase() ).distinct().count() != fields.size() ) {  // has duplicate names", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkxMTg1OQ=="}, "originalCommit": {"oid": "a16dfbe5453916854c1c837e9fe0adf2c0ba8175"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA4NTE3Mg==", "bodyText": "I see, thanks Yi!  I've removed toUpperCase().\nI also checked about case sensitivity in Spark and found that it can be set either way with a param: spark.sqlContext.setConf(\"spark.sql.caseSensitive\", \"true\").", "url": "https://github.com/splicemachine/spliceengine/pull/4449#discussion_r517085172", "createdAt": "2020-11-04T04:00:33Z", "author": {"login": "jpanko1"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkUtils.java", "diffHunk": "@@ -224,19 +220,37 @@ public ExecRow call(ExecRow row) throws Exception {\n         SpliceBaseOperation operation = (SpliceBaseOperation) serverSideResultSet;\n         DataSetProcessor dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n         DataSet<ExecRow> spliceDataSet = operation.getResultDataSet(dsp);\n-        if(spliceDataSet instanceof SparkDataSet) {\n-            JavaRDD<ExecRow> rdd = ((SparkDataSet)spliceDataSet).rdd;\n-            final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n-\n-            // Generate the schema based on the ResultColumnDescriptors\n-            List<StructField> fields = new ArrayList<>();\n+        \n+        final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n+        // Generate the schema based on the ResultColumnDescriptors\n+        List<StructField> fields = new ArrayList<>();\n+        for (ResultColumnDescriptor column : columns) {\n+            fields.add(column.getStructField());\n+        }\n+        if( fields.stream().map( f -> f.name().toUpperCase() ).distinct().count() != fields.size() ) {  // has duplicate names", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkxMTg1OQ=="}, "originalCommit": {"oid": "a16dfbe5453916854c1c837e9fe0adf2c0ba8175"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA5NTI2NA==", "bodyText": "Thanks @jpanko1 for checking!", "url": "https://github.com/splicemachine/spliceengine/pull/4449#discussion_r517095264", "createdAt": "2020-11-04T04:46:57Z", "author": {"login": "yxia92"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkUtils.java", "diffHunk": "@@ -224,19 +220,37 @@ public ExecRow call(ExecRow row) throws Exception {\n         SpliceBaseOperation operation = (SpliceBaseOperation) serverSideResultSet;\n         DataSetProcessor dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n         DataSet<ExecRow> spliceDataSet = operation.getResultDataSet(dsp);\n-        if(spliceDataSet instanceof SparkDataSet) {\n-            JavaRDD<ExecRow> rdd = ((SparkDataSet)spliceDataSet).rdd;\n-            final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n-\n-            // Generate the schema based on the ResultColumnDescriptors\n-            List<StructField> fields = new ArrayList<>();\n+        \n+        final ResultColumnDescriptor[] columns = serverSideResultSet.getResultDescription().getColumnInfo();\n+        // Generate the schema based on the ResultColumnDescriptors\n+        List<StructField> fields = new ArrayList<>();\n+        for (ResultColumnDescriptor column : columns) {\n+            fields.add(column.getStructField());\n+        }\n+        if( fields.stream().map( f -> f.name().toUpperCase() ).distinct().count() != fields.size() ) {  // has duplicate names", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkxMTg1OQ=="}, "originalCommit": {"oid": "a16dfbe5453916854c1c837e9fe0adf2c0ba8175"}, "originalPosition": 37}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2857, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}