{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY4MzkzOTc2", "number": 3168, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxMTozNToxOFrODb4mkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxMTo0NzoxNFrODb4yUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNTY1NTIwOnYy", "diffSide": "RIGHT", "path": "db-engine/src/main/java/com/splicemachine/db/iapi/sql/compile/CompilerContext.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxMTozNToxOFrOFjn9_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxODowMDo0MlrOFj0e4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjg5OTMyNg==", "bodyText": "Why is this change needed?", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r372899326", "createdAt": "2020-01-30T11:35:18Z", "author": {"login": "dgomezferro"}, "path": "db-engine/src/main/java/com/splicemachine/db/iapi/sql/compile/CompilerContext.java", "diffHunk": "@@ -656,9 +656,9 @@\n \t */\n \tboolean isReferenced(SequenceDescriptor sd);\n \n-    void setDataSetProcessorType(DataSetProcessorType type);\n+\tvoid setDataSetProcessorType(DataSetProcessorType type, boolean setDSPTypeinLCC);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA1ODgxOA==", "bodyText": "I think this is an old change I made to avoid using control when we're running in the Olap Server.  But I have since fixed that in CostChoosingDataSetProcessorFactory, so I will remove the above change.", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r373058818", "createdAt": "2020-01-30T16:35:22Z", "author": {"login": "msirek"}, "path": "db-engine/src/main/java/com/splicemachine/db/iapi/sql/compile/CompilerContext.java", "diffHunk": "@@ -656,9 +656,9 @@\n \t */\n \tboolean isReferenced(SequenceDescriptor sd);\n \n-    void setDataSetProcessorType(DataSetProcessorType type);\n+\tvoid setDataSetProcessorType(DataSetProcessorType type, boolean setDSPTypeinLCC);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjg5OTMyNg=="}, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEwNDM1Mg==", "bodyText": "I tried backing out this change, but it broke some Spark Explain tests. CostChoosingDataSetProcessorFactory.chooseProcessor() pulls the DataSetProcessorType from the lcc to determine whether to run on control or spark.  The lcc is not properly updated in all flows, so that is the intent of this change, to make sure that the DataSetProcessorType we've picked in the CompilerContext is propagated to the lcc.  Spark Explain needs to be executed on the Olap Server, so the lcc must always have the proper DataSetProcessorType setting.", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r373104352", "createdAt": "2020-01-30T18:00:42Z", "author": {"login": "msirek"}, "path": "db-engine/src/main/java/com/splicemachine/db/iapi/sql/compile/CompilerContext.java", "diffHunk": "@@ -656,9 +656,9 @@\n \t */\n \tboolean isReferenced(SequenceDescriptor sd);\n \n-    void setDataSetProcessorType(DataSetProcessorType type);\n+\tvoid setDataSetProcessorType(DataSetProcessorType type, boolean setDSPTypeinLCC);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjg5OTMyNg=="}, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNTY2NDgxOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxMTozOTowMVrOFjoD6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxNjozNDoyOFrOFjxrLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwMDg0MA==", "bodyText": "You could return an empty dataset instead if this could be a problem, sampling might actually read the data from Splice anyway", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r372900840", "createdAt": "2020-01-30T11:39:01Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "diffHunk": "@@ -102,6 +101,14 @@ else if (storedAs.equals(\"O\"))\n                 // The predicates have variant qualifiers (or we are reading ORC with our own reader), we couldn't push them down to the scan, process them here\n                 return locatedRows.filter(new TableScanPredicateFunction<>(operationContext));\n             }\n+            if (dsp.isSparkExplain()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA1NjExMA==", "bodyText": "Thanks, I'll keep this in mind.  So far it's not an issue.  I'm not sure if this would change what's printed in the spark explain, for example, would it still indicate which parquet table is being read from for external tables, etc.", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r373056110", "createdAt": "2020-01-30T16:30:45Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "diffHunk": "@@ -102,6 +101,14 @@ else if (storedAs.equals(\"O\"))\n                 // The predicates have variant qualifiers (or we are reading ORC with our own reader), we couldn't push them down to the scan, process them here\n                 return locatedRows.filter(new TableScanPredicateFunction<>(operationContext));\n             }\n+            if (dsp.isSparkExplain()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwMDg0MA=="}, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA1ODM1MQ==", "bodyText": "That's a good point, it wouldn't.", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r373058351", "createdAt": "2020-01-30T16:34:28Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "diffHunk": "@@ -102,6 +101,14 @@ else if (storedAs.equals(\"O\"))\n                 // The predicates have variant qualifiers (or we are reading ORC with our own reader), we couldn't push them down to the scan, process them here\n                 return locatedRows.filter(new TableScanPredicateFunction<>(operationContext));\n             }\n+            if (dsp.isSparkExplain()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwMDg0MA=="}, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNTY2NTUwOnYy", "diffSide": "RIGHT", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxMTozOToxN1rOFjoEVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxODowNDowOFrOFj0lFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwMDk1MA==", "bodyText": "I'd either remove these blocks or make them return an empty dataset", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r372900950", "createdAt": "2020-01-30T11:39:17Z", "author": {"login": "dgomezferro"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "diffHunk": "@@ -113,7 +120,19 @@ else if (storedAs.equals(\"O\"))\n         if (oneSplitPerRegion) {\n             conf.set(MRConstants.ONE_SPLIT_PER_REGION, \"true\");\n         }\n-        if (useSample) {\n+        if (dsp.isSparkExplain()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEwNTk0Mg==", "bodyText": "OK, I've reverted the changes.", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r373105942", "createdAt": "2020-01-30T18:04:08Z", "author": {"login": "msirek"}, "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkScanSetBuilder.java", "diffHunk": "@@ -113,7 +120,19 @@ else if (storedAs.equals(\"O\"))\n         if (oneSplitPerRegion) {\n             conf.set(MRConstants.ONE_SPLIT_PER_REGION, \"true\");\n         }\n-        if (useSample) {\n+        if (dsp.isSparkExplain()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwMDk1MA=="}, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwNTY4NTI5OnYy", "diffSide": "RIGHT", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/execute/operations/RowCountOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxMTo0NzoxNFrOFjoQfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxODowNjozNVrOFj0pnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwNDA2Mw==", "bodyText": "Couldn't you use here dsp.getEmpty() ?", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r372904063", "createdAt": "2020-01-30T11:47:14Z", "author": {"login": "dgomezferro"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/execute/operations/RowCountOperation.java", "diffHunk": "@@ -199,8 +202,31 @@ public void writeExternal(ObjectOutput out) throws IOException {\n         final long fetchLimit = getFetchLimit();\n         long offset = getTotalOffset();\n         OperationContext operationContext = dsp.createOperationContext(this);\n-        DataSet<ExecRow> sourceSet = source.getDataSet(dsp).map(new CloneFunction<>(operationContext));\n-        return sourceSet.zipWithIndex(operationContext).mapPartitions(new OffsetFunction<SpliceOperation, ExecRow>(operationContext, offset, fetchLimit));\n+        dsp.incrementOpDepth();\n+        DataSet<ExecRow> sourceDS = source.getDataSet(dsp);\n+        dsp.decrementOpDepth();\n+        DataSet<ExecRow> sourceSet = sourceDS.map(new CloneFunction<>(operationContext));\n+        if (dsp.isSparkExplain()) {\n+                        DataSet<ExecRow> ds = dsp.createDataSet(Iterators.transform(Collections.emptyIterator(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzEwNzEwMQ==", "bodyText": "Made the change.", "url": "https://github.com/splicemachine/spliceengine/pull/3168#discussion_r373107101", "createdAt": "2020-01-30T18:06:35Z", "author": {"login": "msirek"}, "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/execute/operations/RowCountOperation.java", "diffHunk": "@@ -199,8 +202,31 @@ public void writeExternal(ObjectOutput out) throws IOException {\n         final long fetchLimit = getFetchLimit();\n         long offset = getTotalOffset();\n         OperationContext operationContext = dsp.createOperationContext(this);\n-        DataSet<ExecRow> sourceSet = source.getDataSet(dsp).map(new CloneFunction<>(operationContext));\n-        return sourceSet.zipWithIndex(operationContext).mapPartitions(new OffsetFunction<SpliceOperation, ExecRow>(operationContext, offset, fetchLimit));\n+        dsp.incrementOpDepth();\n+        DataSet<ExecRow> sourceDS = source.getDataSet(dsp);\n+        dsp.decrementOpDepth();\n+        DataSet<ExecRow> sourceSet = sourceDS.map(new CloneFunction<>(operationContext));\n+        if (dsp.isSparkExplain()) {\n+                        DataSet<ExecRow> ds = dsp.createDataSet(Iterators.transform(Collections.emptyIterator(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjkwNDA2Mw=="}, "originalCommit": {"oid": "ad9939797a8379a8c86053fbc138b62d9384fab7"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3345, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}