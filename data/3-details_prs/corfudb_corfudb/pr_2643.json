{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzODMyODAz", "number": 2643, "title": "Config the message size for both snapshot full sync and delta sync.", "bodyText": "Overview\nDescription:\nWhy should this be merged:\nRelated issue(s) (if applicable): #\nChecklist (Definition of Done):\n\n There are no TODOs left in the code\n Coding conventions (e.g. for logging, unit tests) have been followed\n Change is covered by automated tests\n Public API has Javadoc", "createdAt": "2020-07-20T20:59:24Z", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643", "merged": true, "mergeCommit": {"oid": "885f6418e627a23abf94a7deeeda19aa03821b34"}, "closed": true, "closedAt": "2020-07-24T21:48:15Z", "author": {"login": "xiaoqin2012"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc23mLGAH2gAyNDUzODMyODAzOmFjMDVlZWRiOTc4MTVhMTY2MjQzZjc0YzUzMmFjMjRhY2ZjNWUxMWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc4HBiDgH2gAyNDUzODMyODAzOjExZDhlNWI3ODI0MGMyZjI0NjhkNjU2Y2Y0OGZmYzhjNjgwYWJhODY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ac05eedb97815a166243f74c532ac24acfc5e11a", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/ac05eedb97815a166243f74c532ac24acfc5e11a", "committedDate": "2020-07-20T20:24:28Z", "message": "Make snapshot message size configurable.\n   * Make the message size configurable.\n   * While reading a SMR entry, setup its size.\n   * While processing an opaque entry, get its size.\n   * Send snapshot message with the maxMessageSize."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18ed759aaf2de8917a1942c49171ca75628cf5e3", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/18ed759aaf2de8917a1942c49171ca75628cf5e3", "committedDate": "2020-07-20T20:26:23Z", "message": "Some cleanup:\n   * The function getSerializedSize works only for an opaque smr entry.\n   * While passing buffer, truncate it to the proper size."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cad850979100fa3d5b2b949aeebf820e32e3aac7", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/cad850979100fa3d5b2b949aeebf820e32e3aac7", "committedDate": "2020-07-20T20:26:50Z", "message": "Support the configuration for LogEntryDataMessage size.\n  * Add options for input of data message size.\n  * Add unit tests to verify the data message size is correctly set."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fdc9adda2a903080450bd42f219075d9f92e158b", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/fdc9adda2a903080450bd42f219075d9f92e158b", "committedDate": "2020-07-20T20:57:25Z", "message": "Rebase and merge the change."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a2d16f5a512623999593234d0b9d1ddf9d8a482", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/6a2d16f5a512623999593234d0b9d1ddf9d8a482", "committedDate": "2020-07-20T22:00:57Z", "message": "Some cleanup."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d1117d719dd92f90b0c17948a051bf1521d89f6", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/9d1117d719dd92f90b0c17948a051bf1521d89f6", "committedDate": "2020-07-20T22:43:56Z", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99e78c613f007d81a6b4d3d64b516da21e504e13", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/99e78c613f007d81a6b4d3d64b516da21e504e13", "committedDate": "2020-07-20T23:40:13Z", "message": "Fix issues with SMREntry size."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "committedDate": "2020-07-21T00:52:11Z", "message": "Change some logging level from info to debug."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyNzQxNTgz", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#pullrequestreview-452741583", "createdAt": "2020-07-21T18:54:51Z", "commit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "state": "COMMENTED", "comments": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxODo1NDo1MVrOG1FifQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzoxOToxMVrOG1NKcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxODQ2MQ==", "bodyText": "if this is only related to Log Replication, can we rename it to getLogReplication....()?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458318461", "createdAt": "2020-07-21T18:54:51Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -216,16 +219,30 @@ public String getPluginConfigFilePath() {\n         return pluginConfigFilePath == null ? PLUGIN_CONFIG_FILE_PATH : pluginConfigFilePath;\n     }\n \n-    public int getSnapshotSyncBatchSize() {\n-        Integer snapshotSyncBatchSize = getServerConfig(Integer.class, \"--snapshot-batch\");\n-        return snapshotSyncBatchSize == null ? SnapshotSender.DEFAULT_SNAPSHOT_BATCH_SIZE : snapshotSyncBatchSize;\n+    /**\n+     * Get the max number of messages can be sent over per batch.\n+     * @return\n+     */\n+    public int getMaxNumMsgPerBatch() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMDAyOQ==", "bodyText": "can we rename to DEFAULT_TIMEOUT_MS?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458320029", "createdAt": "2020-07-21T18:57:35Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -13,15 +13,40 @@\n @Data\n public class LogReplicationConfig {\n \n+    // Log Replication message timeout time in milliseconds.\n+    public static final int DEFAULT_TIMEOUT = 5000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMTg4OA==", "bodyText": "can we rename this to max_message_per_snapshot or snapshot_batch_num_messages?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458321888", "createdAt": "2020-07-21T19:00:55Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java", "diffHunk": "@@ -44,7 +44,8 @@\n                     + \"\\n\"\n                     + \"Usage:\\n\"\n                     + \"\\tlog_replication_server (-l <path>|-m) [-nsN] [-a <address>|-q <interface-name>] \"\n-                    + \"[--snapshot-batch=<batch-size>]\"\n+                    + \"[--snapshot-batch=<batch-size>] \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjI3OQ==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458322279", "createdAt": "2020-07-21T19:01:42Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java", "diffHunk": "@@ -171,14 +172,20 @@\n                     + \" --metrics-port=<metrics_port>                                            \"\n                     + \"              Metrics provider server port [default: 9999].\\n             \"\n                     + \" --snapshot-batch=<batch-size>                                            \"\n-                    + \"              Snapshot (Full) Sync batch size (number of entries)\\n       \"\n+                    + \"              Snapshot (Full) Sync batch size.\\n                          \"\n+                    + \"              The max number of messages per batch)\\n                      \"\n+                    + \"                                                                          \"\n+                    + \" --max-data-message-size=<msg-size>                                       \"\n+                    + \"              The max size of replication data message in bytes.\\n   \"\n+                    + \"                                                                          \"\n                     + \" --lock-lease=<lease-duration>                                            \"\n                     + \"              Lock lease duration in seconds\\n                            \"\n                     + \" -h, --help                                                               \"\n                     + \"              Show this screen\\n\"\n                     + \" --version                                                                \"\n                     + \"              Show version\\n\";\n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjkyNw==", "bodyText": "why did we reduce it?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458322927", "createdAt": "2020-07-21T19:02:53Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java", "diffHunk": "@@ -45,7 +45,7 @@ private DefaultClusterConfig() {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 20;\n+    private static int logSenderBufferSize = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMzkzMg==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458323932", "createdAt": "2020-07-21T19:04:38Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,25 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyODMzNQ==", "bodyText": "the comment says timestamp but we are checking on version.  Is it intended?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458328335", "createdAt": "2020-07-21T19:12:46Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntryWriter.java", "diffHunk": "@@ -98,21 +93,37 @@ void processMsg(LogReplicationEntry txMessage) {\n \n         lastMsgTs = Math.max(persistLogTS, lastMsgTs);\n \n+\n+        // If this entry's max timestamp is not bigger than the persistLogTs, skip the whole message.\n         if (topologyConfigId != persistSiteConfigID || ts != persistSnapStart || ts != persistSnapDone ||\n-                txMessage.getMetadata().getPreviousTimestamp() != persistLogTS) {\n+                entryTS <= persistLogTS) {\n             log.warn(\"Skip write this msg {} as its timestamp is later than the persisted one \" +\n                     txMessage.getMetadata() +  \" persisteSiteConfig \" + persistSiteConfigID + \" persistSnapStart \" + persistSnapStart +\n                     \" persistSnapDone \" + persistSnapDone + \" persistLogTs \" + persistLogTS);\n             return;\n         }\n \n+        // Skip Opaque entries with timestamp that are not larger than persistedTs\n+        OpaqueEntry[] newOpaqueEntryList = opaqueEntryList.stream().filter(x->x.getVersion() > persistLogTS).toArray(OpaqueEntry[]::new);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyOTUwOA==", "bodyText": "will opaque entry list contain all streams to be replicated, even if they do not have any data?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458329508", "createdAt": "2020-07-21T19:15:02Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntryWriter.java", "diffHunk": "@@ -98,21 +93,37 @@ void processMsg(LogReplicationEntry txMessage) {\n \n         lastMsgTs = Math.max(persistLogTS, lastMsgTs);\n \n+\n+        // If this entry's max timestamp is not bigger than the persistLogTs, skip the whole message.\n         if (topologyConfigId != persistSiteConfigID || ts != persistSnapStart || ts != persistSnapDone ||\n-                txMessage.getMetadata().getPreviousTimestamp() != persistLogTS) {\n+                entryTS <= persistLogTS) {\n             log.warn(\"Skip write this msg {} as its timestamp is later than the persisted one \" +\n                     txMessage.getMetadata() +  \" persisteSiteConfig \" + persistSiteConfigID + \" persistSnapStart \" + persistSnapStart +\n                     \" persistSnapDone \" + persistSnapDone + \" persistLogTs \" + persistLogTS);\n             return;\n         }\n \n+        // Skip Opaque entries with timestamp that are not larger than persistedTs\n+        OpaqueEntry[] newOpaqueEntryList = opaqueEntryList.stream().filter(x->x.getVersion() > persistLogTS).toArray(OpaqueEntry[]::new);\n+\n+        // Check that all opaque entries contain the correct streams\n+        for (OpaqueEntry opaqueEntry : newOpaqueEntryList) {\n+            if (!streamMap.keySet().containsAll(opaqueEntry.getEntries().keySet())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMDkxNg==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458330916", "createdAt": "2020-07-21T19:17:36Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java", "diffHunk": "@@ -131,24 +133,25 @@ public LogReplicationSinkManager(String localCorfuEndpoint, LogReplicationConfig\n          */\n         this.rxState = RxState.LOG_ENTRY_SYNC;\n         this.config = config;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMTExMQ==", "bodyText": "nit - extra newline.  Also, since this is measuring time, can we rename to something like default_ack_delay_ms or something else?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458331111", "createdAt": "2020-07-21T19:18:00Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java", "diffHunk": "@@ -36,10 +36,12 @@\n      */\n     private static final String config_file = \"/config/corfu/corfu_replication_config.properties\";\n \n+    private final int DEFAULT_ACK_CNT = 1;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNTYxMA==", "bodyText": "sequencer -> sequence number", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458335610", "createdAt": "2020-07-21T19:26:33Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java", "diffHunk": "@@ -178,23 +178,28 @@ public void apply(LogReplicationEntry message) {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n-                    message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n+            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNzA5OQ==", "bodyText": "we expect the opaqueEntry size == 1.  Why is the iteration needed?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458337099", "createdAt": "2020-07-21T19:29:19Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java", "diffHunk": "@@ -178,23 +178,28 @@ public void apply(LogReplicationEntry message) {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n-                    message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n+            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n+                    message.getMetadata(), message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n                     message.getMetadata().getMessageMetadataType(), MessageType.SNAPSHOT_MESSAGE);\n             throw new ReplicationWriterException(\"Message is out of order or wrong type\");\n         }\n \n-        byte[] payload = message.getPayload();\n-        OpaqueEntry opaqueEntry = OpaqueEntry.deserialize(Unpooled.wrappedBuffer(payload));\n-\n-        if (opaqueEntry.getEntries().keySet().size() != 1) {\n-            log.error(\"The opaqueEntry has more than one entry {}\", opaqueEntry);\n+        // For snapshot message, it has only one opaque entry.\n+        if (message.getOpaqueEntryList().size() > 1) {\n+            log.error(\" Get {} instead of one opaque entry in Snapshot Message\", message.getOpaqueEntryList().size());\n             return;\n         }\n \n-        UUID uuid = opaqueEntry.getEntries().keySet().stream().findFirst().get();\n-        processOpaqueEntry(opaqueEntry.getEntries().get(uuid), message.getMetadata().getSnapshotSyncSeqNum(), uuidMap.get(uuid));\n-        recvSeq++;\n+        for (OpaqueEntry opaqueEntry : message.getOpaqueEntryList()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NDQ4NA==", "bodyText": "static import of a method is in general not a recommended practice..", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458344484", "createdAt": "2020-07-21T19:43:55Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -14,11 +13,16 @@\n import org.corfudb.runtime.view.stream.OpaqueStream;\n \n import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.ArrayList;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n import java.util.UUID;\n \n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE;\n+import static org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsSnapshotReader.calculateSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NDgxOA==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458344818", "createdAt": "2020-07-21T19:44:27Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NTQzNA==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458345434", "createdAt": "2020-07-21T19:45:33Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NTczNA==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458345734", "createdAt": "2020-07-21T19:46:11Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+        // Set the last timestamp as the max timestamp\n+        currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NjM2Ng==", "bodyText": "nit - can we remove the extra newlines?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458346366", "createdAt": "2020-07-21T19:47:23Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -89,34 +107,88 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n                     entry.getEntries().keySet(), streamUUIDs);\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        hasNoiseData = true;\n+        return false;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+    private int calculateOpaqueEntrySize(OpaqueEntry opaqueEntry) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MDE1NA==", "bodyText": "since this is used by both LogEntryReader and SnapshotReader, can we move it to a common place?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458360154", "createdAt": "2020-07-21T20:13:43Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MTA4MQ==", "bodyText": "why do we check for both default size and user-configured size?  It should be either one, right?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458361081", "createdAt": "2020-07-21T20:15:22Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjE4NQ==", "bodyText": "this is not an error.  the entry will get sent in the next batch.  Can we change to debug or trace?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458362185", "createdAt": "2020-07-21T20:17:28Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ==", "bodyText": "debug or trace?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458362371", "createdAt": "2020-07-21T20:17:50Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE);\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MzEyNQ==", "bodyText": "can it be private?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458363125", "createdAt": "2020-07-21T20:19:16Z", "author": {"login": "pankti-m"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -223,4 +281,21 @@ public void setTopologyConfigId(long topologyConfigId) {\n         this.topologyConfigId = topologyConfigId;\n     }\n \n+    /**\n+     * Record a list of SMR entries\n+     */\n+    static class SMREntryList {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjEwNQ==", "bodyText": "can we change the method name to something like calculateOpaqueSMRSerializedSize or something of that kind?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458422105", "createdAt": "2020-07-21T22:20:08Z", "author": {"login": "pankti-m"}, "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +146,42 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateSerializedSize() {\n+        if (!opaque) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjg5Ng==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458422896", "createdAt": "2020-07-21T22:22:03Z", "author": {"login": "pankti-m"}, "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +146,42 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateSerializedSize() {\n+        if (!opaque) {\n+            log.error(\"This operation only supported for an opaque SMR entry\");\n+            return 0;\n+        }\n+\n+        int size = 0;\n+\n+        for (Object smrArg : SMRArguments) {\n+            size += ((byte[])smrArg).length;\n+        }\n+\n+        size += (SMRMethod.length() * Character.BYTES);\n+        size += Integer.BYTES;\n+\n+        return size;\n+    }\n+\n+    /**\n+     * The serialized size of an opaque SMR entry.\n+     * @return\n+     */\n+    public synchronized int getSerializedSize() {\n+        if (serializedSize == null) {\n+            serializedSize = calculateSerializedSize();\n+        }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyNDEzMQ==", "bodyText": "nit - extra newline", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458424131", "createdAt": "2020-07-21T22:25:04Z", "author": {"login": "pankti-m"}, "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -190,6 +238,8 @@ public void serialize(ByteBuf b) {\n                     b.writeInt(length);\n                     b.writerIndex(lengthIndex + length + 4);\n                 });\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ==", "bodyText": "can we import logReplicationConfig instead?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432289", "createdAt": "2020-07-21T22:45:53Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws\n+            TrimmedException {\n+        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n+        StreamsLogEntryReader reader = new StreamsLogEntryReader(rt, config);\n+        reader.setGlobalBaseSnapshot(Address.NON_ADDRESS, Address.NON_ADDRESS);\n+\n+        LogReplicationEntry entry = null;\n+\n+        do {\n+            entry = reader.read(UUID.randomUUID());\n+\n+            if (entry != null) {\n+                msgQ.add(entry);\n+            }\n+\n+            System.out.println(\" msgQ size \" + msgQ.size());\n+\n+        } while (entry != null);\n+    }\n+\n+    public static void writeLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n+        org.corfudb.infrastructure.logreplication.LogReplicationConfig config = new LogReplicationConfig(streams);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjU2NA==", "bodyText": "is this a test?  what does it test?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432564", "createdAt": "2020-07-21T22:46:44Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjY4OA==", "bodyText": "is this a test?  what does it test?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432688", "createdAt": "2020-07-21T22:47:07Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws\n+            TrimmedException {\n+        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n+        StreamsLogEntryReader reader = new StreamsLogEntryReader(rt, config);\n+        reader.setGlobalBaseSnapshot(Address.NON_ADDRESS, Address.NON_ADDRESS);\n+\n+        LogReplicationEntry entry = null;\n+\n+        do {\n+            entry = reader.read(UUID.randomUUID());\n+\n+            if (entry != null) {\n+                msgQ.add(entry);\n+            }\n+\n+            System.out.println(\" msgQ size \" + msgQ.size());\n+\n+        } while (entry != null);\n+    }\n+\n+    public static void writeLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n+        org.corfudb.infrastructure.logreplication.LogReplicationConfig config = new LogReplicationConfig(streams);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ=="}, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng==", "bodyText": "how was this number derived?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458433056", "createdAt": "2020-07-21T22:48:04Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private static final int SHUTDOWN_RETRIES = 10;\n     private static final long SHUTDOWN_RETRY_WAIT = 500;\n \n+    private static final int MSG_SIZE = 131072;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzUxMA==", "bodyText": "remove", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458433510", "createdAt": "2020-07-21T22:49:24Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -556,7 +564,7 @@ public String getOptionsString() {\n          * @throws IOException\n          */\n         public Process runServer() throws IOException {\n-            final String serverConsoleLogPath = CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";\n+            final String serverConsoleLogPath = \"/Users/maxi/Projects/tmp/test.result\"; //CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNzg0Mw==", "bodyText": "what is the purpose of this method?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458437843", "createdAt": "2020-07-21T23:02:03Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -311,6 +324,16 @@ void verifyTables(HashMap<String, CorfuTable<Long, Long>> tables0, HashMap<Strin\n             }\n     }\n \n+    void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MTkyOA==", "bodyText": "numKeys -> NUM_KEYS", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458441928", "createdAt": "2020-07-21T23:14:42Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1042,9 +1094,12 @@ public void testLogEntrySyncLargeTables() throws Exception {\n \n     /* ********************** AUXILIARY METHODS ********************** */\n \n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys) throws Exception {\n+        generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MjAyNQ==", "bodyText": "numKeys -> NUM_KEYS", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458442025", "createdAt": "2020-07-21T23:14:57Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1042,9 +1094,12 @@ public void testLogEntrySyncLargeTables() throws Exception {\n \n     /* ********************** AUXILIARY METHODS ********************** */\n \n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys) throws Exception {\n+        generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);\n+    }\n \n-    // startCrossTx indicates if we start with a transaction across Tables\n-    private void testSnapshotSyncCrossTables(Set<String> crossTableTransactions, boolean startCrossTx) throws Exception {\n+        // startCrossTx indicates if we start with a transaction across Tables\n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys, int startValue) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 274}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0Mjg4OA==", "bodyText": "numKeys -> NUM_KEYS", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458442888", "createdAt": "2020-07-21T23:17:40Z", "author": {"login": "pankti-m"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1054,20 +1109,20 @@ private void testSnapshotSyncCrossTables(Set<String> crossTableTransactions, boo\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n         if (startCrossTx) {\n-            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n+            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, startValue);\n         }\n \n         // Write data to t0\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n \n         // Write data to t1\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n \n         // Write data to t2\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, numKeys, srcDataRuntime, 0);\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n-        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS*2);\n+        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, numKeys*2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MzM3OA==", "bodyText": "was this intentional?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458443378", "createdAt": "2020-07-21T23:19:11Z", "author": {"login": "pankti-m"}, "path": "test/src/test/resources/logback-test.xml", "diffHunk": "@@ -43,9 +43,9 @@\n         <!--<appender-ref ref=\"MetricsRollingFile\" />-->\n     </logger>\n \n-    <!--<root level=\"DEBUG\">-->\n-        <!--&lt;!&ndash;<appender-ref ref=\"FILE\" />&ndash;&gt;-->\n-        <!--<appender-ref ref=\"STDOUT\" />-->\n-        <!--&lt;!&ndash;<appender-ref ref=\"MetricsRollingFile\" />&ndash;&gt;-->\n-    <!--</root>-->\n+    <root level=\"INFO\">", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/49dc824b6fca4e7bc9c5639308f6db6440016cc1", "committedDate": "2020-07-22T23:39:13Z", "message": "Address comments and fix tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "committedDate": "2020-07-23T04:26:00Z", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/558d9bc783fa25db0791b58efbb4e33da951a052", "committedDate": "2020-07-23T05:06:42Z", "message": "Fix compiling issues."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzNzc4Njc2", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#pullrequestreview-453778676", "createdAt": "2020-07-23T00:58:29Z", "commit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "state": "COMMENTED", "comments": {"totalCount": 48, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMDo1ODoyOVrOG15I5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNTo1MToxMFrOG18_CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2Mzg3OA==", "bodyText": "Thinking on the readers. I'm not sure the right words, because even I was confused the first time I reviewed cause I understood that if it is set to 5 we send 5 messages embedded in a single message (a batch of 5). But, we use it is to send 5 messages and yield the thread, but 5 messages and not 1 fat one. Maybe it's more accurate to say: \"Get the max number of messages sent per Log Replication Runtime in between FSM worker thread yield\" (IDK just giving ideas, something in that line)", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459163878", "createdAt": "2020-07-23T00:58:29Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -216,16 +219,30 @@ public String getPluginConfigFilePath() {\n         return pluginConfigFilePath == null ? PLUGIN_CONFIG_FILE_PATH : pluginConfigFilePath;\n     }\n \n-    public int getSnapshotSyncBatchSize() {\n-        Integer snapshotSyncBatchSize = getServerConfig(Integer.class, \"--snapshot-batch\");\n-        return snapshotSyncBatchSize == null ? SnapshotSender.DEFAULT_SNAPSHOT_BATCH_SIZE : snapshotSyncBatchSize;\n+    /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NDA0Nw==", "bodyText": "Can we shorten it? Since it already has \"LogReplication\" from LogReplicationConfig. --> maybe MAX_DATA_MSG_SIZE", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459164047", "createdAt": "2020-07-23T00:59:11Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -44,6 +43,8 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NDc2NQ==", "bodyText": "message -> messages", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459164765", "createdAt": "2020-07-23T01:02:08Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -13,15 +13,40 @@\n @Data\n public class LogReplicationConfig {\n \n+    // Log Replication message timeout time in milliseconds.\n+    public static final int DEFAULT_TIMEOUT_MS = 5000;\n+\n+    // Log Replication default max number of message generated at the active cluster for each batch.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng==", "bodyText": "We can simplify with:\nthis(streamsToReplicate, DEFAULT_MAX_NUM_MSG_PER_BATCH, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459170886", "createdAt": "2020-07-23T01:27:11Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -30,16 +55,21 @@\n      */\n     public LogReplicationConfig(Set<String> streamsToReplicate) {\n         this.streamsToReplicate = streamsToReplicate;\n+        this.maxNumMsgPerBatch = DEFAULT_MAX_NUM_MSG_PER_BATCH;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTE1Ng==", "bodyText": "What is this buffer?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171156", "createdAt": "2020-07-23T01:28:35Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java", "diffHunk": "@@ -45,7 +45,7 @@ private DefaultClusterConfig() {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 20;\n+    private static int logSenderBufferSize = 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTMxOA==", "bodyText": "Can we add a comment. Not sure what this means.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171318", "createdAt": "2020-07-23T01:29:19Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/fsm/TestLogEntryReader.java", "diffHunk": "@@ -26,4 +26,9 @@ public void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp) {\n     public void setTopologyConfigId(long siteConfigID) {\n \n     }\n+\n+    @Override\n+    public boolean hasNoiseData() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTkyOQ==", "bodyText": "Shouldn't we remove metadata.getTimestamp?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171929", "createdAt": "2020-07-23T01:32:06Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it\n+                buffer.remove(metadata.getPreviousTimestamp());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTU2OQ==", "bodyText": "remove lastProcessedSeq or metadata.getTimestamp? as lastProcessedSeq is the one that has already been processed and we just applied entry.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459181569", "createdAt": "2020-07-23T02:16:46Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it\n+                buffer.remove(metadata.getPreviousTimestamp());\n+            } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n+                sinkManager.processMessage(entry);\n+                ackCnt++;\n+                buffer.remove(lastProcessedSeq);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MjA4Ng==", "bodyText": "This comment might be redundant.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459182086", "createdAt": "2020-07-23T02:19:01Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4Mjk1MQ==", "bodyText": "<= , right? cause if the same last processed entry was resent we can remove or we'll have a leak.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459182951", "createdAt": "2020-07-23T02:22:28Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ==", "bodyText": "What happens if there are remaining messages in the queue which are skipped because they were not the subsequent sequence numbers and no further messages are received? would we ever apply them? Let me illustrate it. For instance:\nUpdates to Stream A: 0, 1, 2, 5, 7, 8\nlastProcessedSeqNum = 2\nLet's say the buffer looks like this (as some messages were resent and they are out of order):\n0, 2, 1, 0, 7, 8, 5\nWe ignore 0, 2, 1, 0... We leave 7 in the buffer but do not process, same with 8... Then we process 5. But now 7 and 8 are left in the queue. And if no more data is received for an hour, 7 and 8 won't be applied, as this is kicked off as part of receiving data.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459183539", "createdAt": "2020-07-23T02:25:25Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTk5Mg==", "bodyText": "mmm I kept looking and I see we have a processBuffer in SinkBufferManager, which would take care of out of order messages, but one question, in that method we get the lastProcessedSeqNum, isn't that the one that we already applied, and that should not be available anymore in the buffer? Thus, we are not really processing the unordered until a new message is received?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459185992", "createdAt": "2020-07-23T02:37:24Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODgzNw==", "bodyText": "haven't -> haven't been applied", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459188837", "createdAt": "2020-07-23T02:51:15Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java", "diffHunk": "@@ -141,7 +141,8 @@ public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage)\n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        if (preTs == lastProcessedSeq) {\n+        // This message contains entries that haven't applied yet", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mjk5Nw==", "bodyText": "Can this happen because a LogEntryMessage might have numerous delta's inside? and it picks the min of all as the previous and the max of all as the timestamp?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459192997", "createdAt": "2020-07-23T03:11:31Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java", "diffHunk": "@@ -141,7 +141,8 @@ public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage)\n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        if (preTs == lastProcessedSeq) {\n+        // This message contains entries that haven't applied yet\n+        if (preTs <= lastProcessedSeq && currentTs > lastProcessedSeq) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NjE0Mw==", "bodyText": "Can you please add a description to this.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459196143", "createdAt": "2020-07-23T03:26:45Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java", "diffHunk": "@@ -23,4 +23,6 @@\n     void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp);\n \n     void setTopologyConfigId(long topologyConfigId);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NzEwMg==", "bodyText": "This class can be removed right?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459197102", "createdAt": "2020-07-23T03:31:16Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/ReplicationReaderWriterWithUFOIT.java", "diffHunk": "@@ -0,0 +1,4 @@\n+package org.corfudb.integration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NzE5Nw==", "bodyText": "Please don't push this.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459197197", "createdAt": "2020-07-23T03:31:42Z", "author": {"login": "annym"}, "path": "test/src/test/resources/logback-test.xml", "diffHunk": "@@ -43,6 +43,12 @@\n         <!--<appender-ref ref=\"MetricsRollingFile\" />-->\n     </logger>\n \n+    <root level=\"INFO\">\n+    <appender-ref ref=\"FILE\" />\n+    <appender-ref ref=\"STDOUT\" />", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTAwNQ==", "bodyText": "curious why did we remove this assertion? it should still hold valid.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459199005", "createdAt": "2020-07-23T03:40:48Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java", "diffHunk": "@@ -349,8 +349,6 @@ public void testSnapshotSyncStreamImplementation() throws Exception {\n \n         Queue<LogReplicationEntry> listenerQueue = ((TestDataSender) dataSender).getEntryQueue();\n \n-        assertThat(LARGE_NUM_ENTRIES/ StreamsSnapshotReader.MAX_NUM_SMR_ENTRY).isLessThanOrEqualTo(listenerQueue.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTA2OQ==", "bodyText": "we can remove this commented code.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459199069", "createdAt": "2020-07-23T03:41:15Z", "author": {"login": "annym"}, "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +145,41 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateOpaqueSMREntrySerializedSize() {\n+        if (!opaque) {\n+            log.error(\"This operation only supported for an opaque SMR entry\");\n+            return 0;\n+        }\n+\n+        int size = 0;\n+\n+        for (Object smrArg : SMRArguments) {\n+            size += ((byte[])smrArg).length;\n+        }\n+\n+        size += (SMRMethod.length() * Character.BYTES);\n+        size += Integer.BYTES;\n+\n+        return size;\n+    }\n+\n+    /**\n+     * The serialized size of an opaque SMR entry.\n+     * @return\n+     */\n+    public synchronized Integer getSerializedSize() {\n+        //if (serializedSize == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMDQ0Ng==", "bodyText": "Shouldn't this be LogEntryReader as it could have any implementation? like the test one we use in some test.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459200446", "createdAt": "2020-07-23T03:48:31Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java", "diffHunk": "@@ -32,7 +33,7 @@\n     /*\n      * Implementation of Log Entry Reader. Default implementation reads at the stream layer.\n      */\n-    private LogEntryReader logEntryReader;\n+    private StreamsLogEntryReader logEntryReader;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMTk2Mw==", "bodyText": "A bit confusing, maybe -> Max number of messages sent in burst during a snapshot cycle. (or something in that line)", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459201963", "createdAt": "2020-07-23T03:56:04Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java", "diffHunk": "@@ -39,15 +42,14 @@\n @Slf4j\n public class SnapshotSender {\n \n-    public static int DEFAULT_SNAPSHOT_BATCH_SIZE = 100;\n-    public static final int DEFAULT_TIMEOUT = 5000;\n-\n     private CorfuRuntime runtime;\n     private SnapshotReader snapshotReader;\n     private SenderBufferManager dataSenderBufferManager;\n     private LogReplicationFSM fsm;\n     private long baseSnapshotTimestamp;\n-    private final int snapshotSyncBatchSize;\n+\n+    // The max number of message can be sent over per cycle run during snapshot full sync state.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzcwMg==", "bodyText": "entry is never used.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203702", "createdAt": "2020-07-23T04:04:48Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzgzNQ==", "bodyText": "please add access modifier, private. As this is always a static analysis error.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203835", "createdAt": "2020-07-23T04:05:15Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzg5Ng==", "bodyText": "private", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203896", "createdAt": "2020-07-23T04:05:38Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +34,58 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNTQyMg==", "bodyText": "private", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459205422", "createdAt": "2020-07-23T04:13:43Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +34,58 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+        // Set the last timestamp as the max timestamp\n+        currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n         LogReplicationEntry txMessage = new LogReplicationEntry(MSG_TYPE, topologyConfigId, logEntryRequestId,\n-                currentMsgTs, preMsgTs, globalBaseSnapshot, sequence, buf.array());\n+                currentMsgTs, preMsgTs, globalBaseSnapshot, sequence, opaqueEntryList);\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        return  txMessage;\n+        log.trace(\"Generate a log entry message {} with {} transactions \", txMessage.getMetadata(), opaqueEntryList.size());\n+        return txMessage;\n     }\n \n-    boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n+\n+    // Check if it has the correct streams.\n+    boolean shouldProcess(OpaqueEntry entry) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNTcyOA==", "bodyText": "maybe rename to checkValidSize?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459205728", "createdAt": "2020-07-23T04:15:13Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg==", "bodyText": "No need to break as it is already in the while condition, or if we want to break before the while loop remove from the loop.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459206762", "createdAt": "2020-07-23T04:19:56Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjkwOQ==", "bodyText": "I think it's better in the while loop.\nwhile(currentMsgSize < maxDataSizePerMsg && !hasNoiseData && txOpaqueStream.hasNext()) ... it's ok if we read a next (when there is no, as that method returns null if !hasNext)\nBut if you want to take it out of the while and leave it here, we might simplify for readiness:\nif (hasNoiseData || !txOpaqueStream.hasNext()) {\nbreak;\n}", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459206909", "createdAt": "2020-07-23T04:20:38Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg=="}, "originalCommit": {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDM5OQ==", "bodyText": "why do we set hasNoiseData to true, the fact that the size of the entry is greater than the boundary does not have implications on this, right?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214399", "createdAt": "2020-07-23T04:56:54Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDcxNQ==", "bodyText": "this is repeated. Same statement as before.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214715", "createdAt": "2020-07-23T04:58:39Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDk1Mw==", "bodyText": "simplify:\nreturn !(currentEntrySize + CurrentMsgSize > maxDataSizePerMsg);", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214953", "createdAt": "2020-07-23T04:59:34Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg==", "bodyText": "what happens if there is noisy data but the list is not empty? we should still abort, right?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459215426", "createdAt": "2020-07-23T05:01:58Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {\n+                    break;\n                 }\n-                LogReplicationEntry txMessage = generateMessage(opaqueEntry, logEntryRequestId);\n-                return txMessage;\n+\n+                if (!txOpaqueStream.hasNext()) {\n+                    break;\n+                }\n+\n+                lastOpaqueEntry = txOpaqueStream.next();\n             }\n+\n+            log.trace(\"Generate LogEntryDataMessage size {} with {} entries for maxDataSizePerMsg {}. lastEnry size {}\",\n+                    currentMsgSize, opaqueEntryList.size(), maxDataSizePerMsg, lastOpaqueEntry == null? 0 : currentEntrySize);\n+\n+            if (opaqueEntryList.size() == 0 && hasNoiseData) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTc0MQ==", "bodyText": "It's cleaner if generateMessageWithOpaqueEntryList accepts the empty list and returns a null. Less if conditions.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459215741", "createdAt": "2020-07-23T05:03:06Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {\n+                    break;\n                 }\n-                LogReplicationEntry txMessage = generateMessage(opaqueEntry, logEntryRequestId);\n-                return txMessage;\n+\n+                if (!txOpaqueStream.hasNext()) {\n+                    break;\n+                }\n+\n+                lastOpaqueEntry = txOpaqueStream.next();\n             }\n+\n+            log.trace(\"Generate LogEntryDataMessage size {} with {} entries for maxDataSizePerMsg {}. lastEnry size {}\",\n+                    currentMsgSize, opaqueEntryList.size(), maxDataSizePerMsg, lastOpaqueEntry == null? 0 : currentEntrySize);\n+\n+            if (opaqueEntryList.size() == 0 && hasNoiseData) {\n+                throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+            }\n+\n+            if (opaqueEntryList.size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNjAxNA==", "bodyText": "can this logger be removed?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459216014", "createdAt": "2020-07-23T05:04:24Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNzEyOA==", "bodyText": "L 151 -157 can be simplified:\nif (!stream.iterator.hasNext()) {\nbreak;\n}\nlastEntry = (OpaqueEntry) stream.iterator.next();", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459217128", "createdAt": "2020-07-23T05:09:31Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                                    currentEntrySize, maxDataSizePerMsg);\n+                        }\n+\n+                        // Skip append this entry in this message. Will process it first at the next round.\n+                        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg && currentMsgSize != 0) {\n+                            break;\n+                        }\n+\n+                        smrList.addAll(smrEntries);\n+                        currentMsgSize += currentEntrySize;\n+                        stream.maxVersion = Math.max(stream.maxVersion, lastEntry.getVersion());\n+                    }\n+                    lastEntry = null;\n+                }\n+\n+                if (stream.iterator.hasNext()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODUyOQ==", "bodyText": "I'm confused, why do we have this same comparison of a single currentEntrySize twice? shouldn't it only compare with maxDataSizePerMsg, as this is the user configured (which could default to MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED)", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459218529", "createdAt": "2020-07-23T05:15:40Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODk2OQ==", "bodyText": "why do we need currentMsgSize != 0? If we already checked that currentEntrySize is bigger previously and we throw an exception.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459218969", "createdAt": "2020-07-23T05:17:45Z", "author": {"login": "annym"}, "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                                    currentEntrySize, maxDataSizePerMsg);\n+                        }\n+\n+                        // Skip append this entry in this message. Will process it first at the next round.\n+                        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg && currentMsgSize != 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMjQ4NA==", "bodyText": "private", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459222484", "createdAt": "2020-07-23T05:32:52Z", "author": {"login": "annym"}, "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/logreplication/LogReplicationEntry.java", "diffHunk": "@@ -13,41 +21,102 @@\n  *\n  * @author annym\n  */\n+@Slf4j\n @Data\n public class LogReplicationEntry implements ICorfuPayload<LogReplicationEntry> {\n \n     private LogReplicationEntryMetadata metadata;\n \n-    private byte[] payload;\n+    private List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n \n-    public LogReplicationEntry(LogReplicationEntryMetadata metadata, byte[] payload) {\n-        this.payload = payload;\n+\n+    // Only used by test cases\n+    @VisibleForTesting\n+    byte[] payload;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzIyOQ==", "bodyText": "Can we add this comment in the code so it is easy to follow?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223229", "createdAt": "2020-07-23T05:35:54Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private static final int SHUTDOWN_RETRIES = 10;\n     private static final long SHUTDOWN_RETRY_WAIT = 500;\n \n+    private static final int MSG_SIZE = 131072;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng=="}, "originalCommit": {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzMxMQ==", "bodyText": "Camel case please.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223311", "createdAt": "2020-07-23T05:36:15Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -313,6 +315,7 @@ public static Process runReplicationServer(int port, String pluginConfigFilePath\n                 .setHost(DEFAULT_HOST)\n                 .setPort(port)\n                 .setPluginConfigFilePath(pluginConfigFilePath)\n+                .setMsg_size(MSG_SIZE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzY4NA==", "bodyText": "Java uses CamelCase.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223684", "createdAt": "2020-07-23T05:37:36Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -507,6 +510,7 @@ public Process runServer() throws IOException {\n         private String compressionCodec = null;\n         private String pluginConfigFilePath = null;\n         private String logPath = null;\n+        private int msg_size = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDM0OA==", "bodyText": "private", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459224348", "createdAt": "2020-07-23T05:40:18Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -311,6 +324,16 @@ void verifyTables(HashMap<String, CorfuTable<Long, Long>> tables0, HashMap<Strin\n             }\n     }\n \n+    void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDYwNQ==", "bodyText": "Why did this change?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459224605", "createdAt": "2020-07-23T05:41:17Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -695,7 +742,7 @@ public void testLogEntrySyncValidCrossTablesWithTriggerTimeout() throws Exceptio\n         expectedAckMessages =  NUM_KEYS*WRITE_CYCLES;\n \n         testConfig.clear().setDropMessageLevel(2);\n-        startLogEntrySync(crossTables, WAIT.ON_ERROR);\n+        startLogEntrySync(crossTables, WAIT.ON_TIMEOUT_ERROR);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNTU1Ng==", "bodyText": "why did this change? there is a method that takes only one condition.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459225556", "createdAt": "2020-07-23T05:45:18Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA==", "bodyText": "The intention of the observables and the WAIT.ON_ACK is to avoid while loops that are just stuck there until a condition is met. Can't we set the expected number of ACKS by knowing the size of what we wrote and how many messages will be sent, hence how many calks we will receive back?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226180", "createdAt": "2020-07-23T05:48:01Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);\n \n         expectedAckTimestamp = Long.MAX_VALUE;\n-        // Verify Data on Destination site\n-        System.out.println(\"****** Verify Data on Destination\");\n+\n         // Because t2 is not specified as a replicated table, we should not see it on the destination\n         srcDataForVerification.get(t2).clear();\n \n+        // Verify Data on Destination site\n+        System.out.println(\"****** Wait Data on Destination\");\n+        waitData(dstCorfuTables, srcDataForVerification);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjI4Mg==", "bodyText": "All you need is to set expectedAckMessages to the correct value.", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226282", "createdAt": "2020-07-23T05:48:29Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);\n \n         expectedAckTimestamp = Long.MAX_VALUE;\n-        // Verify Data on Destination site\n-        System.out.println(\"****** Verify Data on Destination\");\n+\n         // Because t2 is not specified as a replicated table, we should not see it on the destination\n         srcDataForVerification.get(t2).clear();\n \n+        // Verify Data on Destination site\n+        System.out.println(\"****** Wait Data on Destination\");\n+        waitData(dstCorfuTables, srcDataForVerification);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA=="}, "originalCommit": {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjMyMg==", "bodyText": "required?", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226322", "createdAt": "2020-07-23T05:48:44Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -971,6 +998,8 @@ public void testLogEntrySyncWithTrim() throws Exception {\n         // Setup Environment: two corfu servers (source & destination)\n         setupEnv();\n \n+        log.info(\"Have setutEnv Done\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjg4OA==", "bodyText": "droppingNum (not starting with caps)", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226888", "createdAt": "2020-07-23T05:51:10Z", "author": {"login": "annym"}, "path": "test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java", "diffHunk": "@@ -50,7 +50,9 @@\n \n     final static int DROP_INCREMENT = 4;\n \n-    private int firstDrop = DROP_INCREMENT;\n+    private int DroppingNum = 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052"}, "originalPosition": 24}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ccaa7a42499718ded47259cfc73b1c68572706a7", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/ccaa7a42499718ded47259cfc73b1c68572706a7", "committedDate": "2020-07-23T23:06:10Z", "message": "Address all the comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83232ab1f7264f15b0deb2923ae3956b71348901", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/83232ab1f7264f15b0deb2923ae3956b71348901", "committedDate": "2020-07-23T23:19:19Z", "message": "Fix compiling erros."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0NjA4NDI5", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#pullrequestreview-454608429", "createdAt": "2020-07-24T02:43:14Z", "commit": {"oid": "83232ab1f7264f15b0deb2923ae3956b71348901"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11d8e5b78240c2f2468d656cf48ffc8c680aba86", "author": {"user": {"login": "xiaoqin2012", "name": "Xiaoqin Ma"}}, "url": "https://github.com/CorfuDB/CorfuDB/commit/11d8e5b78240c2f2468d656cf48ffc8c680aba86", "committedDate": "2020-07-24T16:56:51Z", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4290, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}