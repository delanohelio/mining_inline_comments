{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg5Mzc4NTIx", "number": 695, "title": "Don't reuse the same producer on interrupted flush in KafkaProducerWrapper", "bodyText": "When the producer.flush() call gets interrupted, we land up reusing the same producer object.\nThe interrupt may leave the producer in a bad state and the recommendation from the Kafka\nteam is to avoid reusing the same Producer after the flush got interrupted.", "createdAt": "2020-03-16T17:04:38Z", "url": "https://github.com/linkedin/brooklin/pull/695", "merged": true, "mergeCommit": {"oid": "9a50e30fe2faec883ba15774358d87a21b8b6fe0"}, "closed": true, "closedAt": "2020-03-24T15:47:16Z", "author": {"login": "somandal"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcOQrTUAH2gAyMzg5Mzc4NTIxOjdmM2U5YjI5MGMwNjIwZTNkM2I1NTQ4M2RlMzk4MzcxNDM2NjZiZWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcQ0joPgFqTM4MDQxODgwNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7f3e9b290c0620e3d3b55483de39837143666bea", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/7f3e9b290c0620e3d3b55483de39837143666bea", "committedDate": "2020-03-16T16:26:48Z", "message": "Don't reuse the same producer on interrupted flush in KafkaProducerWrapper"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDI2NTYy", "url": "https://github.com/linkedin/brooklin/pull/695#pullrequestreview-375426562", "createdAt": "2020-03-16T17:18:25Z", "commit": {"oid": "7f3e9b290c0620e3d3b55483de39837143666bea"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4330544489adea7a7a166f52325aae765f283596", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/4330544489adea7a7a166f52325aae765f283596", "committedDate": "2020-03-19T01:44:02Z", "message": "Initial test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/f7163bc64d5c3197252f71eb11b35013f22b9e5b", "committedDate": "2020-03-19T03:28:16Z", "message": "Fix up test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3ODAzNDY4", "url": "https://github.com/linkedin/brooklin/pull/695#pullrequestreview-377803468", "createdAt": "2020-03-19T14:59:23Z", "commit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "state": "DISMISSED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNDo1OToyM1rOF4ye2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTowNDozOVrOF4yulg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MTY3NQ==", "bodyText": "Please, add a comment explaining why executing this flush() call on a separate thread was necessary.\n\n\nFeel free to ignore this comment: not that it matters much, but would it be slightly more appropriate to use Executors.newSingleThreadExecutor() or even just new Thread().start()?", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395091675", "createdAt": "2020-03-19T14:59:23Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MzY3NA==", "bodyText": "nit: prefix field names with an underscore", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395093674", "createdAt": "2020-03-19T15:02:02Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NTcwMg==", "bodyText": "Just to make it super simple for anyone reading this test, please add a comment summarizing your intent (e.g. calling flush() on the first created producer will throw).", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395095702", "createdAt": "2020-03-19T15:04:39Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;\n+\n+    MockKafkaProducerWrapper(String logSuffix, Properties props, String metricsNamesPrefix) {\n+      super(logSuffix, props, metricsNamesPrefix);\n+    }\n+\n+    @Override\n+    Producer<byte[], byte[]> createKafkaProducer() {\n+      @SuppressWarnings(\"unchecked\")\n+      Producer<byte[], byte[]> producer = (Producer<byte[], byte[]>) mock(Producer.class);\n+      if (!createKafkaProducerCalled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3ODIyMTEw", "url": "https://github.com/linkedin/brooklin/pull/695#pullrequestreview-377822110", "createdAt": "2020-03-19T15:18:07Z", "commit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNToxODowOFrOF4zXBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNToxODowOFrOF4zXBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwNjA1NA==", "bodyText": "@VisibleForTesting", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395106054", "createdAt": "2020-03-19T15:18:08Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/main/java/com/linkedin/datastream/kafka/KafkaProducerWrapper.java", "diffHunk": "@@ -173,13 +174,17 @@ int getTasksSize() {\n     } else {\n       if (_kafkaProducer == null) {\n         _rateLimiter.acquire();\n-        _kafkaProducer = _producerFactory.createProducer(_props);\n+        _kafkaProducer = createKafkaProducer();\n         NUM_PRODUCERS.incrementAndGet();\n       }\n     }\n     return _kafkaProducer;\n   }\n \n+  Producer<K, V> createKafkaProducer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3570fd4ece623e66257fb40476f4350638c7933", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/c3570fd4ece623e66257fb40476f4350638c7933", "committedDate": "2020-03-19T15:38:39Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24683c0dced218313e15c41780025ab7d0a594a6", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/24683c0dced218313e15c41780025ab7d0a594a6", "committedDate": "2020-03-19T15:44:42Z", "message": "Fix typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ba5846f9086a5932a3c435fccb88d6fc3cac607", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/4ba5846f9086a5932a3c435fccb88d6fc3cac607", "committedDate": "2020-03-19T16:05:47Z", "message": "Fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/d01ad1bc5aafbf6893bb3a8fed285004dd374f0b", "committedDate": "2020-03-19T16:06:33Z", "message": "Fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3ODY5OTY5", "url": "https://github.com/linkedin/brooklin/pull/695#pullrequestreview-377869969", "createdAt": "2020-03-19T16:08:06Z", "commit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwNDE4Mjky", "url": "https://github.com/linkedin/brooklin/pull/695#pullrequestreview-380418292", "createdAt": "2020-03-24T15:22:21Z", "commit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNToyMjoyMVrOF61ebg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNToyMjoyMVrOF61ebg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIzNzg3MA==", "bodyText": "nit: resetting send, flush and close (for completeness)", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r397237870", "createdAt": "2020-03-24T15:22:21Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper<byte[], byte[]> producerWrapper =\n+        new MockKafkaProducerWrapper<>(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    // Calling the first flush() on a separate thread because the InterruptException calls Thread interrupt() on the\n+    // currently running thread. If not run on a separate thread, the test thread itself will be interrupted.\n+    ExecutorService executorService = Executors.newSingleThreadExecutor();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocation counts", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwNDE4ODA3", "url": "https://github.com/linkedin/brooklin/pull/695#pullrequestreview-380418807", "createdAt": "2020-03-24T15:22:51Z", "commit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 400, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}