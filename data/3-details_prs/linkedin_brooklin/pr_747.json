{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcyNzE2Njkx", "number": 747, "title": " Change task locks from Ephemeral to Persistent ", "bodyText": "During zookeeper session expiry, the ephemeral nodes gets automatically deleted by the  zookeeper server. Task locks currently creates ephemeral nodes which allows only one brooklin instance to process the task. Another instance will have to wait to acquire the lock. The lock is important for the connectors which have to manage their own synchronization like partition managed Kafka mirror maker connector which does not rely on kafka for partition management. So, if the lock gets deleted because of session expiry, the current owner will take some time to identify the session expiry scenario and stop all the tasks, but the new task owner will not be aware whether current owner has stopped or not and will immediately get the lock (since ephemeral lock node on zk is already deleted.). To avoid that, we are making the task locks persistent and will let the anyone trying to acquire the lock, force release the lock if it is owned by the dead owner (not part of the cluster) after the configurable debounce timer. The coordinator of the instance which had zk session expiry will be responsible to bring down all the tasks within the debounce timer.\nMaking the debounce timer configurable to ensure that we can setup a longer timeout value, in case required in future.", "createdAt": "2020-08-24T19:22:12Z", "url": "https://github.com/linkedin/brooklin/pull/747", "merged": true, "mergeCommit": {"oid": "20c1c147c518ced9977b64097103c3ba31ba6c02"}, "closed": true, "closedAt": "2020-09-01T21:46:24Z", "author": {"login": "vmaheshw"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABboAf5CAH2gAyNDcyNzE2NjkxOmMzMWNkNGExNWNjOGRkNjliMGE2NTNkYmU0MjAxZGU5MzRlYTZkNjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdEqKOlgFqTQ3OTkzMDYzOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "author": {"user": {"login": "vmaheshw", "name": "Vaibhav Maheshwari"}}, "url": "https://github.com/linkedin/brooklin/commit/c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "committedDate": "2019-11-18T20:06:44Z", "message": "Merge pull request #1 from linkedin/master\n\nPull latest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3004d104a697e619b0715de5c135e7544fb4d25", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/f3004d104a697e619b0715de5c135e7544fb4d25", "committedDate": "2020-08-13T19:09:21Z", "message": "Merge branch 'master' of github.com:vmaheshw/Brooklin"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0126f592a7abc5d97016898ee007758f0ac9cba6", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/0126f592a7abc5d97016898ee007758f0ac9cba6", "committedDate": "2020-08-17T21:04:11Z", "message": "Make task lock persistent"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/3fc04bb2d920774ae88df8dea886af59c3808eec", "committedDate": "2020-08-24T19:16:20Z", "message": "Add tests for persistent lock and configurable debounce timer support"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MDE4ODQy", "url": "https://github.com/linkedin/brooklin/pull/747#pullrequestreview-475018842", "createdAt": "2020-08-25T22:44:22Z", "commit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "state": "COMMENTED", "comments": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMjo0NDoyMlrOHGuTJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMzozNjowMFrOHG6kbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjgxMjA2OA==", "bodyText": "Since we are adding lock cleanup support here, can we rename this function (performCleanupOrphanConnectorTasks) to indicate it cleans up orphan tasks and locks? Also, should the config be renamed too?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476812068", "createdAt": "2020-08-25T22:44:22Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -1252,6 +1252,8 @@ void performCleanupOrphanConnectorTasks() {\n     _log.info(\"performCleanupOrphanConnectorTasks called\");\n     int orphanCount = _adapter.cleanUpOrphanConnectorTasks(_config.getZkCleanUpOrphanConnectorTask());\n     _metrics.updateMeter(CoordinatorMetrics.Meter.NUM_ORPHAN_CONNECTOR_TASKS, orphanCount);\n+    int orphanLockCount = _adapter.cleanUpOrphanConnectorTaskLocks(_config.getZkCleanUpOrphanConnectorTask());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1Mzg0Mw==", "bodyText": "nit: delete empty line?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476853843", "createdAt": "2020-08-25T23:26:26Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/CoordinatorConfig.java", "diffHunk": "@@ -50,8 +52,10 @@ public CoordinatorConfig(Properties config) {\n     _zkConnectionTimeout = _properties.getInt(CONFIG_ZK_CONNECTION_TIMEOUT, ZkClient.DEFAULT_CONNECTION_TIMEOUT);\n     _retryIntervalMs = _properties.getInt(CONFIG_RETRY_INTERVAL, 1000 /* 1 second */);\n     _heartbeatPeriodMs = _properties.getLong(CONFIG_HEARTBEAT_PERIOD_MS, Duration.ofMinutes(1).toMillis());\n+    _debounceTimerMs = _properties.getLong(CONFIG_DEBOUNCE_TIMER_MS, Duration.ofSeconds(30).toMillis());\n     _defaultTransportProviderName = _properties.getString(CONFIG_DEFAULT_TRANSPORT_PROVIDER, \"\");\n     _zkCleanUpOrphanConnectorTask = _properties.getBoolean(CONFIG_ZK_CLEANUP_ORPHAN_CONNECTOR_TASK, false);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NDAxMg==", "bodyText": "nit: delete empty line?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476854012", "createdAt": "2020-08-25T23:26:37Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/CoordinatorConfig.java", "diffHunk": "@@ -89,4 +93,9 @@ public long getHeartbeatPeriodMs() {\n   public boolean getZkCleanUpOrphanConnectorTask() {\n     return _zkCleanUpOrphanConnectorTask;\n   }\n+\n+  public long getDebounceTimerMs() {\n+    return _debounceTimerMs;\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NDY5MQ==", "bodyText": "Should we add a comment explaining what this debounce timer is used for?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476854691", "createdAt": "2020-08-25T23:27:14Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/CoordinatorConfig.java", "diffHunk": "@@ -32,6 +33,7 @@\n   private final Properties _config;\n   private final VerifiableProperties _properties;\n   private final int _retryIntervalMs;\n+  private final long _debounceTimerMs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NjA1Mg==", "bodyText": "did you mean to merge these two sets of comments? I thought the first comment was a more overall comment for the file, and the second one specific to this String?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476856052", "createdAt": "2020-08-25T23:28:29Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -38,9 +38,7 @@ private KeyBuilder() {\n    *\n    * Below keys represent the various locations under connectors for parts of\n    * a DatastreamTask can be persisted.\n-   */\n \n-  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NjkzOA==", "bodyText": "nit: {task} -> {taskName}", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476856938", "createdAt": "2020-08-25T23:29:20Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -71,9 +69,14 @@ private KeyBuilder() {\n   private static final String DATASTREAM_TASK_LOCK_ROOT = CONNECTOR + \"/\" + DATASTREAM_TASK_LOCK_ROOT_NAME;\n \n   /**\n-   * Task lock node under connectorType/lock/{taskName}\n+   * Task lock node under connectorType/lock/{taskPrefix}\n+   */\n+  private static final String DATASTREAM_TASK_LOCK_PREFIX = DATASTREAM_TASK_LOCK_ROOT + \"/%s\";\n+\n+  /**\n+   * Task lock node under connectorType/lock/{taskPrefix}/{task}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg2MDAwNQ==", "bodyText": "This comment talks about ephemeral nodes, and we are making the lock persistent. Can we update this comment and all other comments that mention ephemeral locks?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476860005", "createdAt": "2020-08-25T23:32:19Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -271,22 +274,39 @@ public static String datastreamTaskStateKey(String cluster, String connectorType\n    * <pre>Example: /{cluster}/connectors/{connectorType}/lock</pre>\n    * @param cluster Brooklin cluster name\n    * @param connectorType Connector\n-  \\   */\n+   * @return datastream task lock root\n+   */\n   public static String datastreamTaskLockRoot(String cluster, String connectorType) {\n     return String.format(DATASTREAM_TASK_LOCK_ROOT, cluster, connectorType).replaceAll(\"//\", \"/\");\n   }\n \n+  /**\n+   * Get the ZooKeeper znode for a specific datastream task's lock prefix\n+   * The lock is ephemeral node and it should not be stored under task node", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5NTAyOA==", "bodyText": "Let's rephrase this a bit? Feel free to modify the suggestion below, I'm just trying to lay out a suggested flow:\n\nIdentify orphan connector task locks for which there is no corresponding connector task node present and schedule their clean up after the debounce timer. Lock cleanup must be scheduled after a debounce timer because:\n\nThere is no easy way to know if the task is still running even though it is [un/re]assigned\nAll tasks which are unassigned must be stopped within a fixed amount of time which is <= debounce timer\n\nThus waiting for the debounce timer gives the guarantee that reassigned/dead tasks have actually stopped.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476895028", "createdAt": "2020-08-26T00:05:00Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5NjU2OQ==", "bodyText": "nit: the  leader get ->  the leader gets", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476896569", "createdAt": "2020-08-26T00:06:31Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5NzM3NQ==", "bodyText": "nit: remove 'Boolean' since the type is implicit in the function definition.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476897375", "createdAt": "2020-08-26T00:07:23Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5ODkwNQ==", "bodyText": "Just curious, I noticed that you spawn a separate thread to handle orphan lock cleanup, whereas you don't do the same for cleaning up orphan tasks. Any reason for that? Can both be done on separate threads?\nAlso, shall we add a log here indicating that this is being skipped?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476898905", "createdAt": "2020-08-26T00:09:40Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkwODM0NQ==", "bodyText": "nit: should we also print cleanUpOrphanTaskLocksInConnector? If not here, then maybe at the start of the function or the caller should print this", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476908345", "createdAt": "2020-08-26T00:23:43Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkwOTc1Mw==", "bodyText": "You can potentially move this out of the for loop by pre-grouping connector -> valid task lists:\nMap<String, Set>, where the key is the connector, and Set is the list of valid tasks for that connector. We won't have to walk the full set of validTasks for every connector.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476909753", "createdAt": "2020-08-26T00:25:47Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5MA==", "bodyText": "Feel free to ignore this, but wondering if we can simplify this a bit. Something along the lines of:\nint originalTaskListSize = taskList.size()\ntaskList.removeAll(validTaskNamesSet)   // make sure the types are compatible to do this\n\n// Only invalid tasks are left behind\ntaskList.forEach(taskName -> {\n     orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n});\n\nif  (taskList.size() == originalTaskListSize) {\n    // None of the tasks are valid, thus we can safely remove the prefix node\n    orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n}", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476915990", "createdAt": "2020-08-26T00:35:41Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyMDE1Mg==", "bodyText": "nit: reword to:\nWait for the task lock to be released and delete it if the lock is held by a dead owner", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476920152", "createdAt": "2020-08-26T00:41:52Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDQ5Mw==", "bodyText": "What if this connect() races with the ZkClient's internal connect logic that you mentioned earlier? Can't this cause session leaks then (provided we are planning to remove the 'disconnect()' on connect that we currently have)?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476924493", "createdAt": "2020-08-26T00:48:10Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1613,7 +1757,10 @@ private void scheduleExpiryTimerAfterSessionTimeout() {\n   @VisibleForTesting\n   void onSessionExpired() {\n     LOG.error(\"Zookeeper session expired.\");\n+    //cancel the lock clean up\n+    _orphanLockCleanupFuture.cancel(true);\n     onBecomeFollower();\n+    connect();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 395}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDY0NQ==", "bodyText": "nit: space after '//'", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476924645", "createdAt": "2020-08-26T00:48:24Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1613,7 +1757,10 @@ private void scheduleExpiryTimerAfterSessionTimeout() {\n   @VisibleForTesting\n   void onSessionExpired() {\n     LOG.error(\"Zookeeper session expired.\");\n+    //cancel the lock clean up", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNjg5Mw==", "bodyText": "Just to validate, the list of valid DatastreamTasks cannot change between when we get this list here, and when we check for valid tasks under the lock node, right? i.e. no LEADER_DO_ASSIGNMENT can run in parallel with this?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476926893", "createdAt": "2020-08-26T00:51:51Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNzg2OA==", "bodyText": "remove this comment, we don't need to release any lock here", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476927868", "createdAt": "2020-08-26T00:53:24Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNjA5Mg==", "bodyText": "I'm in two minds about this. Feel it's just better to enforce via a Validate that the timeoutMs must be >= the debounceTimerMs. If we decide not to enforce it, then we should clearly document the difference in the behavior in the comment above the class so that users of this API are aware of the behavior differences.\nAlso, can you rephrase:\nOnly try to identify dead owners of the task lock if the timeout is greater than the debounce timer.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476936092", "createdAt": "2020-08-26T01:05:13Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0NTE3OQ==", "bodyText": "shouldn't we check for owner != null here too?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476945179", "createdAt": "2020-08-26T01:18:57Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.\n+      if (timeoutMs >= _debounceTimerMs) {\n+        // check if the owner is dead.\n+        if (_liveInstancesProvider != null && !getLiveInstances().contains(owner)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0ODA1Ng==", "bodyText": "Should we add a warn log indicating the owner changed underneath us?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476948056", "createdAt": "2020-08-26T01:23:11Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.\n+      if (timeoutMs >= _debounceTimerMs) {\n+        // check if the owner is dead.\n+        if (_liveInstancesProvider != null && !getLiveInstances().contains(owner)) {\n+          LOG.info(\"dead owner {} found for the lock on the task {}\", owner, task.getDatastreamTaskName());\n+          deadOwner = true;\n+        } else {\n+          waitForTaskRelease(task, timeoutMs - _debounceTimerMs, lockPath);\n+          if (!_zkclient.exists(lockPath)) {\n+            return;\n+          }\n+          owner = _zkclient.readData(lockPath, true);\n+          if (owner != null && _liveInstancesProvider != null && !getLiveInstances().contains(owner)) {\n+            LOG.info(\"dead owner {} found for the lock on the task {} after waiting {} ms\",\n+                owner, task.getDatastreamTaskName(), timeoutMs - _debounceTimerMs);\n+            deadOwner = true;\n+          }\n+        }\n+        waitTimeout = _debounceTimerMs;\n+      }\n+\n+      waitForTaskRelease(task, waitTimeout, lockPath);\n+      if (deadOwner && _zkclient.exists(lockPath)) {\n+        String tempOwner = _zkclient.readData(lockPath, true);\n+        if (tempOwner != null && tempOwner.equals(owner)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4MjkyMQ==", "bodyText": "Why have we changed this to readData() instead of ensureReadData()? Looks like ensureReadData() handles the situation where the ZK node is being modified by another process.\nDue to changing this to readData(), we need to check for 'null' owners everywhere, and it's not clear how 'null' owners will be handled. E.g. if throughout this whole call and the waitForTaskReleaseOrForceIfOwnerIsDead, if readData() returns 'null', does it mean we don't release the lock? Or is the expectation that eventually it will stabilize and we will retry?\nAdd a comment explaining why ensureReadData() here is not a good idea, if it really isn't a good idea?\nI also noticed that waitForTaskReleaseOrForceIfOwnerIsDead is already calling readData() and checking if the owner matches the instance in question. Can we directly call waitForTaskReleaseOrForceIfOwnerIsDead instead of doing this check here?  [see my comment where we call waitForTaskReleaseOrForceIfOwnerIsDead on waiting for dependencies]", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476982921", "createdAt": "2020-08-26T02:15:02Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1082,34 +1219,38 @@ private void waitForTaskRelease(DatastreamTask task, long timeoutMs, String lock\n    * @see #releaseTask(DatastreamTaskImpl)\n    */\n   public void acquireTask(DatastreamTaskImpl task, Duration timeout) {\n-    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockRoot(_cluster, task.getConnectorType()));\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockPrefix(_cluster, task.getConnectorType(), task.getTaskPrefix()));\n+    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     String owner = null;\n     if (_zkclient.exists(lockPath)) {\n-      owner = _zkclient.ensureReadData(lockPath);\n-      if (owner.equals(_instanceName)) {\n+      owner = _zkclient.readData(lockPath, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4NjI1NQ==", "bodyText": "Is it necessary to validate this again? Won't create thrown an exception if the node already exists?\nZkConnection comments (please do correct me if I'm looking at the wrong thing):\n\n * If a node with the same actual path already exists in the ZooKeeper, a\n * KeeperException with error code KeeperException.NodeExists will be\n * thrown. Note that since a different actual path is used for each\n * invocation of creating sequential node with the same path argument, the\n * call will never throw \"file exists\" KeeperException.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476986255", "createdAt": "2020-08-26T02:19:57Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1082,34 +1219,38 @@ private void waitForTaskRelease(DatastreamTask task, long timeoutMs, String lock\n    * @see #releaseTask(DatastreamTaskImpl)\n    */\n   public void acquireTask(DatastreamTaskImpl task, Duration timeout) {\n-    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockRoot(_cluster, task.getConnectorType()));\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockPrefix(_cluster, task.getConnectorType(), task.getTaskPrefix()));\n+    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     String owner = null;\n     if (_zkclient.exists(lockPath)) {\n-      owner = _zkclient.ensureReadData(lockPath);\n-      if (owner.equals(_instanceName)) {\n+      owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n         LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n         return;\n       }\n \n-      waitForTaskRelease(task, timeout.toMillis(), lockPath);\n+      waitForTaskReleaseOrForceIfOwnerIsDead(task, timeout.toMillis(), lockPath);\n     }\n \n     if (!_zkclient.exists(lockPath)) {\n-      _zkclient.createEphemeral(lockPath, _instanceName);\n-      LOG.info(\"{} successfully acquired the lock on {}\", _instanceName, task);\n-    } else {\n-      String msg = String.format(\"%s failed to acquire task %s in %dms, current owner: %s\", _instanceName, task,\n-          timeout.toMillis(), owner);\n-      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, msg, null);\n+      _zkclient.create(lockPath, _instanceName, CreateMode.PERSISTENT);\n+      owner = _zkclient.readData(lockPath, true);\n+      if ((owner != null && owner.equals(_instanceName))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 327}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk5ODMxMQ==", "bodyText": "Is it correct for us to check if we are the owner of the dependency and then assume we don't need to release anything here? waitForTaskReleaseOrForceIfOwnerIsDead does  the following check:\n    if (_zkclient.exists(lockPath)) {\n      String owner = _zkclient.readData(lockPath, true);\n      if (owner != null && owner.equals(_instanceName)) {\n        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n        //release the lock.\n        return;\n      }\n\nJust because we are the owner of that lock doesn't mean it's okay to grab it, since it's a dependency task here. The dependency may have just been assigned to the same node. We should instead wait for the task lock to be released right?\nPerhaps the code snippet that checks if the instance is the owner in waitForTaskReleaseOrForceIfOwnerIsDead should be moved out of that function.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476998311", "createdAt": "2020-08-26T02:40:36Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1120,9 +1261,9 @@ public boolean checkIsTaskLocked(String connectorType, String taskName) {\n    */\n   public void waitForDependencies(DatastreamTaskImpl task, Duration timeout) {\n     task.getDependencies().forEach(previousTask -> {\n-        String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), previousTask);\n+      String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), previousTask);\n       if (_zkclient.exists(lockPath)) {\n-        waitForTaskRelease(task, timeout.toMillis(), lockPath);\n+        waitForTaskReleaseOrForceIfOwnerIsDead(task, timeout.toMillis(), lockPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMTI1NQ==", "bodyText": "Quick question, if a Session expiry happens, the _instanceName remains the same? Just wondering if we could have a case where we're trying to release the lock but an expiry + connect happened before we call this, creating a  new liveinstance node for this host. Will the task still be releasable?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477001255", "createdAt": "2020-08-26T02:51:13Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1218,20 +1359,23 @@ public void cleanUpPartitionMovement(String connectorType, String datastreamGrou\n    * @see #acquireTask(DatastreamTaskImpl, Duration)\n    */\n   public void releaseTask(DatastreamTaskImpl task) {\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    String lockPath =\n+        KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     if (!_zkclient.exists(lockPath)) {\n-      LOG.info(\"There is no lock on {}\", task);\n+      LOG.info(\"There is no lock on {}-{}/{}\", task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n       return;\n     }\n \n     String owner = _zkclient.ensureReadData(lockPath);\n     if (!owner.equals(_instanceName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 374}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMjYwOQ==", "bodyText": "The comment where this _liveInstancesProvider is declared indicates that we only maintain this for the Leader. Can you fix that comment and add details about why we need this for all nodes?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477002609", "createdAt": "2020-08-26T02:56:19Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -236,6 +245,7 @@ public void connect() {\n     _zkclient = createZkClient();\n     _stateChangeListener = new ZkStateChangeListener();\n     _leaderElectionListener = new ZkLeaderElectionListener();\n+    _liveInstancesProvider = new ZkBackedLiveInstanceListProvider();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTQxMQ==", "bodyText": "nit: space after '//', and all the other // without space comments too", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477011411", "createdAt": "2020-08-26T03:29:32Z", "author": {"login": "somandal"}, "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -808,7 +865,13 @@ public void testDeleteTasksWithPrefix() {\n     leftOverTasks = zkClient.getChildren(KeyBuilder.connector(testCluster, connectorType));\n     Assert.assertEquals(leftOverTasks.size(), 3);\n \n-    adapter.cleanUpOrphanConnectorTasks(true);\n+    //Verify orphan locks, lockTask is the only orphan task.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTY1NQ==", "bodyText": "Should we also add a test for dependency locks? Feel free to defer this to a later point in time or have me take it up.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477011655", "createdAt": "2020-08-26T03:30:24Z", "author": {"login": "somandal"}, "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -751,14 +755,66 @@ public void testZookeeperSessionExpiry() throws InterruptedException {\n     Mockito.verify(adapter, Mockito.times(1)).onSessionExpired();\n   }\n \n+  @Test\n+  public void testZookeeperLockAcquire() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMzEwMw==", "bodyText": "Should this and other such getZkClient().exists() lines be an assert?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477013103", "createdAt": "2020-08-26T03:36:00Z", "author": {"login": "somandal"}, "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -751,14 +755,66 @@ public void testZookeeperSessionExpiry() throws InterruptedException {\n     Mockito.verify(adapter, Mockito.times(1)).onSessionExpired();\n   }\n \n+  @Test\n+  public void testZookeeperLockAcquire() throws InterruptedException {\n+    String testCluster = \"testLockAcquire\";\n+    String connectorType = \"connectorType\";\n+    //\n+    // start two ZkAdapters, which is corresponding to two Coordinator instances\n+    //\n+    ZkClientInterceptingAdapter adapter1 = createInterceptingZkAdapter(testCluster, 5000, ZK_DEBOUNCE_TIMER_MS * 10);\n+    adapter1.connect();\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl();\n+    task.setId(\"3\");\n+    task.setConnectorType(connectorType);\n+    task.setZkAdapter(adapter1);\n+\n+    List<DatastreamTask> tasks = Collections.singletonList(task);\n+    updateInstanceAssignment(adapter1, adapter1.getInstanceName(), tasks);\n+\n+    LOG.info(\"Acquire from instance1 should succeed\");\n+    Duration timeout = Duration.ofSeconds(3);\n+    Assert.assertTrue(expectException(() -> adapter1.acquireTask(task, timeout), false));\n+    String owner = adapter1.getZkClient().readData(KeyBuilder.datastreamTaskLock(testCluster, task.getConnectorType(),\n+        task.getTaskPrefix(), task.getDatastreamTaskName()));\n+\n+    ZkClientInterceptingAdapter adapter2 = createInterceptingZkAdapter(testCluster, 5000, ZK_DEBOUNCE_TIMER_MS * 10);\n+    adapter2.connect();\n+    simulateSessionExpiration(adapter1);\n+\n+    Thread.sleep(1000);\n+    Assert.assertTrue(expectException(() -> adapter1._zkClient.waitUntilConnected(5, TimeUnit.SECONDS), false));\n+\n+    // adapter2 not able to acquire lock\n+    adapter2.getZkClient().exists(KeyBuilder.datastreamTaskLock(testCluster, task.getConnectorType(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 186}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/abf1d61f05842225cd8d218539e3a6acd3dd8e40", "committedDate": "2020-08-26T19:05:03Z", "message": "Address fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2MDU1NzE0", "url": "https://github.com/linkedin/brooklin/pull/747#pullrequestreview-476055714", "createdAt": "2020-08-26T23:13:06Z", "commit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "state": "DISMISSED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMzoxMzowNlrOHHhvpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMzo0MjozMFrOHHkDaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY1NDk1MQ==", "bodyText": "Sorry to bug you again about this again, but I still see mention of ephemeral nodes ...", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477654951", "createdAt": "2020-08-26T23:13:06Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -271,22 +274,39 @@ public static String datastreamTaskStateKey(String cluster, String connectorType\n    * <pre>Example: /{cluster}/connectors/{connectorType}/lock</pre>\n    * @param cluster Brooklin cluster name\n    * @param connectorType Connector\n-  \\   */\n+   * @return datastream task lock root\n+   */\n   public static String datastreamTaskLockRoot(String cluster, String connectorType) {\n     return String.format(DATASTREAM_TASK_LOCK_ROOT, cluster, connectorType).replaceAll(\"//\", \"/\");\n   }\n \n+  /**\n+   * Get the ZooKeeper znode for a specific datastream task's lock prefix\n+   * The lock is ephemeral node and it should not be stored under task node", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg2MDAwNQ=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY1ODA1Nw==", "bodyText": "Thanks for explaining, makes sense.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477658057", "createdAt": "2020-08-26T23:15:57Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5ODkwNQ=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY1OTI4Mg==", "bodyText": "Sure", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477659282", "createdAt": "2020-08-26T23:17:06Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5MA=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NzAxNw==", "bodyText": "Does the getLiveInstances() list type allow nulls? Otherwise contains() throws a NullPointerException.\n\n/**\n * Returns <tt>true</tt> if this list contains the specified element.\n * More formally, returns <tt>true</tt> if and only if this list contains\n * at least one element <tt>e</tt> such that\n * <tt>(o==null&nbsp;?&nbsp;e==null&nbsp;:&nbsp;o.equals(e))</tt>.\n *\n * @param o element whose presence in this list is to be tested\n * @return <tt>true</tt> if this list contains the specified element\n * @throws ClassCastException if the type of the specified element\n *         is incompatible with this list\n * (<a href=\"Collection.html#optional-restrictions\">optional</a>)\n * @throws NullPointerException if the specified element is null and this\n *         list does not permit null elements\n * (<a href=\"Collection.html#optional-restrictions\">optional</a>)\n */\nboolean contains(Object o);", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477667017", "createdAt": "2020-08-26T23:24:22Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.\n+      if (timeoutMs >= _debounceTimerMs) {\n+        // check if the owner is dead.\n+        if (_liveInstancesProvider != null && !getLiveInstances().contains(owner)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0NTE3OQ=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2ODY4OQ==", "bodyText": "Thanks for explaining offline, make sense to me now. Since leaving it as ensureReadData() causes tests to fail I'm okay with either adding a comment or leaving it out to explain why readData() is used here.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477668689", "createdAt": "2020-08-26T23:25:50Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1082,34 +1219,38 @@ private void waitForTaskRelease(DatastreamTask task, long timeoutMs, String lock\n    * @see #releaseTask(DatastreamTaskImpl)\n    */\n   public void acquireTask(DatastreamTaskImpl task, Duration timeout) {\n-    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockRoot(_cluster, task.getConnectorType()));\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockPrefix(_cluster, task.getConnectorType(), task.getTaskPrefix()));\n+    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     String owner = null;\n     if (_zkclient.exists(lockPath)) {\n-      owner = _zkclient.ensureReadData(lockPath);\n-      if (owner.equals(_instanceName)) {\n+      owner = _zkclient.readData(lockPath, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4MjkyMQ=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2OTQ5Nw==", "bodyText": "Sure, are you keep track of all these changes somewhere? Just so that they're not forgotten.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477669497", "createdAt": "2020-08-26T23:26:34Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1218,20 +1359,23 @@ public void cleanUpPartitionMovement(String connectorType, String datastreamGrou\n    * @see #acquireTask(DatastreamTaskImpl, Duration)\n    */\n   public void releaseTask(DatastreamTaskImpl task) {\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    String lockPath =\n+        KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     if (!_zkclient.exists(lockPath)) {\n-      LOG.info(\"There is no lock on {}\", task);\n+      LOG.info(\"There is no lock on {}-{}/{}\", task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n       return;\n     }\n \n     String owner = _zkclient.ensureReadData(lockPath);\n     if (!owner.equals(_instanceName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMTI1NQ=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 374}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY3MDgyNA==", "bodyText": "Got it. In your other PR to bring tasks down, I noticed you had a comment explaining that this is temporary. Want to add it here? Or add this as an action item in some document where you're tracking all the changes needed. Don't want to lose sight of these things.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477670824", "createdAt": "2020-08-26T23:27:46Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1613,7 +1757,10 @@ private void scheduleExpiryTimerAfterSessionTimeout() {\n   @VisibleForTesting\n   void onSessionExpired() {\n     LOG.error(\"Zookeeper session expired.\");\n+    //cancel the lock clean up\n+    _orphanLockCleanupFuture.cancel(true);\n     onBecomeFollower();\n+    connect();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDQ5Mw=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 395}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY3MjczMw==", "bodyText": "Oops didn't check the full file for tests. Thanks for pointing it out.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477672733", "createdAt": "2020-08-26T23:29:04Z", "author": {"login": "somandal"}, "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -751,14 +755,66 @@ public void testZookeeperSessionExpiry() throws InterruptedException {\n     Mockito.verify(adapter, Mockito.times(1)).onSessionExpired();\n   }\n \n+  @Test\n+  public void testZookeeperLockAcquire() throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTY1NQ=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY4NjM4NQ==", "bodyText": "Got it, thanks!", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477686385", "createdAt": "2020-08-26T23:38:09Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNjg5Mw=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 193}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY4ODQ3NQ==", "bodyText": "Sure, let's see what @ahmedahamid thinks about this API and make a call either way.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477688475", "createdAt": "2020-08-26T23:39:37Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNjA5Mg=="}, "originalCommit": {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY5Mjc3OQ==", "bodyText": "Can we print the value of cleanUpOrphanTaskLocksInConnector to know whether we are just identifying orphans vs. cleaning them up?", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477692779", "createdAt": "2020-08-26T23:42:30Z", "author": {"login": "somandal"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1062,135 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Map<String, Set<String>> getDatastreamTaskNamesGroupedByConnector() {\n+    return getAllAssignedDatastreamTasks()\n+        .values()\n+        .stream()\n+        .flatMap(Collection::stream)\n+        .collect(groupingBy(DatastreamTask::getConnectorType, mapping(DatastreamTask::getDatastreamTaskName, toSet())));\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce time.\n+   * Lock cleanup must be scheduled after a debounce timer because:\n+   *\n+   * a. There is no easy way to know if the task is still running even though it is [un/re]assigned.\n+   * b. All tasks which are unassigned must be stopped within a fixed amount of time which is <= debounce timer.\n+   *\n+   * Thus waiting for the debounce timer gives the guarantee that reassigned/dead tasks have actually stopped.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader gets elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      LOG.info(\"Skipping cleanUpOrphanConnectorTaskLocks. isLeader: {} lock cleanup scheduled: {}\",\n+          _isLeader, _orphanLockCleanupFuture.isDone());\n+      return 0;\n+    }\n+\n+    LOG.info(\"cleanUpOrphanConnectorTaskLocks called\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "originalPosition": 234}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NzIxMTAw", "url": "https://github.com/linkedin/brooklin/pull/747#pullrequestreview-478721100", "createdAt": "2020-08-31T15:35:35Z", "commit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "state": "DISMISSED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNTozNTozNVrOHJ9xgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMToyNDo1MlrOHKSW2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIxMTMzMQ==", "bodyText": "nit: remove space after taskName", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480211331", "createdAt": "2020-08-31T15:35:35Z", "author": {"login": "ahmedahamid"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -71,9 +71,14 @@ private KeyBuilder() {\n   private static final String DATASTREAM_TASK_LOCK_ROOT = CONNECTOR + \"/\" + DATASTREAM_TASK_LOCK_ROOT_NAME;\n \n   /**\n-   * Task lock node under connectorType/lock/{taskName}\n+   * Task lock node under connectorType/lock/{taskPrefix}\n    */\n-  private static final String DATASTREAM_TASK_LOCK = DATASTREAM_TASK_LOCK_ROOT + \"/%s\";\n+  private static final String DATASTREAM_TASK_LOCK_PREFIX = DATASTREAM_TASK_LOCK_ROOT + \"/%s\";\n+\n+  /**\n+   * Task lock node under connectorType/lock/{taskPrefix}/{taskName }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIzMjkwNg==", "bodyText": "s/Scheduler/Thread", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480232906", "createdAt": "2020-08-31T16:07:30Z", "author": {"login": "ahmedahamid"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -114,38 +119,43 @@\n \n   private final String _defaultTransportProviderName;\n \n+  private final ZkAdapterListener _listener;\n+  private final Random randomGenerator = new Random();\n   private final String _zkServers;\n   private final String _cluster;\n   private final int _sessionTimeoutMs;\n   private final int _connectionTimeoutMs;\n   private final int _operationRetryTimeoutMs;\n-  private ZkClient _zkclient;\n+  private final long _debounceTimerMs;\n \n+  private ZkClient _zkclient;\n   private String _instanceName;\n   private String _liveInstanceName;\n   private String _hostname;\n   private Set<String> _connectorTypes = new HashSet<>();\n \n   private volatile boolean _isLeader = false;\n-  private final ZkAdapterListener _listener;\n-\n   // the current znode this node is listening to\n   private String _currentSubscription = null;\n \n-  private final Random randomGenerator = new Random();\n-\n   private ZkLeaderElectionListener _leaderElectionListener = null;\n   private ZkBackedTaskListProvider _assignmentList = null;\n   private ZkStateChangeListener _stateChangeListener = null;\n+  private ZkBackedLiveInstanceListProvider _liveInstancesProvider = null;\n \n   // only the leader should maintain this list and listen to the changes of live instances\n   private ZkBackedDMSDatastreamList _datastreamList = null;\n-  private ZkBackedLiveInstanceListProvider _liveInstancesProvider = null;\n   private ZkTargetAssignmentProvider _targetAssignmentProvider = null;\n \n   // Cache all live DatastreamTasks per instance for assignment strategy\n   private Map<String, Set<DatastreamTask>> _liveTaskMap = new HashMap<>();\n \n+  // cleanup orphan lock in separate thread.\n+  private final ScheduledExecutorService _scheduledExecutorServiceOrphanLockCleanup = Executors.newScheduledThreadPool(1,\n+      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"OrphanLockCleanupScheduler-%d\").build());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU0Mjg5MQ==", "bodyText": "s/x/prefix", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480542891", "createdAt": "2020-09-01T01:19:34Z", "author": {"login": "ahmedahamid"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -827,6 +840,14 @@ public void addConnectorType(String connectorType) {\n     return new HashSet<>(_zkclient.getChildren(KeyBuilder.connector(_cluster, connector)));\n   }\n \n+  @VisibleForTesting\n+  Map<String, Set<String>> getAllConnectorTaskLocks(String connector) {\n+    return _zkclient.getChildren(KeyBuilder.datastreamTaskLockRoot(_cluster, connector))\n+        .stream()\n+        .collect(Collectors.toMap(Function.identity(),\n+            x -> new HashSet<>(_zkclient.getChildren(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, x)))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU0ODU2OQ==", "bodyText": "This method is going to make N roundtrips to ZK where N's the number of unique task prefixes in the cluster. This number is close to 1500 in one of our bigger change capture clusters atm. Just calling this out in case extended lock cleanup duration could cause issues.", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480548569", "createdAt": "2020-09-01T01:24:52Z", "author": {"login": "ahmedahamid"}, "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -827,6 +840,14 @@ public void addConnectorType(String connectorType) {\n     return new HashSet<>(_zkclient.getChildren(KeyBuilder.connector(_cluster, connector)));\n   }\n \n+  @VisibleForTesting\n+  Map<String, Set<String>> getAllConnectorTaskLocks(String connector) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40"}, "originalPosition": 145}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dbdb40d96665392013407eed193e27283f825e47", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/dbdb40d96665392013407eed193e27283f825e47", "committedDate": "2020-09-01T07:16:00Z", "message": "Address fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5ODQ3MTQz", "url": "https://github.com/linkedin/brooklin/pull/747#pullrequestreview-479847143", "createdAt": "2020-09-01T15:11:50Z", "commit": {"oid": "dbdb40d96665392013407eed193e27283f825e47"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5OTMwNjM5", "url": "https://github.com/linkedin/brooklin/pull/747#pullrequestreview-479930639", "createdAt": "2020-09-01T16:40:07Z", "commit": {"oid": "dbdb40d96665392013407eed193e27283f825e47"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 279, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}