{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNjEwMjMw", "number": 789, "title": "[issue 788] make start offsets take effect when set in KafkaConnectorTask", "bodyText": "Please see #788 for a description of the problem\nthis changes addresses this issue by overriding the auto.offset.reset kafka consumer config and sets it to none whenever start offsets are set in the AbstractKafkaBasedConnectorTask, so that the set start offsets takes effect while handling the NoCheckpointForPartitoinException upon first poll (given that auto.offset.reset=none)\nModified unit test to verify the override, also validated from the logs of the run that added overriding logs are emitted.", "createdAt": "2020-12-17T05:22:15Z", "url": "https://github.com/linkedin/brooklin/pull/789", "merged": true, "mergeCommit": {"oid": "d83cd674cb8b0f6a70b885a768b23c19aa02a974"}, "closed": true, "closedAt": "2020-12-21T21:07:11Z", "author": {"login": "shenodaguirguis"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdnHaaaAFqTU1NDgzNDE2OA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdocANzgFqTU1NjY0NjM1Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0ODM0MTY4", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-554834168", "createdAt": "2020-12-17T17:37:01Z", "commit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozNzowMVrOIIA8sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzo1ODoxNlrOIIB0iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTA1Nw==", "bodyText": "nit: If this datastream wants", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545275057", "createdAt": "2020-12-17T17:37:01Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTM1Mg==", "bodyText": "nit: typo on the.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545275352", "createdAt": "2020-12-17T17:37:25Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NjY3MQ==", "bodyText": "unrelated to your change, but should we make _startOffsets not Optional? It is a map and we are wrapping it in a Nullable only to be using isPresent everywhere if present needing to access it with the get call which is not serving the purpose of Optional at all.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545276671", "createdAt": "2020-12-17T17:39:26Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng==", "bodyText": "nit: I was originally conflicted about the change because both start position and reset strategy are overridable values and we are basically giving higher precedence to start position by adjusting the reset strategy. But on more thought I do feel like this is the right thing to do, given that reset strategy isn't something a client will provide when they have already specified the start position. Although, it would be confusing is if they provided both as it wouldn't be possible to figure out which one they really wanted to set, I do feel like start position somehow feels like a more granular thing to specify and so should probably take precedence.\nCan we however fetch offset reset strategy first so that in the logs it can be made clear that that was ignored because startPosition was also set?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545283096", "createdAt": "2020-12-17T17:48:53Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ==", "bodyText": "Can we add a specific test case to try the use-cases out? Many cases crop up since it is 2 configs, but may be we need at least the one you are fixing, with start position specified and no reset strategy.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545289355", "createdAt": "2020-12-17T17:58:16Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0ODYyNzI3", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-554862727", "createdAt": "2020-12-17T18:11:57Z", "commit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxODoxMTo1N1rOIICYUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxODoxODowNVrOIICncQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5ODUxMw==", "bodyText": "nit: NoOffsetForPartiionException -> NoOffsetForPartitionException", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545298513", "createdAt": "2020-12-17T18:11:57Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5OTk4OA==", "bodyText": "Can you return a null string instead of \"NOT SET\"?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545299988", "createdAt": "2020-12-17T18:14:19Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -1035,4 +1047,9 @@ public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor gro\n       throw e;\n     }\n   }\n+\n+  @VisibleForTesting\n+  public String getConsumerAutoOffsetResetConfig() {\n+    return _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"NOT SET\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMwMTM0MA==", "bodyText": "+1, I'd go as far as to make sure that we see the expected behavior when start positions are present, and when they aren't.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545301340", "createdAt": "2020-12-17T18:16:23Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMwMjM4NQ==", "bodyText": "+1\nWe could perhaps add some kind of validation that start positions cannot be present if the auto.offset.reset strategy is set to anything other than \"none\" if we feel the need to be careful, but I'll leave the decision to add this or not to you.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545302385", "createdAt": "2020-12-17T18:18:05Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/69ca1503c17f7b80e263fee0d33f2b45a482ac39", "committedDate": "2020-12-17T05:12:37Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}, "afterCommit": {"oid": "05823e9c923f1bef307323c09f3efa840c7faeae", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/05823e9c923f1bef307323c09f3efa840c7faeae", "committedDate": "2020-12-18T01:41:32Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "05823e9c923f1bef307323c09f3efa840c7faeae", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/05823e9c923f1bef307323c09f3efa840c7faeae", "committedDate": "2020-12-18T01:41:32Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}, "afterCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "committedDate": "2020-12-18T02:25:36Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1Njg3Mjk2", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-555687296", "createdAt": "2020-12-18T17:42:26Z", "commit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0MjoyNlrOIIscQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0ODo0NlrOIIspdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NzY0OQ==", "bodyText": "nit:  teh -> the", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545987649", "createdAt": "2020-12-18T17:42:26Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MDE0Mg==", "bodyText": "In case someone adds the datastream metadata too, should we print that here too? It'll be helpful in debugging if someone sets this to something else and is wondering why it isn't taking effect.\nString strategy = _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY);\nThis log is only printing the config.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545990142", "createdAt": "2020-12-18T17:47:05Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MDQ4OQ==", "bodyText": "can we modify getConsumerAutoOffsetResetConfig to set an empty string \"\" instead of null, and call that here? Then instead of null checks we can use isBlank checks.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545990489", "createdAt": "2020-12-18T17:47:46Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n+          ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n+          _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MTAyOA==", "bodyText": "nit: add an empty line after this", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545991028", "createdAt": "2020-12-18T17:48:46Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -448,6 +452,13 @@ static Datastream getDatastream(String broker, String topic) {\n     return datastream;\n   }\n \n+  private KafkaConnectorTask createKafkaConnectorTaskWithAutoOffsetResetConfig(DatastreamTaskImpl task,\n+      String autoOffsetResetStrategy) throws InterruptedException {\n+    Properties consumerProperties = new Properties();\n+    consumerProperties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetResetStrategy);\n+    return createKafkaConnectorTask(task, new KafkaBasedConnectorConfigBuilder()\n+        .setConsumerProps(consumerProperties).build());\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 23}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/94d540e6b277432092bebe4ff0a4ed42b0ce85f3", "committedDate": "2020-12-18T02:25:36Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}, "afterCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/2df71d42261f8e3eefffb82899b21daf417a2818", "committedDate": "2020-12-18T18:27:54Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2MDM0NDc1", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-556034475", "createdAt": "2020-12-20T16:59:28Z", "commit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "state": "DISMISSED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NjI0MDEx", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-556624011", "createdAt": "2020-12-21T19:48:39Z", "commit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxOTo0ODozOVrOIJkBSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxOTo0OTowNFrOIJkCGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODI1MQ==", "bodyText": "was the choice of 100L intentional for start timestamp?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546898251", "createdAt": "2020-12-21T19:48:39Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA==", "bodyText": "The name start position unfortunately is confusing since it is a timestamp. Since you have set it to 100 I am guessing it is returning null? And so what happens when it returns null? What is the code flow when this happens?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546898458", "createdAt": "2020-12-21T19:49:04Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);\n+\n+    LOG.info(\"Sending second set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 100, 100);\n+\n+    //start\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    Datastream datastream = getDatastream(_broker, topic);\n+\n+    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 62}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae7498afad70fdad4efa704519465d1471d20ff4", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/ae7498afad70fdad4efa704519465d1471d20ff4", "committedDate": "2020-12-21T20:24:13Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/2df71d42261f8e3eefffb82899b21daf417a2818", "committedDate": "2020-12-18T18:27:54Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}, "afterCommit": {"oid": "ae7498afad70fdad4efa704519465d1471d20ff4", "author": {"user": null}, "url": "https://github.com/linkedin/brooklin/commit/ae7498afad70fdad4efa704519465d1471d20ff4", "committedDate": "2020-12-21T20:24:13Z", "message": "[issue 788] make start offsets take effect when set in KafkaConnectorTask"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NjQzOTUy", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-556643952", "createdAt": "2020-12-21T20:26:56Z", "commit": {"oid": "ae7498afad70fdad4efa704519465d1471d20ff4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NjQ2MzUz", "url": "https://github.com/linkedin/brooklin/pull/789#pullrequestreview-556646353", "createdAt": "2020-12-21T20:31:47Z", "commit": {"oid": "ae7498afad70fdad4efa704519465d1471d20ff4"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 355, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}