{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNjEwMjMw", "number": 789, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozNzowMVrOFG0xJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxOTo0OTowNFrOFH6dew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzAwMzI3OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozNzowMVrOIIA8sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozNzowMVrOIIA8sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTA1Nw==", "bodyText": "nit: If this datastream wants", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545275057", "createdAt": "2020-12-17T17:37:01Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzAwNTI4OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozNzoyNVrOIIA92A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxOTowNzowNlrOIIEkqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTM1Mg==", "bodyText": "nit: typo on the.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545275352", "createdAt": "2020-12-17T17:37:25Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMzNDQ0MA==", "bodyText": "Nit: Another typo one the exception name", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545334440", "createdAt": "2020-12-17T19:07:06Z", "author": {"login": "nisargthakkar"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NTM1Mg=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzAxNDAxOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozOToyNlrOIIBC_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzoxMjozMVrOIIMmbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NjY3MQ==", "bodyText": "unrelated to your change, but should we make _startOffsets not Optional? It is a map and we are wrapping it in a Nullable only to be using isPresent everywhere if present needing to access it with the get call which is not serving the purpose of Optional at all.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545276671", "createdAt": "2020-12-17T17:39:26Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ2NTk2NQ==", "bodyText": "makes sense. I made _startOffsets not Optional", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545465965", "createdAt": "2020-12-17T23:12:31Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3NjY3MQ=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzA1NTQ1OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzo0ODo1M1rOIIBcGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxODoxMzo1NlrOIJhaOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng==", "bodyText": "nit: I was originally conflicted about the change because both start position and reset strategy are overridable values and we are basically giving higher precedence to start position by adjusting the reset strategy. But on more thought I do feel like this is the right thing to do, given that reset strategy isn't something a client will provide when they have already specified the start position. Although, it would be confusing is if they provided both as it wouldn't be possible to figure out which one they really wanted to set, I do feel like start position somehow feels like a more granular thing to specify and so should probably take precedence.\nCan we however fetch offset reset strategy first so that in the logs it can be made clear that that was ignored because startPosition was also set?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545283096", "createdAt": "2020-12-17T17:48:53Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMwMjM4NQ==", "bodyText": "+1\nWe could perhaps add some kind of validation that start positions cannot be present if the auto.offset.reset strategy is set to anything other than \"none\" if we feel the need to be careful, but I'll leave the decision to add this or not to you.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545302385", "createdAt": "2020-12-17T18:18:05Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMyNjk3NQ==", "bodyText": "I was thinking about validation, but then the stream task would just die, right? I wasn't terribly sure if this was a good idea since clients are not necessarily good at figuring out what's happening on the server, but may be in such cases they are, and may be SREs should in fact get alerted for bad configs.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545326975", "createdAt": "2020-12-17T18:54:58Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMyODk5MA==", "bodyText": "@DEEPTHIKORAT what about checking this during a) initialize datastream and b) datastream update? This way both stream creation/update will fail.\nAgain don't have a strong opinion on whether we should add this or not, but it's just a thought if we're worried about explaining the behavior when both metadata fields are present.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545328990", "createdAt": "2020-12-17T18:58:14Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ5NjQxMg==", "bodyText": "I agree with Deepthi that failing might not be the right thing to do. And that was exactly my thinking @DEEPTHIKORAT  that when users set the start offsets, they mean to use it. I am also doing exactly as you suggest: I am logging the original value before the update. Here is a sample log from the test:\n2020-12-17 16:27:41 INFO  KafkaConnectorTask:176 - Datastream contains startOffsets, override auto.offset.reset with value none (was earliest in the provided configs)\nI will attempt to reword to make it clearer", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545496412", "createdAt": "2020-12-18T00:35:15Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg0MTI3MQ==", "bodyText": "I think that is a nice idea @somandal. @shenodaguirguis feel free to check in this version since Sonam has approved it as well, but we can consider making the change Sonam suggested in the future perhaps. At least then it is a user-facing failure and they won't be caught unaware.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546841271", "createdAt": "2020-12-21T17:42:54Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg1NTQ4MQ==", "bodyText": "we can have a separate change for this", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546855481", "createdAt": "2020-12-21T18:13:56Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (_startOffsets.isPresent()) {\n+      _logger.info(\"Datastream contains startOffsets, override {} with value {} (was {} in the provided configs)\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4MzA5Ng=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzA5NTg1OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzo1ODoxNlrOIIB0iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODo0NDozNlrOIIudkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ==", "bodyText": "Can we add a specific test case to try the use-cases out? Many cases crop up since it is 2 configs, but may be we need at least the one you are fixing, with start position specified and no reset strategy.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545289355", "createdAt": "2020-12-17T17:58:16Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTMwMTM0MA==", "bodyText": "+1, I'd go as far as to make sure that we see the expected behavior when start positions are present, and when they aren't.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545301340", "createdAt": "2020-12-17T18:16:23Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUwNDA4OQ==", "bodyText": "testConsumeBaseCase tests when no startOffsets are present,\ntestConsumeWithTheStartingOffset tests the case with startOffsets are present. It was testing with no auto.offset.rest set, I changed it to set auto.offset.reset set to earliest, and it already verifies the expected behavior (in terms of number of records consumed)\nThe actual value of the auto.offset.reset is irrelevant, we only need to test if it is set that we do override. Whether it was earliest,  latest or none, is orthogonal.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545504089", "createdAt": "2020-12-18T00:58:17Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAyMDc1NA==", "bodyText": "I added a test case for when the auto offset reset strategy is set in metadata, to verify startOffsets takes precedence", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546020754", "createdAt": "2020-12-18T18:44:36Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -172,6 +172,9 @@ public void testConsumeWithStartingOffset() throws Exception {\n \n     KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n \n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI4OTM1NQ=="}, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzE1NTEyOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxODoxMTo1N1rOIICYUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxODoxMTo1N1rOIICYUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5ODUxMw==", "bodyText": "nit: NoOffsetForPartiionException -> NoOffsetForPartitionException", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545298513", "createdAt": "2020-12-17T18:11:57Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -168,6 +164,22 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n         .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n         }));\n+\n+    // if this datastream want to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartiionException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzE2NDkyOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxODoxNDoxOVrOIICeFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxODoxNDoxOVrOIICeFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5OTk4OA==", "bodyText": "Can you return a null string instead of \"NOT SET\"?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545299988", "createdAt": "2020-12-17T18:14:19Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -1035,4 +1047,9 @@ public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor gro\n       throw e;\n     }\n   }\n+\n+  @VisibleForTesting\n+  public String getConsumerAutoOffsetResetConfig() {\n+    return _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"NOT SET\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69ca1503c17f7b80e263fee0d33f2b45a482ac39"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTgwNjYxOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0MjoyNlrOIIscQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0MjoyNlrOIIscQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NzY0OQ==", "bodyText": "nit:  teh -> the", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545987649", "createdAt": "2020-12-18T17:42:26Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTgyMzE0OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0NzowNVrOIIsl_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0NzowNVrOIIsl_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MDE0Mg==", "bodyText": "In case someone adds the datastream metadata too, should we print that here too? It'll be helpful in debugging if someone sets this to something else and is wondering why it isn't taking effect.\nString strategy = _datastream.getMetadata().get(KafkaDatastreamMetadataConstants.CONSUMER_OFFSET_RESET_STRATEGY);\nThis log is only printing the config.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545990142", "createdAt": "2020-12-18T17:47:05Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTgyNTU0OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0Nzo0NlrOIIsnWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0Nzo0NlrOIIsnWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MDQ4OQ==", "bodyText": "can we modify getConsumerAutoOffsetResetConfig to set an empty string \"\" instead of null, and call that here? Then instead of null checks we can use isBlank checks.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545990489", "createdAt": "2020-12-18T17:47:46Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -165,9 +161,30 @@ protected AbstractKafkaBasedConnectorTask(KafkaBasedConnectorConfig config, Data\n     _pausePartitionOnError = config.getPausePartitionOnError();\n     _pauseErrorPartitionDuration = config.getPauseErrorPartitionDuration();\n     _enableAdditionalMetrics = config.getEnableAdditionalMetrics();\n-    _startOffsets = Optional.ofNullable(_datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION))\n-        .map(json -> JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n-        }));\n+\n+    _startOffsets = new HashMap<>();\n+    String json = _datastream.getMetadata().get(DatastreamMetadataConstants.START_POSITION);\n+    if (StringUtils.isNotBlank(json)) {\n+      _startOffsets.putAll(JsonUtils.fromJson(json, new TypeReference<Map<Integer, Long>>() {\n+      }));\n+    }\n+\n+    // if this datastream wants to use specific start offsets, we need to have the auto.offset.reset config\n+    // set to \"none\" in the kafka consumer configs, so that the Kafka consumer throws NoOffsetForPartitionException\n+    // upon first poll, to be handled by seeking to teh startOffsets\n+    if (!_startOffsets.isEmpty()) {\n+      _logger.info(\"Datastream contains startOffsets, setting {} = \\\"{}\\\" in consumer configs \"\n+              + \"(overriding the configs value of \\\"{}\\\")\",\n+          ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, CONSUMER_AUTO_OFFSET_RESET_CONFIG_NONE,\n+          _consumerProps.getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTgyOTMwOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0ODo0NlrOIIspdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzo0ODo0NlrOIIspdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MTAyOA==", "bodyText": "nit: add an empty line after this", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r545991028", "createdAt": "2020-12-18T17:48:46Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -448,6 +452,13 @@ static Datastream getDatastream(String broker, String topic) {\n     return datastream;\n   }\n \n+  private KafkaConnectorTask createKafkaConnectorTaskWithAutoOffsetResetConfig(DatastreamTaskImpl task,\n+      String autoOffsetResetStrategy) throws InterruptedException {\n+    Properties consumerProperties = new Properties();\n+    consumerProperties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetResetStrategy);\n+    return createKafkaConnectorTask(task, new KafkaBasedConnectorConfigBuilder()\n+        .setConsumerProps(consumerProperties).build());\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94d540e6b277432092bebe4ff0a4ed42b0ce85f3"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODQyMDE3OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxOTo0ODozOVrOIJkBSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDoyMDoxNFrOIJk09g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODI1MQ==", "bodyText": "was the choice of 100L intentional for start timestamp?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546898251", "createdAt": "2020-12-21T19:48:39Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMTQ3OA==", "bodyText": "yes intentional, and it is a start offset, not timestamp.\nwe produce 100 events, set startOffsets to 100, produce 200 more and make sure we consume 200 only to verify start offset metadata took effect", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546911478", "createdAt": "2020-12-21T20:20:14Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODI1MQ=="}, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzODQyMTcxOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxOTo0OTowNFrOIJkCGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDoyNjozNVrOIJk-pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA==", "bodyText": "The name start position unfortunately is confusing since it is a timestamp. Since you have set it to 100 I am guessing it is returning null? And so what happens when it returns null? What is the code flow when this happens?", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546898458", "createdAt": "2020-12-21T19:49:04Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);\n+\n+    LOG.info(\"Sending second set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 100, 100);\n+\n+    //start\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    Datastream datastream = getDatastream(_broker, topic);\n+\n+    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMjgwNA==", "bodyText": "a) I copied and modfied testConsumeWithStartOffset, so not sure what this comment refers to, I removed it.\nb) it is not a timestamp, it is an offset. The metadata key is: DatastreamMetadataConstants.START_POSITION = \"system.start.position\"", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546912804", "createdAt": "2020-12-21T20:23:42Z", "author": {"login": "shenodaguirguis"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);\n+\n+    LOG.info(\"Sending second set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 100, 100);\n+\n+    //start\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    Datastream datastream = getDatastream(_broker, topic);\n+\n+    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA=="}, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxMzk1Nw==", "bodyText": "I see. So it does seem to be offset in the implementation and there are some other connectors storing time. I was guessing that is where the comment about offsetForTimestamp was coming from. Good to know it actually causes no issues.", "url": "https://github.com/linkedin/brooklin/pull/789#discussion_r546913957", "createdAt": "2020-12-21T20:26:35Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/TestKafkaConnectorTask.java", "diffHunk": "@@ -170,7 +171,58 @@ public void testConsumeWithStartingOffset() throws Exception {\n     DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n     task.setEventProducer(datastreamProducer);\n \n-    KafkaConnectorTask connectorTask = createKafkaConnectorTask(task);\n+    KafkaConnectorTask connectorTask = createKafkaConnectorTaskWithAutoOffsetResetConfig(task, \"earliest\");\n+\n+    // validate auto.offset.reset config is overridden to none (given the start offsets)\n+    Assert.assertEquals(connectorTask.getConsumerAutoOffsetResetConfig(), \"none\");\n+\n+\n+    LOG.info(\"Sending third set of events\");\n+\n+    //send 100 more msgs\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 1000, 100);\n+\n+    if (!PollUtils.poll(() -> datastreamProducer.getEvents().size() == 200, 100, POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"did not transfer 200 msgs within timeout. transferred \" + datastreamProducer.getEvents().size());\n+    }\n+\n+    connectorTask.stop();\n+    Assert.assertTrue(connectorTask.awaitStop(CONNECTOR_AWAIT_STOP_TIMEOUT_MS, TimeUnit.MILLISECONDS),\n+        \"did not shut down on time\");\n+  }\n+\n+  @Test\n+  public void testConsumeWithStartingOffsetAndResetStrategy() throws Exception {\n+    String topic = \"pizza1\";\n+    createTopic(_zkUtils, topic);\n+\n+    LOG.info(\"Sending first set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 0, 100);\n+    Map<Integer, Long> startOffsets = Collections.singletonMap(0, 100L);\n+\n+    LOG.info(\"Sending second set of events\");\n+\n+    //produce 100 msgs to topic before start\n+    produceEvents(_kafkaCluster, _zkUtils, topic, 100, 100);\n+\n+    //start\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    Datastream datastream = getDatastream(_broker, topic);\n+\n+    // Unable to set the start position, OffsetToTimestamp is returning null in the embedded Kafka cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5ODQ1OA=="}, "originalCommit": {"oid": "2df71d42261f8e3eefffb82899b21daf417a2818"}, "originalPosition": 62}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 942, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}