{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg5Mzc4NTIx", "number": 695, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNDo1OToyM1rODpi8FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNToyMjoyMVrODq12DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODkwNjQ1OnYy", "diffSide": "RIGHT", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNDo1OToyM1rOF4ye2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTozNTowN1rOF40JEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MTY3NQ==", "bodyText": "Please, add a comment explaining why executing this flush() call on a separate thread was necessary.\n\n\nFeel free to ignore this comment: not that it matters much, but would it be slightly more appropriate to use Executors.newSingleThreadExecutor() or even just new Thread().start()?", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395091675", "createdAt": "2020-03-19T14:59:23Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExODg2NA==", "bodyText": "done", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395118864", "createdAt": "2020-03-19T15:35:07Z", "author": {"login": "somandal"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MTY3NQ=="}, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODkxODMxOnYy", "diffSide": "RIGHT", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTowMjowMlrOF4ymqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTozNToxNlrOF40Jbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MzY3NA==", "bodyText": "nit: prefix field names with an underscore", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395093674", "createdAt": "2020-03-19T15:02:02Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExODk1OA==", "bodyText": "woops, done!", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395118958", "createdAt": "2020-03-19T15:35:16Z", "author": {"login": "somandal"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MzY3NA=="}, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODkzMDQ5OnYy", "diffSide": "RIGHT", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTowNDozOVrOF4yulg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTozNToyNlrOF40J-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NTcwMg==", "bodyText": "Just to make it super simple for anyone reading this test, please add a comment summarizing your intent (e.g. calling flush() on the first created producer will throw).", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395095702", "createdAt": "2020-03-19T15:04:39Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;\n+\n+    MockKafkaProducerWrapper(String logSuffix, Properties props, String metricsNamesPrefix) {\n+      super(logSuffix, props, metricsNamesPrefix);\n+    }\n+\n+    @Override\n+    Producer<byte[], byte[]> createKafkaProducer() {\n+      @SuppressWarnings(\"unchecked\")\n+      Producer<byte[], byte[]> producer = (Producer<byte[], byte[]>) mock(Producer.class);\n+      if (!createKafkaProducerCalled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExOTA5OQ==", "bodyText": "done", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395119099", "createdAt": "2020-03-19T15:35:26Z", "author": {"login": "somandal"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;\n+\n+    MockKafkaProducerWrapper(String logSuffix, Properties props, String metricsNamesPrefix) {\n+      super(logSuffix, props, metricsNamesPrefix);\n+    }\n+\n+    @Override\n+    Producer<byte[], byte[]> createKafkaProducer() {\n+      @SuppressWarnings(\"unchecked\")\n+      Producer<byte[], byte[]> producer = (Producer<byte[], byte[]>) mock(Producer.class);\n+      if (!createKafkaProducerCalled) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NTcwMg=="}, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0ODk5MjIzOnYy", "diffSide": "RIGHT", "path": "datastream-kafka/src/main/java/com/linkedin/datastream/kafka/KafkaProducerWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNToxODowOFrOF4zXBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTozNTozM1rOF40KVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwNjA1NA==", "bodyText": "@VisibleForTesting", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395106054", "createdAt": "2020-03-19T15:18:08Z", "author": {"login": "ahmedahamid"}, "path": "datastream-kafka/src/main/java/com/linkedin/datastream/kafka/KafkaProducerWrapper.java", "diffHunk": "@@ -173,13 +174,17 @@ int getTasksSize() {\n     } else {\n       if (_kafkaProducer == null) {\n         _rateLimiter.acquire();\n-        _kafkaProducer = _producerFactory.createProducer(_props);\n+        _kafkaProducer = createKafkaProducer();\n         NUM_PRODUCERS.incrementAndGet();\n       }\n     }\n     return _kafkaProducer;\n   }\n \n+  Producer<K, V> createKafkaProducer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExOTE5MQ==", "bodyText": "done", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395119191", "createdAt": "2020-03-19T15:35:33Z", "author": {"login": "somandal"}, "path": "datastream-kafka/src/main/java/com/linkedin/datastream/kafka/KafkaProducerWrapper.java", "diffHunk": "@@ -173,13 +174,17 @@ int getTasksSize() {\n     } else {\n       if (_kafkaProducer == null) {\n         _rateLimiter.acquire();\n-        _kafkaProducer = _producerFactory.createProducer(_props);\n+        _kafkaProducer = createKafkaProducer();\n         NUM_PRODUCERS.incrementAndGet();\n       }\n     }\n     return _kafkaProducer;\n   }\n \n+  Producer<K, V> createKafkaProducer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwNjA1NA=="}, "originalCommit": {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2MjQ4OTczOnYy", "diffSide": "RIGHT", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNToyMjoyMVrOF61ebg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNTo0NzowMFrOF62rGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIzNzg3MA==", "bodyText": "nit: resetting send, flush and close (for completeness)", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r397237870", "createdAt": "2020-03-24T15:22:21Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper<byte[], byte[]> producerWrapper =\n+        new MockKafkaProducerWrapper<>(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    // Calling the first flush() on a separate thread because the InterruptException calls Thread interrupt() on the\n+    // currently running thread. If not run on a separate thread, the test thread itself will be interrupted.\n+    ExecutorService executorService = Executors.newSingleThreadExecutor();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocation counts", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI1NzQ5Ng==", "bodyText": "As discussed offline, I'll take care of this on the next review. :)", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r397257496", "createdAt": "2020-03-24T15:47:00Z", "author": {"login": "somandal"}, "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper<byte[], byte[]> producerWrapper =\n+        new MockKafkaProducerWrapper<>(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    // Calling the first flush() on a separate thread because the InterruptException calls Thread interrupt() on the\n+    // currently running thread. If not run on a separate thread, the test thread itself will be interrupted.\n+    ExecutorService executorService = Executors.newSingleThreadExecutor();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocation counts", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIzNzg3MA=="}, "originalCommit": {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b"}, "originalPosition": 83}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 971, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}