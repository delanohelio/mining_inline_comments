{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEwNDI5MzUx", "number": 705, "title": "Avoid accessing consumer in send callback to avoid ConcurrentModificationException in the Kafka connector", "bodyText": "This change just avoids allowing access to the consumer from different threads to avoid the ConcurrentModificationException we sometimes see. It basically tracks all the TopicPartitions which have seen send failures, and attempts to rewind their offsets to the last checkpoint after flush() or before the next poll.\nThe flush() enabled mode can still land up committing offsets for TopicPartitions that have seen send failures in case rewind to last checkpoint fails. Addressing this will be done as part of a separate PR.\nImportant: DO NOT REPORT SECURITY ISSUES DIRECTLY ON GITHUB.\nFor reporting security issues and contributing security fixes,\nplease, email security@linkedin.com instead, as described in\nthe contribution guidelines.\nPlease, take a minute to review the contribution guidelines at:\nhttps://github.com/linkedin/Brooklin/blob/master/CONTRIBUTING.md", "createdAt": "2020-04-29T00:36:25Z", "url": "https://github.com/linkedin/brooklin/pull/705", "merged": true, "mergeCommit": {"oid": "a006b992aac27f9e635c33f9402d0a459854f0d9"}, "closed": true, "closedAt": "2020-05-04T15:49:15Z", "author": {"login": "somandal"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccNXB-gH2gAyNDEwNDI5MzUxOjA1Nzk3OTQzY2ZhM2RjNmZjYjY3M2M2MGY1MTQ0NDllOGIyYjcxYWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcd4a0oAFqTQwNDcxMjU1Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/05797943cfa3dc6fcb673c60f514449e8b2b71aa", "committedDate": "2020-04-29T00:29:53Z", "message": "Avoid accessing consumer in send callback to avoid ConcurrentModificationException in the Kafka connector"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzMDYzMTc3", "url": "https://github.com/linkedin/brooklin/pull/705#pullrequestreview-403063177", "createdAt": "2020-04-29T21:46:07Z", "commit": {"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQyMTo0NjowN1rOGOSNqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQyMTo0NjowN1rOGOSNqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzMTY1Ng==", "bodyText": "Do you see a value in clarifying in the warning whether skipping the record because of autoPausedSourcePartition or send failure ?", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417631656", "createdAt": "2020-04-29T21:46:07Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -229,8 +232,13 @@ protected void translateAndSendBatch(ConsumerRecords<?, ?> records, Instant read\n     for (TopicPartition topicPartition : records.partitions()) {\n       for (ConsumerRecord<?, ?> record : records.records(topicPartition)) {\n         try {\n-          if (_autoPausedSourcePartitions.containsKey(topicPartition)) {\n-            _logger.warn(\"Abort sending as {} is auto-paused, rewind offset\", topicPartition);\n+          boolean skipRecord;\n+          synchronized (_sendFailureTopicPartitionExceptionMap) {\n+            skipRecord = _autoPausedSourcePartitions.containsKey(topicPartition)\n+                || _sendFailureTopicPartitionExceptionMap.containsKey(topicPartition);\n+          }\n+          if (skipRecord) {\n+            _logger.warn(\"Abort sending as {} is auto-paused or saw a send failure, rewind offset\", topicPartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa"}, "originalPosition": 22}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "committedDate": "2020-04-29T22:55:49Z", "message": "Enhance log message"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzMDY0Njk0", "url": "https://github.com/linkedin/brooklin/pull/705#pullrequestreview-403064694", "createdAt": "2020-04-29T21:49:03Z", "commit": {"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQyMTo0OTowM1rOGOSSxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQyMzoyNToyNVrOGOUh7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzMjk2NA==", "bodyText": "nit: Can you please make both the comment lines of similar length ?", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417632964", "createdAt": "2020-04-29T21:49:03Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -257,18 +265,26 @@ protected void rewindAndPausePartitionOnException(TopicPartition srcTopicPartiti\n       seekToLastCheckpoint(Collections.singleton(srcTopicPartition));\n       partitionRewound = true;\n     } catch (Exception e) {\n-      _logger.error(\"Partition rewind failed due to \", e);\n+      _logger.error(String.format(\"Partition rewind for %s failed due to \", srcTopicPartition), e);\n     }\n     if (_pausePartitionOnError && (!partitionRewound || !containsTransientException(ex))) {\n-      // if doesn't contain DatastreamTransientException and it's configured to pause partition on error conditions,\n-      // add to auto-paused set\n+      // If partition rewind failed or the exception is not of type DatastreamTransientException and it is configured to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzI3Mw==", "bodyText": "Can you add a comment explaining why is it important to check this after flush?", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417637273", "createdAt": "2020-04-29T21:58:26Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -522,6 +540,7 @@ protected void maybeCommitOffsetsInternal(Consumer<?, ?> consumer, boolean force\n     if (force || timeSinceLastCommit > _offsetCommitInterval) {\n       _logger.info(\"Trying to flush the producer and commit offsets.\");\n       _producer.flush();\n+      rewindAndPausePartitionsOnSendException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NTgxOA==", "bodyText": "We should set both the values in sychronized above and use them to display.", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417665818", "createdAt": "2020-04-29T23:13:40Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -229,8 +232,14 @@ protected void translateAndSendBatch(ConsumerRecords<?, ?> records, Instant read\n     for (TopicPartition topicPartition : records.partitions()) {\n       for (ConsumerRecord<?, ?> record : records.records(topicPartition)) {\n         try {\n-          if (_autoPausedSourcePartitions.containsKey(topicPartition)) {\n-            _logger.warn(\"Abort sending as {} is auto-paused, rewind offset\", topicPartition);\n+          boolean skipRecord;\n+          synchronized (_sendFailureTopicPartitionExceptionMap) {\n+            skipRecord = _autoPausedSourcePartitions.containsKey(topicPartition)\n+                || _sendFailureTopicPartitionExceptionMap.containsKey(topicPartition);\n+          }\n+          if (skipRecord) {\n+            _logger.warn(\"Abort sending as {} {}, rewind offset\", topicPartition,\n+                _autoPausedSourcePartitions.containsKey(topicPartition) ? \"is auto-paused\" : \"saw a send failure\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NjUwNQ==", "bodyText": "I think you should move this in a function in AbstractKafkaBasedConnectorTask and use that function and avoid using the map outside the class. You should also set the visibilty to private in that case.", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417666505", "createdAt": "2020-04-29T23:15:41Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTask.java", "diffHunk": "@@ -264,9 +264,12 @@ protected void sendDatastreamProducerRecord(DatastreamProducerRecord datastreamP\n           ((metadata, exception) -> {\n             if (exception != null) {\n               _logger.warn(\n-                  String.format(\"Detected exception being throw from callback for src partition: %s while sending producer \"\n-                      + \"record: %s, exception: \", srcTopicPartition, datastreamProducerRecord), exception);\n-              rewindAndPausePartitionOnException(srcTopicPartition, exception);\n+                  String.format(\"Detected exception being throw from flushless send callback for source \"\n+                      + \"topic-partition: %s with metadata: %s, exception: \", srcTopicPartition, metadata),\n+                  exception);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NzQ3OA==", "bodyText": "You can extract this out into another function and reuse between both connectorTasks", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417667478", "createdAt": "2020-04-29T23:18:47Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -283,10 +300,12 @@ protected void sendDatastreamProducerRecord(DatastreamProducerRecord datastreamP\n       TopicPartition srcTopicPartition, int numBytes, SendCallback sendCallback) {\n     _producer.send(datastreamProducerRecord, ((metadata, exception) -> {\n       if (exception != null) {\n-        String msg = String.format(\"Detect exception being thrown from callback for src partition: %s while \"\n-            + \"sending, metadata: %s , exception: \", srcTopicPartition, metadata);\n+        String msg = String.format(\"Detected exception being thrown from send callback for source topic-partition: %s \"\n+            + \"with metadata: %s, exception: \", srcTopicPartition, metadata);\n         _logger.warn(msg, exception);\n-        rewindAndPausePartitionOnException(srcTopicPartition, exception);\n+        synchronized (_sendFailureTopicPartitionExceptionMap) {\n+          _sendFailureTopicPartitionExceptionMap.put(srcTopicPartition, exception);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg==", "bodyText": "You should check if the topic-partition is already in the pause list and skip the rewind process.  Reason: suppose for a topic-partition, 10 records are polled and got 1 callback with error before next poll, this topic partition will be rewind and paused and poll will continue. Suppose the other 9 callbacks are coming with delay and come before next 2-3 poll iterations, we will do this rewind multiple-times, even though the partition is already paused and rewind. We should try to skip the same no-op  operation multiple times.", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417669612", "createdAt": "2020-04-29T23:25:25Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -257,18 +266,26 @@ protected void rewindAndPausePartitionOnException(TopicPartition srcTopicPartiti\n       seekToLastCheckpoint(Collections.singleton(srcTopicPartition));\n       partitionRewound = true;\n     } catch (Exception e) {\n-      _logger.error(\"Partition rewind failed due to \", e);\n+      _logger.error(String.format(\"Partition rewind for %s failed due to \", srcTopicPartition), e);\n     }\n     if (_pausePartitionOnError && (!partitionRewound || !containsTransientException(ex))) {\n-      // if doesn't contain DatastreamTransientException and it's configured to pause partition on error conditions,\n-      // add to auto-paused set\n+      // If partition rewind failed or the exception is not of type DatastreamTransientException and it is configured to\n+      // pause partition on error conditions, add it to the auto-paused set\n       _logger.warn(\"Adding source topic partition {} to auto-pause set\", srcTopicPartition);\n       _autoPausedSourcePartitions.put(srcTopicPartition,\n           PausedSourcePartitionMetadata.sendError(start, _pauseErrorPartitionDuration, ex));\n       _taskUpdates.add(DatastreamConstants.UpdateType.PAUSE_RESUME_PARTITIONS);\n     }\n   }\n \n+  protected void rewindAndPausePartitionsOnSendException() {\n+    // For all topic partitions which have seen send exceptions, attempt to rewind them to the last checkpoint\n+    synchronized (_sendFailureTopicPartitionExceptionMap) {\n+      _sendFailureTopicPartitionExceptionMap.forEach(this::rewindAndPausePartitionOnException);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c"}, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9037fad190aabc304c4b5f143c016f6a2f8b509d", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/9037fad190aabc304c4b5f143c016f6a2f8b509d", "committedDate": "2020-04-30T00:41:06Z", "message": "Address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzNzM4OTI4", "url": "https://github.com/linkedin/brooklin/pull/705#pullrequestreview-403738928", "createdAt": "2020-04-30T17:40:39Z", "commit": {"oid": "9037fad190aabc304c4b5f143c016f6a2f8b509d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzo0MDozOVrOGOzqDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQxNzo0MDozOVrOGOzqDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE3OTU5Nw==", "bodyText": "You might have mentioned this to me before, but isnt a ConcurrentHashMap useful here?", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r418179597", "createdAt": "2020-04-30T17:40:39Z", "author": {"login": "DEEPTHIKORAT"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -109,6 +109,9 @@\n   protected Consumer<?, ?> _consumer;\n   protected final Set<TopicPartition> _consumerAssignment = new HashSet<>();\n \n+  // TopicPartitions which have seen exceptions on send. Access to this map must be synchronized.\n+  private final Map<TopicPartition, Exception> _sendFailureTopicPartitionExceptionMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9037fad190aabc304c4b5f143c016f6a2f8b509d"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69be31a94b9fa2ace31df0cb9569bcd651484bd1", "author": {"user": {"login": "somandal", "name": "Sonam Mandal"}}, "url": "https://github.com/linkedin/brooklin/commit/69be31a94b9fa2ace31df0cb9569bcd651484bd1", "committedDate": "2020-04-30T18:04:39Z", "message": "Add comment explaining why HashMap vs. ConcurrentHashMap"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzNzY0MDY0", "url": "https://github.com/linkedin/brooklin/pull/705#pullrequestreview-403764064", "createdAt": "2020-04-30T18:15:02Z", "commit": {"oid": "69be31a94b9fa2ace31df0cb9569bcd651484bd1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NzEwODA3", "url": "https://github.com/linkedin/brooklin/pull/705#pullrequestreview-404710807", "createdAt": "2020-05-04T05:04:49Z", "commit": {"oid": "69be31a94b9fa2ace31df0cb9569bcd651484bd1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwNTowNDo1MFrOGPy20A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwNTowNDo1MFrOGPy20A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIxNTA1Ng==", "bodyText": "Do we need this extra explanation? It might become a norm every time to put this in comment. I understand this will make the understanding easier, but do we then want to put every time going forward. It might become cumbersome.", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r419215056", "createdAt": "2020-05-04T05:04:50Z", "author": {"login": "vmaheshw"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -109,6 +109,11 @@\n   protected Consumer<?, ?> _consumer;\n   protected final Set<TopicPartition> _consumerAssignment = new HashSet<>();\n \n+  // TopicPartitions which have seen exceptions on send. Access to this map must be synchronized.\n+  // A ConcurrentHashMap is not used here due to the need for having more than one operation performed together as an", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "69be31a94b9fa2ace31df0cb9569bcd651484bd1"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NzEyNTU3", "url": "https://github.com/linkedin/brooklin/pull/705#pullrequestreview-404712557", "createdAt": "2020-05-04T05:13:52Z", "commit": {"oid": "69be31a94b9fa2ace31df0cb9569bcd651484bd1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 425, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}