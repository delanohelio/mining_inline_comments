{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2NTc0Mjc3", "number": 758, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMjoxNjowM1rOEppVhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxOToyOTo0MlrOEp7PQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMTA0MzI3OnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMjoxNjowM1rOHbca7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMjoxNjowM1rOHbca7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODUzOTI0NA==", "bodyText": "I was considering adding a separate ConsumerOffsets class to hold both consumed offset and committed offset and use that in the nested dictionary. But synchronizing the writes to the nested dictionaries was a not easy. onPartitionsPolled and onOffsetsCommitted calls would hold locks on the dictionaries and it would make this code bug prone and non-efficient. So I thought it would be better for the client-side to merge these dictionaries (if necessary for presentation purposes).", "url": "https://github.com/linkedin/brooklin/pull/758#discussion_r498539244", "createdAt": "2020-10-01T22:16:03Z", "author": {"login": "jzakaryan"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -32,8 +33,8 @@\n   private final String _consumerGroupId;\n \n   private final Map<String, Set<Integer>> _topicPartitions = new ConcurrentHashMap<>();\n-\n-  private final Map<String, Map<Integer, Long>> _consumerOffsets = new ConcurrentHashMap<>();\n+  private final Map<String, Map<Integer, Long>> _consumedOffsets = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbc0df9f198f4c83954877bddf094a81771d0824"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzk3NjMzOnYy", "diffSide": "RIGHT", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxOToyOTo0MlrOHb5P_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQxNzoxNToyNFrOHfSULw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxMTU4MA==", "bodyText": "Nitpick. May be you need to call this just offsets since its being used by both _consumedOffsets and _committedOffsets", "url": "https://github.com/linkedin/brooklin/pull/758#discussion_r499011580", "createdAt": "2020-10-02T19:29:42Z", "author": {"login": "vishwajith-s"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -75,22 +76,30 @@ public void onPartitionsRevoked(@NotNull Collection<TopicPartition> topicPartiti\n       }\n     });\n \n-    // Remove consumer offsets for partitions that have been revoked. The reason to remove the consumer offsets\n+    // Remove consumed offsets for partitions that have been revoked. The reason to remove the consumed offsets\n     // here is that another host may handle these partitions due to rebalance, and we don't want to have duplicate\n     // consumer offsets for affected partitions (even though the ones with larger offsets wins).\n+    removeOffsetsForTopicPartition(topicPartitions, _consumedOffsets);\n+\n+    // Remove committed offsets for partitions that have been revoked.\n+    removeOffsetsForTopicPartition(topicPartitions, _committedOffsets);\n+  }\n+\n+  private void removeOffsetsForTopicPartition(@NotNull Collection<TopicPartition> topicPartitions,\n+      Map<String, Map<Integer, Long>> committedOffsets) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cbc0df9f198f4c83954877bddf094a81771d0824"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjU2Nzk4Mw==", "bodyText": "+1 make this generic. We also intend to add end offsets to this list.", "url": "https://github.com/linkedin/brooklin/pull/758#discussion_r502567983", "createdAt": "2020-10-09T17:15:24Z", "author": {"login": "somandal"}, "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -75,22 +76,30 @@ public void onPartitionsRevoked(@NotNull Collection<TopicPartition> topicPartiti\n       }\n     });\n \n-    // Remove consumer offsets for partitions that have been revoked. The reason to remove the consumer offsets\n+    // Remove consumed offsets for partitions that have been revoked. The reason to remove the consumed offsets\n     // here is that another host may handle these partitions due to rebalance, and we don't want to have duplicate\n     // consumer offsets for affected partitions (even though the ones with larger offsets wins).\n+    removeOffsetsForTopicPartition(topicPartitions, _consumedOffsets);\n+\n+    // Remove committed offsets for partitions that have been revoked.\n+    removeOffsetsForTopicPartition(topicPartitions, _committedOffsets);\n+  }\n+\n+  private void removeOffsetsForTopicPartition(@NotNull Collection<TopicPartition> topicPartitions,\n+      Map<String, Map<Integer, Long>> committedOffsets) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxMTU4MA=="}, "originalCommit": {"oid": "cbc0df9f198f4c83954877bddf094a81771d0824"}, "originalPosition": 34}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 887, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}