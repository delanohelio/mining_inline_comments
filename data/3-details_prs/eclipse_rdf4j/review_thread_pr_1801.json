{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4NTIyOTYx", "number": 1801, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDoyODoxN1rODVyUUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxMTo0MjozN1rODgJ-Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcxMDg5OnYy", "diffSide": "RIGHT", "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDoyODoxN1rOFaMtAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQyMjoyODozNlrOF1LdUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw==", "bodyText": "What is this? It looks awfully platform-dependent. I see it in several places.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015427", "createdAt": "2020-01-04T04:28:17Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDg4MjU3NA==", "bodyText": "Yeah. Because of how benchmark forks the jvm, the JAVA_HOME env isn't being passed through as expected. This was a dirty hack to get it to function.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r364882574", "createdAt": "2020-01-09T18:01:56Z", "author": {"login": "hmottestad"}, "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkyODM0NQ==", "bodyText": "I'm not going to try to fix this.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r390928345", "createdAt": "2020-03-11T12:13:08Z", "author": {"login": "hmottestad"}, "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTI0ODA2NA==", "bodyText": "Can you add a comment to explain why this is there?", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r391248064", "createdAt": "2020-03-11T20:25:29Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMwNjU3OA==", "bodyText": "Will do.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r391306578", "createdAt": "2020-03-11T22:28:36Z", "author": {"login": "hmottestad"}, "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw=="}, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcxMTI5OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/EvaluationStisticsWrapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDozMDowNFrOFaMtOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDozMDowNFrOFaMtOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQ4Mw==", "bodyText": "Typo, should be: EvaluationStatisticsWrapper", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015483", "createdAt": "2020-01-04T04:30:04Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/EvaluationStisticsWrapper.java", "diffHunk": "@@ -0,0 +1,68 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import org.eclipse.rdf4j.common.iteration.CloseableIteration;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.sail.SailException;\n+import org.eclipse.rdf4j.sail.extensiblestore.DataStructureInterface;\n+\n+public class EvaluationStisticsWrapper implements DataStructureInterface {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcxNDA4OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDo0MDoxNlrOFaMujg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDo0MDoxNlrOFaMujg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTgyMg==", "bodyText": "Typo: StatementQueueItem.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015822", "createdAt": "2020-01-04T04:40:16Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();\n+\t\t}\n+\n+\t\t/*\n+\t\t * byte[] statementHash = hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes();\n+\t\t *\n+\t\t * size.add(statementHash); int subjectHash = statement.getSubject().hashCode(); int predicateHash =\n+\t\t * statement.getPredicate().hashCode(); int objectHash = statement.getObject().hashCode();\n+\t\t *\n+\t\t * indexOneValue(statementHash, subjectIndex, subjectHash); indexOneValue(statementHash, predicateIndex,\n+\t\t * predicateHash); indexOneValue(statementHash, objectIndex, objectHash);\n+\t\t *\n+\t\t * indexTwoValues(statementHash, subjectPredicateIndex, subjectHash, predicateHash);\n+\t\t * indexTwoValues(statementHash, predicateObjectIndex, predicateHash, objectHash);\n+\t\t *\n+\t\t * if (statement.getContext() == null) { defaultContext.add(statementHash); } else {\n+\t\t * indexOneValue(statementHash, contextIndex, statement.getContext().hashCode()); }\n+\t\t *\n+\t\t */\n+\n+\t\t// logger.info(\"added: {} : {} \", statement, inferred ? \"INFERRED\" : \"REAL\");\n+\t}\n+\n+\tsynchronized private void startQueueThread() {\n+\t\tif (queueThread == null) {\n+\t\t\tqueueThread = new Thread(() -> {\n+\t\t\t\ttry {\n+\t\t\t\t\twhile (!queue.isEmpty()) {\n+\t\t\t\t\t\tStatemetQueueItem poll = queue.poll();\n+\t\t\t\t\t\tqueueSize.decrementAndGet();\n+\t\t\t\t\t\tStatement statement = poll.statement;\n+\t\t\t\t\t\tbyte[] statementHash = hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8)\n+\t\t\t\t\t\t\t\t.asBytes();\n+\n+\t\t\t\t\t\tif (poll.added) {\n+\n+\t\t\t\t\t\t\tsize.add(statementHash);\n+\t\t\t\t\t\t\tint subjectHash = statement.getSubject().hashCode();\n+\t\t\t\t\t\t\tint predicateHash = statement.getPredicate().hashCode();\n+\t\t\t\t\t\t\tint objectHash = statement.getObject().hashCode();\n+\n+\t\t\t\t\t\t\tindexOneValue(statementHash, subjectIndex, subjectHash);\n+\t\t\t\t\t\t\tindexOneValue(statementHash, predicateIndex, predicateHash);\n+\t\t\t\t\t\t\tindexOneValue(statementHash, objectIndex, objectHash);\n+\n+\t\t\t\t\t\t\tindexTwoValues(statementHash, subjectPredicateIndex, subjectHash, predicateHash);\n+\t\t\t\t\t\t\tindexTwoValues(statementHash, predicateObjectIndex, predicateHash, objectHash);\n+\n+\t\t\t\t\t\t\tif (statement.getContext() == null) {\n+\t\t\t\t\t\t\t\tdefaultContext.add(statementHash);\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tindexOneValue(statementHash, contextIndex, statement.getContext().hashCode());\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t} finally {\n+\t\t\t\t\tqueueThread = null;\n+\t\t\t\t}\n+\n+\t\t\t});\n+\n+\t\t\tqueueThread.start();\n+\n+\t\t}\n+\t}\n+\n+\tclass StatemetQueueItem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 246}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcxNTI4OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/EvaluationStatisticsTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDo0NDozNFrOFaMvIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDo0NDozNFrOFaMvIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTk3MA==", "bodyText": "I realize this is all WIP, but can you please make sure to use logger.debug instead of System.out.println?\nMore generally: this test - I assume you're looking for it to produce a certain optimized query plan. Can we put the expectation in in the form of an assertion?", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015970", "createdAt": "2020-01-04T04:44:34Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/EvaluationStatisticsTest.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*******************************************************************************\n+ * Copyright (c) 2019 Eclipse RDF4J contributors.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Distribution License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/org/documents/edl-v10.php.\n+ *******************************************************************************/\n+package org.eclipse.rdf4j.sail.extensiblestoreimpl;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.IsolationLevels;\n+import org.eclipse.rdf4j.common.iteration.Iterations;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.impl.SimpleValueFactory;\n+import org.eclipse.rdf4j.model.vocabulary.RDF;\n+import org.eclipse.rdf4j.query.TupleQuery;\n+import org.eclipse.rdf4j.query.TupleQueryResult;\n+import org.eclipse.rdf4j.query.impl.IteratingTupleQueryResult;\n+import org.eclipse.rdf4j.repository.sail.SailRepository;\n+import org.eclipse.rdf4j.repository.sail.SailRepositoryConnection;\n+import org.eclipse.rdf4j.rio.RDFFormat;\n+import org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics.ExtensibleDynamicEvaluationStatistics;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+\n+import static junit.framework.TestCase.assertEquals;\n+\n+public class EvaluationStatisticsTest {\n+\n+\tprivate static final Logger logger = LoggerFactory.getLogger(EvaluationStatisticsTest.class);\n+\n+\t@Test\n+\tpublic void temp() {\n+\n+\t\tExtensibleStoreImplForTests extensibleStoreImplForTests = new ExtensibleStoreImplForTests();\n+\n+\t\tSailRepository sailRepository = new SailRepository(extensibleStoreImplForTests);\n+\n+\t\ttry (SailRepositoryConnection connection = sailRepository.getConnection()) {\n+\n+\t\t\tconnection.begin();\n+\t\t\tconnection.add(RDF.TYPE, RDF.TYPE, RDF.PROPERTY);\n+\t\t\tconnection.commit();\n+\t\t}\n+\n+\t\tsailRepository.shutDown();\n+\t}\n+\n+\t@Test\n+\tpublic void hllTest() {\n+\n+\t\tStatement statement = SimpleValueFactory.getInstance().createStatement(RDF.TYPE, RDF.TYPE, RDF.PROPERTY);\n+\t\tHyperLogLogCollector collector = HyperLogLogCollector.makeLatestCollector();\n+\n+\t\tHashFunction hashFunction = Hashing.murmur3_128();\n+\t\tcollector.add(hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes());\n+\n+\t\tdouble cardinality = collector.estimateCardinality();\n+\n+\t\tcollector.add(hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes());\n+\n+\t\tassertEquals(cardinality, collector.estimateCardinality());\n+\n+\t}\n+\n+\t@Test\n+\tpublic void queryPlanTest() throws IOException {\n+\n+\t\tExtensibleStoreImplForTests extensibleStoreImplForTests = new ExtensibleStoreImplForTests();\n+\n+\t\tSailRepository sailRepository = new SailRepository(extensibleStoreImplForTests);\n+\n+\t\ttry (SailRepositoryConnection connection = sailRepository.getConnection()) {\n+\n+\t\t\tconnection.begin(IsolationLevels.NONE);\n+\t\t\tconnection.add(EvaluationStatisticsTest.class.getClassLoader().getResourceAsStream(\"bsbm-100.ttl\"), \"\",\n+\t\t\t\t\tRDFFormat.TURTLE);\n+\t\t\tconnection.commit();\n+\n+\t\t\tTupleQuery tupleQuery = connection.prepareTupleQuery(getQuery(\"evaluation-statistics/query1.rq\"));\n+\t\t\tSystem.out.println(tupleQuery.toString());\n+\t\t\ttry (IteratingTupleQueryResult evaluate = (IteratingTupleQueryResult) tupleQuery.evaluate()) {\n+\t\t\t\tSystem.out.println(evaluate.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcxNTQ2OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/benchmark/ExtensibleDynamicEvaluationStatisticsBenchmark.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDo0NToxMFrOFaMvOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDo0NToxMFrOFaMvOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTk5Mw==", "bodyText": "Heh - time to update our code templates to 2020 :) I haven't done mine either yet.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015993", "createdAt": "2020-01-04T04:45:10Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/benchmark/ExtensibleDynamicEvaluationStatisticsBenchmark.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*******************************************************************************\n+ * Copyright (c) 2019 Eclipse RDF4J contributors.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcxOTY4OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNTowNDowOFrOFaMxZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNTowNDowOFrOFaMxZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjU1MA==", "bodyText": "Copyright header is missing here, and a couple of other places as well.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016550", "createdAt": "2020-01-04T05:04:08Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcyMDE0OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNTowNjo0M1rOFaMxrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNTowNjo0M1rOFaMxrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjYyMQ==", "bodyText": "I believe this class, and some of the other implementation classes as well, deserve some extensive javadoc that goes into the how and why of the algorithm used.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016621", "createdAt": "2020-01-04T05:06:43Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcyMDYzOnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNTowODozNlrOFaMx7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wOVQxNzo1OToxMlrOFb-l9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjY4Nw==", "bodyText": "I'll have to believe you if you say that it does what it should do. Seriously though: I wonder if you could maybe split this out into more than one line of code to make it easier to read.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016687", "createdAt": "2020-01-04T05:08:36Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDg4MTM5OA==", "bodyText": "Will do :D - it got a lot worse when I added in the index_removed support.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r364881398", "createdAt": "2020-01-09T17:59:12Z", "author": {"login": "hmottestad"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjY4Nw=="}, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MTcyMTE3OnYy", "diffSide": "RIGHT", "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNToxMDo1NVrOFaMyKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wOVQxODoyMjo1NlrOFb_MYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjc0Nw==", "bodyText": "What is the role of this thread? It updates the various indexes I gather, is that such an expensive operation that it should be handled asynchronously? I also note that further down you wait for the thread to re-join before continuing - does the separate thread really help, then?", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016747", "createdAt": "2020-01-04T05:10:55Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDg5MTIzNQ==", "bodyText": "It's to do async stats updates. The thread is started if the size of the queue is likely to be above the limit, then runs until the queue is empty. Since the stats are only estimates there is no guarantee for timeliness. Those guarantees are needed when running tests and benchmarking, so there is a poor mans latch that waits for the thread to finish.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r364891235", "createdAt": "2020-01-09T18:22:56Z", "author": {"login": "hmottestad"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjc0Nw=="}, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1MDQ0MzU1OnYy", "diffSide": "RIGHT", "path": "core/sail/base/src/main/java/org/eclipse/rdf4j/sail/base/SailSourceConnection.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxMTo0MjozN1rOFqOyCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QwNjozNDo0OVrOFqZovA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ==", "bodyText": "@jeenbroekstra\nI found out that the SailSourceConnection will get all statements and remove them from the inferred sink, instead of getting just the inferred statements. So I fixed that by introducing a tri-state enum, essentially you can now specify: all, inferredOnly or explicitOnly. This is only for this class....and branch(...) is a private method, so no breaking changes.\nI don't have any decent benchmarks for the rdfs sail that add and remove statements, but I do have some for the SPIN sail and that's seeing a 20% speed bump for some of the benchmarks.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r379826699", "createdAt": "2020-02-15T11:42:37Z", "author": {"login": "hmottestad"}, "path": "core/sail/base/src/main/java/org/eclipse/rdf4j/sail/base/SailSourceConnection.java", "diffHunk": "@@ -657,17 +656,17 @@ public void clearInferred(Resource... contexts) throws SailException {\n \t\tverifyIsOpen();\n \t\tverifyIsActive();\n \t\tsynchronized (datasets) {\n-\t\t\tif (inferredSink == null) {\n+\t\t\tif (inferredOnlySink == null) {\n \t\t\t\tIsolationLevel level = getIsolationLevel();\n-\t\t\t\tSailSource branch = branch(true);\n-\t\t\t\tinferredDataset = branch.dataset(level);\n-\t\t\t\tinferredSink = branch.sink(level);\n-\t\t\t\texplicitOnlyDataset = branch(false).dataset(level);\n+\t\t\t\tSailSource branch = branch(IncludeInferred.inferredOnly);\n+\t\t\t\tinferredOnlyDataset = branch.dataset(level);\n+\t\t\t\tinferredOnlySink = branch.sink(level);\n+\t\t\t\texplicitOnlyDataset = branch(IncludeInferred.explicitOnly).dataset(level);\n \t\t\t}\n \t\t\tif (this.hasConnectionListeners()) {\n-\t\t\t\tremove(null, null, null, inferredDataset, inferredSink, contexts);\n+\t\t\t\tremove(null, null, null, inferredOnlyDataset, inferredOnlySink, contexts);\n \t\t\t}\n-\t\t\tinferredSink.clear(contexts);\n+\t\t\tinferredOnlySink.clear(contexts);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2MzEwMA==", "bodyText": "Is this related to #1795 ?", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r379963100", "createdAt": "2020-02-17T02:29:18Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/base/src/main/java/org/eclipse/rdf4j/sail/base/SailSourceConnection.java", "diffHunk": "@@ -657,17 +656,17 @@ public void clearInferred(Resource... contexts) throws SailException {\n \t\tverifyIsOpen();\n \t\tverifyIsActive();\n \t\tsynchronized (datasets) {\n-\t\t\tif (inferredSink == null) {\n+\t\t\tif (inferredOnlySink == null) {\n \t\t\t\tIsolationLevel level = getIsolationLevel();\n-\t\t\t\tSailSource branch = branch(true);\n-\t\t\t\tinferredDataset = branch.dataset(level);\n-\t\t\t\tinferredSink = branch.sink(level);\n-\t\t\t\texplicitOnlyDataset = branch(false).dataset(level);\n+\t\t\t\tSailSource branch = branch(IncludeInferred.inferredOnly);\n+\t\t\t\tinferredOnlyDataset = branch.dataset(level);\n+\t\t\t\tinferredOnlySink = branch.sink(level);\n+\t\t\t\texplicitOnlyDataset = branch(IncludeInferred.explicitOnly).dataset(level);\n \t\t\t}\n \t\t\tif (this.hasConnectionListeners()) {\n-\t\t\t\tremove(null, null, null, inferredDataset, inferredSink, contexts);\n+\t\t\t\tremove(null, null, null, inferredOnlyDataset, inferredOnlySink, contexts);\n \t\t\t}\n-\t\t\tinferredSink.clear(contexts);\n+\t\t\tinferredOnlySink.clear(contexts);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ=="}, "originalCommit": {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDAwNDU0MA==", "bodyText": "Not related no. This just makes clearing inferred statements faster.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r380004540", "createdAt": "2020-02-17T06:34:49Z", "author": {"login": "hmottestad"}, "path": "core/sail/base/src/main/java/org/eclipse/rdf4j/sail/base/SailSourceConnection.java", "diffHunk": "@@ -657,17 +656,17 @@ public void clearInferred(Resource... contexts) throws SailException {\n \t\tverifyIsOpen();\n \t\tverifyIsActive();\n \t\tsynchronized (datasets) {\n-\t\t\tif (inferredSink == null) {\n+\t\t\tif (inferredOnlySink == null) {\n \t\t\t\tIsolationLevel level = getIsolationLevel();\n-\t\t\t\tSailSource branch = branch(true);\n-\t\t\t\tinferredDataset = branch.dataset(level);\n-\t\t\t\tinferredSink = branch.sink(level);\n-\t\t\t\texplicitOnlyDataset = branch(false).dataset(level);\n+\t\t\t\tSailSource branch = branch(IncludeInferred.inferredOnly);\n+\t\t\t\tinferredOnlyDataset = branch.dataset(level);\n+\t\t\t\tinferredOnlySink = branch.sink(level);\n+\t\t\t\texplicitOnlyDataset = branch(IncludeInferred.explicitOnly).dataset(level);\n \t\t\t}\n \t\t\tif (this.hasConnectionListeners()) {\n-\t\t\t\tremove(null, null, null, inferredDataset, inferredSink, contexts);\n+\t\t\t\tremove(null, null, null, inferredOnlyDataset, inferredOnlySink, contexts);\n \t\t\t}\n-\t\t\tinferredSink.clear(contexts);\n+\t\t\tinferredOnlySink.clear(contexts);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ=="}, "originalCommit": {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d"}, "originalPosition": 226}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1671, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}