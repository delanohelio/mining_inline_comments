{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4NTIyOTYx", "number": 1801, "title": "GH-1784 eval stats", "bodyText": "GitHub issue resolved: #1784  \nBriefly describe the changes proposed in this PR:\n\nthree stats implementations: constant, direct, dynamic\n\nconstant reuses existing implementation\ndirect queries the underlying data structure\ndynamic uses arrays of HyperLogLog counters to keep estimates", "createdAt": "2020-01-01T22:33:16Z", "url": "https://github.com/eclipse/rdf4j/pull/1801", "merged": true, "mergeCommit": {"oid": "676bad3827df297c9a853a2cd4ea0fa760e55fc6"}, "closed": true, "closedAt": "2020-03-12T07:30:05Z", "author": {"login": "hmottestad"}, "timelineItems": {"totalCount": 43, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb0SZPTgH2gAyMzU4NTIyOTYxOjYzNjNiYTlhZmE1YjQ5OWQ2ZDUwNDM0MzU4Y2MwM2UwMTc5ZDlhMTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcM2j98AH2gAyMzU4NTIyOTYxOmVhY2NiZjE2Y2RmYzA5NDEwYmE0N2NmZTc2MDQzYTkyY2FmYjBmNmE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6363ba9afa5b499d6d50434358cc03e0179d9a15", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/6363ba9afa5b499d6d50434358cc03e0179d9a15", "committedDate": "2019-12-26T23:44:51Z", "message": "started developing eval stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c594ec4ab8f4a6e1ccfa5b63cd7cc2c7caac980c", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/c594ec4ab8f4a6e1ccfa5b63cd7cc2c7caac980c", "committedDate": "2019-12-28T08:02:39Z", "message": "Merge branch 'develop' into issues/1784_eval_stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2046f8622c19f47659fbe50146f2cb35052cd2c", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/f2046f8622c19f47659fbe50146f2cb35052cd2c", "committedDate": "2019-12-29T16:59:20Z", "message": "WIP\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61332a4cb193c1046fa6f95f8e95233704dc8c90", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/61332a4cb193c1046fa6f95f8e95233704dc8c90", "committedDate": "2020-01-01T13:42:38Z", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a118c7fe2c1c260460e82c3fb9bc1c5ff872c3d7", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/a118c7fe2c1c260460e82c3fb9bc1c5ff872c3d7", "committedDate": "2020-01-01T18:06:08Z", "message": "better dynamic stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "448c2342e74426826db1192586442c74ccce952a", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/448c2342e74426826db1192586442c74ccce952a", "committedDate": "2020-01-01T18:29:18Z", "message": "wip\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f813372a930b59f284439843e7da5e9d79b6c01a", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/f813372a930b59f284439843e7da5e9d79b6c01a", "committedDate": "2020-01-01T22:24:20Z", "message": "wip\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db2c7917a5752b946dfdedd8c08e673cbd302b52", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/db2c7917a5752b946dfdedd8c08e673cbd302b52", "committedDate": "2020-01-01T23:06:24Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41a9153b2bd7e0754e11494d61bd1963a1dfba2f", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/41a9153b2bd7e0754e11494d61bd1963a1dfba2f", "committedDate": "2020-01-01T23:07:48Z", "message": "Merge branch 'develop' into issues/1784_eval_stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7b9a03ce4e5748eaa203c1e1eeb2e2447a28a3e", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/a7b9a03ce4e5748eaa203c1e1eeb2e2447a28a3e", "committedDate": "2020-01-01T23:21:22Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d338217bc0466cea25d36188d73df85f29974cd", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/8d338217bc0466cea25d36188d73df85f29974cd", "committedDate": "2020-01-01T23:24:53Z", "message": "wip\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ffda07dd5643b3355fb6a646e01ef9f72544f08", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/9ffda07dd5643b3355fb6a646e01ef9f72544f08", "committedDate": "2020-01-02T21:25:38Z", "message": "sparse indexes\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2094628fbfd9981253dba506e9c62a0dbfe22e0e", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/2094628fbfd9981253dba506e9c62a0dbfe22e0e", "committedDate": "2020-01-02T21:26:08Z", "message": "Merge branch 'develop' into issues/1784_eval_stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/990c4496ef9414ab5728db87620e730042f167a4", "committedDate": "2020-01-02T21:49:30Z", "message": "low mem benchmark\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM4MzM3MDE0", "url": "https://github.com/eclipse/rdf4j/pull/1801#pullrequestreview-338337014", "createdAt": "2020-01-04T04:28:16Z", "commit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNDoyODoxN1rOFaMtAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNFQwNToxMDo1NVrOFaMyKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQyNw==", "bodyText": "What is this? It looks awfully platform-dependent. I see it in several places.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015427", "createdAt": "2020-01-04T04:28:17Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/elasticsearch-store/src/test/java/org/eclipse/rdf4j/sail/elasticsearchstore/benchmark/DeleteBenchmark.java", "diffHunk": "@@ -60,7 +61,7 @@\n \tpublic void beforeClass() throws IOException, InterruptedException {\n \n \t\tembeddedElastic = TestHelpers.startElasticsearch(installLocation,\n-\t\t\t\t\"/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home\");\n+\t\t\t\t\"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTQ4Mw==", "bodyText": "Typo, should be: EvaluationStatisticsWrapper", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015483", "createdAt": "2020-01-04T04:30:04Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/EvaluationStisticsWrapper.java", "diffHunk": "@@ -0,0 +1,68 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import org.eclipse.rdf4j.common.iteration.CloseableIteration;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.sail.SailException;\n+import org.eclipse.rdf4j.sail.extensiblestore.DataStructureInterface;\n+\n+public class EvaluationStisticsWrapper implements DataStructureInterface {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTgyMg==", "bodyText": "Typo: StatementQueueItem.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015822", "createdAt": "2020-01-04T04:40:16Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();\n+\t\t}\n+\n+\t\t/*\n+\t\t * byte[] statementHash = hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes();\n+\t\t *\n+\t\t * size.add(statementHash); int subjectHash = statement.getSubject().hashCode(); int predicateHash =\n+\t\t * statement.getPredicate().hashCode(); int objectHash = statement.getObject().hashCode();\n+\t\t *\n+\t\t * indexOneValue(statementHash, subjectIndex, subjectHash); indexOneValue(statementHash, predicateIndex,\n+\t\t * predicateHash); indexOneValue(statementHash, objectIndex, objectHash);\n+\t\t *\n+\t\t * indexTwoValues(statementHash, subjectPredicateIndex, subjectHash, predicateHash);\n+\t\t * indexTwoValues(statementHash, predicateObjectIndex, predicateHash, objectHash);\n+\t\t *\n+\t\t * if (statement.getContext() == null) { defaultContext.add(statementHash); } else {\n+\t\t * indexOneValue(statementHash, contextIndex, statement.getContext().hashCode()); }\n+\t\t *\n+\t\t */\n+\n+\t\t// logger.info(\"added: {} : {} \", statement, inferred ? \"INFERRED\" : \"REAL\");\n+\t}\n+\n+\tsynchronized private void startQueueThread() {\n+\t\tif (queueThread == null) {\n+\t\t\tqueueThread = new Thread(() -> {\n+\t\t\t\ttry {\n+\t\t\t\t\twhile (!queue.isEmpty()) {\n+\t\t\t\t\t\tStatemetQueueItem poll = queue.poll();\n+\t\t\t\t\t\tqueueSize.decrementAndGet();\n+\t\t\t\t\t\tStatement statement = poll.statement;\n+\t\t\t\t\t\tbyte[] statementHash = hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8)\n+\t\t\t\t\t\t\t\t.asBytes();\n+\n+\t\t\t\t\t\tif (poll.added) {\n+\n+\t\t\t\t\t\t\tsize.add(statementHash);\n+\t\t\t\t\t\t\tint subjectHash = statement.getSubject().hashCode();\n+\t\t\t\t\t\t\tint predicateHash = statement.getPredicate().hashCode();\n+\t\t\t\t\t\t\tint objectHash = statement.getObject().hashCode();\n+\n+\t\t\t\t\t\t\tindexOneValue(statementHash, subjectIndex, subjectHash);\n+\t\t\t\t\t\t\tindexOneValue(statementHash, predicateIndex, predicateHash);\n+\t\t\t\t\t\t\tindexOneValue(statementHash, objectIndex, objectHash);\n+\n+\t\t\t\t\t\t\tindexTwoValues(statementHash, subjectPredicateIndex, subjectHash, predicateHash);\n+\t\t\t\t\t\t\tindexTwoValues(statementHash, predicateObjectIndex, predicateHash, objectHash);\n+\n+\t\t\t\t\t\t\tif (statement.getContext() == null) {\n+\t\t\t\t\t\t\t\tdefaultContext.add(statementHash);\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tindexOneValue(statementHash, contextIndex, statement.getContext().hashCode());\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t} finally {\n+\t\t\t\t\tqueueThread = null;\n+\t\t\t\t}\n+\n+\t\t\t});\n+\n+\t\t\tqueueThread.start();\n+\n+\t\t}\n+\t}\n+\n+\tclass StatemetQueueItem {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 246}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTk3MA==", "bodyText": "I realize this is all WIP, but can you please make sure to use logger.debug instead of System.out.println?\nMore generally: this test - I assume you're looking for it to produce a certain optimized query plan. Can we put the expectation in in the form of an assertion?", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015970", "createdAt": "2020-01-04T04:44:34Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/EvaluationStatisticsTest.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*******************************************************************************\n+ * Copyright (c) 2019 Eclipse RDF4J contributors.\n+ * All rights reserved. This program and the accompanying materials\n+ * are made available under the terms of the Eclipse Distribution License v1.0\n+ * which accompanies this distribution, and is available at\n+ * http://www.eclipse.org/org/documents/edl-v10.php.\n+ *******************************************************************************/\n+package org.eclipse.rdf4j.sail.extensiblestoreimpl;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.IsolationLevels;\n+import org.eclipse.rdf4j.common.iteration.Iterations;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.impl.SimpleValueFactory;\n+import org.eclipse.rdf4j.model.vocabulary.RDF;\n+import org.eclipse.rdf4j.query.TupleQuery;\n+import org.eclipse.rdf4j.query.TupleQueryResult;\n+import org.eclipse.rdf4j.query.impl.IteratingTupleQueryResult;\n+import org.eclipse.rdf4j.repository.sail.SailRepository;\n+import org.eclipse.rdf4j.repository.sail.SailRepositoryConnection;\n+import org.eclipse.rdf4j.rio.RDFFormat;\n+import org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics.ExtensibleDynamicEvaluationStatistics;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+\n+import static junit.framework.TestCase.assertEquals;\n+\n+public class EvaluationStatisticsTest {\n+\n+\tprivate static final Logger logger = LoggerFactory.getLogger(EvaluationStatisticsTest.class);\n+\n+\t@Test\n+\tpublic void temp() {\n+\n+\t\tExtensibleStoreImplForTests extensibleStoreImplForTests = new ExtensibleStoreImplForTests();\n+\n+\t\tSailRepository sailRepository = new SailRepository(extensibleStoreImplForTests);\n+\n+\t\ttry (SailRepositoryConnection connection = sailRepository.getConnection()) {\n+\n+\t\t\tconnection.begin();\n+\t\t\tconnection.add(RDF.TYPE, RDF.TYPE, RDF.PROPERTY);\n+\t\t\tconnection.commit();\n+\t\t}\n+\n+\t\tsailRepository.shutDown();\n+\t}\n+\n+\t@Test\n+\tpublic void hllTest() {\n+\n+\t\tStatement statement = SimpleValueFactory.getInstance().createStatement(RDF.TYPE, RDF.TYPE, RDF.PROPERTY);\n+\t\tHyperLogLogCollector collector = HyperLogLogCollector.makeLatestCollector();\n+\n+\t\tHashFunction hashFunction = Hashing.murmur3_128();\n+\t\tcollector.add(hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes());\n+\n+\t\tdouble cardinality = collector.estimateCardinality();\n+\n+\t\tcollector.add(hashFunction.hashString(statement.toString(), StandardCharsets.UTF_8).asBytes());\n+\n+\t\tassertEquals(cardinality, collector.estimateCardinality());\n+\n+\t}\n+\n+\t@Test\n+\tpublic void queryPlanTest() throws IOException {\n+\n+\t\tExtensibleStoreImplForTests extensibleStoreImplForTests = new ExtensibleStoreImplForTests();\n+\n+\t\tSailRepository sailRepository = new SailRepository(extensibleStoreImplForTests);\n+\n+\t\ttry (SailRepositoryConnection connection = sailRepository.getConnection()) {\n+\n+\t\t\tconnection.begin(IsolationLevels.NONE);\n+\t\t\tconnection.add(EvaluationStatisticsTest.class.getClassLoader().getResourceAsStream(\"bsbm-100.ttl\"), \"\",\n+\t\t\t\t\tRDFFormat.TURTLE);\n+\t\t\tconnection.commit();\n+\n+\t\t\tTupleQuery tupleQuery = connection.prepareTupleQuery(getQuery(\"evaluation-statistics/query1.rq\"));\n+\t\t\tSystem.out.println(tupleQuery.toString());\n+\t\t\ttry (IteratingTupleQueryResult evaluate = (IteratingTupleQueryResult) tupleQuery.evaluate()) {\n+\t\t\t\tSystem.out.println(evaluate.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNTk5Mw==", "bodyText": "Heh - time to update our code templates to 2020 :) I haven't done mine either yet.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363015993", "createdAt": "2020-01-04T04:45:10Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/test/java/org/eclipse/rdf4j/sail/extensiblestoreimpl/benchmark/ExtensibleDynamicEvaluationStatisticsBenchmark.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*******************************************************************************\n+ * Copyright (c) 2019 Eclipse RDF4J contributors.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjU1MA==", "bodyText": "Copyright header is missing here, and a couple of other places as well.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016550", "createdAt": "2020-01-04T05:04:08Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjYyMQ==", "bodyText": "I believe this class, and some of the other implementation classes as well, deserve some extensive javadoc that goes into the how and why of the algorithm used.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016621", "createdAt": "2020-01-04T05:06:43Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjY4Nw==", "bodyText": "I'll have to believe you if you say that it does what it should do. Seriously though: I wonder if you could maybe split this out into more than one line of code to make it easier to read.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016687", "createdAt": "2020-01-04T05:08:36Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzAxNjc0Nw==", "bodyText": "What is the role of this thread? It updates the various indexes I gather, is that such an expensive operation that it should be handled asynchronously? I also note that further down you wait for the thread to re-join before continuing - does the separate thread really help, then?", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r363016747", "createdAt": "2020-01-04T05:10:55Z", "author": {"login": "jeenbroekstra"}, "path": "core/sail/extensible-store/src/main/java/org/eclipse/rdf4j/sail/extensiblestore/evaluationstatistics/ExtensibleDynamicEvaluationStatistics.java", "diffHunk": "@@ -0,0 +1,337 @@\n+package org.eclipse.rdf4j.sail.extensiblestore.evaluationstatistics;\n+\n+import com.google.common.hash.HashFunction;\n+import com.google.common.hash.Hashing;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.eclipse.rdf4j.model.IRI;\n+import org.eclipse.rdf4j.model.Resource;\n+import org.eclipse.rdf4j.model.Statement;\n+import org.eclipse.rdf4j.model.Value;\n+import org.eclipse.rdf4j.query.algebra.StatementPattern;\n+import org.eclipse.rdf4j.query.algebra.Var;\n+import org.eclipse.rdf4j.sail.extensiblestore.ExtensibleSailStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+public class ExtensibleDynamicEvaluationStatistics extends ExtensibleEvaluationStatistics implements DynamicStatistics {\n+\tprivate static final Logger logger = LoggerFactory.getLogger(ExtensibleDynamicEvaluationStatistics.class);\n+\tprivate static final int QUEUE_LIMIT = 128;\n+\tprivate static final int ONE_DIMENSION_INDEX_SIZE = 1024;\n+\n+\tConcurrentLinkedQueue<StatemetQueueItem> queue = new ConcurrentLinkedQueue<StatemetQueueItem>();\n+\n+\tAtomicInteger queueSize = new AtomicInteger();\n+\n+\tprivate final HashFunction hashFunction = Hashing.murmur3_128();\n+\n+\tprivate final HyperLogLogCollector EMPTY_HLL = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector size = HyperLogLogCollector.makeLatestCollector();\n+\tprivate final HyperLogLogCollector size_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex = new HyperLogLogCollector[64][64];\n+\n+\tprivate final Map<Integer, HyperLogLogCollector> subjectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> predicateIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> objectIndex_removed = new HashMap<>();\n+\tprivate final Map<Integer, HyperLogLogCollector> contextIndex_removed = new HashMap<>();\n+\tprivate final HyperLogLogCollector defaultContext_removed = HyperLogLogCollector.makeLatestCollector();\n+\n+\tprivate final HyperLogLogCollector[][] subjectPredicateIndex_removed = new HyperLogLogCollector[64][64];\n+\tprivate final HyperLogLogCollector[][] predicateObjectIndex_removed = new HyperLogLogCollector[64][64];\n+\tvolatile private Thread queueThread;\n+\n+\tpublic ExtensibleDynamicEvaluationStatistics(ExtensibleSailStore extensibleSailStore) {\n+\t\tsuper(extensibleSailStore);\n+\n+//\t\tStream.of(subjectIndex, predicateIndex, objectIndex, contextIndex,\n+//\t\t\tsubjectIndex_removed, predicateIndex_removed, objectIndex_removed, contextIndex_removed)\n+//\t\t\t.forEach(index -> {\n+//\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+//\t\t\t\t\tindex[i] = HyperLogLogCollector.makeLatestCollector();\n+//\t\t\t\t}\n+//\t\t\t});\n+\n+\t\tStream.of(subjectPredicateIndex, predicateObjectIndex, subjectPredicateIndex_removed,\n+\t\t\t\tpredicateObjectIndex_removed).forEach(index -> {\n+\t\t\t\t\tfor (int i = 0; i < index.length; i++) {\n+\t\t\t\t\t\tfor (int j = 0; j < index[i].length; j++) {\n+\t\t\t\t\t\t\tindex[i][j] = HyperLogLogCollector.makeLatestCollector();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t});\n+\n+\t}\n+\n+\t@Override\n+\tprotected CardinalityCalculator createCardinalityCalculator() {\n+\t\treturn cardinalityCalculator;\n+\t}\n+\n+\tCardinalityCalculator cardinalityCalculator = new CardinalityCalculator() {\n+\n+\t\t@Override\n+\t\tprotected double getCardinality(StatementPattern sp) {\n+\t\t\tdouble min = size.estimateCardinality() - size_removed.estimateCardinality();\n+\n+\t\t\tmin = Math.min(min, getSubjectCardinality(sp.getSubjectVar()));\n+\t\t\tmin = Math.min(min, getPredicateCardinality(sp.getPredicateVar()));\n+\t\t\tmin = Math.min(min, getObjectCardinality(sp.getObjectVar()));\n+\n+\t\t\tif (sp.getSubjectVar().getValue() != null && sp.getPredicateVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(subjectPredicateIndex, subjectPredicateIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getSubjectVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue()));\n+\t\t\t}\n+\n+\t\t\tif (sp.getPredicateVar().getValue() != null && sp.getObjectVar().getValue() != null) {\n+\t\t\t\tmin = Math.min(min,\n+\t\t\t\t\t\tgetHllCardinality(predicateObjectIndex, predicateObjectIndex_removed,\n+\t\t\t\t\t\t\t\tsp.getPredicateVar().getValue(),\n+\t\t\t\t\t\t\t\tsp.getObjectVar().getValue()));\n+\t\t\t}\n+\n+\t\t\treturn min;\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getSubjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(subjectIndex, subjectIndex_removed, var.getValue());\n+\t\t\t}\n+\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getPredicateCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(predicateIndex, predicateIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getObjectCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn size.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(objectIndex, objectIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\n+\t\t@Override\n+\t\tprotected double getContextCardinality(Var var) {\n+\t\t\tif (var.getValue() == null) {\n+\t\t\t\treturn defaultContext.estimateCardinality() - defaultContext_removed.estimateCardinality();\n+\t\t\t} else {\n+\t\t\t\treturn getHllCardinality(contextIndex, contextIndex_removed, var.getValue());\n+\t\t\t}\n+\t\t}\n+\t};\n+\n+\tprivate double getHllCardinality(HyperLogLogCollector[][] index, HyperLogLogCollector[][] index_removed,\n+\t\t\tValue value1, Value value2) {\n+\t\treturn index[Math.abs(value1.hashCode() % index.length)][Math.abs(value2.hashCode() % index.length)]\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed[Math.abs(value1.hashCode() % index_removed.length)][Math\n+\t\t\t\t\t\t.abs(value2.hashCode() % index_removed.length)]\n+\t\t\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\tprivate double getHllCardinality(Map<Integer, HyperLogLogCollector> index,\n+\t\t\tMap<Integer, HyperLogLogCollector> index_removed, Value value) {\n+\n+\t\treturn index.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t.estimateCardinality()\n+\t\t\t\t- index_removed.getOrDefault(Math.abs(value.hashCode() % ONE_DIMENSION_INDEX_SIZE), EMPTY_HLL)\n+\t\t\t\t\t\t.estimateCardinality();\n+\t}\n+\n+\t@Override\n+\tpublic void add(Statement statement, boolean inferred) {\n+\n+\t\tqueue.add(new StatemetQueueItem(statement, inferred, true));\n+\n+\t\tint size = queueSize.incrementAndGet();\n+\t\tif (size > QUEUE_LIMIT && queueThread == null) {\n+\t\t\tstartQueueThread();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "990c4496ef9414ab5728db87620e730042f167a4"}, "originalPosition": 179}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8421600ff8d5a7ae5475b7d912c8164a49b99a9", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/f8421600ff8d5a7ae5475b7d912c8164a49b99a9", "committedDate": "2020-01-04T13:37:13Z", "message": "more tests\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fda585e56529e306a44264902f3f33002d1720d", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/2fda585e56529e306a44264902f3f33002d1720d", "committedDate": "2020-01-04T13:37:34Z", "message": "Merge branch 'develop' into issues/1784_eval_stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4cb6b96b020f78b27d45d0d9d3d91788af0bde07", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/4cb6b96b020f78b27d45d0d9d3d91788af0bde07", "committedDate": "2020-01-05T12:22:06Z", "message": "wip"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "561a53aad05b2a4ac1b2fe3e545991d2583e8a1c", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/561a53aad05b2a4ac1b2fe3e545991d2583e8a1c", "committedDate": "2020-01-12T11:49:58Z", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc345bc5ecae4f5580e741564b6d77fcb5b10445", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/bc345bc5ecae4f5580e741564b6d77fcb5b10445", "committedDate": "2020-01-14T11:24:18Z", "message": "Merge branch 'develop' into issues/1784_eval_stats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db6ee47581e3ceed57c4940c229cc223fb88f667", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/db6ee47581e3ceed57c4940c229cc223fb88f667", "committedDate": "2020-01-18T10:24:03Z", "message": "testing out ways to detect stalenss\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c54c196e287fda13796d88065474c24b73f6001", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/2c54c196e287fda13796d88065474c24b73f6001", "committedDate": "2020-02-10T06:46:15Z", "message": "#1784 better staleness\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a953326e5b68935c4f536954123e5dc877a18abb", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/a953326e5b68935c4f536954123e5dc877a18abb", "committedDate": "2020-02-10T07:32:38Z", "message": "#1784 more tests\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "912584274d0794af206309bc443b33eecd629dd0", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/912584274d0794af206309bc443b33eecd629dd0", "committedDate": "2020-02-10T07:33:55Z", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f1b0ba83aef3eca19778d1e144d61e262ec3bb7", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/5f1b0ba83aef3eca19778d1e144d61e262ec3bb7", "committedDate": "2020-02-13T12:33:35Z", "message": "#1784 support inferred deeper into the datastructure because we need it for for the eval stats engine\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "36b2031ff518b91083c1785acf22b18ba13a2fee", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/36b2031ff518b91083c1785acf22b18ba13a2fee", "committedDate": "2020-02-14T14:28:57Z", "message": "WIP inferred triples\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "688b4f968abd57c8deb0785a3512477ce5028289", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/688b4f968abd57c8deb0785a3512477ce5028289", "committedDate": "2020-02-14T16:50:40Z", "message": "#1784 fixed inferred statements in elasticsearch\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba51e3a26a205bfce12bbef0b52779701415e4c9", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/ba51e3a26a205bfce12bbef0b52779701415e4c9", "committedDate": "2020-02-14T17:14:22Z", "message": "#1784 introduced concept of inferredOnly to make clearing inferred statements faster\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6bba7cd6e5dd6ff98eb7ad90aa1effbec0679a6", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/f6bba7cd6e5dd6ff98eb7ad90aa1effbec0679a6", "committedDate": "2020-02-14T23:36:19Z", "message": "Update IteratingTupleQueryResult.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7be9746988ca4b59ef92225c8db4cbaeb32937b", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/a7be9746988ca4b59ef92225c8db4cbaeb32937b", "committedDate": "2020-02-15T08:55:48Z", "message": "code cleanup\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/4751135ed4653dec9b7c94254c4ab568b1a7064d", "committedDate": "2020-02-15T09:29:13Z", "message": "fix read committed wrapper\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5MzQxNDM3", "url": "https://github.com/eclipse/rdf4j/pull/1801#pullrequestreview-359341437", "createdAt": "2020-02-15T11:42:37Z", "commit": {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxMTo0MjozN1rOFqOyCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNVQxMTo0MjozN1rOFqOyCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTgyNjY5OQ==", "bodyText": "@jeenbroekstra\nI found out that the SailSourceConnection will get all statements and remove them from the inferred sink, instead of getting just the inferred statements. So I fixed that by introducing a tri-state enum, essentially you can now specify: all, inferredOnly or explicitOnly. This is only for this class....and branch(...) is a private method, so no breaking changes.\nI don't have any decent benchmarks for the rdfs sail that add and remove statements, but I do have some for the SPIN sail and that's seeing a 20% speed bump for some of the benchmarks.", "url": "https://github.com/eclipse/rdf4j/pull/1801#discussion_r379826699", "createdAt": "2020-02-15T11:42:37Z", "author": {"login": "hmottestad"}, "path": "core/sail/base/src/main/java/org/eclipse/rdf4j/sail/base/SailSourceConnection.java", "diffHunk": "@@ -657,17 +656,17 @@ public void clearInferred(Resource... contexts) throws SailException {\n \t\tverifyIsOpen();\n \t\tverifyIsActive();\n \t\tsynchronized (datasets) {\n-\t\t\tif (inferredSink == null) {\n+\t\t\tif (inferredOnlySink == null) {\n \t\t\t\tIsolationLevel level = getIsolationLevel();\n-\t\t\t\tSailSource branch = branch(true);\n-\t\t\t\tinferredDataset = branch.dataset(level);\n-\t\t\t\tinferredSink = branch.sink(level);\n-\t\t\t\texplicitOnlyDataset = branch(false).dataset(level);\n+\t\t\t\tSailSource branch = branch(IncludeInferred.inferredOnly);\n+\t\t\t\tinferredOnlyDataset = branch.dataset(level);\n+\t\t\t\tinferredOnlySink = branch.sink(level);\n+\t\t\t\texplicitOnlyDataset = branch(IncludeInferred.explicitOnly).dataset(level);\n \t\t\t}\n \t\t\tif (this.hasConnectionListeners()) {\n-\t\t\t\tremove(null, null, null, inferredDataset, inferredSink, contexts);\n+\t\t\t\tremove(null, null, null, inferredOnlyDataset, inferredOnlySink, contexts);\n \t\t\t}\n-\t\t\tinferredSink.clear(contexts);\n+\t\t\tinferredOnlySink.clear(contexts);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4751135ed4653dec9b7c94254c4ab568b1a7064d"}, "originalPosition": 226}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a05c21b6267f08d09fa84b93f788dad4f9ba5e70", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/a05c21b6267f08d09fa84b93f788dad4f9ba5e70", "committedDate": "2020-03-10T13:55:17Z", "message": "Merge branch 'develop' into issues/1784_eval_stats\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "274476446d4e74d48d746b21f41763186b9ea2af", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/274476446d4e74d48d746b21f41763186b9ea2af", "committedDate": "2020-03-10T15:49:07Z", "message": "trigger dynamic stats refresh\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ddc91f0e7cc81bc749a31bc0807f5b50fb567f5", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/3ddc91f0e7cc81bc749a31bc0807f5b50fb567f5", "committedDate": "2020-03-11T10:12:35Z", "message": "refresh stats every 60 seconds\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f3d82c58e03926523472c0444663e2fa4d0d338", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/8f3d82c58e03926523472c0444663e2fa4d0d338", "committedDate": "2020-03-11T10:18:35Z", "message": "code review cleanup\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27f64bd17849987fa3e1838e39849ace05ed8e10", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/27f64bd17849987fa3e1838e39849ace05ed8e10", "committedDate": "2020-03-11T10:48:22Z", "message": "cleaned up some code and added some documentation\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21b4d488d1a7e1d37d34dd5a0974a63d9cac7117", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/21b4d488d1a7e1d37d34dd5a0974a63d9cac7117", "committedDate": "2020-03-11T11:16:43Z", "message": "more documentation\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9784b6a16259bbe8ab4ea1259c83f350b0a11873", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/9784b6a16259bbe8ab4ea1259c83f350b0a11873", "committedDate": "2020-03-11T11:26:42Z", "message": "cleanup unused code and code used for testing during development\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "749c8a2f9da74d5786dcde19c6893e1ed644c324", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/749c8a2f9da74d5786dcde19c6893e1ed644c324", "committedDate": "2020-03-11T12:09:57Z", "message": "updated some tests\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53675cd833bc5bb193818c557bd13bce1c9a5a6a", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/53675cd833bc5bb193818c557bd13bce1c9a5a6a", "committedDate": "2020-03-11T13:33:03Z", "message": "add recommended sorting on ES scroll\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "502333a0bea663bbb12de03e14f51bb34870d325", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/502333a0bea663bbb12de03e14f51bb34870d325", "committedDate": "2020-03-11T13:54:43Z", "message": "code cleanup\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eaccbf16cdfc09410ba47cfe76043a92cafb0f6a", "author": {"user": {"login": "hmottestad", "name": null}}, "url": "https://github.com/eclipse/rdf4j/commit/eaccbf16cdfc09410ba47cfe76043a92cafb0f6a", "committedDate": "2020-03-12T07:27:20Z", "message": "comments about JMH not passing JAVA_HOME correctly\n\nSigned-off-by: Ha\u030avard Ottestad <hmottestad@gmail.com>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 74, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}