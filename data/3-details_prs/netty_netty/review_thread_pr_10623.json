{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1MzY4ODY1", "number": 10623, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzozMTozOVrOEpeekA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo1OVrOE1R1gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTI2NDE2OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzozMTozOVrOHbKibg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMToxODoxMVrOHcG5Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA==", "bodyText": "@fredericBregier can you explain why setting an upper limit improves things here ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r498246254", "createdAt": "2020-10-01T13:31:39Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1035,7 +1035,7 @@ private static String readLine(ByteBuf undecodedChunk, Charset charset) {\n         }\n         SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n         int readerIndex = undecodedChunk.readerIndex();\n-        ByteBuf line = undecodedChunk.alloc().heapBuffer(64);\n+        ByteBuf line = undecodedChunk.alloc().heapBuffer(64, 64);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI3MDAyOA==", "bodyText": "@normanmaurer\nI don't have any real clue, but it seems it does. That's why I'm asking you to look at this why.\nThe change is huge on performances in PARANOID mode (using the example test given by the final user), about 10 times better... I test several times, and without it, the times are about 1000ms, while with it, it gives about 100ms.\nUsing my old test (a bit different, where a lot of items, about 6000, are passed and not only one file), the improvement is there also on PARANOID mode, but not that much (not 10 times better, comparing 146.000 old code vs 105.000 new code).\nThere, the issue (and change) is on the undecodedChunk.write(buf).\nBut if I change it to something like wrappedBuffers(undecodedChunk, buf), the time is close to 1000ms again in PARANOID mode, but increases sensibly in other modes (500ms against 200ms without any change).\nIt was strange for me to see that comparing:\n\nundecodedChunk = isLast? ... : buf.alloc.buffer(buf.readableSize(), buf.readableSize()).write(buf) and undecodedChunk = Unpooled.wrapped(undecodedChunk, buf.alloc.buffer(buf.readableSize(), buf.readableSize()).write(buf))\nvs original ones (still in place at lines 338 and 340\n\ngives impact performances (better for PARANOID, about 1000 times, but worst in other modes, about 2 times). I feel like it was the same in both cases (allocation and writing), but it seems not. Note that in current code, undecodedChunk in line 338 is still allocated using no upper bound.\nThe main \"same root cause\" seems to be \"no upper bound for buffer\" when using PARANOID mode, but in different ways.\nI feel like this proposal is safe as it enhances a bit the general performances and takes down the PARANOID issue, at least for the user's example.\nBut I feel also that there is something else, since in my test (high number of items) this is really not enough.\nSo the reason I try to test it and produce at least this in order to get help.", "url": "https://github.com/netty/netty/pull/10623#discussion_r498270028", "createdAt": "2020-10-01T14:04:20Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1035,7 +1035,7 @@ private static String readLine(ByteBuf undecodedChunk, Charset charset) {\n         }\n         SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n         int readerIndex = undecodedChunk.readerIndex();\n-        ByteBuf line = undecodedChunk.alloc().heapBuffer(64);\n+        ByteBuf line = undecodedChunk.alloc().heapBuffer(64, 64);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA=="}, "originalCommit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODYyODA0MA==", "bodyText": "@normanmaurer\nI added a fake commit to let you run the different cases at once:\nThis commit is only intend to show the result on tests using 3 cases:\n\nHttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\nHttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\nHttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where write is replaced by wrapped and the last \"no upper bounded\" buffer is replaced too\n\nTest to run is testRegressionMultipleLevelLeakDetector.\nOn my side results are:\nTimer: DISABLED NotUsingDisk1 => 220.995585\nTimer: SIMPLE NotUsingDisk1 => 222.134248\nTimer: ADVANCED NotUsingDisk1 => 208.68989\nTimer: PARANOID NotUsingDisk1 => 100044.293876\nTimer: DISABLED UsingDisk1 => 707.480873\nTimer: DISABLED UsingDisk1 => 518.695771\nTimer: SIMPLE UsingDisk1 => 522.790849\nTimer: ADVANCED UsingDisk1 => 585.759164\nTimer: PARANOID UsingDisk1 => 105090.135276\nTimer: DISABLED NotUsingDisk2 => 136.461289\nTimer: SIMPLE NotUsingDisk2 => 126.793194\nTimer: ADVANCED NotUsingDisk2 => 129.771282\nTimer: PARANOID NotUsingDisk2 => 973.38734\nTimer: DISABLED UsingDisk2 => 74.279745\nTimer: DISABLED UsingDisk2 => 71.332363\nTimer: SIMPLE UsingDisk2 => 114.811827\nTimer: ADVANCED UsingDisk2 => 89.290387\nTimer: PARANOID UsingDisk2 => 1155.278418\nTimer: DISABLED NotUsingDisk3 => 1829.364849\nTimer: SIMPLE NotUsingDisk3 => 1949.512746\nTimer: ADVANCED NotUsingDisk3 => 2120.573799\nTimer: PARANOID NotUsingDisk3 => 2073.506771\nTimer: DISABLED UsingDisk3 => 1998.837595\nTimer: DISABLED UsingDisk3 => 1952.342478\nTimer: SIMPLE UsingDisk3 => 2099.951834\nTimer: ADVANCED UsingDisk3 => 2058.028121\nTimer: PARANOID UsingDisk3 => 2295.847557", "url": "https://github.com/netty/netty/pull/10623#discussion_r498628040", "createdAt": "2020-10-02T05:53:30Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1035,7 +1035,7 @@ private static String readLine(ByteBuf undecodedChunk, Charset charset) {\n         }\n         SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n         int readerIndex = undecodedChunk.readerIndex();\n-        ByteBuf line = undecodedChunk.alloc().heapBuffer(64);\n+        ByteBuf line = undecodedChunk.alloc().heapBuffer(64, 64);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA=="}, "originalCommit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTIzNTA3OQ==", "bodyText": "Last fake commit:\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where temptative to reuse more the existing ByteBuf is done\nTest to run is testRegressionMultipleLevelLeakDetectorNoDisk or testHighNumberCheckLeakDetectorVersions\nCurrent results (stability is correct but numbers depend on host)\nI've done various tests:\n\nusing wrapped (or through buf.alloc().compositeByteBuf()): worst timers\nusing fixed allocation (but high enough to fit,the necessary elements, just for test): not good timers (probably because the size is too huge for a lot of cases, and whatever, it is not acceptable to have fixed size, it was just for testing purpose)\nusing various ways to discard read bytes: it gives a bit improvement (TEST_TEMP_ITEM=3): almost best results, but not that much\n\nIn particular, in the \"big\" test (testHighNumberCheckLeakDetectorVersions where more than 16000 items are sent, while the other test is only one item from a big file), I am not able to reduce the timer in PARANOID mode, while in \"unique\" item test ( testRegressionMultipleLevelLeakDetectorNoDisk), the PARANOID decreases a lot.\nEven if I cannot understand why setting the upper bound brings such a better improvement, I feel like this is at least a good change.\nHowever, for the other try using discardReadBytes (or tries using wrapped buffers), it seems to not enhanced the timers.\n@normanmaurer If you have any clue to continue, I will. I would propose to at least have the minimal change (upper bounds) in order to fix a bit this issue in PARANOID mode (and does not changed a lot, better or worst, for other levels).\nHere are my results (I tried to use profiling to understand the reasons, but the profiling just kill all results, therefore I was unable to deep in buffer implementations to see where is the main difference).\ntestHighNumberCheckLeakDetectorVersions\nHighItemNumberDISABLED1=506.43838600000004,\nHighItemNumberDISABLED2=496.729877,\nHighItemNumberDISABLED3=503.72193200000004,\nHighItemNumberSIMPLE1=460.46265600000004,\nHighItemNumberSIMPLE2=475.950721,\nHighItemNumberSIMPLE3=467.69606699999997,\nHighItemNumberADVANCED1=465.508472,\nHighItemNumberADVANCED2=611.797092,\nHighItemNumberADVANCED3=467.812221,\nHighItemNumberPARANOID1=304098.68770899996,\nHighItemNumberPARANOID2=304140.256149,\nHighItemNumberPARANOID3=303301.14581200003,\nBigItemDISABLED1=426.205971,\nBigItemDISABLED2=441.343974,\nBigItemDISABLED3=420.528978,\nBigItemSIMPLE1=420.825118,\nBigItemSIMPLE2=432.968082,\nBigItemSIMPLE3=418.955781,\nBigItemADVANCED1=420.391646,\nBigItemADVANCED2=445.089119,\nBigItemADVANCED3=428.97732599999995,\nBigItemPARANOID1=318339.78715600003\nBigItemPARANOID2=319121.713933\nBigItemPARANOID3=316866.262051\ntestRegressionMultipleLevelLeakDetectorNoDisk\nTimer: DISABLED NotUsingDisk1 => 195.185296\nTimer: DISABLED NotUsingDisk2 => 206.28149\nTimer: DISABLED NotUsingDisk3 => 142.354934\nTimer: SIMPLE NotUsingDisk1 => 185.459894\nTimer: SIMPLE NotUsingDisk2 => 82.243331\nTimer: SIMPLE NotUsingDisk3 => 83.65489\nTimer: ADVANCED NotUsingDisk1 => 175.245971\nTimer: ADVANCED NotUsingDisk2 => 214.114222\nTimer: ADVANCED NotUsingDisk3 => 89.005829\nTimer: PARANOID NotUsingDisk1 => 119286.301819\nTimer: PARANOID NotUsingDisk2 => 1604.135822\nTimer: PARANOID NotUsingDisk3 => 2489.555437", "url": "https://github.com/netty/netty/pull/10623#discussion_r499235079", "createdAt": "2020-10-04T11:18:11Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1035,7 +1035,7 @@ private static String readLine(ByteBuf undecodedChunk, Charset charset) {\n         }\n         SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n         int readerIndex = undecodedChunk.readerIndex();\n-        ByteBuf line = undecodedChunk.alloc().heapBuffer(64);\n+        ByteBuf line = undecodedChunk.alloc().heapBuffer(64, 64);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA=="}, "originalCommit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNTY4Nzc2OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMjo0MDo0N1rOHcHV3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxNTowNTozOFrOHcIL5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw==", "bodyText": "Use final here, it would change a lot how the JIT would optimize it.\nUse a sys property to set this and just use different runs with different JVMs (probably using a good profiler + JMH bench would be ideal to be sure of the impact/meaning of changes)", "url": "https://github.com/netty/netty/pull/10623#discussion_r499242463", "createdAt": "2020-10-04T12:40:47Z", "author": {"login": "franz1981"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -306,6 +308,7 @@ public InterfaceHttpData getBodyHttpData(String name) {\n         return null;\n     }\n \n+    public static int TEST_TEMP_ITEM = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0NDQ4MA==", "bodyText": "@franz1981 Thank you !\nI agree that making it final shall be done to achieve final checks. This variable was only intend to allow 3 kinds of tests, not to be keeped of course in final code (and if final, I couldn't change the value of course).\nNow, using a final here, 3 different runs (changing value of course) and a good profiler could be far better and ideal, I agree.\nHowever, as I said, when using a profiler (the one I have is from Oracle - VisualVM-, and I believe it is not the best around, but the only one I've got), the performances drop severely (even in DISCARDED mode), therefore nothing to look at correctly. For my other tasks, this one is great, giving the information I need and with not that much impact. But there, the performances are almost all at the same level than PARANOID, even in DISCARDED or SIMPLE mode.\nCan you suggest some tools (free, sadly) that could help in this research?", "url": "https://github.com/netty/netty/pull/10623#discussion_r499244480", "createdAt": "2020-10-04T13:02:42Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -306,6 +308,7 @@ public InterfaceHttpData getBodyHttpData(String name) {\n         return null;\n     }\n \n+    public static int TEST_TEMP_ITEM = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw=="}, "originalCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0OTA3Ng==", "bodyText": "Sure!\nhttps://github.com/jvm-profiling-tools/async-profiler is probably one of the most complete, accurate and cheap (;)) ones I know.\nHighly suggested to add -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints", "url": "https://github.com/netty/netty/pull/10623#discussion_r499249076", "createdAt": "2020-10-04T13:50:54Z", "author": {"login": "franz1981"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -306,6 +308,7 @@ public InterfaceHttpData getBodyHttpData(String name) {\n         return null;\n     }\n \n+    public static int TEST_TEMP_ITEM = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw=="}, "originalCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1NjI5NA==", "bodyText": "@njhill now you read my mind bud?:)", "url": "https://github.com/netty/netty/pull/10623#discussion_r499256294", "createdAt": "2020-10-04T15:05:38Z", "author": {"login": "franz1981"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -306,6 +308,7 @@ public InterfaceHttpData getBodyHttpData(String name) {\n         return null;\n     }\n \n+    public static int TEST_TEMP_ITEM = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw=="}, "originalCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MjAyNjQ0OnYy", "diffSide": "RIGHT", "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMTo1NjozOFrOHf7nBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNTo0NDo1MFrOHgEAZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NDU1MQ==", "bodyText": "2020", "url": "https://github.com/netty/netty/pull/10623#discussion_r503244551", "createdAt": "2020-10-12T11:56:38Z", "author": {"login": "chrisvest"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2019 The Netty Project", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MjExOQ==", "bodyText": "OK", "url": "https://github.com/netty/netty/pull/10623#discussion_r503382119", "createdAt": "2020-10-12T15:44:50Z", "author": {"login": "fredericBregier"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2019 The Netty Project", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NDU1MQ=="}, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MjAzNDEyOnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMTo1ODo1N1rOHf7rzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNTo0NjoyOVrOHgEEUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NTc3Mg==", "bodyText": "It's suspicious to see a while-loop for releasing a reference counted object. Increments and decrements of the ref count are supposed to be pair-wise, following the structure of the code.", "url": "https://github.com/netty/netty/pull/10623#discussion_r503245772", "createdAt": "2020-10-12T11:58:57Z", "author": {"login": "chrisvest"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -689,7 +695,9 @@ public void destroy() {\n         destroyed = true;\n \n         if (undecodedChunk != null && undecodedChunk.refCnt() > 0) {\n-            undecodedChunk.release();\n+            while (undecodedChunk.refCnt() > 0) {\n+                undecodedChunk.release();\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MzEyMg==", "bodyText": "Indeed, and I did not found why in some cases I have a huge number of refCnt(), so this.\nHowever, I will remove it, going back to previous situation (probably bad programming on my side).", "url": "https://github.com/netty/netty/pull/10623#discussion_r503383122", "createdAt": "2020-10-12T15:46:29Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -689,7 +695,9 @@ public void destroy() {\n         destroyed = true;\n \n         if (undecodedChunk != null && undecodedChunk.refCnt() > 0) {\n-            undecodedChunk.release();\n+            while (undecodedChunk.refCnt() > 0) {\n+                undecodedChunk.release();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NTc3Mg=="}, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MjAzODU3OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMTo1OTo1OVrOHf7uSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNTo0Njo1MFrOHgEFDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NjQxMA==", "bodyText": "Odd formatting that the constants ended up on their own line.", "url": "https://github.com/netty/netty/pull/10623#discussion_r503246410", "createdAt": "2020-10-12T11:59:59Z", "author": {"login": "chrisvest"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1290,96 +1159,89 @@ private static String readDelimiter(ByteBuf undecodedChunk, String delimiter) {\n     }\n \n     /**\n-     * Load the field value or file data from a Multipart request\n+     * @param undecodedChunk the source where the delimiter is to be found\n+     * @param delimiter the string to find out\n+     * @param offset the offset from readerIndex within the undecodedChunk to\n+     *     start from to find out the delimiter\n      *\n-     * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n-     * @throws ErrorDataDecoderException\n+     * @return a number >= 0 if found, else new offset with negative value\n+     *     (to inverse), both from readerIndex\n      */\n-    private static boolean loadDataMultipartStandard(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n+    private static int findDelimiter(ByteBuf undecodedChunk, String delimiter,\n+                                     int offset) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n         final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastPosition = startReaderIndex;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (undecodedChunk.isReadable()) {\n-            final byte nextByte = undecodedChunk.readByte();\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n+        final int toRead = undecodedChunk.readableBytes();\n+        int newOffset = offset;\n+        boolean delimiterNotFound = true;\n+        while (delimiterNotFound && newOffset + delimeterLength <= toRead) {\n+            int posFirstChar = undecodedChunk\n+                .bytesBefore(startReaderIndex + newOffset, toRead - newOffset,\n+                             (byte) delimiter.codePointAt(0));\n+            if (posFirstChar == -1) {\n+                newOffset = toRead;\n+                return -newOffset;\n+            }\n+            newOffset = posFirstChar + offset;\n+            if (newOffset + delimeterLength > toRead) {\n+                return -newOffset;\n+            }\n+            // assume will found it\n+            delimiterNotFound = false;\n+            for (int index = 1; index < delimeterLength; index++) {\n+                if (undecodedChunk\n+                        .getByte(startReaderIndex + newOffset + index) !=\n+                    delimiter.codePointAt(index)) {\n+                    // ignore first found offset and redo search from next char\n+                    newOffset++;\n+                    delimiterNotFound = true;\n                     break;\n                 }\n-                continue;\n             }\n-            lastPosition = undecodedChunk.readerIndex();\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastPosition -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n-        }\n-        if (prevByte == HttpConstants.CR) {\n-            lastPosition--;\n         }\n-        ByteBuf content = undecodedChunk.retainedSlice(startReaderIndex, lastPosition - startReaderIndex);\n-        try {\n-            httpData.addContent(content, delimiterFound);\n-        } catch (IOException e) {\n-            throw new ErrorDataDecoderException(e);\n+        if (delimiterNotFound || newOffset + delimeterLength > toRead) {\n+            return -newOffset;\n         }\n-        undecodedChunk.readerIndex(lastPosition);\n-        return delimiterFound;\n+        return newOffset;\n     }\n \n     /**\n      * Load the field value from a Multipart request\n      *\n      * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n+     *\n      * @throws ErrorDataDecoderException\n      */\n-    private static boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n-        if (!undecodedChunk.hasArray()) {\n-            return loadDataMultipartStandard(undecodedChunk, delimiter, httpData);\n-        }\n-        final SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n+    private boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter,\n+                                      HttpData httpData) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n-        final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastRealPos = sao.pos;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (sao.pos < sao.limit) {\n-            final byte nextByte = sao.bytes[sao.pos++];\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n-                    break;\n-                }\n-                continue;\n-            }\n-            lastRealPos = sao.pos;\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastRealPos -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n+        int newOffset =\n+            findDelimiter(undecodedChunk, delimiter, lastDataPosition);\n+        if (newOffset < 0) {\n+            // delimiter not found\n+            lastDataPosition = -newOffset;\n+            return false;\n         }\n-        if (prevByte == HttpConstants.CR) {\n-            lastRealPos--;\n+        // found delimiter but still need to check if CRLF before\n+        int startDelimiter = newOffset;\n+        if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+            HttpConstants.LF) {\n+            startDelimiter--;\n+            if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+                HttpConstants.CR) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 491}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MzMxMA==", "bodyText": "Right, wrong indentation through IJ", "url": "https://github.com/netty/netty/pull/10623#discussion_r503383310", "createdAt": "2020-10-12T15:46:50Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1290,96 +1159,89 @@ private static String readDelimiter(ByteBuf undecodedChunk, String delimiter) {\n     }\n \n     /**\n-     * Load the field value or file data from a Multipart request\n+     * @param undecodedChunk the source where the delimiter is to be found\n+     * @param delimiter the string to find out\n+     * @param offset the offset from readerIndex within the undecodedChunk to\n+     *     start from to find out the delimiter\n      *\n-     * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n-     * @throws ErrorDataDecoderException\n+     * @return a number >= 0 if found, else new offset with negative value\n+     *     (to inverse), both from readerIndex\n      */\n-    private static boolean loadDataMultipartStandard(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n+    private static int findDelimiter(ByteBuf undecodedChunk, String delimiter,\n+                                     int offset) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n         final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastPosition = startReaderIndex;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (undecodedChunk.isReadable()) {\n-            final byte nextByte = undecodedChunk.readByte();\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n+        final int toRead = undecodedChunk.readableBytes();\n+        int newOffset = offset;\n+        boolean delimiterNotFound = true;\n+        while (delimiterNotFound && newOffset + delimeterLength <= toRead) {\n+            int posFirstChar = undecodedChunk\n+                .bytesBefore(startReaderIndex + newOffset, toRead - newOffset,\n+                             (byte) delimiter.codePointAt(0));\n+            if (posFirstChar == -1) {\n+                newOffset = toRead;\n+                return -newOffset;\n+            }\n+            newOffset = posFirstChar + offset;\n+            if (newOffset + delimeterLength > toRead) {\n+                return -newOffset;\n+            }\n+            // assume will found it\n+            delimiterNotFound = false;\n+            for (int index = 1; index < delimeterLength; index++) {\n+                if (undecodedChunk\n+                        .getByte(startReaderIndex + newOffset + index) !=\n+                    delimiter.codePointAt(index)) {\n+                    // ignore first found offset and redo search from next char\n+                    newOffset++;\n+                    delimiterNotFound = true;\n                     break;\n                 }\n-                continue;\n             }\n-            lastPosition = undecodedChunk.readerIndex();\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastPosition -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n-        }\n-        if (prevByte == HttpConstants.CR) {\n-            lastPosition--;\n         }\n-        ByteBuf content = undecodedChunk.retainedSlice(startReaderIndex, lastPosition - startReaderIndex);\n-        try {\n-            httpData.addContent(content, delimiterFound);\n-        } catch (IOException e) {\n-            throw new ErrorDataDecoderException(e);\n+        if (delimiterNotFound || newOffset + delimeterLength > toRead) {\n+            return -newOffset;\n         }\n-        undecodedChunk.readerIndex(lastPosition);\n-        return delimiterFound;\n+        return newOffset;\n     }\n \n     /**\n      * Load the field value from a Multipart request\n      *\n      * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n+     *\n      * @throws ErrorDataDecoderException\n      */\n-    private static boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n-        if (!undecodedChunk.hasArray()) {\n-            return loadDataMultipartStandard(undecodedChunk, delimiter, httpData);\n-        }\n-        final SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n+    private boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter,\n+                                      HttpData httpData) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n-        final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastRealPos = sao.pos;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (sao.pos < sao.limit) {\n-            final byte nextByte = sao.bytes[sao.pos++];\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n-                    break;\n-                }\n-                continue;\n-            }\n-            lastRealPos = sao.pos;\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastRealPos -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n+        int newOffset =\n+            findDelimiter(undecodedChunk, delimiter, lastDataPosition);\n+        if (newOffset < 0) {\n+            // delimiter not found\n+            lastDataPosition = -newOffset;\n+            return false;\n         }\n-        if (prevByte == HttpConstants.CR) {\n-            lastRealPos--;\n+        // found delimiter but still need to check if CRLF before\n+        int startDelimiter = newOffset;\n+        if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+            HttpConstants.LF) {\n+            startDelimiter--;\n+            if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+                HttpConstants.CR) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NjQxMA=="}, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 491}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzU5OTc3OnYy", "diffSide": "RIGHT", "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzoyOTozOFrOHoPf_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozMjoyN1rOHoPnxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1OTAzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *   http://www.apache.org/licenses/LICENSE-2.0\n          \n          \n            \n             *   https://www.apache.org/licenses/LICENSE-2.0", "url": "https://github.com/netty/netty/pull/10623#discussion_r511959038", "createdAt": "2020-10-26T13:29:38Z", "author": {"login": "chrisvest"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MTAyOA==", "bodyText": "Already done ;-)", "url": "https://github.com/netty/netty/pull/10623#discussion_r511961028", "createdAt": "2020-10-26T13:32:27Z", "author": {"login": "fredericBregier"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1OTAzOA=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYyMzg0OnYy", "diffSide": "RIGHT", "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNDo0NlrOHoPuFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxOTozMDoyNFrOHpNQdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng==", "bodyText": "nit: we use 4 spaces ... Please change everywhere to be consistent with our code-styling", "url": "https://github.com/netty/netty/pull/10623#discussion_r511962646", "createdAt": "2020-10-26T13:34:46Z", "author": {"login": "normanmaurer"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.http.multipart;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpRequest;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpHeaderNames;\n+import io.netty.handler.codec.http.HttpMethod;\n+import io.netty.handler.codec.http.HttpVersion;\n+import io.netty.microbench.util.AbstractMicrobenchmark;\n+import io.netty.util.ResourceLeakDetector;\n+import io.netty.util.ResourceLeakDetector.Level;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+\n+@Threads(1)\n+@Warmup(iterations = 2)\n+@Measurement(iterations = 3)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class HttpPostMultipartRequestDecoderBenchmark\n+    extends AbstractMicrobenchmark {\n+\n+  public double testHighNumberChunks(boolean big, boolean noDisk) {\n+    String BOUNDARY = \"01f136d9282f\";\n+    int size = 8 * 1024;\n+    int chunkNumber = 64;\n+    StringBuilder stringBuilder = new StringBuilder(size);\n+    stringBuilder.setLength(size);\n+    String data = stringBuilder.toString();\n+\n+    byte[] bodyStartBytes = (\"--\" + BOUNDARY + \"\\n\" +\n+                             \"Content-Disposition: form-data; name=\\\"msg_id\\\"\\n\\n15200\\n--\" +\n+                             BOUNDARY +\n+                             \"\\nContent-Disposition: form-data; name=\\\"msg1\\\"; filename=\\\"file1.txt\\\"\\n\\n\" +\n+                             data).getBytes();\n+    byte[] bodyPartBigBytes = data.getBytes();\n+    byte[] intermediaryBytes = (\"\\n--\" + BOUNDARY +\n+                                \"\\nContent-Disposition: form-data; name=\\\"msg2\\\"; filename=\\\"file2.txt\\\"\\n\\n\" +\n+                                data).getBytes();\n+    byte[] finalBigBytes = (\"\\n\" + \"--\" + BOUNDARY + \"--\\n\").getBytes();\n+    ByteBuf firstBuf = Unpooled.wrappedBuffer(bodyStartBytes);\n+    ByteBuf finalBuf = Unpooled.wrappedBuffer(finalBigBytes);\n+    ByteBuf nextBuf;\n+    if (big) {\n+      nextBuf = Unpooled.wrappedBuffer(bodyPartBigBytes);\n+    } else {\n+      nextBuf = Unpooled.wrappedBuffer(intermediaryBytes);\n+    }\n+    DefaultHttpRequest req =\n+        new DefaultHttpRequest(HttpVersion.HTTP_1_0, HttpMethod.POST, \"/up\");\n+    req.headers().add(HttpHeaderNames.CONTENT_TYPE,\n+                      \"multipart/form-data; boundary=\" + BOUNDARY);\n+\n+    long start = System.nanoTime();\n+\n+    DefaultHttpDataFactory defaultHttpDataFactory =\n+        new DefaultHttpDataFactory(noDisk? 1024 * 1024 : 16 * 1024);\n+    HttpPostRequestDecoder decoder =\n+        new HttpPostRequestDecoder(defaultHttpDataFactory, req);\n+    firstBuf.retain();\n+    decoder.offer(new DefaultHttpContent(firstBuf));\n+    firstBuf.release();\n+    for (int i = 1; i < chunkNumber; i++) {\n+      nextBuf.retain();\n+      decoder.offer(new DefaultHttpContent(nextBuf));\n+      nextBuf.release();\n+      nextBuf.readerIndex(0);\n+    }\n+    finalBuf.retain();\n+    decoder.offer(new DefaultLastHttpContent(finalBuf));\n+    finalBuf.release();\n+    while (decoder.hasNext()) {\n+      InterfaceHttpData httpData = decoder.next();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    while (nextBuf.refCnt() > 0) {\n+      nextBuf.release();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    long stop = System.nanoTime();\n+    double time = (stop - start) / 1000000.0;\n+    defaultHttpDataFactory.cleanAllHttpData();\n+    defaultHttpDataFactory.cleanRequestHttpData(req);\n+    decoder.destroy();\n+    return time;\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighAdvancedLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NTY0OA==", "bodyText": "OK, I will double check.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511965648", "createdAt": "2020-10-26T13:39:00Z", "author": {"login": "fredericBregier"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.http.multipart;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpRequest;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpHeaderNames;\n+import io.netty.handler.codec.http.HttpMethod;\n+import io.netty.handler.codec.http.HttpVersion;\n+import io.netty.microbench.util.AbstractMicrobenchmark;\n+import io.netty.util.ResourceLeakDetector;\n+import io.netty.util.ResourceLeakDetector.Level;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+\n+@Threads(1)\n+@Warmup(iterations = 2)\n+@Measurement(iterations = 3)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class HttpPostMultipartRequestDecoderBenchmark\n+    extends AbstractMicrobenchmark {\n+\n+  public double testHighNumberChunks(boolean big, boolean noDisk) {\n+    String BOUNDARY = \"01f136d9282f\";\n+    int size = 8 * 1024;\n+    int chunkNumber = 64;\n+    StringBuilder stringBuilder = new StringBuilder(size);\n+    stringBuilder.setLength(size);\n+    String data = stringBuilder.toString();\n+\n+    byte[] bodyStartBytes = (\"--\" + BOUNDARY + \"\\n\" +\n+                             \"Content-Disposition: form-data; name=\\\"msg_id\\\"\\n\\n15200\\n--\" +\n+                             BOUNDARY +\n+                             \"\\nContent-Disposition: form-data; name=\\\"msg1\\\"; filename=\\\"file1.txt\\\"\\n\\n\" +\n+                             data).getBytes();\n+    byte[] bodyPartBigBytes = data.getBytes();\n+    byte[] intermediaryBytes = (\"\\n--\" + BOUNDARY +\n+                                \"\\nContent-Disposition: form-data; name=\\\"msg2\\\"; filename=\\\"file2.txt\\\"\\n\\n\" +\n+                                data).getBytes();\n+    byte[] finalBigBytes = (\"\\n\" + \"--\" + BOUNDARY + \"--\\n\").getBytes();\n+    ByteBuf firstBuf = Unpooled.wrappedBuffer(bodyStartBytes);\n+    ByteBuf finalBuf = Unpooled.wrappedBuffer(finalBigBytes);\n+    ByteBuf nextBuf;\n+    if (big) {\n+      nextBuf = Unpooled.wrappedBuffer(bodyPartBigBytes);\n+    } else {\n+      nextBuf = Unpooled.wrappedBuffer(intermediaryBytes);\n+    }\n+    DefaultHttpRequest req =\n+        new DefaultHttpRequest(HttpVersion.HTTP_1_0, HttpMethod.POST, \"/up\");\n+    req.headers().add(HttpHeaderNames.CONTENT_TYPE,\n+                      \"multipart/form-data; boundary=\" + BOUNDARY);\n+\n+    long start = System.nanoTime();\n+\n+    DefaultHttpDataFactory defaultHttpDataFactory =\n+        new DefaultHttpDataFactory(noDisk? 1024 * 1024 : 16 * 1024);\n+    HttpPostRequestDecoder decoder =\n+        new HttpPostRequestDecoder(defaultHttpDataFactory, req);\n+    firstBuf.retain();\n+    decoder.offer(new DefaultHttpContent(firstBuf));\n+    firstBuf.release();\n+    for (int i = 1; i < chunkNumber; i++) {\n+      nextBuf.retain();\n+      decoder.offer(new DefaultHttpContent(nextBuf));\n+      nextBuf.release();\n+      nextBuf.readerIndex(0);\n+    }\n+    finalBuf.retain();\n+    decoder.offer(new DefaultLastHttpContent(finalBuf));\n+    finalBuf.release();\n+    while (decoder.hasNext()) {\n+      InterfaceHttpData httpData = decoder.next();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    while (nextBuf.refCnt() > 0) {\n+      nextBuf.release();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    long stop = System.nanoTime();\n+    double time = (stop - start) / 1000000.0;\n+    defaultHttpDataFactory.cleanAllHttpData();\n+    defaultHttpDataFactory.cleanRequestHttpData(req);\n+    decoder.destroy();\n+    return time;\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighAdvancedLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk3MDg3MA==", "bodyText": "Done", "url": "https://github.com/netty/netty/pull/10623#discussion_r512970870", "createdAt": "2020-10-27T19:30:24Z", "author": {"login": "fredericBregier"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.http.multipart;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpRequest;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpHeaderNames;\n+import io.netty.handler.codec.http.HttpMethod;\n+import io.netty.handler.codec.http.HttpVersion;\n+import io.netty.microbench.util.AbstractMicrobenchmark;\n+import io.netty.util.ResourceLeakDetector;\n+import io.netty.util.ResourceLeakDetector.Level;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+\n+@Threads(1)\n+@Warmup(iterations = 2)\n+@Measurement(iterations = 3)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class HttpPostMultipartRequestDecoderBenchmark\n+    extends AbstractMicrobenchmark {\n+\n+  public double testHighNumberChunks(boolean big, boolean noDisk) {\n+    String BOUNDARY = \"01f136d9282f\";\n+    int size = 8 * 1024;\n+    int chunkNumber = 64;\n+    StringBuilder stringBuilder = new StringBuilder(size);\n+    stringBuilder.setLength(size);\n+    String data = stringBuilder.toString();\n+\n+    byte[] bodyStartBytes = (\"--\" + BOUNDARY + \"\\n\" +\n+                             \"Content-Disposition: form-data; name=\\\"msg_id\\\"\\n\\n15200\\n--\" +\n+                             BOUNDARY +\n+                             \"\\nContent-Disposition: form-data; name=\\\"msg1\\\"; filename=\\\"file1.txt\\\"\\n\\n\" +\n+                             data).getBytes();\n+    byte[] bodyPartBigBytes = data.getBytes();\n+    byte[] intermediaryBytes = (\"\\n--\" + BOUNDARY +\n+                                \"\\nContent-Disposition: form-data; name=\\\"msg2\\\"; filename=\\\"file2.txt\\\"\\n\\n\" +\n+                                data).getBytes();\n+    byte[] finalBigBytes = (\"\\n\" + \"--\" + BOUNDARY + \"--\\n\").getBytes();\n+    ByteBuf firstBuf = Unpooled.wrappedBuffer(bodyStartBytes);\n+    ByteBuf finalBuf = Unpooled.wrappedBuffer(finalBigBytes);\n+    ByteBuf nextBuf;\n+    if (big) {\n+      nextBuf = Unpooled.wrappedBuffer(bodyPartBigBytes);\n+    } else {\n+      nextBuf = Unpooled.wrappedBuffer(intermediaryBytes);\n+    }\n+    DefaultHttpRequest req =\n+        new DefaultHttpRequest(HttpVersion.HTTP_1_0, HttpMethod.POST, \"/up\");\n+    req.headers().add(HttpHeaderNames.CONTENT_TYPE,\n+                      \"multipart/form-data; boundary=\" + BOUNDARY);\n+\n+    long start = System.nanoTime();\n+\n+    DefaultHttpDataFactory defaultHttpDataFactory =\n+        new DefaultHttpDataFactory(noDisk? 1024 * 1024 : 16 * 1024);\n+    HttpPostRequestDecoder decoder =\n+        new HttpPostRequestDecoder(defaultHttpDataFactory, req);\n+    firstBuf.retain();\n+    decoder.offer(new DefaultHttpContent(firstBuf));\n+    firstBuf.release();\n+    for (int i = 1; i < chunkNumber; i++) {\n+      nextBuf.retain();\n+      decoder.offer(new DefaultHttpContent(nextBuf));\n+      nextBuf.release();\n+      nextBuf.readerIndex(0);\n+    }\n+    finalBuf.retain();\n+    decoder.offer(new DefaultLastHttpContent(finalBuf));\n+    finalBuf.release();\n+    while (decoder.hasNext()) {\n+      InterfaceHttpData httpData = decoder.next();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    while (nextBuf.refCnt() > 0) {\n+      nextBuf.release();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    long stop = System.nanoTime();\n+    double time = (stop - start) / 1000000.0;\n+    defaultHttpDataFactory.cleanAllHttpData();\n+    defaultHttpDataFactory.cleanRequestHttpData(req);\n+    decoder.destroy();\n+    return time;\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighAdvancedLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYyODQ1OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo0MlrOHoPw1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxOTozMDo1NVrOHpNRgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963350", "createdAt": "2020-10-26T13:35:42Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk3MDM1NQ==", "bodyText": "As I wrote, there is a \"old\" bug, rarely catched, when the buffer undecodedChunk exceed the memory limit. When it reached this limit, there is a call to undecodedChunk.discardReadBytes();. I found this bug accidently, through my other new calls to the very same methods and errors in unit testing.\nWhen this call occurs, the underlying buffers are changed, changing all HttpData values (of course, incorrectly).\nTo prevent this, I propose to copy it. I know this is higher in memory copy, but correct from final values, instead of current status.\nAlso, I changed this \"high limit only\" call to \"each time it is relevant\" (when new data can be added while there are enough space within the current buffer to fill it with this new data).\nAnother way would be to igonre totally this high limit and therefore not calling, nowhere, this undecodedChunk.discardReadBytes();, in order to keep final consistency.\nWDYT?", "url": "https://github.com/netty/netty/pull/10623#discussion_r511970355", "createdAt": "2020-10-26T13:45:34Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk3NTkxMg==", "bodyText": "Of course, the second option (no more calls to discardReadBytes()) would lead to higher memory usage since the underlying buffer will be kept until the end.\nUsing \"to disk\" (or mixed way) management of HttpData can decrease this memory pressure, espcially for File upload (but not limited to), but as in mixed way, some HttpData could stay using the original undecodedChunk, this could lead in wrong data at the end, so my original proposition.\nAny direction or idea ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r511975912", "createdAt": "2020-10-26T13:53:21Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE4OTAwMQ==", "bodyText": "I think we should only call undecoded.discardReadBytes() if refCnt() == 1.", "url": "https://github.com/netty/netty/pull/10623#discussion_r512189001", "createdAt": "2020-10-26T18:43:22Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjIwNzA5NQ==", "bodyText": "It could be a good idea !\nI know this will be almost never true (due to retainedSlice() that will increment refCnt()), but at least, it will give a chance for it (as with SeekAhead algorithm that is almost never usable) and obviously limits the copy as much as possible.\nI will git it a try. Thanks @normanmaurer for this idea !", "url": "https://github.com/netty/netty/pull/10623#discussion_r512207095", "createdAt": "2020-10-26T19:14:57Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk3MTEzOQ==", "bodyText": "Done:\nNew benchmark results:\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,248 \u00b1 0,198  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,067 \u00b1 1,219  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,109 \u00b1 0,038  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,326 \u00b1 0,314  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,444 \u00b1 0,226  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,462 \u00b1 0,642  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,159 \u00b1 0,003  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,522 \u00b1 0,049  ops/ms\nAlmost the same as in previous code.", "url": "https://github.com/netty/netty/pull/10623#discussion_r512971139", "createdAt": "2020-10-27T19:30:55Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYyODc1OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo0N1rOHoPxAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo0N1rOHoPxAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM5NA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963394", "createdAt": "2020-10-26T13:35:47Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -461,7 +464,7 @@ private void parseBodyAttributesStandard() {\n                     } else if (read == HttpConstants.LF) {\n                         currentStatus = MultiPartStatus.PREEPILOGUE;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYyOTExOnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo1M1rOHoPxPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo1M1rOHoPxPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzQ1Mg==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963452", "createdAt": "2020-10-26T13:35:53Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -475,16 +478,15 @@ private void parseBodyAttributesStandard() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYyOTM4OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo1OFrOHoPxdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNTo1OFrOHoPxdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzUwOQ==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963509", "createdAt": "2020-10-26T13:35:58Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -475,16 +478,15 @@ private void parseBodyAttributesStandard() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));\n                 } else if (!currentAttribute.isCompleted()) {\n                     setFinalBuffer(Unpooled.EMPTY_BUFFER);\n                 }\n                 firstpos = currentpos;\n                 currentStatus = MultiPartStatus.EPILOGUE;\n             } else if (contRead && currentAttribute != null && currentStatus == MultiPartStatus.FIELD) {\n                 // reset index except if to continue in case of FIELD getStatus\n-                currentAttribute.addContent(undecodedChunk.retainedSlice(firstpos, currentpos - firstpos),\n-                                            false);\n+                currentAttribute.addContent(undecodedChunk.copy(firstpos, currentpos - firstpos), false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYyOTkyOnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjowM1rOHoPxvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjowM1rOHoPxvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzU4MQ==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963581", "createdAt": "2020-10-26T13:36:03Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -558,7 +560,7 @@ private void parseBodyAttributes() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYzMTAyOnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjoxOVrOHoPyaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjoxOVrOHoPyaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2Mzc1Mw==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963753", "createdAt": "2020-10-26T13:36:19Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -569,7 +571,7 @@ private void parseBodyAttributes() {\n                                 currentStatus = MultiPartStatus.PREEPILOGUE;\n                                 ampersandpos = currentpos - 2;\n                                 sao.setReadPosition(0);\n-                                setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                                setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYzMTM2OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjoyNFrOHoPypg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjoyNFrOHoPypg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzgxNA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963814", "createdAt": "2020-10-26T13:36:24Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -587,7 +589,7 @@ private void parseBodyAttributes() {\n                         currentStatus = MultiPartStatus.PREEPILOGUE;\n                         ampersandpos = currentpos - 1;\n                         sao.setReadPosition(0);\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYzMTcyOnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjoyOVrOHoPy3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjoyOVrOHoPy3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2Mzg2OQ==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963869", "createdAt": "2020-10-26T13:36:29Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -604,16 +606,15 @@ private void parseBodyAttributes() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYzMjA0OnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjozNFrOHoPzCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNjozNFrOHoPzCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzkxNA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963914", "createdAt": "2020-10-26T13:36:34Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -604,16 +606,15 @@ private void parseBodyAttributes() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));\n                 } else if (!currentAttribute.isCompleted()) {\n                     setFinalBuffer(Unpooled.EMPTY_BUFFER);\n                 }\n                 firstpos = currentpos;\n                 currentStatus = MultiPartStatus.EPILOGUE;\n             } else if (contRead && currentAttribute != null && currentStatus == MultiPartStatus.FIELD) {\n                 // reset index except if to continue in case of FIELD getStatus\n-                currentAttribute.addContent(undecodedChunk.retainedSlice(firstpos, currentpos - firstpos),\n-                                            false);\n+                currentAttribute.addContent(undecodedChunk.copy(firstpos, currentpos - firstpos), false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzYzNDMzOnYy", "diffSide": "RIGHT", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNzowMlrOHoP0YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzo0Njo1MVrOHoQPzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NDI1Ng==", "bodyText": "I think the b.readable() for the second arg is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511964256", "createdAt": "2020-10-26T13:37:02Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -661,7 +662,7 @@ private static ByteBuf decodeAttribute(ByteBuf b, Charset charset) {\n             return null; // nothing to decode\n         }\n \n-        ByteBuf buf = b.alloc().buffer(b.readableBytes());\n+        ByteBuf buf = b.alloc().buffer(b.readableBytes(), b.readableBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk3MTI3Ng==", "bodyText": "Not mandatory, but there, it is valid (there is no need to allocate more than this in this specific case).\nIf you prefer, I could remove it (while valid).", "url": "https://github.com/netty/netty/pull/10623#discussion_r511971276", "createdAt": "2020-10-26T13:46:51Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -661,7 +662,7 @@ private static ByteBuf decodeAttribute(ByteBuf b, Charset charset) {\n             return null; // nothing to decode\n         }\n \n-        ByteBuf buf = b.alloc().buffer(b.readableBytes());\n+        ByteBuf buf = b.alloc().buffer(b.readableBytes(), b.readableBytes());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NDI1Ng=="}, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMDIwOnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjozMFrOHtdnkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MToyMlrOHtggVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzIzMg==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433232", "createdAt": "2020-11-04T15:36:30Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -704,6 +710,7 @@ public void testDecodeMalformedEmptyContentTypeFieldParameters() throws Exceptio\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp-0.txt\", fileUpload.getFilename());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MDUzNQ==", "bodyText": "Yes, not related. I was just getting from time to time (before and after changes) an error in PARANOID mode, so I tried to fix as much as possible the tests to be more coherent.\nThe reason is: as the test is not going through network but directly, so there is no handler that take care of incoming messages, there is no clean of the request after receiving it by the main handler as it should.", "url": "https://github.com/netty/netty/pull/10623#discussion_r517480535", "createdAt": "2020-11-04T16:41:22Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -704,6 +710,7 @@ public void testDecodeMalformedEmptyContentTypeFieldParameters() throws Exceptio\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp-0.txt\", fileUpload.getFilename());\n+        req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzIzMg=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMDczOnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo0MFrOHtdn6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MTo1OFrOHtgh_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzMyMw==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433323", "createdAt": "2020-11-04T15:36:40Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -427,6 +432,7 @@ public void testMultipartRequestWithoutContentTypeBody() {\n         // Create decoder instance to test without any exception.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MDk1Ng==", "bodyText": "Same answear: not related, just fix the test", "url": "https://github.com/netty/netty/pull/10623#discussion_r517480956", "createdAt": "2020-11-04T16:41:58Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -427,6 +432,7 @@ public void testMultipartRequestWithoutContentTypeBody() {\n         // Create decoder instance to test without any exception.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzMyMw=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMDkyOnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo0NFrOHtdoDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MTo1MFrOHtghow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM1Ng==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433356", "createdAt": "2020-11-04T15:36:44Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -397,6 +401,7 @@ public void testFilenameContainingSemicolon2() throws Exception {\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp 0.txt\", fileUpload.getFilename());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MDg2Nw==", "bodyText": "Same answear: not related, just fix the test", "url": "https://github.com/netty/netty/pull/10623#discussion_r517480867", "createdAt": "2020-11-04T16:41:50Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -397,6 +401,7 @@ public void testFilenameContainingSemicolon2() throws Exception {\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp 0.txt\", fileUpload.getFilename());\n+        req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM1Ng=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMTIwOnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo0OFrOHtdoNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MjowNFrOHtgiWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM5Ng==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433396", "createdAt": "2020-11-04T15:36:48Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -368,6 +371,7 @@ public void testFilenameContainingSemicolon() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTA1MQ==", "bodyText": "Same answear: not related, just fix the test", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481051", "createdAt": "2020-11-04T16:42:04Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -368,6 +371,7 @@ public void testFilenameContainingSemicolon() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM5Ng=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMTU1OnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo1MlrOHtdoYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MjoxMFrOHtgimg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ0Mw==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433443", "createdAt": "2020-11-04T15:36:52Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -211,6 +213,7 @@ public void testQuotedBoundary() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTExNA==", "bodyText": "Same answear: not related, just fix the test", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481114", "createdAt": "2020-11-04T16:42:10Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -211,6 +213,7 @@ public void testQuotedBoundary() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ0Mw=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMTc1OnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo1NVrOHtdoiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MjoxNlrOHtgi3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ4Mw==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433483", "createdAt": "2020-11-04T15:36:55Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -178,6 +179,7 @@ public void testMultipartCodecWithCRasEndOfAttribute() throws Exception {\n             assertNotNull(datar);\n             assertEquals(datas[i].getBytes(CharsetUtil.UTF_8).length, datar.length);\n \n+            req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTE4Mw==", "bodyText": "Same answear: not related, just fix the test", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481183", "createdAt": "2020-11-04T16:42:16Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -178,6 +179,7 @@ public void testMultipartCodecWithCRasEndOfAttribute() throws Exception {\n             assertNotNull(datar);\n             assertEquals(datas[i].getBytes(CharsetUtil.UTF_8).length, datar.length);\n \n+            req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ4Mw=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MzAyMjA5OnYy", "diffSide": "RIGHT", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo1OVrOHtdoyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo0MjoyMVrOHtgjLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzU0NQ==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433545", "createdAt": "2020-11-04T15:36:59Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -132,6 +132,7 @@ public void testFullHttpRequestUpload() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTI2MA==", "bodyText": "Same answear: not related, just fix the test", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481260", "createdAt": "2020-11-04T16:42:21Z", "author": {"login": "fredericBregier"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -132,6 +132,7 @@ public void testFullHttpRequestUpload() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzU0NQ=="}, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3782, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}