{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzMzE2NDI2", "number": 10082, "title": "Let LzfEncoder support length aware ability.", "bodyText": "Motivation:\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\nModification:\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well.\nResult:\nGives users control over compression capabilities", "createdAt": "2020-03-04T03:18:20Z", "url": "https://github.com/netty/netty/pull/10082", "merged": true, "mergeCommit": {"oid": "60cbe8b7b2f7e54cb25218229f0e6af407441735"}, "closed": true, "closedAt": "2020-03-11T20:05:23Z", "author": {"login": "carryxyh"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcMTV0UAFqTM3MTk5NzU2Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcMneHfABqjMxMTg4OTIzMjc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxOTk3NTY2", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-371997566", "createdAt": "2020-03-10T14:22:17Z", "commit": {"oid": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNDoyMjoxN1rOF0RCVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNDoyNDoyNFrOF0RI-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM0OTM5Ng==", "bodyText": "we need to also keep the old constructor for backward combat.", "url": "https://github.com/netty/netty/pull/10082#discussion_r390349396", "createdAt": "2020-03-10T14:22:17Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -55,58 +70,73 @@\n      * non-standard platforms it may be necessary to use {@link #LzfEncoder(boolean)} with {@code true} param.\n      */\n     public LzfEncoder() {\n-        this(false, MAX_CHUNK_LEN);\n+        this(false, MAX_CHUNK_LEN, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified encoding instance.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses\n+     *                     standard JDK access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n      */\n     public LzfEncoder(boolean safeInstance) {\n-        this(safeInstance, MAX_CHUNK_LEN);\n+        this(safeInstance, MAX_CHUNK_LEN, -1);\n+    }\n+\n+    /**\n+     * Creates a new LZF encoder with specified encoding instance and compressThreshold.\n+     *\n+     * @param safeInstance      If {@code true} encoder will use {@link ChunkEncoder} that only uses standard\n+     *                          JDK access methods, and should work on all Java platforms and JVMs.\n+     *                          Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                          implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                          class (which may be included by other JDK's as well).\n+     * @param compressThreshold compress threshold for compression. see {@link #compressThreshold}.\n+     */\n+    public LzfEncoder(boolean safeInstance, int compressThreshold) {\n+        this(safeInstance, MAX_CHUNK_LEN, compressThreshold);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified total length of encoded chunk. You can configure it to encode\n      * your data flow more efficient if you know the average size of messages that you send.\n      *\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param totalLength Expected total length of content to compress;\n+     *                    only matters for outgoing messages that is smaller than maximum chunk size (64k),\n+     *                    to optimize encoding hash tables.\n      */\n     public LzfEncoder(int totalLength) {\n-        this(false, totalLength);\n+        this(false, totalLength, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified settings.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK\n+     *                     access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n+     * @param totalLength  Expected total length of content to compress; only matters for outgoing messages\n+     *                     that is smaller than maximum chunk size (64k), to optimize encoding hash tables.\n      */\n-    public LzfEncoder(boolean safeInstance, int totalLength) {\n+    public LzfEncoder(boolean safeInstance, int totalLength, int compressThreshold) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM1MDcyOA==", "bodyText": "I would prefer to not allow this for now... I think there is really no good reason to adjust this on the fly.", "url": "https://github.com/netty/netty/pull/10082#discussion_r390350728", "createdAt": "2020-03-10T14:23:57Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -138,6 +176,48 @@ protected void encode(ChannelHandlerContext ctx, ByteBuf in, ByteBuf out) throws\n         }\n     }\n \n+    private int encodeCompress(byte[] input, int inputPtr, int length, byte[] output, int outputPtr) {\n+        return LZFEncoder.appendEncoded(encoder,\n+                input, inputPtr, length, output, outputPtr) - outputPtr;\n+    }\n+\n+    /**\n+     * Use lzf uncompressed format to encode a piece of input.\n+     */\n+    private static int encodeNonCompress(byte[] input, int inputPtr, int length, byte[] output, int outputPtr) {\n+        int left = length;\n+        int chunkLen = Math.min(LZFChunk.MAX_CHUNK_LEN, left);\n+        outputPtr = LZFChunk.appendNonCompressed(input, inputPtr, length, output, outputPtr);\n+        left -= chunkLen;\n+        if (left < 1) {\n+            return outputPtr;\n+        }\n+        inputPtr += chunkLen;\n+        do {\n+            chunkLen = Math.min(left, LZFChunk.MAX_CHUNK_LEN);\n+            outputPtr = LZFChunk.appendNonCompressed(input, inputPtr, length, output, outputPtr);\n+            inputPtr += chunkLen;\n+            left -= chunkLen;\n+        } while (left > 0);\n+        return outputPtr;\n+    }\n+\n+    public int getCompressThreshold() {\n+        return compressThreshold;\n+    }\n+\n+    /**\n+     * Since we could set this threshold at runtime, so we keep set method.\n+     */\n+    public void setCompressThreshold(int compressThreshold) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM1MTA5OA==", "bodyText": "remove...", "url": "https://github.com/netty/netty/pull/10082#discussion_r390351098", "createdAt": "2020-03-10T14:24:24Z", "author": {"login": "normanmaurer"}, "path": "codec/src/test/java/io/netty/handler/codec/compression/LengthAwareLzfIntegrationTest.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.compression;\n+\n+import io.netty.channel.embedded.EmbeddedChannel;\n+\n+/**\n+ * LengthAwareLzfIntegrationTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be"}, "originalPosition": 21}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "912f552c979d3791e8dde2e2f173c205aff7fb92", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/912f552c979d3791e8dde2e2f173c205aff7fb92", "committedDate": "2020-03-11T02:35:46Z", "message": "Fix review request."}, "afterCommit": {"oid": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "committedDate": "2020-03-11T03:27:21Z", "message": "Fix bug"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNTY0NDMy", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-372564432", "createdAt": "2020-03-11T08:48:06Z", "commit": {"oid": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwODo0ODowNlrOF0traA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwODo0OTowN1rOF0ttbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxODY2NA==", "bodyText": "should we validate this on construction and if so throw ?", "url": "https://github.com/netty/netty/pull/10082#discussion_r390818664", "createdAt": "2020-03-11T08:48:06Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -17,27 +17,42 @@\n \n import com.ning.compress.BufferRecycler;\n import com.ning.compress.lzf.ChunkEncoder;\n+import com.ning.compress.lzf.LZFChunk;\n import com.ning.compress.lzf.LZFEncoder;\n import com.ning.compress.lzf.util.ChunkEncoderFactory;\n import io.netty.buffer.ByteBuf;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.handler.codec.MessageToByteEncoder;\n+import io.netty.util.internal.logging.InternalLogger;\n+import io.netty.util.internal.logging.InternalLoggerFactory;\n \n-import static com.ning.compress.lzf.LZFChunk.*;\n+import static com.ning.compress.lzf.LZFChunk.MAX_CHUNK_LEN;\n \n /**\n  * Compresses a {@link ByteBuf} using the LZF format.\n- *\n+ * <p>\n  * See original <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LZF package</a>\n  * and <a href=\"https://github.com/ning/compress/wiki/LZFFormat\">LZF format</a> for full description.\n  */\n public class LzfEncoder extends MessageToByteEncoder<ByteBuf> {\n+\n+    private static final InternalLogger logger = InternalLoggerFactory.getInstance(LzfEncoder.class);\n+\n     /**\n      * Minimum block size ready for compression. Blocks with length\n      * less than {@link #MIN_BLOCK_TO_COMPRESS} will write as uncompressed.\n      */\n     private static final int MIN_BLOCK_TO_COMPRESS = 16;\n \n+    /**\n+     * Compress threshold for LZF format. When the amount of input data is less than compressThreshold,\n+     * we will construct an uncompressed output according to the LZF format.\n+     * <p>\n+     * When the value is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, since LZF will not compress data\n+     * that is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, compressThreshold will not work.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxODkxMA==", "bodyText": "final", "url": "https://github.com/netty/netty/pull/10082#discussion_r390818910", "createdAt": "2020-03-11T08:48:35Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -17,27 +17,42 @@\n \n import com.ning.compress.BufferRecycler;\n import com.ning.compress.lzf.ChunkEncoder;\n+import com.ning.compress.lzf.LZFChunk;\n import com.ning.compress.lzf.LZFEncoder;\n import com.ning.compress.lzf.util.ChunkEncoderFactory;\n import io.netty.buffer.ByteBuf;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.handler.codec.MessageToByteEncoder;\n+import io.netty.util.internal.logging.InternalLogger;\n+import io.netty.util.internal.logging.InternalLoggerFactory;\n \n-import static com.ning.compress.lzf.LZFChunk.*;\n+import static com.ning.compress.lzf.LZFChunk.MAX_CHUNK_LEN;\n \n /**\n  * Compresses a {@link ByteBuf} using the LZF format.\n- *\n+ * <p>\n  * See original <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LZF package</a>\n  * and <a href=\"https://github.com/ning/compress/wiki/LZFFormat\">LZF format</a> for full description.\n  */\n public class LzfEncoder extends MessageToByteEncoder<ByteBuf> {\n+\n+    private static final InternalLogger logger = InternalLoggerFactory.getInstance(LzfEncoder.class);\n+\n     /**\n      * Minimum block size ready for compression. Blocks with length\n      * less than {@link #MIN_BLOCK_TO_COMPRESS} will write as uncompressed.\n      */\n     private static final int MIN_BLOCK_TO_COMPRESS = 16;\n \n+    /**\n+     * Compress threshold for LZF format. When the amount of input data is less than compressThreshold,\n+     * we will construct an uncompressed output according to the LZF format.\n+     * <p>\n+     * When the value is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, since LZF will not compress data\n+     * that is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, compressThreshold will not work.\n+     */\n+    private int compressThreshold;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxOTE4MA==", "bodyText": "add javadocs for param compressThreshold as well.", "url": "https://github.com/netty/netty/pull/10082#discussion_r390819180", "createdAt": "2020-03-11T08:49:07Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -55,58 +70,79 @@\n      * non-standard platforms it may be necessary to use {@link #LzfEncoder(boolean)} with {@code true} param.\n      */\n     public LzfEncoder() {\n-        this(false, MAX_CHUNK_LEN);\n+        this(false, MAX_CHUNK_LEN, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified encoding instance.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses\n+     *                     standard JDK access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n      */\n     public LzfEncoder(boolean safeInstance) {\n-        this(safeInstance, MAX_CHUNK_LEN);\n+        this(safeInstance, MAX_CHUNK_LEN, -1);\n+    }\n+\n+    /**\n+     * Creates a new LZF encoder with specified encoding instance and compressThreshold.\n+     *\n+     * @param safeInstance      If {@code true} encoder will use {@link ChunkEncoder} that only uses standard\n+     *                          JDK access methods, and should work on all Java platforms and JVMs.\n+     *                          Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                          implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                          class (which may be included by other JDK's as well).\n+     * @param totalLength       Expected total length of content to compress; only matters for outgoing messages\n+     *                          that is smaller than maximum chunk size (64k), to optimize encoding hash tables.\n+     */\n+    public LzfEncoder(boolean safeInstance, int totalLength) {\n+        this(safeInstance, totalLength, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified total length of encoded chunk. You can configure it to encode\n      * your data flow more efficient if you know the average size of messages that you send.\n      *\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param totalLength Expected total length of content to compress;\n+     *                    only matters for outgoing messages that is smaller than maximum chunk size (64k),\n+     *                    to optimize encoding hash tables.\n      */\n     public LzfEncoder(int totalLength) {\n-        this(false, totalLength);\n+        this(false, totalLength, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified settings.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK\n+     *                     access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n+     * @param totalLength  Expected total length of content to compress; only matters for outgoing messages\n+     *                     that is smaller than maximum chunk size (64k), to optimize encoding hash tables.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af"}, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNTc1NTI2", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-372575526", "createdAt": "2020-03-11T09:04:57Z", "commit": {"oid": "173bbde9794eb304239181903052980c2e8846a2"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwOTowNDo1OFrOF0uNpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwOTowNDo1OFrOF0uNpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgyNzQzMQ==", "bodyText": "I think we should not use -1 as a special number here and use MIN_BLOCK_TO_COMPRESS  as our default.", "url": "https://github.com/netty/netty/pull/10082#discussion_r390827431", "createdAt": "2020-03-11T09:04:58Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -131,10 +133,10 @@ public LzfEncoder(boolean safeInstance, int totalLength, int compressThreshold)\n                     \" (expected: \" + MIN_BLOCK_TO_COMPRESS + '-' + MAX_CHUNK_LEN + ')');\n         }\n \n-        if (compressThreshold >= 0 && compressThreshold < MIN_BLOCK_TO_COMPRESS) {\n+        if (compressThreshold > 0 && compressThreshold < MIN_BLOCK_TO_COMPRESS) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "173bbde9794eb304239181903052980c2e8846a2"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNTg5MDAy", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-372589002", "createdAt": "2020-03-11T09:24:07Z", "commit": {"oid": "5d706c6245b71a84c4aede2c3906d90ef50b331c"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwOToyNDowN1rOF0u3zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwOToyNDowN1rOF0u3zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgzODIyMw==", "bodyText": "nit: make this final", "url": "https://github.com/netty/netty/pull/10082#discussion_r390838223", "createdAt": "2020-03-11T09:24:07Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -128,8 +166,16 @@ protected void encode(ChannelHandlerContext ctx, ByteBuf in, ByteBuf out) throws\n         out.ensureWritable(maxOutputLength);\n         final byte[] output = out.array();\n         final int outputPtr = out.arrayOffset() + out.writerIndex();\n-        final int outputLength = LZFEncoder.appendEncoded(encoder,\n-                        input, inputPtr, length,  output, outputPtr) - outputPtr;\n+\n+        int outputLength;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d706c6245b71a84c4aede2c3906d90ef50b331c"}, "originalPosition": 154}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNTk1NTM1", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-372595535", "createdAt": "2020-03-11T09:33:17Z", "commit": {"oid": "575f2e61c4ab3ffc8a65168f5158d3ec51601e02"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNjk4MDY3", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-372698067", "createdAt": "2020-03-11T12:05:21Z", "commit": {"oid": "05f61dc01343b56135db5bf28bc8c22f9e5e1d02"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjowNToyMVrOF00J9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjowNToyMVrOF00J9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkyNDc4OQ==", "bodyText": "@carryxyh sorry I missed this before... Please remove this declaration and the related imports as its not used anymore.", "url": "https://github.com/netty/netty/pull/10082#discussion_r390924789", "createdAt": "2020-03-11T12:05:21Z", "author": {"login": "normanmaurer"}, "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -17,27 +17,42 @@\n \n import com.ning.compress.BufferRecycler;\n import com.ning.compress.lzf.ChunkEncoder;\n+import com.ning.compress.lzf.LZFChunk;\n import com.ning.compress.lzf.LZFEncoder;\n import com.ning.compress.lzf.util.ChunkEncoderFactory;\n import io.netty.buffer.ByteBuf;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.handler.codec.MessageToByteEncoder;\n+import io.netty.util.internal.logging.InternalLogger;\n+import io.netty.util.internal.logging.InternalLoggerFactory;\n \n-import static com.ning.compress.lzf.LZFChunk.*;\n+import static com.ning.compress.lzf.LZFChunk.MAX_CHUNK_LEN;\n \n /**\n  * Compresses a {@link ByteBuf} using the LZF format.\n- *\n+ * <p>\n  * See original <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LZF package</a>\n  * and <a href=\"https://github.com/ning/compress/wiki/LZFFormat\">LZF format</a> for full description.\n  */\n public class LzfEncoder extends MessageToByteEncoder<ByteBuf> {\n+\n+    private static final InternalLogger logger = InternalLoggerFactory.getInstance(LzfEncoder.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05f61dc01343b56135db5bf28bc8c22f9e5e1d02"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzQ3NjE2", "url": "https://github.com/netty/netty/pull/10082#pullrequestreview-372747616", "createdAt": "2020-03-11T13:18:44Z", "commit": {"oid": "d8214d6617ab1ebf86ce61d5b26a1bee8802f0e1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d8214d6617ab1ebf86ce61d5b26a1bee8802f0e1", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/d8214d6617ab1ebf86ce61d5b26a1bee8802f0e1", "committedDate": "2020-03-11T12:07:57Z", "message": "remove useless logger."}, "afterCommit": {"oid": "c7270b921ab1ff1c15cc966c0322388b74a38db7", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/c7270b921ab1ff1c15cc966c0322388b74a38db7", "committedDate": "2020-03-11T13:46:37Z", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96a65a003881f73d4896ba104644f796217fe82f", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/96a65a003881f73d4896ba104644f796217fe82f", "committedDate": "2020-03-11T13:50:56Z", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c7270b921ab1ff1c15cc966c0322388b74a38db7", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/c7270b921ab1ff1c15cc966c0322388b74a38db7", "committedDate": "2020-03-11T13:46:37Z", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well."}, "afterCommit": {"oid": "96a65a003881f73d4896ba104644f796217fe82f", "author": {"user": {"login": "carryxyh", "name": "\u65f6\u65e0\u4e24\u4e36"}}, "url": "https://github.com/netty/netty/commit/96a65a003881f73d4896ba104644f796217fe82f", "committedDate": "2020-03-11T13:50:56Z", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 386, "cost": 1, "resetAt": "2021-11-01T15:33:45Z"}}}