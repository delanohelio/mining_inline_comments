{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1MzY4ODY1", "number": 10623, "title": "Fix performance regression on HttpPost RequestDecoder", "bodyText": "Fix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\nMotivation:\nToo many readByte() method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using bytesBefore(firstByte) instead of looping externally).\nModification:\nChanges done:\n\nmajor change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using bytesBefore() and keep the last unfound position to skeep already parsed parts (algorithms are kept the same but scan implementations are different, minimizing access to the undecodedChunk buffer)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\nResult:\nObservations using Async-Profiler:\n\nWithout optimizations, most of the time (more than 95%) is through readByte() method within loadDataMultipartStandard method.\nWith using bytesBefore(byte) instead of readByte() to find various delimiter, the loadDataMultipartStandard method is going down to 19 to 33% depending on the test used. the readByte() method or equivalent getByte(pos) method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n\n1 vs 2: With optimizations, in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through bytesBefore(byte) method)\n\nExtract of Benchmark run:\nRun complete. Total time: 00:13:27\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,925 \u00b1 0,233  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  3,498 \u00b1 1,484  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,443 \u00b1 0,027  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,983 \u00b1 0,103  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,690 \u00b1 0,060  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,935 \u00b1 0,030  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,201 \u00b1 0,005  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,858 \u00b1 0,055  ops/ms", "createdAt": "2020-09-30T09:16:13Z", "url": "https://github.com/netty/netty/pull/10623", "merged": true, "mergeCommit": {"oid": "1c230405fd4f7c445773b662beeccebc18f85f98"}, "closed": true, "closedAt": "2020-11-19T07:00:36Z", "author": {"login": "fredericBregier"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdOOtv-gBqjM4Mjg3MDE1MzI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABddvkPtABqjQwMTExNTk0NzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "66b3d379f56e8bd70185133bc62c7f274bd5a96b", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/66b3d379f56e8bd70185133bc62c7f274bd5a96b", "committedDate": "2020-09-30T09:04:52Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nTemptative to fix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\n- Instead of appending each time the new buffer, propostion is to wrapped it\n- Instead of allocating with no upper limit buffers, try to alocate as much as possible with effective and coherent limit\n\nThe performances drop twice in normal condition (not PARANOID):\n- Without this patch, the very same test gives 200 to 240 ms each check, except in PARANOID mode where it is about 170.000 ms\n- With this patch, the timers are about 430 ms each check (so double), and 710 ms in PARANOID mode (so 400 times better)\n\nSo this might not be a good idea considering usual times without PARANOID level, but allows to keep PARANOID in all tests.\nAt least it might give a clue on expert to know where the issue is.\n\nConsider that the included test shall not be keeped as it does nothing except performance check.\nNote also there is no given test for the original mode since it relies on old code but the test code is the same."}, "afterCommit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/4b2f1db62ce5bab675dc9d82e9bb00d1bd855633", "committedDate": "2020-10-01T10:16:30Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nInstead of allocating with no upper limit buffers, try to alocate as much as possible with effective and coherent limit\n\nThe performances are better in all conditions (even PARANOID):\n- Without this patch, the very same test gives 300 to 450 ms each check, except in PARANOID mode where it is about 105.000 ms\n- With this patch, the timers are about 100 to 200 ms each check (so almost 2 times better), and 1000 to 1300 ms in PARANOID mode (so about 100 times better)\n\nSo this might be a good idea considering better times without PARANOID level, and allowing to keep PARANOID in all tests.\n\nConsider that the included test shall not be keeped as it does nothing except performance check."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwMzQ5MjUz", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-500349253", "createdAt": "2020-10-01T13:31:39Z", "commit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzozMTozOVrOHbKibg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzozMTozOVrOHbKibg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA==", "bodyText": "@fredericBregier can you explain why setting an upper limit improves things here ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r498246254", "createdAt": "2020-10-01T13:31:39Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1035,7 +1035,7 @@ private static String readLine(ByteBuf undecodedChunk, Charset charset) {\n         }\n         SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n         int readerIndex = undecodedChunk.readerIndex();\n-        ByteBuf line = undecodedChunk.alloc().heapBuffer(64);\n+        ByteBuf line = undecodedChunk.alloc().heapBuffer(64, 64);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633"}, "originalPosition": 14}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ac562ff3d9e78c8a0502b0784f71373377667c1c", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/ac562ff3d9e78c8a0502b0784f71373377667c1c", "committedDate": "2020-10-02T05:49:01Z", "message": "Fake commit\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where write is replaced by wrapped and the last \"no upper bounded\" buffer is replaced too\n\nTest to run is `testRegressionMultipleLevelLeakDetector`"}, "afterCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/be0651e98dc09d3056441b30ed90a184a5ccf512", "committedDate": "2020-10-04T11:00:37Z", "message": "Fake commit\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where temptative to reuse more the existing ByteBuf is done\n\nTest to run is `testRegressionMultipleLevelLeakDetectorNoDisk` or `testHighNumberCheckLeakDetectorVersions`\n\nCurrent results (stability is correct but numbers depend on host)\n\ntestHighNumberCheckLeakDetectorVersions\n=======================================\n\nHighItemNumberDISABLED1=506.43838600000004,\nHighItemNumberDISABLED2=496.729877,\nHighItemNumberDISABLED3=503.72193200000004,\n\nHighItemNumberSIMPLE1=460.46265600000004,\nHighItemNumberSIMPLE2=475.950721,\nHighItemNumberSIMPLE3=467.69606699999997,\n\nHighItemNumberADVANCED1=465.508472,\nHighItemNumberADVANCED2=611.797092,\nHighItemNumberADVANCED3=467.812221,\n\nHighItemNumberPARANOID1=304098.68770899996,\nHighItemNumberPARANOID2=304140.256149,\nHighItemNumberPARANOID3=303301.14581200003,\n\nBigItemDISABLED1=426.205971,\nBigItemDISABLED2=441.343974,\nBigItemDISABLED3=420.528978,\n\nBigItemSIMPLE1=420.825118,\nBigItemSIMPLE2=432.968082,\nBigItemSIMPLE3=418.955781,\n\nBigItemADVANCED1=420.391646,\nBigItemADVANCED2=445.089119,\nBigItemADVANCED3=428.97732599999995,\n\nBigItemPARANOID1=318339.78715600003\nBigItemPARANOID2=319121.713933\nBigItemPARANOID3=316866.262051\n\ntestRegressionMultipleLevelLeakDetectorNoDisk\n=============================================\n\nTimer: DISABLED NotUsingDisk1 => 195.185296\nTimer: DISABLED NotUsingDisk2 => 206.28149\nTimer: DISABLED NotUsingDisk3 => 142.354934\n\nTimer: SIMPLE NotUsingDisk1 => 185.459894\nTimer: SIMPLE NotUsingDisk2 => 82.243331\nTimer: SIMPLE NotUsingDisk3 => 83.65489\n\nTimer: ADVANCED NotUsingDisk1 => 175.245971\nTimer: ADVANCED NotUsingDisk2 => 214.114222\nTimer: ADVANCED NotUsingDisk3 => 89.005829\n\nTimer: PARANOID NotUsingDisk1 => 119286.301819\nTimer: PARANOID NotUsingDisk2 => 1604.135822\nTimer: PARANOID NotUsingDisk3 => 2489.555437"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNjE5ODg2", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-501619886", "createdAt": "2020-10-04T12:40:47Z", "commit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMjo0MDo0N1rOHcHV3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQxMjo0MDo0N1rOHcHV3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw==", "bodyText": "Use final here, it would change a lot how the JIT would optimize it.\nUse a sys property to set this and just use different runs with different JVMs (probably using a good profiler + JMH bench would be ideal to be sure of the impact/meaning of changes)", "url": "https://github.com/netty/netty/pull/10623#discussion_r499242463", "createdAt": "2020-10-04T12:40:47Z", "author": {"login": "franz1981"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -306,6 +308,7 @@ public InterfaceHttpData getBodyHttpData(String name) {\n         return null;\n     }\n \n+    public static int TEST_TEMP_ITEM = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512"}, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/be0651e98dc09d3056441b30ed90a184a5ccf512", "committedDate": "2020-10-04T11:00:37Z", "message": "Fake commit\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where temptative to reuse more the existing ByteBuf is done\n\nTest to run is `testRegressionMultipleLevelLeakDetectorNoDisk` or `testHighNumberCheckLeakDetectorVersions`\n\nCurrent results (stability is correct but numbers depend on host)\n\ntestHighNumberCheckLeakDetectorVersions\n=======================================\n\nHighItemNumberDISABLED1=506.43838600000004,\nHighItemNumberDISABLED2=496.729877,\nHighItemNumberDISABLED3=503.72193200000004,\n\nHighItemNumberSIMPLE1=460.46265600000004,\nHighItemNumberSIMPLE2=475.950721,\nHighItemNumberSIMPLE3=467.69606699999997,\n\nHighItemNumberADVANCED1=465.508472,\nHighItemNumberADVANCED2=611.797092,\nHighItemNumberADVANCED3=467.812221,\n\nHighItemNumberPARANOID1=304098.68770899996,\nHighItemNumberPARANOID2=304140.256149,\nHighItemNumberPARANOID3=303301.14581200003,\n\nBigItemDISABLED1=426.205971,\nBigItemDISABLED2=441.343974,\nBigItemDISABLED3=420.528978,\n\nBigItemSIMPLE1=420.825118,\nBigItemSIMPLE2=432.968082,\nBigItemSIMPLE3=418.955781,\n\nBigItemADVANCED1=420.391646,\nBigItemADVANCED2=445.089119,\nBigItemADVANCED3=428.97732599999995,\n\nBigItemPARANOID1=318339.78715600003\nBigItemPARANOID2=319121.713933\nBigItemPARANOID3=316866.262051\n\ntestRegressionMultipleLevelLeakDetectorNoDisk\n=============================================\n\nTimer: DISABLED NotUsingDisk1 => 195.185296\nTimer: DISABLED NotUsingDisk2 => 206.28149\nTimer: DISABLED NotUsingDisk3 => 142.354934\n\nTimer: SIMPLE NotUsingDisk1 => 185.459894\nTimer: SIMPLE NotUsingDisk2 => 82.243331\nTimer: SIMPLE NotUsingDisk3 => 83.65489\n\nTimer: ADVANCED NotUsingDisk1 => 175.245971\nTimer: ADVANCED NotUsingDisk2 => 214.114222\nTimer: ADVANCED NotUsingDisk3 => 89.005829\n\nTimer: PARANOID NotUsingDisk1 => 119286.301819\nTimer: PARANOID NotUsingDisk2 => 1604.135822\nTimer: PARANOID NotUsingDisk3 => 2489.555437"}, "afterCommit": {"oid": "a5cc7f20bd5824638f91b66fcc50b1fc773cb787", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/a5cc7f20bd5824638f91b66fcc50b1fc773cb787", "committedDate": "2020-10-08T18:13:27Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers.\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only undecodedChunk temptatives of reusing already allocated buffers are made\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using bytesBefore() and keep the last unfound position to skeep already parsed parts\n\nTest to run is `testRegressionMultipleLevelLeakDetectorNoDisk` or `testHighNumberCheckLeakDetectorVersions`\n\nCurrent results:\n\ntestHighNumberCheckLeakDetectorVersions\n=======================================\n\n{BigItemSIMPLE1=4100, BigItemADVANCED1=4100, BigItemDISABLED1=4100, HighItemNumberADVANCED1=4500, HighItemNumberSIMPLE1=4500, HighItemNumberDISABLED1=4100, HighItemNumberPARANOID1=1480000, BigItemPARANOID1=1450000}\n\n{BigItemDISABLED2=2339, BigItemADVANCED2=2345, BigItemSIMPLE2=2383, HighItemNumberDISABLED2=2840, HighItemNumberSIMPLE2=2864, HighItemNumberADVANCED2=3048, HighItemNumberPARANOID2=1511640, BigItemPARANOID2=1588672}\n\nWithout optim level 2\n{BigItemDISABLED3=423, BigItemADVANCED3=429, BigItemSIMPLE3=453, HighItemNumberSIMPLE3=834, HighItemNumberDISABLED3=936, HighItemNumberADVANCED3=978, BigItemPARANOID3=2720, HighItemNumberPARANOID3=17182}\n\nWith Optim level 2\n{BigItemDISABLED3=420, BigItemSIMPLE3=422, BigItemADVANCED3=451, HighItemNumberSIMPLE3=821, HighItemNumberDISABLED3=930, HighItemNumberADVANCED3=1003, BigItemPARANOID3=2660, HighItemNumberPARANOID3=16712}\n\ntestRegressionMultipleLevelLeakDetectorNoDisk\n=============================================\n\nTimer: DISABLED NotUsingDisk1 => 1903.912934\nTimer: SIMPLE NotUsingDisk1 => 1926.971844\nTimer: ADVANCED NotUsingDisk1 => 26489.845075\nTimer: PARANOID NotUsingDisk1 => 1082819.185679\n\nTimer: DISABLED NotUsingDisk2 => 1538.593234\nTimer: SIMPLE NotUsingDisk2 => 1590.055765\nTimer: ADVANCED NotUsingDisk2 => 1538.786221\nTimer: PARANOID NotUsingDisk2 => 1132602.00827\n\nTimer: DISABLED NotUsingDisk3 => 653.744671\nTimer: SIMPLE NotUsingDisk3 => 785.152185\nTimer: ADVANCED NotUsingDisk3 => 572.136954\nTimer: PARANOID NotUsingDisk3 => 1039.148579\n\nObservations using Async-Profiler:\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With level 2 (reusing as much as possible already allocated buffers and limit the allocation as small as possible), about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With level 3 (using `bytesBefore(byte)` instead of `readByte()` to find the delimiter), the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- V1 vs V2: In SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- V1 vs V3: In SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a5cc7f20bd5824638f91b66fcc50b1fc773cb787", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/a5cc7f20bd5824638f91b66fcc50b1fc773cb787", "committedDate": "2020-10-08T18:13:27Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers.\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only undecodedChunk temptatives of reusing already allocated buffers are made\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using bytesBefore() and keep the last unfound position to skeep already parsed parts\n\nTest to run is `testRegressionMultipleLevelLeakDetectorNoDisk` or `testHighNumberCheckLeakDetectorVersions`\n\nCurrent results:\n\ntestHighNumberCheckLeakDetectorVersions\n=======================================\n\n{BigItemSIMPLE1=4100, BigItemADVANCED1=4100, BigItemDISABLED1=4100, HighItemNumberADVANCED1=4500, HighItemNumberSIMPLE1=4500, HighItemNumberDISABLED1=4100, HighItemNumberPARANOID1=1480000, BigItemPARANOID1=1450000}\n\n{BigItemDISABLED2=2339, BigItemADVANCED2=2345, BigItemSIMPLE2=2383, HighItemNumberDISABLED2=2840, HighItemNumberSIMPLE2=2864, HighItemNumberADVANCED2=3048, HighItemNumberPARANOID2=1511640, BigItemPARANOID2=1588672}\n\nWithout optim level 2\n{BigItemDISABLED3=423, BigItemADVANCED3=429, BigItemSIMPLE3=453, HighItemNumberSIMPLE3=834, HighItemNumberDISABLED3=936, HighItemNumberADVANCED3=978, BigItemPARANOID3=2720, HighItemNumberPARANOID3=17182}\n\nWith Optim level 2\n{BigItemDISABLED3=420, BigItemSIMPLE3=422, BigItemADVANCED3=451, HighItemNumberSIMPLE3=821, HighItemNumberDISABLED3=930, HighItemNumberADVANCED3=1003, BigItemPARANOID3=2660, HighItemNumberPARANOID3=16712}\n\ntestRegressionMultipleLevelLeakDetectorNoDisk\n=============================================\n\nTimer: DISABLED NotUsingDisk1 => 1903.912934\nTimer: SIMPLE NotUsingDisk1 => 1926.971844\nTimer: ADVANCED NotUsingDisk1 => 26489.845075\nTimer: PARANOID NotUsingDisk1 => 1082819.185679\n\nTimer: DISABLED NotUsingDisk2 => 1538.593234\nTimer: SIMPLE NotUsingDisk2 => 1590.055765\nTimer: ADVANCED NotUsingDisk2 => 1538.786221\nTimer: PARANOID NotUsingDisk2 => 1132602.00827\n\nTimer: DISABLED NotUsingDisk3 => 653.744671\nTimer: SIMPLE NotUsingDisk3 => 785.152185\nTimer: ADVANCED NotUsingDisk3 => 572.136954\nTimer: PARANOID NotUsingDisk3 => 1039.148579\n\nObservations using Async-Profiler:\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With level 2 (reusing as much as possible already allocated buffers and limit the allocation as small as possible), about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With level 3 (using `bytesBefore(byte)` instead of `readByte()` to find the delimiter), the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- V1 vs V2: In SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- V1 vs V3: In SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)"}, "afterCommit": {"oid": "d515b3eaf67059ff58135ed2c606997ff921d9d4", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/d515b3eaf67059ff58135ed2c606997ff921d9d4", "committedDate": "2020-10-10T15:05:20Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers minimizes a bit the memory footprint and allocations.\n\n2 kinds of change were done:\n- undecodedChunk temptatives of reusing already allocated buffers are made\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With reusing as much as possible already allocated buffers and limit the allocation as small as possible, about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With reusing already allocated buffers as much as possible, in SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,977 \u00b1 0,843  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,997 \u00b1 0,240  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,010 \u00b1 1,398  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,768 \u00b1 0,072  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,706 \u00b1 0,047  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,954 \u00b1 0,041  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,197 \u00b1 0,008  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,967 \u00b1 0,428  ops/ms"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d515b3eaf67059ff58135ed2c606997ff921d9d4", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/d515b3eaf67059ff58135ed2c606997ff921d9d4", "committedDate": "2020-10-10T15:05:20Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers minimizes a bit the memory footprint and allocations.\n\n2 kinds of change were done:\n- undecodedChunk temptatives of reusing already allocated buffers are made\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With reusing as much as possible already allocated buffers and limit the allocation as small as possible, about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With reusing already allocated buffers as much as possible, in SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,977 \u00b1 0,843  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,997 \u00b1 0,240  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,010 \u00b1 1,398  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,768 \u00b1 0,072  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,706 \u00b1 0,047  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,954 \u00b1 0,041  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,197 \u00b1 0,008  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,967 \u00b1 0,428  ops/ms"}, "afterCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/00565439760dce1a5cfdafc542e94311dfeca68b", "committedDate": "2020-10-10T15:29:33Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers minimizes a bit the memory footprint and allocations.\n\n2 kinds of change were done:\n- undecodedChunk temptatives of reusing already allocated buffers are made\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With reusing as much as possible already allocated buffers and limit the allocation as small as possible, about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With reusing already allocated buffers as much as possible, in SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,977 \u00b1 0,843  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,997 \u00b1 0,240  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,010 \u00b1 1,398  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,768 \u00b1 0,072  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,706 \u00b1 0,047  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,954 \u00b1 0,041  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,197 \u00b1 0,008  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,967 \u00b1 0,428  ops/ms"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2NTUyNjA4", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-506552608", "createdAt": "2020-10-12T11:56:38Z", "commit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMTo1NjozOFrOHf7nBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMTo1OTo1OVrOHf7uSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NDU1MQ==", "bodyText": "2020", "url": "https://github.com/netty/netty/pull/10623#discussion_r503244551", "createdAt": "2020-10-12T11:56:38Z", "author": {"login": "chrisvest"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2019 The Netty Project", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NTc3Mg==", "bodyText": "It's suspicious to see a while-loop for releasing a reference counted object. Increments and decrements of the ref count are supposed to be pair-wise, following the structure of the code.", "url": "https://github.com/netty/netty/pull/10623#discussion_r503245772", "createdAt": "2020-10-12T11:58:57Z", "author": {"login": "chrisvest"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -689,7 +695,9 @@ public void destroy() {\n         destroyed = true;\n \n         if (undecodedChunk != null && undecodedChunk.refCnt() > 0) {\n-            undecodedChunk.release();\n+            while (undecodedChunk.refCnt() > 0) {\n+                undecodedChunk.release();\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NjQxMA==", "bodyText": "Odd formatting that the constants ended up on their own line.", "url": "https://github.com/netty/netty/pull/10623#discussion_r503246410", "createdAt": "2020-10-12T11:59:59Z", "author": {"login": "chrisvest"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1290,96 +1159,89 @@ private static String readDelimiter(ByteBuf undecodedChunk, String delimiter) {\n     }\n \n     /**\n-     * Load the field value or file data from a Multipart request\n+     * @param undecodedChunk the source where the delimiter is to be found\n+     * @param delimiter the string to find out\n+     * @param offset the offset from readerIndex within the undecodedChunk to\n+     *     start from to find out the delimiter\n      *\n-     * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n-     * @throws ErrorDataDecoderException\n+     * @return a number >= 0 if found, else new offset with negative value\n+     *     (to inverse), both from readerIndex\n      */\n-    private static boolean loadDataMultipartStandard(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n+    private static int findDelimiter(ByteBuf undecodedChunk, String delimiter,\n+                                     int offset) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n         final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastPosition = startReaderIndex;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (undecodedChunk.isReadable()) {\n-            final byte nextByte = undecodedChunk.readByte();\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n+        final int toRead = undecodedChunk.readableBytes();\n+        int newOffset = offset;\n+        boolean delimiterNotFound = true;\n+        while (delimiterNotFound && newOffset + delimeterLength <= toRead) {\n+            int posFirstChar = undecodedChunk\n+                .bytesBefore(startReaderIndex + newOffset, toRead - newOffset,\n+                             (byte) delimiter.codePointAt(0));\n+            if (posFirstChar == -1) {\n+                newOffset = toRead;\n+                return -newOffset;\n+            }\n+            newOffset = posFirstChar + offset;\n+            if (newOffset + delimeterLength > toRead) {\n+                return -newOffset;\n+            }\n+            // assume will found it\n+            delimiterNotFound = false;\n+            for (int index = 1; index < delimeterLength; index++) {\n+                if (undecodedChunk\n+                        .getByte(startReaderIndex + newOffset + index) !=\n+                    delimiter.codePointAt(index)) {\n+                    // ignore first found offset and redo search from next char\n+                    newOffset++;\n+                    delimiterNotFound = true;\n                     break;\n                 }\n-                continue;\n             }\n-            lastPosition = undecodedChunk.readerIndex();\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastPosition -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n-        }\n-        if (prevByte == HttpConstants.CR) {\n-            lastPosition--;\n         }\n-        ByteBuf content = undecodedChunk.retainedSlice(startReaderIndex, lastPosition - startReaderIndex);\n-        try {\n-            httpData.addContent(content, delimiterFound);\n-        } catch (IOException e) {\n-            throw new ErrorDataDecoderException(e);\n+        if (delimiterNotFound || newOffset + delimeterLength > toRead) {\n+            return -newOffset;\n         }\n-        undecodedChunk.readerIndex(lastPosition);\n-        return delimiterFound;\n+        return newOffset;\n     }\n \n     /**\n      * Load the field value from a Multipart request\n      *\n      * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n+     *\n      * @throws ErrorDataDecoderException\n      */\n-    private static boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n-        if (!undecodedChunk.hasArray()) {\n-            return loadDataMultipartStandard(undecodedChunk, delimiter, httpData);\n-        }\n-        final SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n+    private boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter,\n+                                      HttpData httpData) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n-        final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastRealPos = sao.pos;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (sao.pos < sao.limit) {\n-            final byte nextByte = sao.bytes[sao.pos++];\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n-                    break;\n-                }\n-                continue;\n-            }\n-            lastRealPos = sao.pos;\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastRealPos -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n+        int newOffset =\n+            findDelimiter(undecodedChunk, delimiter, lastDataPosition);\n+        if (newOffset < 0) {\n+            // delimiter not found\n+            lastDataPosition = -newOffset;\n+            return false;\n         }\n-        if (prevByte == HttpConstants.CR) {\n-            lastRealPos--;\n+        // found delimiter but still need to check if CRLF before\n+        int startDelimiter = newOffset;\n+        if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+            HttpConstants.LF) {\n+            startDelimiter--;\n+            if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+                HttpConstants.CR) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b"}, "originalPosition": 491}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/00565439760dce1a5cfdafc542e94311dfeca68b", "committedDate": "2020-10-10T15:29:33Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers minimizes a bit the memory footprint and allocations.\n\n2 kinds of change were done:\n- undecodedChunk temptatives of reusing already allocated buffers are made\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With reusing as much as possible already allocated buffers and limit the allocation as small as possible, about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With reusing already allocated buffers as much as possible, in SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,977 \u00b1 0,843  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,997 \u00b1 0,240  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,010 \u00b1 1,398  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,768 \u00b1 0,072  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,706 \u00b1 0,047  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,954 \u00b1 0,041  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,197 \u00b1 0,008  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,967 \u00b1 0,428  ops/ms"}, "afterCommit": {"oid": "785e9d99946ad25e2cfc1b2b248dfdadb04d59d4", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/785e9d99946ad25e2cfc1b2b248dfdadb04d59d4", "committedDate": "2020-10-12T16:15:34Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\n\nChanges done:\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,925 \u00b1 0,233  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  3,498 \u00b1 1,484  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,443 \u00b1 0,027  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,983 \u00b1 0,103  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,690 \u00b1 0,060  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,935 \u00b1 0,030  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,201 \u00b1 0,005  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,858 \u00b1 0,055  ops/ms"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f6798beba46b8ad47e5e00487c2c54a41c4471ac", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/f6798beba46b8ad47e5e00487c2c54a41c4471ac", "committedDate": "2020-10-16T12:52:46Z", "message": "Create package-info.java"}, "afterCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/7b14cfb7c113f916dd019d623912c19fff66ba04", "committedDate": "2020-10-26T13:23:13Z", "message": "Create package-info.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Nzc1MzM1", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-516775335", "createdAt": "2020-10-26T13:29:38Z", "commit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzoyOTozOFrOHoPf_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzoyOTozOFrOHoPf_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1OTAzOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *   http://www.apache.org/licenses/LICENSE-2.0\n          \n          \n            \n             *   https://www.apache.org/licenses/LICENSE-2.0", "url": "https://github.com/netty/netty/pull/10623#discussion_r511959038", "createdAt": "2020-10-26T13:29:38Z", "author": {"login": "chrisvest"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Nzc5OTY0", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-516779964", "createdAt": "2020-10-26T13:34:46Z", "commit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNDo0NlrOHoPuFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMzozNzowMlrOHoP0YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng==", "bodyText": "nit: we use 4 spaces ... Please change everywhere to be consistent with our code-styling", "url": "https://github.com/netty/netty/pull/10623#discussion_r511962646", "createdAt": "2020-10-26T13:34:46Z", "author": {"login": "normanmaurer"}, "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.http.multipart;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpRequest;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpHeaderNames;\n+import io.netty.handler.codec.http.HttpMethod;\n+import io.netty.handler.codec.http.HttpVersion;\n+import io.netty.microbench.util.AbstractMicrobenchmark;\n+import io.netty.util.ResourceLeakDetector;\n+import io.netty.util.ResourceLeakDetector.Level;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+\n+@Threads(1)\n+@Warmup(iterations = 2)\n+@Measurement(iterations = 3)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class HttpPostMultipartRequestDecoderBenchmark\n+    extends AbstractMicrobenchmark {\n+\n+  public double testHighNumberChunks(boolean big, boolean noDisk) {\n+    String BOUNDARY = \"01f136d9282f\";\n+    int size = 8 * 1024;\n+    int chunkNumber = 64;\n+    StringBuilder stringBuilder = new StringBuilder(size);\n+    stringBuilder.setLength(size);\n+    String data = stringBuilder.toString();\n+\n+    byte[] bodyStartBytes = (\"--\" + BOUNDARY + \"\\n\" +\n+                             \"Content-Disposition: form-data; name=\\\"msg_id\\\"\\n\\n15200\\n--\" +\n+                             BOUNDARY +\n+                             \"\\nContent-Disposition: form-data; name=\\\"msg1\\\"; filename=\\\"file1.txt\\\"\\n\\n\" +\n+                             data).getBytes();\n+    byte[] bodyPartBigBytes = data.getBytes();\n+    byte[] intermediaryBytes = (\"\\n--\" + BOUNDARY +\n+                                \"\\nContent-Disposition: form-data; name=\\\"msg2\\\"; filename=\\\"file2.txt\\\"\\n\\n\" +\n+                                data).getBytes();\n+    byte[] finalBigBytes = (\"\\n\" + \"--\" + BOUNDARY + \"--\\n\").getBytes();\n+    ByteBuf firstBuf = Unpooled.wrappedBuffer(bodyStartBytes);\n+    ByteBuf finalBuf = Unpooled.wrappedBuffer(finalBigBytes);\n+    ByteBuf nextBuf;\n+    if (big) {\n+      nextBuf = Unpooled.wrappedBuffer(bodyPartBigBytes);\n+    } else {\n+      nextBuf = Unpooled.wrappedBuffer(intermediaryBytes);\n+    }\n+    DefaultHttpRequest req =\n+        new DefaultHttpRequest(HttpVersion.HTTP_1_0, HttpMethod.POST, \"/up\");\n+    req.headers().add(HttpHeaderNames.CONTENT_TYPE,\n+                      \"multipart/form-data; boundary=\" + BOUNDARY);\n+\n+    long start = System.nanoTime();\n+\n+    DefaultHttpDataFactory defaultHttpDataFactory =\n+        new DefaultHttpDataFactory(noDisk? 1024 * 1024 : 16 * 1024);\n+    HttpPostRequestDecoder decoder =\n+        new HttpPostRequestDecoder(defaultHttpDataFactory, req);\n+    firstBuf.retain();\n+    decoder.offer(new DefaultHttpContent(firstBuf));\n+    firstBuf.release();\n+    for (int i = 1; i < chunkNumber; i++) {\n+      nextBuf.retain();\n+      decoder.offer(new DefaultHttpContent(nextBuf));\n+      nextBuf.release();\n+      nextBuf.readerIndex(0);\n+    }\n+    finalBuf.retain();\n+    decoder.offer(new DefaultLastHttpContent(finalBuf));\n+    finalBuf.release();\n+    while (decoder.hasNext()) {\n+      InterfaceHttpData httpData = decoder.next();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    while (nextBuf.refCnt() > 0) {\n+      nextBuf.release();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    long stop = System.nanoTime();\n+    double time = (stop - start) / 1000000.0;\n+    defaultHttpDataFactory.cleanAllHttpData();\n+    defaultHttpDataFactory.cleanRequestHttpData(req);\n+    decoder.destroy();\n+    return time;\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighAdvancedLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963350", "createdAt": "2020-10-26T13:35:42Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM5NA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963394", "createdAt": "2020-10-26T13:35:47Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -461,7 +464,7 @@ private void parseBodyAttributesStandard() {\n                     } else if (read == HttpConstants.LF) {\n                         currentStatus = MultiPartStatus.PREEPILOGUE;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzQ1Mg==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963452", "createdAt": "2020-10-26T13:35:53Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -475,16 +478,15 @@ private void parseBodyAttributesStandard() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzUwOQ==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963509", "createdAt": "2020-10-26T13:35:58Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -475,16 +478,15 @@ private void parseBodyAttributesStandard() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));\n                 } else if (!currentAttribute.isCompleted()) {\n                     setFinalBuffer(Unpooled.EMPTY_BUFFER);\n                 }\n                 firstpos = currentpos;\n                 currentStatus = MultiPartStatus.EPILOGUE;\n             } else if (contRead && currentAttribute != null && currentStatus == MultiPartStatus.FIELD) {\n                 // reset index except if to continue in case of FIELD getStatus\n-                currentAttribute.addContent(undecodedChunk.retainedSlice(firstpos, currentpos - firstpos),\n-                                            false);\n+                currentAttribute.addContent(undecodedChunk.copy(firstpos, currentpos - firstpos), false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzU4MQ==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963581", "createdAt": "2020-10-26T13:36:03Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -558,7 +560,7 @@ private void parseBodyAttributes() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2Mzc1Mw==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963753", "createdAt": "2020-10-26T13:36:19Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -569,7 +571,7 @@ private void parseBodyAttributes() {\n                                 currentStatus = MultiPartStatus.PREEPILOGUE;\n                                 ampersandpos = currentpos - 2;\n                                 sao.setReadPosition(0);\n-                                setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                                setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzgxNA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963814", "createdAt": "2020-10-26T13:36:24Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -587,7 +589,7 @@ private void parseBodyAttributes() {\n                         currentStatus = MultiPartStatus.PREEPILOGUE;\n                         ampersandpos = currentpos - 1;\n                         sao.setReadPosition(0);\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2Mzg2OQ==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963869", "createdAt": "2020-10-26T13:36:29Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -604,16 +606,15 @@ private void parseBodyAttributes() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzkxNA==", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963914", "createdAt": "2020-10-26T13:36:34Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -604,16 +606,15 @@ private void parseBodyAttributes() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));\n                 } else if (!currentAttribute.isCompleted()) {\n                     setFinalBuffer(Unpooled.EMPTY_BUFFER);\n                 }\n                 firstpos = currentpos;\n                 currentStatus = MultiPartStatus.EPILOGUE;\n             } else if (contRead && currentAttribute != null && currentStatus == MultiPartStatus.FIELD) {\n                 // reset index except if to continue in case of FIELD getStatus\n-                currentAttribute.addContent(undecodedChunk.retainedSlice(firstpos, currentpos - firstpos),\n-                                            false);\n+                currentAttribute.addContent(undecodedChunk.copy(firstpos, currentpos - firstpos), false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NDI1Ng==", "bodyText": "I think the b.readable() for the second arg is not needed.", "url": "https://github.com/netty/netty/pull/10623#discussion_r511964256", "createdAt": "2020-10-26T13:37:02Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -661,7 +662,7 @@ private static ByteBuf decodeAttribute(ByteBuf b, Charset charset) {\n             return null; // nothing to decode\n         }\n \n-        ByteBuf buf = b.alloc().buffer(b.readableBytes());\n+        ByteBuf buf = b.alloc().buffer(b.readableBytes(), b.readableBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzNDc5NTc1", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-523479575", "createdAt": "2020-11-04T15:36:30Z", "commit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjozMFrOHtdnkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjozMFrOHtdnkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzIzMg==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433232", "createdAt": "2020-11-04T15:36:30Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -704,6 +710,7 @@ public void testDecodeMalformedEmptyContentTypeFieldParameters() throws Exceptio\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp-0.txt\", fileUpload.getFilename());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzNDc5NzA5", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-523479709", "createdAt": "2020-11-04T15:36:40Z", "commit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo0MFrOHtdn6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNTozNjo1OVrOHtdoyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzMyMw==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433323", "createdAt": "2020-11-04T15:36:40Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -427,6 +432,7 @@ public void testMultipartRequestWithoutContentTypeBody() {\n         // Create decoder instance to test without any exception.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM1Ng==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433356", "createdAt": "2020-11-04T15:36:44Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -397,6 +401,7 @@ public void testFilenameContainingSemicolon2() throws Exception {\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp 0.txt\", fileUpload.getFilename());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM5Ng==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433396", "createdAt": "2020-11-04T15:36:48Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -368,6 +371,7 @@ public void testFilenameContainingSemicolon() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ0Mw==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433443", "createdAt": "2020-11-04T15:36:52Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -211,6 +213,7 @@ public void testQuotedBoundary() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ4Mw==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433483", "createdAt": "2020-11-04T15:36:55Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -178,6 +179,7 @@ public void testMultipartCodecWithCRasEndOfAttribute() throws Exception {\n             assertNotNull(datar);\n             assertEquals(datas[i].getBytes(CharsetUtil.UTF_8).length, datar.length);\n \n+            req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzU0NQ==", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433545", "createdAt": "2020-11-04T15:36:59Z", "author": {"login": "normanmaurer"}, "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -132,6 +132,7 @@ public void testFullHttpRequestUpload() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMzA5MjMw", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-533309230", "createdAt": "2020-11-18T10:35:18Z", "commit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMzU4NTUz", "url": "https://github.com/netty/netty/pull/10623#pullrequestreview-533358553", "createdAt": "2020-11-18T11:37:50Z", "commit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03ef3d0090a51cd79ada6fbbed65cda147e0d595", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/03ef3d0090a51cd79ada6fbbed65cda147e0d595", "committedDate": "2020-11-18T14:21:49Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\nAlso fix a rare issue when internal buffer was growing over a limit, it was partially discarded\nusing `discardReadBytes()` which causes bad changes within previously discovered HttpData.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\n\nChanges done:\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n- Change the condition to discard read bytes when refCnt is at most 1.\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With optimizations, in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,248 \u00b1 0,198 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,067 \u00b1 1,219 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,109 \u00b1 0,038 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,326 \u00b1 0,314 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,444 \u00b1 0,226 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,462 \u00b1 0,642 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,159 \u00b1 0,003 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,522 \u00b1 0,049 ops/ms"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "committedDate": "2020-10-27T23:02:48Z", "message": "Fix https doc"}, "afterCommit": {"oid": "03ef3d0090a51cd79ada6fbbed65cda147e0d595", "author": {"user": {"login": "fredericBregier", "name": "Fr\u00e9d\u00e9ric Br\u00e9gier"}}, "url": "https://github.com/netty/netty/commit/03ef3d0090a51cd79ada6fbbed65cda147e0d595", "committedDate": "2020-11-18T14:21:49Z", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\nAlso fix a rare issue when internal buffer was growing over a limit, it was partially discarded\nusing `discardReadBytes()` which causes bad changes within previously discovered HttpData.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\n\nChanges done:\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n- Change the condition to discard read bytes when refCnt is at most 1.\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With optimizations, in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,248 \u00b1 0,198 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,067 \u00b1 1,219 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,109 \u00b1 0,038 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,326 \u00b1 0,314 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,444 \u00b1 0,226 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,462 \u00b1 0,642 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,159 \u00b1 0,003 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,522 \u00b1 0,049 ops/ms"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 79, "cost": 1, "resetAt": "2021-11-01T15:33:45Z"}}}