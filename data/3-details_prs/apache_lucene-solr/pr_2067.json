{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2OTk3NDI2", "number": 2067, "title": "SOLR-14987: Reuse HttpSolrClient per node vs. one per Solr core when using CloudSolrStream", "bodyText": "Description\nFor collections with many shards (or aliases with many collections and some shards), CloudSolrStream will end up creating a new HttpSolrClient for every SolrStream it opens because the cache key is the full core URL, such as: http://127.0.0.1:63460/solr/collection4_shard4_replica_n6/\nIn addition, CloudSolrStream#getSlices was calling clusterState.getCollectionsMap() which pre-emptively loads all LazyCollectionRef from ZK unnecessarily. This could cause an issue with clusters with many collections and slow down the streaming expression execution.\nSolution\nIn this PR, I've introduced a new ctor in SolrStream to pass the Replica's baseUrl and core as separate parameters. This leads to reusing the same HttpSolrClient for the same node because the cache key is now http://127.0.0.1:63460/solr/. I chose this new ctor approach because CloudSolrStream is not the only consumer of SolrStream and it knows how the list of URLs where constructed from cluster state, so it can safely make the decision about passing the core and reusing clients.\nWhen the request is sent to the remote core, we need to add the core name to the path. This happens in SolrStream#constructParser. This method was public and takes a SolrClient (even though SolrStream already has an HttpSolrClient created in the open method); I've changed the signature to be private and use the client opened in the open method.\nTests\nAdded a new test testCloudStreamClientCache in StreamingTest to verify the SolrStreams created by the CloudStream have the correct baseUrl (without the core).\nChecklist\nPlease review the following and check all that apply:\n\n I have reviewed the guidelines for How to Contribute and my code conforms to the standards described there to the best of my ability.\n I have created a Jira issue and added the issue ID to my pull request title.\n I have given Solr maintainers access to contribute to my PR branch. (optional but recommended)\n I have developed this patch against the master branch.\n I have run ./gradlew check.\n I have added tests for my changes.\n I have added documentation for the Ref Guide (for Solr changes only).", "createdAt": "2020-11-06T21:53:51Z", "url": "https://github.com/apache/lucene-solr/pull/2067", "merged": true, "mergeCommit": {"oid": "30e5e38336de49433a7ecc60fb169c2426278565"}, "closed": true, "closedAt": "2020-12-07T16:03:03Z", "author": {"login": "thelabdude"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZ90gTAH2gAyNTE2OTk3NDI2OmZlZDFlODNmZDdmZmNiZmZhMTg3ZDBjNWFhNzMwMWQ0YjIyZTQyM2M=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdj3bQWAH2gAyNTE2OTk3NDI2OjdiYjYwNjhiMzFkOGY4ODFhNmUxMGNhNjdjMmRkOWM0ZjQ5OTQ3MzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "fed1e83fd7ffcbffa187d0c5aa7301d4b22e423c", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/fed1e83fd7ffcbffa187d0c5aa7301d4b22e423c", "committedDate": "2020-11-06T21:26:54Z", "message": "SOLR-14987: Reuse HttpSolrClient per node vs. one per replica when using CloudSolrStream"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NzUwMzkx", "url": "https://github.com/apache/lucene-solr/pull/2067#pullrequestreview-526750391", "createdAt": "2020-11-10T00:00:31Z", "commit": {"oid": "fed1e83fd7ffcbffa187d0c5aa7301d4b22e423c"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMDowMDozMVrOHwGY-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMDoxMjoxNFrOHwGoKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE5ODM5NQ==", "bodyText": "related: can we update the javadoc on clusterState.getCollectionsMap to be more explicit that it will make a call to zk, instead of the current may", "url": "https://github.com/apache/lucene-solr/pull/2067#discussion_r520198395", "createdAt": "2020-11-10T00:00:31Z", "author": {"login": "madrob"}, "path": "solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/CloudSolrStream.java", "diffHunk": "@@ -334,11 +334,6 @@ private StreamComparator parseComp(String sort, String fl) throws IOException {\n   public static Slice[] getSlices(String collectionName, ZkStateReader zkStateReader, boolean checkAlias) throws IOException {\n     ClusterState clusterState = zkStateReader.getClusterState();\n \n-    Map<String, DocCollection> collectionsMap = clusterState.getCollectionsMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fed1e83fd7ffcbffa187d0c5aa7301d4b22e423c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE5OTA3Mw==", "bodyText": "Should we cache the value of zkStateReader.getAliases below to avoid volatile reads?", "url": "https://github.com/apache/lucene-solr/pull/2067#discussion_r520199073", "createdAt": "2020-11-10T00:02:36Z", "author": {"login": "madrob"}, "path": "solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/CloudSolrStream.java", "diffHunk": "@@ -334,11 +334,6 @@ private StreamComparator parseComp(String sort, String fl) throws IOException {\n   public static Slice[] getSlices(String collectionName, ZkStateReader zkStateReader, boolean checkAlias) throws IOException {\n     ClusterState clusterState = zkStateReader.getClusterState();\n \n-    Map<String, DocCollection> collectionsMap = clusterState.getCollectionsMap();\n-\n-    //TODO we should probably split collection by comma to query more than one\n-    //  which is something already supported in other parts of Solr\n-\n     // check for alias or collection", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fed1e83fd7ffcbffa187d0c5aa7301d4b22e423c"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIwMjI4Mg==", "bodyText": "Can we precomute this in the constructor?", "url": "https://github.com/apache/lucene-solr/pull/2067#discussion_r520202282", "createdAt": "2020-11-10T00:12:14Z", "author": {"login": "madrob"}, "path": "solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/SolrStream.java", "diffHunk": "@@ -126,6 +135,17 @@ public void open() throws IOException {\n     }\n   }\n \n+  private String getNodeUrl() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fed1e83fd7ffcbffa187d0c5aa7301d4b22e423c"}, "originalPosition": 40}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e445554c7e71e4d39b486c427d2efa59dac84dea", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/e445554c7e71e4d39b486c427d2efa59dac84dea", "committedDate": "2020-11-17T21:30:46Z", "message": "Add unit test and use Replica metadata instead of parsing node / core"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c6286911c1147cf59d7a9faca969385c970251c", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/1c6286911c1147cf59d7a9faca969385c970251c", "committedDate": "2020-11-17T21:34:48Z", "message": "Remove unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6507ad92ef6dc6887140de5d5f9076f5c05d653", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/e6507ad92ef6dc6887140de5d5f9076f5c05d653", "committedDate": "2020-11-17T21:37:33Z", "message": "Merge remote-tracking branch 'asf/master' into jira/solr-14987"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be1433745cc37b6999cb3fcf6bf0d770e8bfc242", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/be1433745cc37b6999cb3fcf6bf0d770e8bfc242", "committedDate": "2020-11-19T15:43:26Z", "message": "Merge remote-tracking branch 'asf/master' into jira/solr-14987"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245", "committedDate": "2020-11-19T15:44:17Z", "message": "Update changes.txt"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NjMwNDg1", "url": "https://github.com/apache/lucene-solr/pull/2067#pullrequestreview-534630485", "createdAt": "2020-11-19T16:33:21Z", "commit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNjozMzoyMVrOH2nMIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNjozMzoyMVrOH2nMIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzAyNzIzNA==", "bodyText": "I removed this b/c I don't think we should try to accommodate improperly cased collection names. No tests broke, but let me know if we need this for some reason I don't understand", "url": "https://github.com/apache/lucene-solr/pull/2067#discussion_r527027234", "createdAt": "2020-11-19T16:33:21Z", "author": {"login": "thelabdude"}, "path": "solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/CloudSolrStream.java", "diffHunk": "@@ -334,88 +330,76 @@ private StreamComparator parseComp(String sort, String fl) throws IOException {\n   public static Slice[] getSlices(String collectionName, ZkStateReader zkStateReader, boolean checkAlias) throws IOException {\n     ClusterState clusterState = zkStateReader.getClusterState();\n \n-    Map<String, DocCollection> collectionsMap = clusterState.getCollectionsMap();\n-\n-    //TODO we should probably split collection by comma to query more than one\n-    //  which is something already supported in other parts of Solr\n-\n     // check for alias or collection\n \n     List<String> allCollections = new ArrayList<>();\n     String[] collectionNames = collectionName.split(\",\");\n+    Aliases aliases = checkAlias ? zkStateReader.getAliases() : null;\n+\n     for(String col : collectionNames) {\n-      List<String> collections = checkAlias\n-          ? zkStateReader.getAliases().resolveAliases(col)  // if not an alias, returns collectionName\n+      List<String> collections = (aliases != null)\n+          ? aliases.resolveAliases(col)  // if not an alias, returns collectionName\n           : Collections.singletonList(collectionName);\n       allCollections.addAll(collections);\n     }\n \n     // Lookup all actives slices for these collections\n     List<Slice> slices = allCollections.stream()\n-        .map(collectionsMap::get)\n+        .map(c -> clusterState.getCollectionOrNull(c, true))\n         .filter(Objects::nonNull)\n         .flatMap(docCol -> Arrays.stream(docCol.getActiveSlicesArr()))\n         .collect(Collectors.toList());\n     if (!slices.isEmpty()) {\n-      return slices.toArray(new Slice[slices.size()]);\n-    }\n-\n-    // Check collection case insensitive\n-    for(Entry<String, DocCollection> entry : collectionsMap.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NjMxNzE4", "url": "https://github.com/apache/lucene-solr/pull/2067#pullrequestreview-534631718", "createdAt": "2020-11-19T16:34:40Z", "commit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNjozNDo0MFrOH2nP5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNjozNDo0MFrOH2nP5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzAyODE5OQ==", "bodyText": "Here we're keeping the Replica so we have direct access to its baseUrl and core name instead of parsing those out of the shardUrl", "url": "https://github.com/apache/lucene-solr/pull/2067#discussion_r527028199", "createdAt": "2020-11-19T16:34:40Z", "author": {"login": "thelabdude"}, "path": "solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/CloudSolrStream.java", "diffHunk": "@@ -334,88 +330,76 @@ private StreamComparator parseComp(String sort, String fl) throws IOException {\n   public static Slice[] getSlices(String collectionName, ZkStateReader zkStateReader, boolean checkAlias) throws IOException {\n     ClusterState clusterState = zkStateReader.getClusterState();\n \n-    Map<String, DocCollection> collectionsMap = clusterState.getCollectionsMap();\n-\n-    //TODO we should probably split collection by comma to query more than one\n-    //  which is something already supported in other parts of Solr\n-\n     // check for alias or collection\n \n     List<String> allCollections = new ArrayList<>();\n     String[] collectionNames = collectionName.split(\",\");\n+    Aliases aliases = checkAlias ? zkStateReader.getAliases() : null;\n+\n     for(String col : collectionNames) {\n-      List<String> collections = checkAlias\n-          ? zkStateReader.getAliases().resolveAliases(col)  // if not an alias, returns collectionName\n+      List<String> collections = (aliases != null)\n+          ? aliases.resolveAliases(col)  // if not an alias, returns collectionName\n           : Collections.singletonList(collectionName);\n       allCollections.addAll(collections);\n     }\n \n     // Lookup all actives slices for these collections\n     List<Slice> slices = allCollections.stream()\n-        .map(collectionsMap::get)\n+        .map(c -> clusterState.getCollectionOrNull(c, true))\n         .filter(Objects::nonNull)\n         .flatMap(docCol -> Arrays.stream(docCol.getActiveSlicesArr()))\n         .collect(Collectors.toList());\n     if (!slices.isEmpty()) {\n-      return slices.toArray(new Slice[slices.size()]);\n-    }\n-\n-    // Check collection case insensitive\n-    for(Entry<String, DocCollection> entry : collectionsMap.entrySet()) {\n-      if(entry.getKey().equalsIgnoreCase(collectionName)) {\n-        return entry.getValue().getActiveSlicesArr();\n-      }\n+      return slices.toArray(new Slice[0]);\n     }\n \n     throw new IOException(\"Slices not found for \" + collectionName);\n   }\n \n   protected void constructStreams() throws IOException {\n+    final ModifiableSolrParams mParams = adjustParams(new ModifiableSolrParams(params));\n+    mParams.set(DISTRIB, \"false\"); // We are the aggregator.\n     try {\n+      final Stream<SolrStream> streamOfSolrStream;\n+      if (streamContext != null && streamContext.get(\"shards\") != null) {\n+        // stream of shard url with core\n+        streamOfSolrStream = getShards(this.zkHost, this.collection, this.streamContext, mParams).stream()\n+            .map(s -> new SolrStream(s, mParams));\n+      } else {\n+        // stream of replicas to reuse the same SolrHttpClient per baseUrl\n+        // avoids re-parsing data we already have in the replicas\n+        streamOfSolrStream = getReplicas(this.zkHost, this.collection, this.streamContext, mParams).stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NjMyOTQ4", "url": "https://github.com/apache/lucene-solr/pull/2067#pullrequestreview-534632948", "createdAt": "2020-11-19T16:35:57Z", "commit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNjozNTo1N1rOH2nT0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNjozNTo1N1rOH2nT0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzAyOTIwMQ==", "bodyText": "Didn't seem like this method needed to be public and we already get a SolrClient in the open method, so no need to pass it. However, this breaks a public method signature, so is only for Solr 9.x and shouldn't be back-ported to 8.x", "url": "https://github.com/apache/lucene-solr/pull/2067#discussion_r527029201", "createdAt": "2020-11-19T16:35:57Z", "author": {"login": "thelabdude"}, "path": "solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/SolrStream.java", "diffHunk": "@@ -268,8 +275,7 @@ private Map mapFields(Map fields, Map<String,String> mappings) {\n     return fields;\n   }\n \n-  // temporary...\n-  public TupleStreamParser constructParser(SolrClient server, SolrParams requestParams) throws IOException, SolrServerException {\n+  private TupleStreamParser constructParser(SolrParams requestParams) throws IOException, SolrServerException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9c1d7ec3453a46a33ea9f5123d8882a8dbf8245"}, "originalPosition": 61}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ccd1292b7f63948f7e6b66bf9da38c303bf0f2b", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/1ccd1292b7f63948f7e6b66bf9da38c303bf0f2b", "committedDate": "2020-12-07T15:09:43Z", "message": "Merge remote-tracking branch 'asf/master' into jira/solr-14987"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7bb6068b31d8f881a6e10ca67c2dd9c4f4994735", "author": {"user": {"login": "thelabdude", "name": "Timothy Potter"}}, "url": "https://github.com/apache/lucene-solr/commit/7bb6068b31d8f881a6e10ca67c2dd9c4f4994735", "committedDate": "2020-12-07T15:39:08Z", "message": "Improvement in 8.8"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2579, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}