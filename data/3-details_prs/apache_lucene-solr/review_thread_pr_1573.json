{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNzQxODU5", "number": 1573, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMTo1MFrOEFQ02Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1ODo1MVrOEFV2Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MDA5OnYy", "diffSide": "LEFT", "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMTo1MFrOGjYSsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwODoxMzowMFrOGjbgBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM0Nw==", "bodyText": "Hmm why not keep this assertion (to confirm that if the field is not somehow indexed we are not accidentally/incorrectly running this code)?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751347", "createdAt": "2020-06-13T16:31:50Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -207,8 +202,6 @@ public void newPostingsArray() {\n \n   @Override\n   ParallelPostingsArray createPostingsArray(int size) {\n-    IndexOptions indexOptions = fieldInfo.getIndexOptions();\n-    assert indexOptions != IndexOptions.NONE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwMzkxMA==", "bodyText": "I moved it in a better place in the ctor of the base class. I think that's enough?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439803910", "createdAt": "2020-06-14T08:13:00Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -207,8 +202,6 @@ public void newPostingsArray() {\n \n   @Override\n   ParallelPostingsArray createPostingsArray(int size) {\n-    IndexOptions indexOptions = fieldInfo.getIndexOptions();\n-    assert indexOptions != IndexOptions.NONE;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM0Nw=="}, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MDMxOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMjoyMVrOGjYS0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMjoyMVrOGjYS0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM3Ng==", "bodyText": "s/terms text/terms's text", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751376", "createdAt": "2020-06-13T16:32:21Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "diffHunk": "@@ -22,14 +22,14 @@\n   final static int BYTES_PER_POSTING = 3 * Integer.BYTES;\n \n   final int size;\n-  final int[] textStarts;\n-  final int[] intStarts;\n-  final int[] byteStarts;\n+  final int[] textStarts; // maps term ID to the terms text start in the bytesHash", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MDYwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMjo0NVrOGjYS8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMjo0NVrOGjYS8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQxMQ==", "bodyText": "+1 for this renaming!", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751411", "createdAt": "2020-06-13T16:32:45Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "diffHunk": "@@ -22,14 +22,14 @@\n   final static int BYTES_PER_POSTING = 3 * Integer.BYTES;\n \n   final int size;\n-  final int[] textStarts;\n-  final int[] intStarts;\n-  final int[] byteStarts;\n+  final int[] textStarts; // maps term ID to the terms text start in the bytesHash\n+  final int[] addressOffset; // maps term ID to current stream address", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MDc4OnYy", "diffSide": "LEFT", "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMzozMFrOGjYTDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwODozMjozNVrOGjblxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg==", "bodyText": "Hmm, did these aggregations move somewhere else?  Oh, they look entirely removed?  Were they redundant (computed elsewhere) and these ones were unused?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751436", "createdAt": "2020-06-13T16:33:30Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -56,12 +56,6 @@ public FreqProxTermsWriterPerField(FieldInvertState invertState, TermsHash terms\n   @Override\n   void finish() throws IOException {\n     super.finish();\n-    sumDocFreq += fieldState.uniqueTermCount;\n-    sumTotalTermFreq += fieldState.length;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNDMxNQ==", "bodyText": "sumDocFreq and sumTotalTermFreq are unused. They were used in FreqProxFields in the past but not anymore for a while now. I removed their commented out usage so you can see it in a followup commit", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439804315", "createdAt": "2020-06-14T08:18:50Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -56,12 +56,6 @@ public FreqProxTermsWriterPerField(FieldInvertState invertState, TermsHash terms\n   @Override\n   void finish() throws IOException {\n     super.finish();\n-    sumDocFreq += fieldState.uniqueTermCount;\n-    sumTotalTermFreq += fieldState.length;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg=="}, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNTM4MQ==", "bodyText": "see this https://github.com/apache/lucene-solr/pull/1573/files#diff-aa6c5376b6b755262430916164fd0088L84", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439805381", "createdAt": "2020-06-14T08:32:35Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -56,12 +56,6 @@ public FreqProxTermsWriterPerField(FieldInvertState invertState, TermsHash terms\n   @Override\n   void finish() throws IOException {\n     super.finish();\n-    sumDocFreq += fieldState.uniqueTermCount;\n-    sumTotalTermFreq += fieldState.length;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg=="}, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MjAyOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozNjowNFrOGjYTtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwODoxOTo1MFrOGjbh5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTYwNw==", "bodyText": "Hmm docID is unused in this method?  But I guess the other impl (normal postings) needs it?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751607", "createdAt": "2020-06-13T16:36:04Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java", "diffHunk": "@@ -222,7 +234,7 @@ void writeProx(TermVectorsPostingsArray postings, int termID) {\n   }\n \n   @Override\n-  void newTerm(final int termID) {\n+  void newTerm(final int termID, final int docID) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNDM5MA==", "bodyText": "that's correct.", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439804390", "createdAt": "2020-06-14T08:19:50Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java", "diffHunk": "@@ -222,7 +234,7 @@ void writeProx(TermVectorsPostingsArray postings, int termID) {\n   }\n \n   @Override\n-  void newTerm(final int termID) {\n+  void newTerm(final int termID, final int docID) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTYwNw=="}, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MjgxOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozNzo0M1rOGjYUIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozNzo0M1rOGjYUIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTcxMg==", "bodyText": "Thank you for the javadocs/comments for such cryptic and ancient code ;)\nMaybe just stores instead of allows to store?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751712", "createdAt": "2020-06-13T16:37:43Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU0MzIwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozOToxNFrOGjYUYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQwODoyMDoyMVrOGjbiGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTc3OA==", "bodyText": "Ahh I see, we just moved the assertion to a better place, awesome.", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751778", "createdAt": "2020-06-13T16:39:14Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgwNDQ0Mw==", "bodyText": "yeah :)", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439804443", "createdAt": "2020-06-14T08:20:21Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTc3OA=="}, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTU4NzA1OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxODowNjo0N1rOGjYqzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxODowNjo0N1rOGjYqzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1NzUxNg==", "bodyText": "s/allocated/allocate", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439757516", "createdAt": "2020-06-13T18:06:47Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;\n+    this.indexOptions = indexOptions;\n     PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);\n     bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);\n   }\n \n   void reset() {\n     bytesHash.clear(false);\n+    sortedTermIDs = null;\n     if (nextPerField != null) {\n       nextPerField.reset();\n     }\n   }\n \n-  public void initReader(ByteSliceReader reader, int termID, int stream) {\n+  final void initReader(ByteSliceReader reader, int termID, int stream) {\n     assert stream < streamCount;\n-    int intStart = postingsArray.intStarts[termID];\n-    final int[] ints = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n-    final int upto = intStart & IntBlockPool.INT_BLOCK_MASK;\n+    int streamStartOffset = postingsArray.addressOffset[termID];\n+    final int[] streamAddressBuffer = intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];\n+    final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;\n     reader.init(bytePool,\n                 postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,\n-                ints[upto+stream]);\n+                streamAddressBuffer[offsetInAddressBuffer+stream]);\n   }\n \n-  int[] sortedTermIDs;\n+  private int[] sortedTermIDs;\n \n   /** Collapse the hash table and sort in-place; also sets\n-   * this.sortedTermIDs to the results */\n-  public int[] sortPostings() {\n+   * this.sortedTermIDs to the results\n+   * This method should not be called twice unless {@link #reset()}\n+   * or {@link #reinitHash()} was called. */\n+  final void sortTerms() {\n+    assert sortedTermIDs == null;\n     sortedTermIDs = bytesHash.sort();\n+  }\n+\n+  /**\n+   * Returns the sorted term IDs. {@link #sortTerms()} must be called before\n+   */\n+  final int[] getSortedTermIDs() {\n+    assert sortedTermIDs != null;\n     return sortedTermIDs;\n   }\n \n+  final void reinitHash() {\n+    sortedTermIDs = null;\n+    bytesHash.reinit();\n+  }\n+\n   private boolean doNextCall;\n \n   // Secondary entry point (for 2nd & subsequent TermsHash),\n   // because token text has already been \"interned\" into\n   // textStart, so we hash by textStart.  term vectors use\n   // this API.\n-  public void add(int textStart) throws IOException {\n+  private void add(int textStart, final int docID) throws IOException {\n     int termID = bytesHash.addByPoolOffset(textStart);\n     if (termID >= 0) {      // New posting\n       // First time we are seeing this token since we last\n       // flushed the hash.\n-      // Init stream slices\n-      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n-        intPool.nextBuffer();\n-      }\n-\n-      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n-        bytePool.nextBuffer();\n-      }\n+      initStreamSlices(termID, docID);\n+    } else {\n+      positionStreamSlice(termID, docID);\n+    }\n+  }\n \n-      intUptos = intPool.buffer;\n-      intUptoStart = intPool.intUpto;\n-      intPool.intUpto += streamCount;\n+  private void initStreamSlices(int termID, int docID) throws IOException {\n+    // Init stream slices\n+    // TODO: figure out why this is 2*streamCount here. streamCount should be enough?\n+    if ((2*streamCount) + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n+      // can we fit all the streams in the current buffer?\n+      intPool.nextBuffer();\n+    }\n \n-      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n+    if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < (2*streamCount) * ByteBlockPool.FIRST_LEVEL_SIZE) {\n+      // can we fit at least one byte per stream in the current buffer, if not allocated a new one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDM2MTIwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NTo0MlrOGjezAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NTo0MlrOGjezAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1NzkyMQ==", "bodyText": "this the?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439857921", "createdAt": "2020-06-14T18:55:42Z", "author": {"login": "dweiss"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class stores streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDM2MjA5OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NzoxNFrOGjezfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NzoxNFrOGjezfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1ODA0Ng==", "bodyText": "should not -> must not perhaps?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439858046", "createdAt": "2020-06-14T18:57:14Z", "author": {"login": "dweiss"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class stores streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;\n+    this.indexOptions = indexOptions;\n     PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);\n     bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);\n   }\n \n   void reset() {\n     bytesHash.clear(false);\n+    sortedTermIDs = null;\n     if (nextPerField != null) {\n       nextPerField.reset();\n     }\n   }\n \n-  public void initReader(ByteSliceReader reader, int termID, int stream) {\n+  final void initReader(ByteSliceReader reader, int termID, int stream) {\n     assert stream < streamCount;\n-    int intStart = postingsArray.intStarts[termID];\n-    final int[] ints = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n-    final int upto = intStart & IntBlockPool.INT_BLOCK_MASK;\n+    int streamStartOffset = postingsArray.addressOffset[termID];\n+    final int[] streamAddressBuffer = intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];\n+    final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;\n     reader.init(bytePool,\n                 postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,\n-                ints[upto+stream]);\n+                streamAddressBuffer[offsetInAddressBuffer+stream]);\n   }\n \n-  int[] sortedTermIDs;\n+  private int[] sortedTermIDs;\n \n   /** Collapse the hash table and sort in-place; also sets\n-   * this.sortedTermIDs to the results */\n-  public int[] sortPostings() {\n+   * this.sortedTermIDs to the results\n+   * This method should not be called twice unless {@link #reset()}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDM2MzAyOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/test/org/apache/lucene/index/TestTermsHashPerField.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1ODo1MlrOGjez_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1ODo1MlrOGjez_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1ODE3Mw==", "bodyText": "extra space?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439858173", "createdAt": "2020-06-14T18:58:52Z", "author": {"login": "dweiss"}, "path": "lucene/core/src/test/org/apache/lucene/index/TestTermsHashPerField.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.lucene.index;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import com.carrotsearch.randomizedtesting.generators.RandomPicks;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.Counter;\n+import org.apache.lucene.util.IntBlockPool;\n+import org.apache.lucene.util.LuceneTestCase;\n+\n+public class TestTermsHashPerField extends LuceneTestCase  {\n+\n+  private static TermsHashPerField createNewHash(AtomicInteger newCalled, AtomicInteger addCalled) {\n+    IntBlockPool intBlockPool = new IntBlockPool();\n+    ByteBlockPool byteBlockPool = new ByteBlockPool(new ByteBlockPool.DirectAllocator());\n+    ByteBlockPool termBlockPool = new ByteBlockPool(new ByteBlockPool.DirectAllocator());\n+\n+    TermsHashPerField hash = new TermsHashPerField(1, intBlockPool, byteBlockPool, termBlockPool, Counter.newCounter(),\n+        null, \"testfield\", IndexOptions.DOCS_AND_FREQS) {\n+\n+      private FreqProxTermsWriterPerField.FreqProxPostingsArray freqProxPostingsArray;\n+\n+      @Override\n+      void newTerm(int termID, int docID) {\n+        newCalled.incrementAndGet();\n+        FreqProxTermsWriterPerField.FreqProxPostingsArray postings = freqProxPostingsArray;\n+        postings.lastDocIDs[termID] = docID;\n+        postings.lastDocCodes[termID] = docID << 1;\n+        postings.termFreqs[termID] = 1;\n+      }\n+\n+      @Override\n+      void addTerm(int termID, int docID) {\n+        addCalled.incrementAndGet();\n+        FreqProxTermsWriterPerField.FreqProxPostingsArray postings = freqProxPostingsArray;\n+        if (docID != postings.lastDocIDs[termID]) {\n+          if (1 == postings.termFreqs[termID]) {\n+            writeVInt(0, postings.lastDocCodes[termID]|1);\n+          } else {\n+            writeVInt(0, postings.lastDocCodes[termID]);\n+            writeVInt(0, postings.termFreqs[termID]);\n+          }\n+          postings.termFreqs[termID] = 1;\n+          postings.lastDocCodes[termID] = (docID - postings.lastDocIDs[termID]) << 1;\n+          postings.lastDocIDs[termID] = docID;\n+        } else {\n+          postings.termFreqs[termID] = Math.addExact(postings.termFreqs[termID], 1);\n+        }\n+      }\n+\n+      @Override\n+      void newPostingsArray() {\n+        freqProxPostingsArray = (FreqProxTermsWriterPerField.FreqProxPostingsArray) postingsArray;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "originalPosition": 81}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1477, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}