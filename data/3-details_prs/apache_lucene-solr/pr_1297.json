{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgwOTk1Njgz", "number": 1297, "title": "SOLR-14253 Replace various sleep calls with ZK waits", "bodyText": "Description\nThere are lots of places in the code where we poll and sleep rather than using proper ZK callbacks.\nSolution\nReplace poll loops with waitForState", "createdAt": "2020-02-27T18:08:02Z", "url": "https://github.com/apache/lucene-solr/pull/1297", "merged": true, "mergeCommit": {"oid": "99748384cfb16cdef2c5a116243cddc23cedf11c"}, "closed": true, "closedAt": "2021-02-01T19:25:18Z", "author": {"login": "madrob"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcIkbu4gFqTM2NjA3NzQ3OA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABd0_9EOgBqjQyNjgwNjI4Nzk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2MDc3NDc4", "url": "https://github.com/apache/lucene-solr/pull/1297#pullrequestreview-366077478", "createdAt": "2020-02-27T23:44:23Z", "commit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QyMzo0NDoyM1rOFvlDvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQwMDowNDoxMFrOFvlbpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDU1Ng==", "bodyText": "Do we need any synchronization, since this will now be running on a different thread?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385434556", "createdAt": "2020-02-27T23:44:23Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDY1MA==", "bodyText": "We still want to reset interruption, right?\nAlso, am I reading right? before we'd just continue running normally even after a \"timeout\", while now we throw an exception. Sounds like a good change, but that's intended, right?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385434650", "createdAt": "2020-02-27T23:44:42Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNTk0NA==", "bodyText": "Do we need any synchronization?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385435944", "createdAt": "2020-02-27T23:48:42Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n+    getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n   }\n \n-  private void waitForShardId(CoreDescriptor cd) {\n+  private void waitForShardId(final CoreDescriptor cd) {\n     log.debug(\"waiting to find shard id in clusterstate for \" + cd.getName());\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final String shardId = zkStateReader.getClusterState().getShardId(cd.getCollectionName(), getNodeName(), cd.getName());\n-      if (shardId != null) {\n-        cd.getCloudDescriptor().setShardId(shardId);\n-        return;\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+    try {\n+      zkStateReader.waitForState(cd.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        if (c == null) return false;\n+        final String shardId = c.getShardId(getNodeName(), cd.getName());\n+        if (shardId != null) {\n+          cd.getCloudDescriptor().setShardId(shardId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNjA5NA==", "bodyText": "Same as before, we should probably re set the interruption. Also, did you intentionally not wrap the exception?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385436094", "createdAt": "2020-02-27T23:49:12Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n+    getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n   }\n \n-  private void waitForShardId(CoreDescriptor cd) {\n+  private void waitForShardId(final CoreDescriptor cd) {\n     log.debug(\"waiting to find shard id in clusterstate for \" + cd.getName());\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final String shardId = zkStateReader.getClusterState().getShardId(cd.getCollectionName(), getNodeName(), cd.getName());\n-      if (shardId != null) {\n-        cd.getCloudDescriptor().setShardId(shardId);\n-        return;\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+    try {\n+      zkStateReader.waitForState(cd.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        if (c == null) return false;\n+        final String shardId = c.getShardId(getNodeName(), cd.getName());\n+        if (shardId != null) {\n+          cd.getCloudDescriptor().setShardId(shardId);\n+          return true;\n+        }\n+        return false;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not get shard id for core: \" + cd.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNjU1OA==", "bodyText": "same comment/questions as with the other methods", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385436558", "createdAt": "2020-02-27T23:50:49Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -471,29 +471,21 @@ void checkResults(String label, NamedList<Object> results, boolean failureIsFata\n   private void migrateStateFormat(ClusterState state, ZkNodeProps message, NamedList results) throws Exception {\n     final String collectionName = message.getStr(COLLECTION_PROP);\n \n-    boolean firstLoop = true;\n-    // wait for a while until the state format changes\n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    while (! timeout.hasTimedOut()) {\n-      DocCollection collection = zkStateReader.getClusterState().getCollection(collectionName);\n-      if (collection == null) {\n-        throw new SolrException(ErrorCode.BAD_REQUEST, \"Collection: \" + collectionName + \" not found\");\n-      }\n-      if (collection.getStateFormat() == 2) {\n-        // Done.\n-        results.add(\"success\", new SimpleOrderedMap<>());\n-        return;\n-      }\n+    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, MIGRATESTATEFORMAT.toLower(), COLLECTION_PROP, collectionName);\n+    overseer.offerStateUpdate(Utils.toJSON(m));\n \n-      if (firstLoop) {\n-        // Actually queue the migration command.\n-        firstLoop = false;\n-        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, MIGRATESTATEFORMAT.toLower(), COLLECTION_PROP, collectionName);\n-        overseer.offerStateUpdate(Utils.toJSON(m));\n-      }\n-      timeout.sleep(100);\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNjg1Ng==", "bodyText": "Thread.currentThread().interrupt()?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385436856", "createdAt": "2020-02-27T23:51:39Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -526,57 +518,31 @@ static UpdateResponse softCommit(String url) throws SolrServerException, IOExcep\n   }\n \n   String waitForCoreNodeName(String collectionName, String msgNodeName, String msgCore) {\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collectionName);\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        Map<String,Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            if (nodeName.equals(msgNodeName) && core.equals(msgCore)) {\n-              return replica.getName();\n-            }\n-          }\n+    AtomicReference<String> coreNodeName = new AtomicReference<>();\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, msgNodeName, msgCore);\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        coreNodeName.set(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNzE5NA==", "bodyText": "interruption?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385437194", "createdAt": "2020-02-27T23:52:42Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -526,57 +518,31 @@ static UpdateResponse softCommit(String url) throws SolrServerException, IOExcep\n   }\n \n   String waitForCoreNodeName(String collectionName, String msgNodeName, String msgCore) {\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collectionName);\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        Map<String,Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            if (nodeName.equals(msgNodeName) && core.equals(msgCore)) {\n-              return replica.getName();\n-            }\n-          }\n+    AtomicReference<String> coreNodeName = new AtomicReference<>();\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, msgNodeName, msgCore);\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        coreNodeName.set(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n-    throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not find coreNodeName\");\n+    return coreNodeName.get();\n   }\n \n-  ClusterState waitForNewShard(String collectionName, String sliceName) throws KeeperException, InterruptedException {\n+  ClusterState waitForNewShard(String collectionName, String sliceName) {\n     log.debug(\"Waiting for slice {} of collection {} to be available\", sliceName, collectionName);\n-    RTimer timer = new RTimer();\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      ClusterState clusterState = zkStateReader.getClusterState();\n-      DocCollection collection = clusterState.getCollection(collectionName);\n-\n-      if (collection == null) {\n-        throw new SolrException(ErrorCode.SERVER_ERROR,\n-            \"Unable to find collection: \" + collectionName + \" in clusterstate\");\n-      }\n-      Slice slice = collection.getSlice(sliceName);\n-      if (slice != null) {\n-        log.debug(\"Waited for {}ms for slice {} of collection {} to be available\",\n-            timer.getTime(), sliceName, collectionName);\n-        return clusterState;\n-      }\n-      Thread.sleep(1000);\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> c != null && c.getSlice(sliceName) != null);\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for new slice\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzODIxMA==", "bodyText": "Isn't that the case with most of this methods? While the predicate is being executed for example, there is no watch in ZooKeeper AFAICT, unless we go back and write in ZooKeeper and use the version.", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385438210", "createdAt": "2020-02-27T23:55:58Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -526,57 +518,31 @@ static UpdateResponse softCommit(String url) throws SolrServerException, IOExcep\n   }\n \n   String waitForCoreNodeName(String collectionName, String msgNodeName, String msgCore) {\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collectionName);\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        Map<String,Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            if (nodeName.equals(msgNodeName) && core.equals(msgCore)) {\n-              return replica.getName();\n-            }\n-          }\n+    AtomicReference<String> coreNodeName = new AtomicReference<>();\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, msgNodeName, msgCore);\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        coreNodeName.set(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n-    throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not find coreNodeName\");\n+    return coreNodeName.get();\n   }\n \n-  ClusterState waitForNewShard(String collectionName, String sliceName) throws KeeperException, InterruptedException {\n+  ClusterState waitForNewShard(String collectionName, String sliceName) {\n     log.debug(\"Waiting for slice {} of collection {} to be available\", sliceName, collectionName);\n-    RTimer timer = new RTimer();\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      ClusterState clusterState = zkStateReader.getClusterState();\n-      DocCollection collection = clusterState.getCollection(collectionName);\n-\n-      if (collection == null) {\n-        throw new SolrException(ErrorCode.SERVER_ERROR,\n-            \"Unable to find collection: \" + collectionName + \" in clusterstate\");\n-      }\n-      Slice slice = collection.getSlice(sliceName);\n-      if (slice != null) {\n-        log.debug(\"Waited for {}ms for slice {} of collection {} to be available\",\n-            timer.getTime(), sliceName, collectionName);\n-        return clusterState;\n-      }\n-      Thread.sleep(1000);\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> c != null && c.getSlice(sliceName) != null);\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for new slice\", e);\n     }\n-    throw new SolrException(ErrorCode.SERVER_ERROR,\n-        \"Could not find new slice \" + sliceName + \" in collection \" + collectionName\n-            + \" even after waiting for \" + timer.getTime() + \"ms\"\n-    );\n+    // nocommit is there a race condition here since we're not returning the same clusterstate we inspected?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzODcxMw==", "bodyText": "Can this happen?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385438713", "createdAt": "2020-02-27T23:57:34Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -630,34 +596,32 @@ private void modifyCollection(ClusterState clusterState, ZkNodeProps message, Na\n \n     overseer.offerStateUpdate(Utils.toJSON(message));\n \n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    boolean areChangesVisible = true;\n-    while (!timeout.hasTimedOut()) {\n-      DocCollection collection = cloudManager.getClusterStateProvider().getClusterState().getCollection(collectionName);\n-      areChangesVisible = true;\n-      for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n-        String updateKey = updateEntry.getKey();\n-\n-        if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n-            && !updateKey.equals(Overseer.QUEUE_OPERATION)\n-            && updateEntry.getValue() != null // handled below in a separate conditional\n-            && !updateEntry.getValue().equals(collection.get(updateKey))) {\n-          areChangesVisible = false;\n-          break;\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {\n+        if (c == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzOTA4NQ==", "bodyText": "Can we use parametrized logging here?", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385439085", "createdAt": "2020-02-27T23:58:43Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -630,34 +596,32 @@ private void modifyCollection(ClusterState clusterState, ZkNodeProps message, Na\n \n     overseer.offerStateUpdate(Utils.toJSON(message));\n \n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    boolean areChangesVisible = true;\n-    while (!timeout.hasTimedOut()) {\n-      DocCollection collection = cloudManager.getClusterStateProvider().getClusterState().getCollection(collectionName);\n-      areChangesVisible = true;\n-      for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n-        String updateKey = updateEntry.getKey();\n-\n-        if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n-            && !updateKey.equals(Overseer.QUEUE_OPERATION)\n-            && updateEntry.getValue() != null // handled below in a separate conditional\n-            && !updateEntry.getValue().equals(collection.get(updateKey))) {\n-          areChangesVisible = false;\n-          break;\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {\n+        if (c == null) {\n+          return false;\n         }\n+        for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n+          String updateKey = updateEntry.getKey();\n+\n+          if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n+                  && !updateKey.equals(Overseer.QUEUE_OPERATION)\n+                  && updateEntry.getValue() != null // handled below in a separate conditional\n+                  && !updateEntry.getValue().equals(c.get(updateKey))) {\n+            return false;\n+          }\n \n-        if (updateEntry.getValue() == null && collection.containsKey(updateKey)) {\n-          areChangesVisible = false;\n-          break;\n+          if (updateEntry.getValue() == null && c.containsKey(updateKey)) {\n+            return false;\n+          }\n         }\n-      }\n-      if (areChangesVisible) break;\n-      timeout.sleep(100);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      log.debug(\"modifyCollection(ClusterState=\" + clusterState + \", ZkNodeProps=\" + message + \", NamedList=\" + results + \")\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzOTMzNg==", "bodyText": "reset interruption", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385439336", "createdAt": "2020-02-27T23:59:32Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -630,34 +596,32 @@ private void modifyCollection(ClusterState clusterState, ZkNodeProps message, Na\n \n     overseer.offerStateUpdate(Utils.toJSON(message));\n \n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    boolean areChangesVisible = true;\n-    while (!timeout.hasTimedOut()) {\n-      DocCollection collection = cloudManager.getClusterStateProvider().getClusterState().getCollection(collectionName);\n-      areChangesVisible = true;\n-      for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n-        String updateKey = updateEntry.getKey();\n-\n-        if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n-            && !updateKey.equals(Overseer.QUEUE_OPERATION)\n-            && updateEntry.getValue() != null // handled below in a separate conditional\n-            && !updateEntry.getValue().equals(collection.get(updateKey))) {\n-          areChangesVisible = false;\n-          break;\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {\n+        if (c == null) {\n+          return false;\n         }\n+        for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n+          String updateKey = updateEntry.getKey();\n+\n+          if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n+                  && !updateKey.equals(Overseer.QUEUE_OPERATION)\n+                  && updateEntry.getValue() != null // handled below in a separate conditional\n+                  && !updateEntry.getValue().equals(c.get(updateKey))) {\n+            return false;\n+          }\n \n-        if (updateEntry.getValue() == null && collection.containsKey(updateKey)) {\n-          areChangesVisible = false;\n-          break;\n+          if (updateEntry.getValue() == null && c.containsKey(updateKey)) {\n+            return false;\n+          }\n         }\n-      }\n-      if (areChangesVisible) break;\n-      timeout.sleep(100);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      log.debug(\"modifyCollection(ClusterState=\" + clusterState + \", ZkNodeProps=\" + message + \", NamedList=\" + results + \")\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ0MDY3OA==", "bodyText": "Can't we iterate shards/replicas, and for each one check if coreNames .contains(replica.getStr(ZkStateReader.CORE_NAME_PROP)). Maybe make a set with all the elements in coreNames and remove them as you find them, and break if empty? I guess it depends on how big coreNames will be", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385440678", "createdAt": "2020-02-28T00:04:10Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -672,35 +636,35 @@ void cleanupCollection(String collectionName, NamedList results) throws Exceptio\n     commandMap.get(DELETE).call(zkStateReader.getClusterState(), new ZkNodeProps(props), results);\n   }\n \n-  Map<String, Replica> waitToSeeReplicasInState(String collectionName, Collection<String> coreNames) throws InterruptedException {\n-    assert coreNames.size() > 0;\n-    Map<String, Replica> result = new HashMap<>();\n-    TimeOut timeout = new TimeOut(Integer.getInteger(\"solr.waitToSeeReplicasInStateTimeoutSeconds\", 120), TimeUnit.SECONDS, timeSource); // could be a big cluster\n-    while (true) {\n-      DocCollection coll = zkStateReader.getClusterState().getCollection(collectionName);\n-      for (String coreName : coreNames) {\n-        if (result.containsKey(coreName)) continue;\n-        for (Slice slice : coll.getSlices()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            if (coreName.equals(replica.getStr(ZkStateReader.CORE_NAME_PROP))) {\n-              result.put(coreName, replica);\n-              break;\n+  Map<String, Replica> waitToSeeReplicasInState(String collectionName, Collection<String> coreNames) {\n+    final Map<String, Replica> result = new HashMap<>();\n+    int timeout = Integer.getInteger(\"solr.waitToSeeReplicasInStateTimeoutSeconds\", 120); // could be a big cluster\n+    try {\n+      zkStateReader.waitForState(collectionName, timeout, TimeUnit.SECONDS, c -> {\n+        // todo this is ugly, but I'm not sure there is a better way to fix it?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 239}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY2NTU5MDQ3", "url": "https://github.com/apache/lucene-solr/pull/1297#pullrequestreview-366559047", "createdAt": "2020-02-28T17:20:25Z", "commit": {"oid": "df7af8e457bb5c6dbbd3b43593bc00b8382e9e2c"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxNzoyMDoyNVrOFv8vEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxNzoyMzowOFrOFv80HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgyMjQ4Mw==", "bodyText": "OK. I was thinking more on data visibility than race conditions", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385822483", "createdAt": "2020-02-28T17:20:25Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDU1Ng=="}, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgyMzc3Mw==", "bodyText": "+1 for the changes. Just wanted to make sure they were intentional. Regarding the InterruptedException handling, This LGTM, but if you want to keep the multicatch you could use SolrZkClient.checkInterrupted(Throwable e)", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385823773", "createdAt": "2020-02-28T17:23:08Z", "author": {"login": "tflobbe"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDY1MA=="}, "originalCommit": {"oid": "531383c9010a739093639a0c825dc59d618517b7"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwNjEyMjE1", "url": "https://github.com/apache/lucene-solr/pull/1297#pullrequestreview-370612215", "createdAt": "2020-03-06T20:39:56Z", "commit": {"oid": "5efd9737f4943aacd0524fbf590951672ce0657f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozOTo1NlrOFzGxlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozOTo1NlrOFzGxlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjY5Mw==", "bodyText": "If this change is being made, should the number of retries be configurable?  This hardcoded value seems to be used a lot in the code.", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r389132693", "createdAt": "2020-03-06T20:39:56Z", "author": {"login": "beettlle"}, "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,39 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5efd9737f4943aacd0524fbf590951672ce0657f"}, "originalPosition": 47}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5efd9737f4943aacd0524fbf590951672ce0657f", "author": {"user": null}, "url": "https://github.com/apache/lucene-solr/commit/5efd9737f4943aacd0524fbf590951672ce0657f", "committedDate": "2020-03-03T00:00:38Z", "message": "review feedback & precommit"}, "afterCommit": {"oid": "b2a2b16d04e3ead029b4948496c52fa3fc48606a", "author": {"user": {"login": "madrob", "name": "Mike Drob"}}, "url": "https://github.com/apache/lucene-solr/commit/b2a2b16d04e3ead029b4948496c52fa3fc48606a", "committedDate": "2020-03-11T15:53:58Z", "message": "Localized some exception logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "author": {"user": {"login": "madrob", "name": "Mike Drob"}}, "url": "https://github.com/apache/lucene-solr/commit/21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "committedDate": "2021-01-29T21:11:22Z", "message": "SOLR-14253 Replace sleep calls with ZK waits\n\nCo-Authored-By: markrmiller <markrmiller@apache.org>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b2a2b16d04e3ead029b4948496c52fa3fc48606a", "author": {"user": {"login": "madrob", "name": "Mike Drob"}}, "url": "https://github.com/apache/lucene-solr/commit/b2a2b16d04e3ead029b4948496c52fa3fc48606a", "committedDate": "2020-03-11T15:53:58Z", "message": "Localized some exception logic"}, "afterCommit": {"oid": "21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "author": {"user": {"login": "madrob", "name": "Mike Drob"}}, "url": "https://github.com/apache/lucene-solr/commit/21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "committedDate": "2021-01-29T21:11:22Z", "message": "SOLR-14253 Replace sleep calls with ZK waits\n\nCo-Authored-By: markrmiller <markrmiller@apache.org>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2160, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}