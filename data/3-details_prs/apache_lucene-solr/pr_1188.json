{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY1MDEzMzMz", "number": 1188, "title": "SOLR-14044: Support collection and shard deletion in shared storage", "bodyText": "This PR addS support for shard and collection deletion in shared storage (SOLR-14044) and also includes a major refactor of the existing BlobDeleteManager and deletion code.\nThe BlobDeleteManager uses refactors the existing async processing machinery and BlobDeleteManager manages two deletion pools now - the existing one for handling normal indexing flow deletion (as we push) and a pool used specifically by the Overseer for handling collection and shard deletion.\nCurrently working on a another test that I should add soon but the rest is review-able.", "createdAt": "2020-01-20T21:46:48Z", "url": "https://github.com/apache/lucene-solr/pull/1188", "merged": true, "mergeCommit": {"oid": "3e8ca67f6f5d87b103148824e2bf158d5d5330da"}, "closed": true, "closedAt": "2020-02-06T02:20:57Z", "author": {"login": "andyvuong"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb8ToTOAH2gAyMzY1MDEzMzMzOmE3ZTlkNjhjNWQ0MDg2NTA0NzcwMTBkYzI5NGVlYmIxNjI4MjhlNGQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcBeXDGAFqTM1NDEwMjM1MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a7e9d68c5d408650477010dc294eebb162828e4d", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/a7e9d68c5d408650477010dc294eebb162828e4d", "committedDate": "2020-01-20T21:42:36Z", "message": "Support collection and shard deletion in shared storage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcfc41a4e84758a3609f218356efc634844526c0", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/fcfc41a4e84758a3609f218356efc634844526c0", "committedDate": "2020-01-22T07:12:19Z", "message": "Add end to end collection api delete tests and fix local client test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "523dd2c3f91ddea7988e9af6980031f5aafe36a1", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/523dd2c3f91ddea7988e9af6980031f5aafe36a1", "committedDate": "2020-01-27T19:08:17Z", "message": "Merge branch 'jira/SOLR-13101' into jira/SOLR-13101-data-delete"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8404a5be395dadd93d2500dd14468cdafdb96d84", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/8404a5be395dadd93d2500dd14468cdafdb96d84", "committedDate": "2020-01-28T01:17:37Z", "message": "Enable if configured only"}, "afterCommit": {"oid": "523dd2c3f91ddea7988e9af6980031f5aafe36a1", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/523dd2c3f91ddea7988e9af6980031f5aafe36a1", "committedDate": "2020-01-27T19:08:17Z", "message": "Merge branch 'jira/SOLR-13101' into jira/SOLR-13101-data-delete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "45e58815651e6fe955ff6fa3fb4f9b6b9dcf09bf", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/45e58815651e6fe955ff6fa3fb4f9b6b9dcf09bf", "committedDate": "2020-02-01T00:29:36Z", "message": "Fix timestamps"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/b9a43e67ea7a610f6b78622fcf69afd6e2d974d5", "committedDate": "2020-02-03T19:33:39Z", "message": "Remove debug log line and fix timestamps"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzMTk5MDQy", "url": "https://github.com/apache/lucene-solr/pull/1188#pullrequestreview-353199042", "createdAt": "2020-02-04T18:23:14Z", "commit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQxODoyMzoxNFrOFlekWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo1ODozNFrOFlk7Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0MjQ1OA==", "bodyText": "I only see new imports. Is there any functional change in this file?", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374842458", "createdAt": "2020-02-04T18:23:14Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/cloud/Overseer.java", "diffHunk": "@@ -70,6 +70,9 @@\n import org.apache.solr.handler.admin.CollectionsHandler;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NDUzNw==", "bodyText": "Shouldn't this be warning instead of exception? Same as line#167. I believe intention is to not block deletion and leave orphan files behind, correct?", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374844537", "createdAt": "2020-02-04T18:27:08Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/DeleteCollectionCmd.java", "diffHunk": "@@ -142,6 +148,34 @@ public void call(ClusterState state, ZkNodeProps message, NamedList results) thr\n           break;\n         }\n       }\n+      \n+      // Delete the collection files from shared store. We want to delete all of the files before we delete\n+      // the collection state from ZooKeeper.\n+      DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collection);\n+      if (docCollection != null && docCollection.getSharedIndex()) {\n+        SharedStoreManager sharedStoreManager = ocmh.overseer.getCoreContainer().getSharedStoreManager();\n+        BlobDeleteManager deleteManager = sharedStoreManager.getBlobDeleteManager();\n+        BlobDeleteProcessor deleteProcessor = deleteManager.getOverseerDeleteProcessor();\n+        // deletes all files belonging to this collection\n+        CompletableFuture<BlobDeleterTaskResult> deleteFuture = \n+            deleteProcessor.deleteCollection(collection, false);\n+        \n+        try {\n+          // TODO: Find a reasonable timeout value\n+          BlobDeleterTaskResult result = deleteFuture.get(60, TimeUnit.SECONDS);\n+          if (!result.isSuccess()) {\n+            log.warn(\"Deleting all files belonging to shared collection \" + collection + \n+                \" was not successful! Files belonging to this collection may be orphaned.\");\n+          }\n+        } catch (TimeoutException tex) {\n+          // We can orphan files here if we don't delete everything in time but what matters for potentially\n+          // reusing the collection name is that the zookeeper state of the collection gets deleted which \n+          // will happen in the finally block\n+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not complete deleting collection\" + ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NTE1OQ==", "bodyText": "Same comment for DeleteShardCmd.java.", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374845159", "createdAt": "2020-02-04T18:28:18Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/DeleteCollectionCmd.java", "diffHunk": "@@ -142,6 +148,34 @@ public void call(ClusterState state, ZkNodeProps message, NamedList results) thr\n           break;\n         }\n       }\n+      \n+      // Delete the collection files from shared store. We want to delete all of the files before we delete\n+      // the collection state from ZooKeeper.\n+      DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collection);\n+      if (docCollection != null && docCollection.getSharedIndex()) {\n+        SharedStoreManager sharedStoreManager = ocmh.overseer.getCoreContainer().getSharedStoreManager();\n+        BlobDeleteManager deleteManager = sharedStoreManager.getBlobDeleteManager();\n+        BlobDeleteProcessor deleteProcessor = deleteManager.getOverseerDeleteProcessor();\n+        // deletes all files belonging to this collection\n+        CompletableFuture<BlobDeleterTaskResult> deleteFuture = \n+            deleteProcessor.deleteCollection(collection, false);\n+        \n+        try {\n+          // TODO: Find a reasonable timeout value\n+          BlobDeleterTaskResult result = deleteFuture.get(60, TimeUnit.SECONDS);\n+          if (!result.isSuccess()) {\n+            log.warn(\"Deleting all files belonging to shared collection \" + collection + \n+                \" was not successful! Files belonging to this collection may be orphaned.\");\n+          }\n+        } catch (TimeoutException tex) {\n+          // We can orphan files here if we don't delete everything in time but what matters for potentially\n+          // reusing the collection name is that the zookeeper state of the collection gets deleted which \n+          // will happen in the finally block\n+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Could not complete deleting collection\" + ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0NDUzNw=="}, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg5NTM3NQ==", "bodyText": "Can we put this conversion in some utility and put a reason why we use nanoTime and not currentTimeMillis?", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374895375", "createdAt": "2020-02-04T20:07:58Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/store/blob/metadata/CorePushPull.java", "diffHunk": "@@ -137,7 +139,7 @@ public BlobCoreMetadata pushToBlobStore(String currentMetadataSuffix, String new\n          */\n         for (BlobCoreMetadata.BlobFile d : resolvedMetadataResult.getFilesToDelete()) {\n             bcmBuilder.removeFile(d);\n-            BlobCoreMetadata.BlobFileToDelete bftd = new BlobCoreMetadata.BlobFileToDelete(d, System.currentTimeMillis());\n+            BlobCoreMetadata.BlobFileToDelete bftd = new BlobCoreMetadata.BlobFileToDelete(d, System.nanoTime() / 1000000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkwODY0OQ==", "bodyText": "I don't see CorePushTest anymore. If that is the case, then we should delete this note. And if some test needs a non-default value, it should override.", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374908649", "createdAt": "2020-02-04T20:36:30Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleteManager.java", "diffHunk": "@@ -18,40 +18,77 @@\n package org.apache.solr.store.blob.process;\n \n import java.lang.invoke.MethodHandles;\n-import java.util.Set;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.LinkedBlockingDeque;\n-import java.util.concurrent.RejectedExecutionException;\n-import java.util.concurrent.ThreadFactory;\n-import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n-import org.apache.lucene.util.NamedThreadFactory;\n-import org.apache.solr.common.util.ExecutorUtil.MDCAwareThreadPoolExecutor;\n+import org.apache.solr.core.SolrCore;\n import org.apache.solr.store.blob.client.CoreStorageClient;\n import org.apache.solr.store.blob.metadata.CorePushPull;\n+import org.apache.solr.store.blob.metadata.ServerSideMetadata;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.annotations.VisibleForTesting;\n+\n /**\n- * Manager of blobs (files) to delete, putting them in a queue (if space left on the queue) then consumed and processed\n- * by {@link BlobDeleterTask}\n+ * This class manages the deletion machinery required by shared storage enabled collections. Its responsibilities\n+ * include the allocation and management of bounded deletion task queues and their consumers. \n+ * \n+ * Deletion of blob files from shared store happen on two paths:\n+ *  1. In the indexing path, the local {@link SolrCore}'s index files represented by an instance of a\n+ *  {@link ServerSideMetadata} object is resolved against the blob store's core.metadata file, or the\n+ *  the source of truth for what index files a {@link SolrCore} should have. As the difference between \n+ *  these two metadata instances are resolved, we add files to be deleted to the BlobDeleteManager which\n+ *  enqueues a {@link BlobDeleterTask} for asynchronous processing.\n+ *  2. In the collection admin API, we may delete a collection or collection shard. In the former, all index\n+ *  files belonging to the specified collection on shared storage should be deleted while in the latter \n+ *  all index files belonging to a particular collection/shard pair should be deleted.   \n+ * \n+ * Shard leaders are the only replicas receiving indexing traffic and pushing to shared store in a shared collection\n+ * so all Solr nodes in a cluster may be sending deletion requests to the shared storage provider at a given moment.\n+ * Collection commands are only processed by the Overseer and therefore only the Overseer should be deleting entire\n+ * collections or shard files from shared storage.\n+ * \n+ * The BlobDeleteManager maintains two queues to prevent any potential starvation, one for the incremental indexing \n+ * deletion path that is always initiated when a Solr node with shared collections starts up and one that is only\n+ * used when the current node is Overseer and handles Overseer specific actions.\n  */\n public class BlobDeleteManager {\n \n   private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n   \n+  /**\n+   * Identifier for a BlobDeleteProcessor that runs on all Solr nodes containing shared collections\n+   */\n+  public static final String BLOB_FILE_DELETER = \"BlobFileDeleter\";\n+  \n+  /**\n+   * Identifier for a BlobDeleteProcessor that runs on the Overseer if the Solr cluster contains\n+   * any shared collection\n+   */\n+  public static final String OVERSEER_BLOB_FILE_DELETER = \"OverseerBlobFileDeleter\";\n+  \n   /**\n    * Limit to the number of blob files to delete accepted on the delete queue (and lost in case of server crash). When\n    * the queue reaches that size, no more deletes are accepted (will be retried later for a core, next time it is pushed).\n    * (note that tests in searchserver.blobstore.metadata.CorePushTest trigger a merge that enqueues more than 100 files to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkwOTU0NA==", "bodyText": "minor: deleteDelayMs", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374909544", "createdAt": "2020-02-04T20:38:29Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleteManager.java", "diffHunk": "@@ -71,92 +108,81 @@\n    * delete until we know for sure the file can be resuscitated...\n    */\n   private final long deleteDelayMs;\n+  \n+  private AtomicBoolean isShutdown; \n \n   /**\n-   * TODO : Creates a default delete client, should have config based one  \n+   * Creates a new BlobDeleteManager with the provided {@link CoreStorageClient} and instantiates\n+   * it with a default deletedelayMs, queue size, and thread pool size. A default {@link BlobDeleteProcessor}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkyNjc2Mw==", "bodyText": "default prefix in the name does not make sense. I guess it can just be maxDeleteAttempts.", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374926763", "createdAt": "2020-02-04T21:15:23Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleteProcessor.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.solr.store.blob.process;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.Set;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.lucene.util.NamedThreadFactory;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.common.util.ExecutorUtil.MDCAwareThreadPoolExecutor;\n+import org.apache.solr.store.blob.client.BlobClientUtils;\n+import org.apache.solr.store.blob.client.CoreStorageClient;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobDeleterTaskResult;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobFileDeletionTask;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobPrefixedFileDeletionTask;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+\n+/**\n+ * A generic deletion processor used for deleting object files from shared\n+ * storage. Each processor manages its own task bounded thread pool for processing\n+ * {@link BlobDeleterTask} asynchronously. Processors support retrying tasks if \n+ * necessary but retry decisions are left to the individual task implementations.  \n+ * \n+ * Instances of {@link BlobDeleteProcessor} are managed by the {@link BlobDeleteManager}.\n+ */\n+public class BlobDeleteProcessor {\n+  \n+  private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n+  \n+  private final String name;\n+  private final int almostMaxQueueSize;\n+  /**\n+   * Note we sleep() after each failed attempt, so multiply this value by {@link #fixedRetryDelay} to find\n+   * out how long we'll retry (at least) if Blob access fails for some reason (\"at least\" because we\n+   * re-enqueue at the tail of the queue ({@link BlobDeleteManager} creates a list), so there might be additional\n+   * processing delay if the queue is not empty and is processed before the enqueued retry is processed).\n+   */\n+  private final int defaultMaxDeleteAttempts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkyOTc2Mg==", "bodyText": "typo: gaurantee", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374929762", "createdAt": "2020-02-04T21:21:41Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/java/org/apache/solr/store/blob/process/BlobDeleterTask.java", "diffHunk": "@@ -18,94 +18,244 @@\n package org.apache.solr.store.blob.process;\n \n import java.lang.invoke.MethodHandles;\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Locale;\n import java.util.Set;\n-import java.util.concurrent.ThreadPoolExecutor;\n-import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.solr.store.blob.client.CoreStorageClient;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobDeleterTaskResult;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * Task in charge of deleting Blobs (files) from blob store.\n+ * Generic deletion task for files located on shared storage\n  */\n-class BlobDeleterTask implements Runnable {\n+public abstract class BlobDeleterTask implements Callable<BlobDeleterTaskResult> {\n \n   private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n-\n-  /**\n-   * Note we sleep() after each failed attempt, so multiply this value by {@link #SLEEP_MS_FAILED_ATTEMPT} to find\n-   * out how long we'll retry (at least) if Blob access fails for some reason (\"at least\" because we\n-   * re-enqueue at the tail of the queue ({@link BlobDeleteManager} creates a list), so there might be additional\n-   * processing delay if the queue is not empty and is processed before the enqueued retry is processed).\n-   */\n-  private static int MAX_DELETE_ATTEMPTS = 50;\n-  private static long SLEEP_MS_FAILED_ATTEMPT = TimeUnit.SECONDS.toMillis(10);\n-\n+  \n   private final CoreStorageClient client;\n-  private final String sharedBlobName;\n-  private final Set<String> blobNames;\n+  private final String collectionName;\n   private final AtomicInteger attempt;\n-  private final ThreadPoolExecutor executor;\n+  \n   private final long queuedTimeMs;\n+  private final int maxAttempts;\n+  private final boolean allowRetry;\n+  private Throwable err;\n \n-  BlobDeleterTask(CoreStorageClient client, String sharedBlobName, Set<String> blobNames, ThreadPoolExecutor executor) {\n-    this.client = client; \n-    this.sharedBlobName = sharedBlobName;\n-    this.blobNames = blobNames;\n+  public BlobDeleterTask(CoreStorageClient client, String collectionName, boolean allowRetry,\n+      int maxAttempts) {\n+    this.client = client;\n+    this.collectionName = collectionName;\n     this.attempt = new AtomicInteger(0);\n-    this.executor = executor;\n-    this.queuedTimeMs = System.nanoTime();\n+    this.queuedTimeMs = System.nanoTime() / 1000000;\n+    this.allowRetry = allowRetry;\n+    this.maxAttempts = maxAttempts;\n   }\n-\n+  \n+  /**\n+   * Performs a deletion action and request against the shared storage for the given collection\n+   * and returns the list of file paths deleted\n+   */\n+  public abstract Collection<String> doDelete() throws Exception;\n+  \n+  /**\n+   * Return a String representing the action performed by the BlobDeleterTask for logging purposes\n+   */\n+  public abstract String getActionName();\n+  \n   @Override\n-  public void run() {\n-    final long startTimeMs = System.nanoTime();\n+  public BlobDeleterTaskResult call() {\n+    List<String> filesDeleted = new LinkedList<>();\n+    final long startTimeMs = System.nanoTime() / 1000000;\n     boolean isSuccess = true;\n-      \n+    boolean shouldRetry = false;\n     try {\n+      filesDeleted.addAll(doDelete());\n+      attempt.incrementAndGet();\n+      return new BlobDeleterTaskResult(this, filesDeleted, isSuccess, shouldRetry, err);\n+    } catch (Exception ex) {\n+      if (err == null) {\n+        err = ex;\n+      } else {\n+        err.addSuppressed(ex);\n+      }\n+      int attempts = attempt.incrementAndGet();\n+      isSuccess = false;\n+      log.warn(\"BlobDeleterTask failed on attempt=\" + attempts  + \" collection=\" + collectionName\n+          + \" task=\" + toString(), ex);\n+      if (allowRetry) {\n+        if (attempts < maxAttempts) {\n+          shouldRetry = true;\n+        } else {\n+          log.warn(\"Reached \" + maxAttempts + \" attempt limit for deletion task \" + toString() + \n+              \". This task won't be retried.\");\n+        }\n+      }\n+    } finally {\n+      long now = System.nanoTime() / 1000000;\n+      long runTime = now - startTimeMs;\n+      long startLatency = now - this.queuedTimeMs;\n+      log(getActionName(), collectionName, runTime, startLatency, isSuccess, getAdditionalLogMessage());\n+    }\n+    return new BlobDeleterTaskResult(this, filesDeleted, isSuccess, shouldRetry, err);\n+  }\n+  \n+  /**\n+   * Override-able by deletion tasks to provide additional action specific logging\n+   */\n+  public String getAdditionalLogMessage() {\n+    return \"\";\n+  }\n+  \n+  @Override\n+  public String toString() {\n+    return \"collectionName=\" + collectionName + \" allowRetry=\" + allowRetry + \n+        \" queuedTimeMs=\" + queuedTimeMs + \" attemptsTried=\" + attempt.get();\n+  }\n+  \n+  public int getAttempts() {\n+    return attempt.get();\n+  }\n+\n+  public void log(String action, String collectionName, long runTime, long startLatency, boolean isSuccess, \n+      String additionalMessage) {\n+    String message = String.format(Locale.ROOT, \n+        \"action=%s storageProvider=%s bucketRegion=%s bucketName=%s, runTime=%s \"\n+        + \"startLatency=%s attempt=%s isSuccess=%s %s\",\n+        action, client.getStorageProvider().name(), client.getBucketRegion(), client.getBucketName(),\n+        runTime, startLatency, attempt.get(), isSuccess, additionalMessage);\n+    log.info(message);\n+  }\n+  \n+  /**\n+   * Represents the result of a deletion task\n+   */\n+  public static class BlobDeleterTaskResult {\n+    private final BlobDeleterTask task;\n+    private final Collection<String> filesDeleted;\n+    private final boolean isSuccess;\n+    private final boolean shouldRetry;\n+    private final Throwable err;\n+    \n+    public BlobDeleterTaskResult(BlobDeleterTask task, Collection<String> filesDeleted, \n+        boolean isSuccess, boolean shouldRetry, Throwable errs) {\n+      this.task = task;\n+      this.filesDeleted = filesDeleted;\n+      this.isSuccess = isSuccess;\n+      this.shouldRetry = shouldRetry;\n+      this.err = errs;\n+    }\n+    \n+    public boolean isSuccess() {\n+      return isSuccess;\n+    }\n+    \n+    public boolean shouldRetry() {\n+      return shouldRetry;\n+    }\n+    \n+    public BlobDeleterTask getTask() {\n+      return task;\n+    }\n+    \n+    /**\n+     * @return the files that are being deleted. Note if the task wasn't successful there is no gaurantee", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNDk0NA==", "bodyText": "To improve it little further, maybe add one non-matching prefix entry and make sure we don't get that in the result.", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374934944", "createdAt": "2020-02-04T21:32:59Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/test/org/apache/solr/store/blob/client/CoreStorageClientTest.java", "diffHunk": "@@ -87,6 +88,22 @@ public void testPushStreamReturnsPath() throws Exception {\n     int expectedBlobKeyLength = TEST_CORE_NAME_1.length() + uuid4length + 1 + 4;\n     Assert.assertEquals(blobPath.length(), expectedBlobKeyLength);\n   }\n+  \n+  @Test\n+  public void testListBlobFiles() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0NjU5OA==", "bodyText": "This needs to be non-null otherwise it will through NPE in BlobDeleterTaskResult#call.\nBecause of this, testRetryableTaskSucceeds is failing. It gets enqueued 5 time instead of 4.\nAlso, the failed test is leaking a thread. To fix that processor.shutdown needs to be inside finally block.\ncom.carrotsearch.randomizedtesting.ThreadLeakError: There are still zombie threads that couldn't be terminated:\n\nThread[id=24, name=DeleterForTest-1-thread-1, state=WAITING, group=TGRP-BlobDeleteProcessorTest]\nat sun.misc.Unsafe.park(Native Method)\nat java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\nat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\nat java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)\nat java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)\nat java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)", "url": "https://github.com/apache/lucene-solr/pull/1188#discussion_r374946598", "createdAt": "2020-02-04T21:58:34Z", "author": {"login": "mbwaheed"}, "path": "solr/core/src/test/org/apache/solr/store/blob/process/BlobDeleteProcessorTest.java", "diffHunk": "@@ -0,0 +1,472 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.solr.store.blob.process;\n+\n+import java.nio.file.Path;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.solr.SolrTestCaseJ4;\n+import org.apache.solr.store.blob.client.BlobException;\n+import org.apache.solr.store.blob.client.CoreStorageClient;\n+import org.apache.solr.store.blob.client.LocalStorageClient;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobDeleterTaskResult;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobFileDeletionTask;\n+import org.apache.solr.store.blob.process.BlobDeleterTask.BlobPrefixedFileDeletionTask;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Unit tests for {@link BlobDeleteProcessor}\n+ */\n+public class BlobDeleteProcessorTest extends SolrTestCaseJ4 {\n+  \n+  private static String DEFAULT_PROCESSOR_NAME = \"DeleterForTest\";\n+  private static Path sharedStoreRootPath;\n+  private static CoreStorageClient blobClient;\n+  \n+  private static List<BlobDeleterTask> enqueuedTasks;\n+\n+  @BeforeClass\n+  public static void setupTestClass() throws Exception {\n+    sharedStoreRootPath = createTempDir(\"tempDir\");\n+    System.setProperty(LocalStorageClient.BLOB_STORE_LOCAL_FS_ROOT_DIR_PROPERTY, sharedStoreRootPath.resolve(\"LocalBlobStore/\").toString());\n+    blobClient = new LocalStorageClient() {\n+       \n+      // no ops for BlobFileDeletionTask and BlobPrefixedFileDeletionTask to execute successfully\n+      @Override\n+      public void deleteBlobs(Collection<String> paths) throws BlobException {\n+        return;\n+      }\n+\n+      // no ops for BlobFileDeletionTask and BlobPrefixedFileDeletionTask to execute successfully\n+      @Override\n+      public List<String> listCoreBlobFiles(String prefix) throws BlobException {\n+        return new LinkedList<>();\n+      }\n+    };\n+  }\n+  \n+  @Before\n+  public void setup() {\n+    enqueuedTasks = new LinkedList<BlobDeleterTask>();\n+  }\n+  \n+  /**\n+   * Verify we enqueue a {@link BlobFileDeletionTask} with the correct parameters.\n+   * Note we're not testing the functionality of the deletion task here only that the processor successfully\n+   * handles the task. End to end blob deletion tests can be found {@link SharedStoreDeletionProcessTest} \n+   */\n+  @Test\n+  public void testDeleteFilesEnqueueTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 500; \n+    String name = \"testName\";\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    Set<String> names = new HashSet<>();\n+    names.add(\"test1\");\n+    names.add(\"test2\");\n+    // uses the specified defaultMaxAttempts at the processor (not task) level \n+    CompletableFuture<BlobDeleterTaskResult> cf = processor.deleteFiles(name, names, true);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    assertEquals(1, enqueuedTasks.size());\n+    \n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify we enqueue a {@link BlobPrefixedFileDeletionTask} with the correct parameters.\n+   * Note we're not testing the functionality of the deletion task here only that the processor successfully\n+   * handles the task. End to end blob deletion tests can be found {@link SharedStoreDeletionProcessTest} \n+   */\n+  @Test\n+  public void testDeleteShardEnqueueTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 500; \n+    String name = \"testName\";\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // uses the specified defaultMaxAttempts at the processor (not task) level \n+    CompletableFuture<BlobDeleterTaskResult> cf = processor.deleteCollection(name, true);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    assertEquals(1, enqueuedTasks.size());\n+    \n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify we enqueue a {@link BlobPrefixedFileDeletionTask} with the correct parameters.\n+   * Note we're not testing the functionality of the deletion task here only that the processor successfully\n+   * handles the task. End to end blob deletion tests can be found {@link SharedStoreDeletionProcessTest} \n+   */\n+  @Test\n+  public void testDeleteCollectionEnqueueTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 500; \n+    String name = \"testName\";\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // uses the specified defaultMaxAttempts at the processor (not task) level \n+    CompletableFuture<BlobDeleterTaskResult> cf = processor.deleteShard(name, name, true);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    assertEquals(1, enqueuedTasks.size());\n+    \n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that we don't retry tasks that are not configured to be retried\n+   * and end up failing\n+   */\n+  @Test\n+  public void testNonRetryableTask() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1; // ignored when we build the test task\n+    int retryDelay = 500;\n+    int totalAttempts = 5; // total number of attempts the task should be tried \n+\n+    String name = \"testName\";\n+    boolean isRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    \n+    // enqueue a task that fails and is not retryable\n+    CompletableFuture<BlobDeleterTaskResult> cf = \n+        processor.enqueue(buildFailingTaskForTest(blobClient, name, totalAttempts, false), isRetry);\n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    \n+    // the first fails\n+    assertEquals(1, enqueuedTasks.size());\n+    assertNotNull(res);\n+    assertEquals(1, res.getTask().getAttempts());\n+    assertEquals(false, res.isSuccess());\n+    assertEquals(false, res.shouldRetry());\n+\n+    // initial error + 0 retry errors suppressed\n+    assertNotNull(res.getError());\n+    assertEquals(0, res.getError().getSuppressed().length);\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that the retry logic kicks in for tasks configured to retry\n+   * and subsequent retry succeeds\n+   */\n+  @Test\n+  public void testRetryableTaskSucceeds() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1; // ignored when we build the test task\n+    int retryDelay = 500;\n+    int totalAttempts = 5; // total number of attempts the task should be tried \n+    int totalFails = 3; // total number of times the task should fail\n+    \n+    String name = \"testName\";\n+    boolean isRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // enqueue a task that fails totalFails number of times before succeeding\n+    CompletableFuture<BlobDeleterTaskResult> cf = \n+        processor.enqueue(buildScheduledFailingTaskForTest(blobClient, name, totalAttempts, true, totalFails), isRetry);\n+    \n+    // wait for this task and all its potential retries to finish\n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    \n+    // the first 3 fail and last one succeeds\n+    assertEquals(4, enqueuedTasks.size());\n+    \n+    assertNotNull(res);\n+    assertEquals(4, res.getTask().getAttempts());\n+    assertEquals(true, res.isSuccess());\n+    \n+    // initial error + 2 retry errors suppressed\n+    assertNotNull(res.getError());\n+    assertEquals(2, res.getError().getSuppressed().length);\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that after all task attempts are exhausted we bail out\n+   */\n+  @Test\n+  public void testRetryableTaskFails() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1; // ignored when we build the test task\n+    int retryDelay = 500;\n+    int totalAttempts = 5; // total number of attempts the task should be tried\n+    \n+    String name = \"testName\";\n+    boolean isRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // enqueue a task that fails every time it runs but is configured to retry\n+    CompletableFuture<BlobDeleterTaskResult> cf = \n+        processor.enqueue(buildFailingTaskForTest(blobClient, name, totalAttempts, true), isRetry);\n+    \n+    // wait for this task and all its potential retries to finish \n+    BlobDeleterTaskResult res = cf.get(5000, TimeUnit.MILLISECONDS);\n+    // 1 initial enqueue + 4 retries\n+    assertEquals(5, enqueuedTasks.size());\n+    \n+    assertNotNull(res);\n+    assertEquals(5, res.getTask().getAttempts());\n+    assertEquals(false, res.isSuccess());\n+    // circuit breaker should be false after all attempts are exceeded\n+    assertEquals(false, res.shouldRetry());\n+    \n+    // initial error + 4 retry errors suppressed\n+    assertNotNull(res.getError());\n+    assertEquals(4, res.getError().getSuppressed().length);\n+    \n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that we cannot add more deletion tasks to the processor if the work queue\n+   * is at its target max but that we can re-add tasks that are retries to the queue\n+   */\n+  @Test\n+  public void testWorkQueueFull() throws Exception {\n+    int maxQueueSize = 3;\n+    int numThreads = 1;\n+    int defaultMaxAttempts = 1;\n+    int retryDelay = 1000;\n+    \n+    String name = \"testName\";\n+    boolean allowRetry = false;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    // numThreads is 1 and we'll enqueue a blocking task that ensures our pool\n+    // will be occupied while we add new tasks subsequently to test enqueue rejection\n+    CountDownLatch tasklatch = new CountDownLatch(1);\n+    processor.enqueue(buildBlockingTaskForTest(tasklatch), allowRetry);\n+\n+    // Fill the internal work queue beyond the maxQueueSize, the internal queue size is not \n+    // approximate so we'll just add beyond the max\n+    for (int i = 0; i < maxQueueSize*2; i++) {\n+      try {\n+        processor.deleteCollection(name, allowRetry);\n+      } catch (Exception ex) {\n+        // ignore\n+      }\n+    }\n+    \n+    // verify adding a new task is rejected\n+    try {\n+      processor.deleteCollection(name, allowRetry);\n+      fail(\"Task should have been rejected\");\n+    } catch (Exception ex) {\n+      assertTrue(ex.getMessage().contains(\"Unable to enqueue deletion\"));\n+    }\n+    CompletableFuture<BlobDeleterTaskResult> cf = null;\n+    try {\n+      // verify adding a task that is marked as a retry is not rejected \n+       cf = processor.enqueue(buildFailingTaskForTest(blobClient, name, 5, true), /* isRetry */ true);\n+    } catch (Exception ex) {\n+      fail(\"Task should not have been rejected\");\n+    }\n+    \n+    // clean up and unblock the task\n+    tasklatch.countDown();\n+    processor.shutdown();\n+  }\n+  \n+  /**\n+   * Verify that with a continuous stream of delete tasks being enqueued, all eventually complete\n+   * successfully in the face of failing tasks and retries without locking up our pool anywhere\n+   */\n+  @Test\n+  public void testSimpleConcurrentDeletionEnqueues() throws Exception {\n+    int maxQueueSize = 200;\n+    int numThreads = 5;\n+    int defaultMaxAttempts = 5;\n+    int retryDelay = 100;\n+    int numberOfTasks = 200;\n+    \n+    BlobDeleteProcessor processor = buildBlobDeleteProcessorForTest(enqueuedTasks, blobClient,\n+        maxQueueSize, numThreads, defaultMaxAttempts, retryDelay);\n+    List<BlobDeleterTask> tasks = generateRandomTasks(defaultMaxAttempts, numberOfTasks);\n+    List<CompletableFuture<BlobDeleterTaskResult>> taskResultsFutures = new LinkedList<>();\n+    List<BlobDeleterTaskResult> results = new LinkedList<>();\n+    for (BlobDeleterTask t : tasks) {\n+      taskResultsFutures.add(processor.enqueue(t, false));\n+    }\n+    \n+    taskResultsFutures.forEach(cf -> {\n+      try {\n+        results.add(cf.get(20000, TimeUnit.MILLISECONDS));\n+      } catch (Exception ex) {\n+        fail(\"We timed out on some task!\");\n+      }\n+    });\n+    \n+    // we shouldn't enqueue more than (numberOfTasks * defaultMaxAttempts) tasks to the pool \n+    assertTrue(enqueuedTasks.size() < (numberOfTasks * defaultMaxAttempts));\n+    assertEquals(numberOfTasks, results.size());\n+    int totalAttempts = 0;\n+    for (BlobDeleterTaskResult res : results) {\n+      assertNotNull(res);\n+      assertNotNull(res.getTask());\n+      assertEquals(\"scheduledFailingTask\", res.getTask().getActionName());\n+      totalAttempts += res.getTask().getAttempts();\n+    }\n+    // total task attempts should be consistent with our test scaffolding\n+    assertTrue(totalAttempts < (numberOfTasks * defaultMaxAttempts));\n+      \n+    processor.shutdown();\n+  }\n+  \n+  private List<BlobDeleterTask> generateRandomTasks(int defaultMaxAttempts, int taskCount) {\n+    List<BlobDeleterTask> tasks = new LinkedList<>();\n+    for (int i = 0; i < taskCount; i++) {\n+      BlobDeleterTask task = null;\n+      int totalAttempts = random().nextInt(defaultMaxAttempts);\n+      int totalFails = random().nextInt(defaultMaxAttempts + 1);\n+      task = buildScheduledFailingTaskForTest(blobClient, \"test\"+i, totalAttempts, true, totalFails);\n+      tasks.add(task);\n+    }\n+    return tasks;\n+  }\n+  \n+  /**\n+   * Returns a test-only task for just holding onto a resource for test purposes\n+   */\n+  private BlobDeleterTask buildBlockingTaskForTest(CountDownLatch latch) {\n+    return new BlobDeleterTask(null, null, false, 0) {\n+      @Override\n+      public Collection<String> doDelete() throws Exception {\n+        // block until something forces this latch to count down\n+        latch.await();\n+        return null;\n+      }\n+      \n+      @Override\n+      public String getActionName() { return \"blockingTask\"; }\n+    };\n+  }\n+  \n+  /**\n+   * Returns a test-only task that always fails on action execution by throwing an\n+   * exception\n+   */\n+  private BlobDeleterTask buildFailingTaskForTest(CoreStorageClient client, \n+      String collectionName, int maxRetries, boolean allowRetries) {\n+    return new BlobDeleterTask(client, collectionName, allowRetries, maxRetries) {\n+      @Override\n+      public Collection<String> doDelete() throws Exception {\n+        throw new Exception(\"\");\n+      }\n+      \n+      @Override\n+      public String getActionName() { return \"failingTask\"; }\n+    };\n+  }\n+  \n+  /**\n+   * Returns a test-only task that fails a specified number of times before succeeding\n+   */\n+  private BlobDeleterTask buildScheduledFailingTaskForTest(CoreStorageClient client, \n+      String collectionName, int maxRetries, boolean allowRetries, int failTotal) {\n+    return new BlobDeleterTask(client, collectionName, allowRetries, maxRetries) {\n+      private AtomicInteger failCount = new AtomicInteger(0);\n+      \n+      @Override\n+      public Collection<String> doDelete() throws Exception {\n+        while (failCount.get() < failTotal) {\n+          failCount.incrementAndGet();\n+          throw new Exception(\"\");\n+        }\n+        return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9a43e67ea7a610f6b78622fcf69afd6e2d974d5"}, "originalPosition": 441}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c03e85e35fe859ed3339e589a58ecd7a4bc7c749", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/c03e85e35fe859ed3339e589a58ecd7a4bc7c749", "committedDate": "2020-02-05T18:10:30Z", "message": "Merge branch 'jira/SOLR-13101' into jira/SOLR-13101-data-delete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e822f98f7a50c0281b3ea3db43a0930556a0ab25", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/e822f98f7a50c0281b3ea3db43a0930556a0ab25", "committedDate": "2020-02-05T18:48:15Z", "message": "Address review comments and fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd034879d7892223ec57db75d5df1314839f95de", "author": {"user": {"login": "andyvuong", "name": "Andy Vuong"}}, "url": "https://github.com/apache/lucene-solr/commit/cd034879d7892223ec57db75d5df1314839f95de", "committedDate": "2020-02-05T22:08:39Z", "message": "Close resource and throw exception on failure"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0MTAyMzUw", "url": "https://github.com/apache/lucene-solr/pull/1188#pullrequestreview-354102350", "createdAt": "2020-02-05T23:02:20Z", "commit": {"oid": "cd034879d7892223ec57db75d5df1314839f95de"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2247, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}