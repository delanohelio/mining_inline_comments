{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEyMDI1NTI4", "number": 1473, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNDoxMzoxN1rOD4YMRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzo0NjozOFrOD4_vVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDQzMjA0OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNDoxMzoxN1rOGPKywA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QwMzowODowNFrOGPoHew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1ODY1Ng==", "bodyText": "do we have a test that tickles these cases?", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r418558656", "createdAt": "2020-05-01T14:13:17Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "diffHunk": "@@ -148,56 +155,80 @@ public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState\n       CodecUtil.retrieveChecksum(termsIn);\n \n       // Read per-field details\n-      seekDir(termsIn);\n-      seekDir(indexIn);\n+      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n+      Map<String, FieldReader> fieldMap = null;\n+      Throwable priorE = null;\n+      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n+        try {\n+          final IndexInput indexMetaIn, termsMetaIn;\n+          if (version >= VERSION_META_FILE) {\n+            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n+            indexMetaIn = termsMetaIn = metaIn;\n+          } else {\n+            seekDir(termsIn);\n+            seekDir(indexIn);\n+            indexMetaIn = indexIn;\n+            termsMetaIn = termsIn;\n+          }\n \n-      final int numFields = termsIn.readVInt();\n-      if (numFields < 0) {\n-        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n-      }\n-      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n-      for (int i = 0; i < numFields; ++i) {\n-        final int field = termsIn.readVInt();\n-        final long numTerms = termsIn.readVLong();\n-        if (numTerms <= 0) {\n-          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n-        }\n-        final BytesRef rootCode = readBytesRef(termsIn);\n-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n-        if (fieldInfo == null) {\n-          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n-        }\n-        final long sumTotalTermFreq = termsIn.readVLong();\n-        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n-        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n-        final int docCount = termsIn.readVInt();\n-        if (version < VERSION_META_LONGS_REMOVED) {\n-          final int longsSize = termsIn.readVInt();\n-          if (longsSize < 0) {\n-            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n+          final int numFields = termsMetaIn.readVInt();\n+          if (numFields < 0) {\n+            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n+          }\n+          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n+          for (int i = 0; i < numFields; ++i) {\n+            final int field = termsMetaIn.readVInt();\n+            final long numTerms = termsMetaIn.readVLong();\n+            if (numTerms <= 0) {\n+              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n+            }\n+            final BytesRef rootCode = readBytesRef(termsMetaIn);\n+            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n+            if (fieldInfo == null) {\n+              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n+            }\n+            final long sumTotalTermFreq = termsMetaIn.readVLong();\n+            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n+            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n+            final int docCount = termsMetaIn.readVInt();\n+            if (version < VERSION_META_LONGS_REMOVED) {\n+              final int longsSize = termsMetaIn.readVInt();\n+              if (longsSize < 0) {\n+                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n+              }\n+            }\n+            BytesRef minTerm = readBytesRef(termsMetaIn);\n+            BytesRef maxTerm = readBytesRef(termsMetaIn);\n+            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n+              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk2MzM2OA==", "bodyText": "Not directly, and these things are hard to test, though I agree we could do better. I opened https://issues.apache.org/jira/browse/LUCENE-9356 to try to improve the coverage of these code paths.", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r418963368", "createdAt": "2020-05-02T14:09:31Z", "author": {"login": "jpountz"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "diffHunk": "@@ -148,56 +155,80 @@ public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState\n       CodecUtil.retrieveChecksum(termsIn);\n \n       // Read per-field details\n-      seekDir(termsIn);\n-      seekDir(indexIn);\n+      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n+      Map<String, FieldReader> fieldMap = null;\n+      Throwable priorE = null;\n+      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n+        try {\n+          final IndexInput indexMetaIn, termsMetaIn;\n+          if (version >= VERSION_META_FILE) {\n+            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n+            indexMetaIn = termsMetaIn = metaIn;\n+          } else {\n+            seekDir(termsIn);\n+            seekDir(indexIn);\n+            indexMetaIn = indexIn;\n+            termsMetaIn = termsIn;\n+          }\n \n-      final int numFields = termsIn.readVInt();\n-      if (numFields < 0) {\n-        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n-      }\n-      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n-      for (int i = 0; i < numFields; ++i) {\n-        final int field = termsIn.readVInt();\n-        final long numTerms = termsIn.readVLong();\n-        if (numTerms <= 0) {\n-          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n-        }\n-        final BytesRef rootCode = readBytesRef(termsIn);\n-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n-        if (fieldInfo == null) {\n-          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n-        }\n-        final long sumTotalTermFreq = termsIn.readVLong();\n-        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n-        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n-        final int docCount = termsIn.readVInt();\n-        if (version < VERSION_META_LONGS_REMOVED) {\n-          final int longsSize = termsIn.readVInt();\n-          if (longsSize < 0) {\n-            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n+          final int numFields = termsMetaIn.readVInt();\n+          if (numFields < 0) {\n+            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n+          }\n+          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n+          for (int i = 0; i < numFields; ++i) {\n+            final int field = termsMetaIn.readVInt();\n+            final long numTerms = termsMetaIn.readVLong();\n+            if (numTerms <= 0) {\n+              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n+            }\n+            final BytesRef rootCode = readBytesRef(termsMetaIn);\n+            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n+            if (fieldInfo == null) {\n+              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n+            }\n+            final long sumTotalTermFreq = termsMetaIn.readVLong();\n+            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n+            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n+            final int docCount = termsMetaIn.readVInt();\n+            if (version < VERSION_META_LONGS_REMOVED) {\n+              final int longsSize = termsMetaIn.readVInt();\n+              if (longsSize < 0) {\n+                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n+              }\n+            }\n+            BytesRef minTerm = readBytesRef(termsMetaIn);\n+            BytesRef maxTerm = readBytesRef(termsMetaIn);\n+            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n+              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1ODY1Ng=="}, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTAzOTA5OQ==", "bodyText": "Thanks, Adrien", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419039099", "createdAt": "2020-05-03T03:08:04Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "diffHunk": "@@ -148,56 +155,80 @@ public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState\n       CodecUtil.retrieveChecksum(termsIn);\n \n       // Read per-field details\n-      seekDir(termsIn);\n-      seekDir(indexIn);\n+      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n+      Map<String, FieldReader> fieldMap = null;\n+      Throwable priorE = null;\n+      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n+        try {\n+          final IndexInput indexMetaIn, termsMetaIn;\n+          if (version >= VERSION_META_FILE) {\n+            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n+            indexMetaIn = termsMetaIn = metaIn;\n+          } else {\n+            seekDir(termsIn);\n+            seekDir(indexIn);\n+            indexMetaIn = indexIn;\n+            termsMetaIn = termsIn;\n+          }\n \n-      final int numFields = termsIn.readVInt();\n-      if (numFields < 0) {\n-        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n-      }\n-      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n-      for (int i = 0; i < numFields; ++i) {\n-        final int field = termsIn.readVInt();\n-        final long numTerms = termsIn.readVLong();\n-        if (numTerms <= 0) {\n-          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n-        }\n-        final BytesRef rootCode = readBytesRef(termsIn);\n-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n-        if (fieldInfo == null) {\n-          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n-        }\n-        final long sumTotalTermFreq = termsIn.readVLong();\n-        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n-        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n-        final int docCount = termsIn.readVInt();\n-        if (version < VERSION_META_LONGS_REMOVED) {\n-          final int longsSize = termsIn.readVInt();\n-          if (longsSize < 0) {\n-            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n+          final int numFields = termsMetaIn.readVInt();\n+          if (numFields < 0) {\n+            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n+          }\n+          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n+          for (int i = 0; i < numFields; ++i) {\n+            final int field = termsMetaIn.readVInt();\n+            final long numTerms = termsMetaIn.readVLong();\n+            if (numTerms <= 0) {\n+              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n+            }\n+            final BytesRef rootCode = readBytesRef(termsMetaIn);\n+            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n+            if (fieldInfo == null) {\n+              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n+            }\n+            final long sumTotalTermFreq = termsMetaIn.readVLong();\n+            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n+            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n+            final int docCount = termsMetaIn.readVInt();\n+            if (version < VERSION_META_LONGS_REMOVED) {\n+              final int longsSize = termsMetaIn.readVInt();\n+              if (longsSize < 0) {\n+                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n+              }\n+            }\n+            BytesRef minTerm = readBytesRef(termsMetaIn);\n+            BytesRef maxTerm = readBytesRef(termsMetaIn);\n+            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n+              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1ODY1Ng=="}, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNDQzNzAwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNDoxNDo0OVrOGPK1xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNDoxNDo0OVrOGPK1xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODU1OTQzMA==", "bodyText": "again, I don't know if we have test coverage for the corrputed metadata?", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r418559430", "createdAt": "2020-05-01T14:14:49Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java", "diffHunk": "@@ -148,56 +155,80 @@ public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState\n       CodecUtil.retrieveChecksum(termsIn);\n \n       // Read per-field details\n-      seekDir(termsIn);\n-      seekDir(indexIn);\n+      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n+      Map<String, FieldReader> fieldMap = null;\n+      Throwable priorE = null;\n+      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n+        try {\n+          final IndexInput indexMetaIn, termsMetaIn;\n+          if (version >= VERSION_META_FILE) {\n+            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n+            indexMetaIn = termsMetaIn = metaIn;\n+          } else {\n+            seekDir(termsIn);\n+            seekDir(indexIn);\n+            indexMetaIn = indexIn;\n+            termsMetaIn = termsIn;\n+          }\n \n-      final int numFields = termsIn.readVInt();\n-      if (numFields < 0) {\n-        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n-      }\n-      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n-      for (int i = 0; i < numFields; ++i) {\n-        final int field = termsIn.readVInt();\n-        final long numTerms = termsIn.readVLong();\n-        if (numTerms <= 0) {\n-          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n-        }\n-        final BytesRef rootCode = readBytesRef(termsIn);\n-        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n-        if (fieldInfo == null) {\n-          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n-        }\n-        final long sumTotalTermFreq = termsIn.readVLong();\n-        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n-        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n-        final int docCount = termsIn.readVInt();\n-        if (version < VERSION_META_LONGS_REMOVED) {\n-          final int longsSize = termsIn.readVInt();\n-          if (longsSize < 0) {\n-            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n+          final int numFields = termsMetaIn.readVInt();\n+          if (numFields < 0) {\n+            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n+          }\n+          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n+          for (int i = 0; i < numFields; ++i) {\n+            final int field = termsMetaIn.readVInt();\n+            final long numTerms = termsMetaIn.readVLong();\n+            if (numTerms <= 0) {\n+              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n+            }\n+            final BytesRef rootCode = readBytesRef(termsMetaIn);\n+            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n+            if (fieldInfo == null) {\n+              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n+            }\n+            final long sumTotalTermFreq = termsMetaIn.readVLong();\n+            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n+            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n+            final int docCount = termsMetaIn.readVInt();\n+            if (version < VERSION_META_LONGS_REMOVED) {\n+              final int longsSize = termsMetaIn.readVInt();\n+              if (longsSize < 0) {\n+                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n+              }\n+            }\n+            BytesRef minTerm = readBytesRef(termsMetaIn);\n+            BytesRef maxTerm = readBytesRef(termsMetaIn);\n+            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n+              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);\n+            }\n+            if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n+              throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsMetaIn);\n+            }\n+            if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n+              throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsMetaIn);\n+            }\n+            final long indexStartFP = indexMetaIn.readVLong();\n+            FieldReader previous = fieldMap.put(fieldInfo.name,\n+                new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n+                    indexStartFP, indexIn, minTerm, maxTerm));\n+            if (previous != null) {\n+              throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsMetaIn);\n+            }\n+          }\n+        } catch (Throwable exception) {\n+          priorE = exception;\n+        } finally {\n+          if (metaIn != null) {\n+            CodecUtil.checkFooter(metaIn, priorE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDIwNTczOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMDoxNzo0MFrOGP6WWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QwODozOToxMFrOGR0VLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ==", "bodyText": "@jpountz here I see the same lack of serializer write/read code, could it be possible to have such thing, It would improve readability and unit testing by only mocking fieldMetadatas and check serialization is correctly applied.", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419337819", "createdAt": "2020-05-04T10:17:40Z", "author": {"login": "juanka588"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {\n+      CodecUtil.writeIndexHeader(metaOut, BlockTreeTermsReader.TERMS_META_CODEC_NAME, BlockTreeTermsReader.VERSION_CURRENT,\n+          state.segmentInfo.getId(), state.segmentSuffix);\n \n-      termsOut.writeVInt(fields.size());\n+      metaOut.writeVInt(fields.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTM5MTcyMQ==", "bodyText": "Your proposal sounds orthogonal to this pull request to me?", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419391721", "createdAt": "2020-05-04T12:15:03Z", "author": {"login": "jpountz"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {\n+      CodecUtil.writeIndexHeader(metaOut, BlockTreeTermsReader.TERMS_META_CODEC_NAME, BlockTreeTermsReader.VERSION_CURRENT,\n+          state.segmentInfo.getId(), state.segmentSuffix);\n \n-      termsOut.writeVInt(fields.size());\n+      metaOut.writeVInt(fields.size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ=="}, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0OTg2MQ==", "bodyText": "yes but maybe is the opportunity to move the code and add more testability in BlockTreeTermsReader.java", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419449861", "createdAt": "2020-05-04T13:48:49Z", "author": {"login": "juanka588"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {\n+      CodecUtil.writeIndexHeader(metaOut, BlockTreeTermsReader.TERMS_META_CODEC_NAME, BlockTreeTermsReader.VERSION_CURRENT,\n+          state.segmentInfo.getId(), state.segmentSuffix);\n \n-      termsOut.writeVInt(fields.size());\n+      metaOut.writeVInt(fields.size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ=="}, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTMzNjM2Ng==", "bodyText": "I prefer decoupling changes when possible.", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r421336366", "createdAt": "2020-05-07T08:39:10Z", "author": {"login": "jpountz"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {\n+      CodecUtil.writeIndexHeader(metaOut, BlockTreeTermsReader.TERMS_META_CODEC_NAME, BlockTreeTermsReader.VERSION_CURRENT,\n+          state.segmentInfo.getId(), state.segmentSuffix);\n \n-      termsOut.writeVInt(fields.size());\n+      metaOut.writeVInt(fields.size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTMzNzgxOQ=="}, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMDkxMTU2OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxMzo0NjozOFrOGQBF1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQyMDowMzoxMVrOGQQF3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0ODI3Nw==", "bodyText": "why this file is not created at the same time with the indexOut, termOut?", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419448277", "createdAt": "2020-05-04T13:46:38Z", "author": {"login": "juanka588"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY5NDA0NQ==", "bodyText": "That would work too. I like keeping the index output open for as little time as possible when it doesn't make things worse otherwise.", "url": "https://github.com/apache/lucene-solr/pull/1473#discussion_r419694045", "createdAt": "2020-05-04T20:03:11Z", "author": {"login": "jpountz"}, "path": "lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java", "diffHunk": "@@ -1060,36 +1052,35 @@ public void close() throws IOException {\n       return;\n     }\n     closed = true;\n-    \n+\n+    final String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTreeTermsReader.TERMS_META_EXTENSION);\n     boolean success = false;\n-    try {\n-      \n-      final long dirStart = termsOut.getFilePointer();\n-      final long indexDirStart = indexOut.getFilePointer();\n+    try (IndexOutput metaOut = state.directory.createOutput(metaName, state.context)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTQ0ODI3Nw=="}, "originalCommit": {"oid": "9a4df99cc3f2611315aa016192b921985ea94e42"}, "originalPosition": 45}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1502, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}