{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQwOTY4MzI3", "number": 1623, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMjowMzowOFrOEYQIdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjozOTo0MlrOEZkJTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzODY1NTg5OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMjowMzowOFrOHAfQEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QyMjowMzowOFrOHAfQEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDI3NDA2Nw==", "bodyText": "This Javadoc will need to be updated to reflect the broader use of this method.\nAlso, is preparePointInTimeMerge (without the on) a better name?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470274067", "createdAt": "2020-08-13T22:03:08Z", "author": {"login": "msfroh"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3321,8 +3395,11 @@ private long prepareCommitInternal() throws IOException {\n    * below.  We also ensure that we pull the merge readers while holding {@code IndexWriter}'s lock.  Otherwise\n    * we could see concurrent deletions/updates applied that do not belong to the segment.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b701264ff304a4c0483b36b6dbc36d3cc8ed1075"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjUxNDE3OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMToxOToxOVrOHBDneA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNjo1OToxMFrOHCF6DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg2OTg4MA==", "bodyText": "Thank you for good success variable naming instead of the usual success1 and success2 etc.!", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470869880", "createdAt": "2020-08-14T21:19:19Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk1NTk4MA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471955980", "createdAt": "2020-08-18T06:59:10Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg2OTg4MA=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjUyMjczOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMToyMjo0N1rOHBDsag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMjo1NDo0M1rOHCShMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MTE0Ng==", "bodyText": "Maybe we need to rename this IWC option and variables?  Maybe we need two timeouts, one for commit, one for getReader?  Or, maybe we somehow make this a property of the MergeSpecification so MergePolicy can decide case by case what the timeout should be?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470871146", "createdAt": "2020-08-14T21:22:47Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk5MTY2NQ==", "bodyText": "I'd not want to add another option to IWC unless absolutely necessary. Maybe we can just keep one for now and if somebody has a good usecase we can still add? I think we have the ability to disable it entirely for one or the other trigger which should be enough in most cases?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471991665", "createdAt": "2020-08-18T08:02:48Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MTE0Ng=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE2MjYxMA==", "bodyText": "OK let's leave this be for now.", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472162610", "createdAt": "2020-08-18T12:54:43Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MTE0Ng=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjU4Njk3OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1MjozN1rOHBERyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1MjozN1rOHBERyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4MDcxNQ==", "bodyText": "Good name!  Maybe rename onCommitMerges to pointInTimeMerges?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470880715", "createdAt": "2020-08-14T21:52:37Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3244,7 +3322,7 @@ private long prepareCommitInternal() throws IOException {\n               if (anyChanges && maxCommitMergeWaitMillis > 0) {\n                 // we can safely call prepareOnCommitMerge since writeReaderPool(true) above wrote all\n                 // necessary files to disk and checkpointed them.\n-                onCommitMerges = prepareOnCommitMerge(toCommit, includeInCommit);\n+                onCommitMerges = preparePointInTimeMerge(toCommit, includeInCommit, MergeTrigger.COMMIT, sci->{});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjU4OTM2OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1Mzo1OVrOHBETRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowMjo1MFrOHCGA0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4MTA5Mg==", "bodyText": "Hmm why don't we need to deleter.incRef for NRT reader case?  I guess the NRT reader we opened holds a reference already?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470881092", "createdAt": "2020-08-14T21:53:59Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3335,49 +3416,60 @@ public void mergeFinished(boolean committed, boolean segmentDropped) throws IOEx\n             // includedInCommit will be set (above, by our caller) to false if the allowed max wall clock\n             // time (IWC.getMaxCommitMergeWaitMillis()) has elapsed, which means we did not make the timeout\n             // and will not commit our merge to the to-be-commited SegmentInfos\n-            \n             if (segmentDropped == false\n                 && committed\n-                && includeInCommit.get()) {\n+                && includeMergeResult.get()) {\n+\n+              // make sure onMergeComplete really was called:\n+              assert origInfo != null;\n \n               if (infoStream.isEnabled(\"IW\")) {\n                 infoStream.message(\"IW\", \"now apply merge during commit: \" + toWrap.segString());\n               }\n \n-              // make sure onMergeComplete really was called:\n-              assert origInfo != null;\n-\n-              deleter.incRef(origInfo.files());\n+              if (trigger == MergeTrigger.COMMIT) { // if we do this in a getReader call here this is obsolete\n+                deleter.incRef(origInfo.files());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk1NzcxMw==", "bodyText": "I extended the comment", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471957713", "createdAt": "2020-08-18T07:02:50Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3335,49 +3416,60 @@ public void mergeFinished(boolean committed, boolean segmentDropped) throws IOEx\n             // includedInCommit will be set (above, by our caller) to false if the allowed max wall clock\n             // time (IWC.getMaxCommitMergeWaitMillis()) has elapsed, which means we did not make the timeout\n             // and will not commit our merge to the to-be-commited SegmentInfos\n-            \n             if (segmentDropped == false\n                 && committed\n-                && includeInCommit.get()) {\n+                && includeMergeResult.get()) {\n+\n+              // make sure onMergeComplete really was called:\n+              assert origInfo != null;\n \n               if (infoStream.isEnabled(\"IW\")) {\n                 infoStream.message(\"IW\", \"now apply merge during commit: \" + toWrap.segString());\n               }\n \n-              // make sure onMergeComplete really was called:\n-              assert origInfo != null;\n-\n-              deleter.incRef(origInfo.files());\n+              if (trigger == MergeTrigger.COMMIT) { // if we do this in a getReader call here this is obsolete\n+                deleter.incRef(origInfo.files());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4MTA5Mg=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjU5MjExOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1NToyNFrOHBEU6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1NToyNFrOHBEU6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4MTUxMw==", "bodyText": "Re-indent?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470881513", "createdAt": "2020-08-14T21:55:24Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java", "diffHunk": "@@ -82,7 +82,8 @@ protected DirectoryReader doBody(String segmentFileName) throws IOException {\n   }\n \n   /** Used by near real-time search */\n-  static DirectoryReader open(IndexWriter writer, SegmentInfos infos, boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n+  static StandardDirectoryReader open(IndexWriter writer, IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFunction,\n+                              SegmentInfos infos, boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjU5NjU4OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1NzoyNVrOHBEXgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1NzoyNVrOHBEXgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4MjE3OQ==", "bodyText": "Maybe move the assert to top of the method?  We should always hold IW's monitor lock on entry?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470882179", "createdAt": "2020-08-14T21:57:25Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3335,49 +3416,60 @@ public void mergeFinished(boolean committed, boolean segmentDropped) throws IOEx\n             // includedInCommit will be set (above, by our caller) to false if the allowed max wall clock\n             // time (IWC.getMaxCommitMergeWaitMillis()) has elapsed, which means we did not make the timeout\n             // and will not commit our merge to the to-be-commited SegmentInfos\n-            \n             if (segmentDropped == false\n                 && committed\n-                && includeInCommit.get()) {\n+                && includeMergeResult.get()) {\n+\n+              // make sure onMergeComplete really was called:\n+              assert origInfo != null;\n \n               if (infoStream.isEnabled(\"IW\")) {\n                 infoStream.message(\"IW\", \"now apply merge during commit: \" + toWrap.segString());\n               }\n \n-              // make sure onMergeComplete really was called:\n-              assert origInfo != null;\n-\n-              deleter.incRef(origInfo.files());\n+              if (trigger == MergeTrigger.COMMIT) { // if we do this in a getReader call here this is obsolete\n+                deleter.incRef(origInfo.files());\n+              }\n               Set<String> mergedSegmentNames = new HashSet<>();\n               for (SegmentCommitInfo sci : segments) {\n                 mergedSegmentNames.add(sci.info.name);\n               }\n               List<SegmentCommitInfo> toCommitMergedAwaySegments = new ArrayList<>();\n-              for (SegmentCommitInfo sci : committingSegmentInfos) {\n+              for (SegmentCommitInfo sci : mergingSegmentInfos) {\n                 if (mergedSegmentNames.contains(sci.info.name)) {\n                   toCommitMergedAwaySegments.add(sci);\n-                  deleter.decRef(sci.files());\n+                  if (trigger == MergeTrigger.COMMIT) { // if we do this in a getReader call here this is obsolete\n+                    deleter.decRef(sci.files());\n+                  }\n                 }\n               }\n               // Construct a OneMerge that applies to toCommit\n               MergePolicy.OneMerge applicableMerge = new MergePolicy.OneMerge(toCommitMergedAwaySegments);\n               applicableMerge.info = origInfo;\n               long segmentCounter = Long.parseLong(origInfo.info.name.substring(1), Character.MAX_RADIX);\n-              committingSegmentInfos.counter = Math.max(committingSegmentInfos.counter, segmentCounter + 1);\n-              committingSegmentInfos.applyMergeChanges(applicableMerge, false);\n+              mergingSegmentInfos.counter = Math.max(mergingSegmentInfos.counter, segmentCounter + 1);\n+              mergingSegmentInfos.applyMergeChanges(applicableMerge, false);\n             } else {\n               if (infoStream.isEnabled(\"IW\")) {\n                 infoStream.message(\"IW\", \"skip apply merge during commit: \" + toWrap.segString());\n               }\n             }\n-            toWrap.mergeFinished(committed, false);\n+            toWrap.mergeFinished(committed, segmentDropped);\n             super.mergeFinished(committed, segmentDropped);\n           }\n \n           @Override\n-          void onMergeComplete() {\n-            // clone the target info to make sure we have the original info without the updated del and update gens\n-            origInfo = info.clone();\n+          void onMergeComplete() throws IOException {\n+            if (includeMergeResult.get()\n+                && isAborted() == false\n+                && info.info.maxDoc() > 0/* never do this if the segment if dropped / empty */) {\n+              assert Thread.holdsLock(IndexWriter.this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 247}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjU5OTk5OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1OTowM1rOHBEZiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1OTowM1rOHBEZiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4MjY5Nw==", "bodyText": "s/refInced/incRef'd?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470882697", "createdAt": "2020-08-14T21:59:03Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+          assert openingSegmentInfos != null;\n+          synchronized (this) {\n+            includeMergeReader.set(false);\n+            boolean openNewReader = mergedReaders.isEmpty() == false;\n+            if (openNewReader) {\n+              StandardDirectoryReader mergedReader = StandardDirectoryReader.open(this,\n+                  sci -> {\n+                    // as soon as we remove the reader and return it the StandardDirectoryReader#open\n+                    // will take care of closing it. We only need to handle the readers that remain in the\n+                    // mergedReaders map and close them.\n+                    SegmentReader remove = mergedReaders.remove(sci.info.name);\n+                    if (remove == null) {\n+                      remove = openedReadOnlyClones.remove(sci.info.name);\n+                      assert remove != null;\n+                      // each of the readers we reuse from the previous reader needs to be refInced", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjYwMDQwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1OToxOVrOHBEZzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMTo1OToxOVrOHBEZzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4Mjc2NQ==", "bodyText": "s/refInc/incRef?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r470882765", "createdAt": "2020-08-14T21:59:19Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+          assert openingSegmentInfos != null;\n+          synchronized (this) {\n+            includeMergeReader.set(false);\n+            boolean openNewReader = mergedReaders.isEmpty() == false;\n+            if (openNewReader) {\n+              StandardDirectoryReader mergedReader = StandardDirectoryReader.open(this,\n+                  sci -> {\n+                    // as soon as we remove the reader and return it the StandardDirectoryReader#open\n+                    // will take care of closing it. We only need to handle the readers that remain in the\n+                    // mergedReaders map and close them.\n+                    SegmentReader remove = mergedReaders.remove(sci.info.name);\n+                    if (remove == null) {\n+                      remove = openedReadOnlyClones.remove(sci.info.name);\n+                      assert remove != null;\n+                      // each of the readers we reuse from the previous reader needs to be refInced\n+                      // since we reuse them but don't have an implicit refInc in the SDR:open call", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzYzNzEwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQxNzozMzozMlrOHBMmRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQyMzo0MjowM1rOHBWMmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAxNzAzMA==", "bodyText": "There's no need to check whether we timed out here, since we effectively abort all of the point-in-time merges we created by setting includeMergeReader to false below, right? I wonder if it would be cleaner to eliminate this AtomicBoolean that is shared with this and the merge threads, and instead using the existing merge abort technique that we have? OTOH IDK how that other mechanism works - is it aborting all outstanding merges?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471017030", "createdAt": "2020-08-15T17:33:32Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0NTk5NQ==", "bodyText": "we actually never abort these merges. there is no reason why we should do that. we might have done most of the work, we only abort merges if we shutdown our writers. I hope that makes sense?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471145995", "createdAt": "2020-08-16T18:54:48Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAxNzAzMA=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE3NDI5OA==", "bodyText": "OK, I misunderstood the usage of includeMergeReader. I guess the idea is that we let them continue, but keep the original (merging) segments in the reader we're opening, so we do need a separate condition for that", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471174298", "createdAt": "2020-08-16T23:42:03Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAxNzAzMA=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzYzOTk5OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQxNzozNzo1MVrOHBMnnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQxNzozNzo1MVrOHBMnnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAxNzM3Mg==", "bodyText": "and this one, closeMergedReaders?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471017372", "createdAt": "2020-08-15T17:37:51Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+          assert openingSegmentInfos != null;\n+          synchronized (this) {\n+            includeMergeReader.set(false);\n+            boolean openNewReader = mergedReaders.isEmpty() == false;\n+            if (openNewReader) {\n+              StandardDirectoryReader mergedReader = StandardDirectoryReader.open(this,\n+                  sci -> {\n+                    // as soon as we remove the reader and return it the StandardDirectoryReader#open\n+                    // will take care of closing it. We only need to handle the readers that remain in the\n+                    // mergedReaders map and close them.\n+                    SegmentReader remove = mergedReaders.remove(sci.info.name);\n+                    if (remove == null) {\n+                      remove = openedReadOnlyClones.remove(sci.info.name);\n+                      assert remove != null;\n+                      // each of the readers we reuse from the previous reader needs to be refInced\n+                      // since we reuse them but don't have an implicit refInc in the SDR:open call\n+                      remove.incRef();\n+                    }\n+                    return remove;\n+                  }, openingSegmentInfos, applyAllDeletes, writeAllDeletes);\n+              try {\n+                r.close(); // close and swap in the new reader... close is cool here since we didn't leak this reader yet\n+              } finally {\n+                r = mergedReader;\n+              }\n+            }\n+          }\n+          replaceReaderSuccess = true;\n+        } finally {\n+          synchronized (this) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MzY0MDIxOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQxNzozODozMVrOHBMnvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNlQxOTowMzozN1rOHBUhTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAxNzQwNA==", "bodyText": "This method is getting pretty big, and it might help readability if we named these synchronized blocks as functions. This one could be replaceReader()? OTOH maybe it requires too many parameters - it's hard to tell in code review", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471017404", "createdAt": "2020-08-15T17:38:31Z", "author": {"login": "msokolov"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+          assert openingSegmentInfos != null;\n+          synchronized (this) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE0NjgyOA==", "bodyText": "I added one new method. I don't think we should extract more WDYT", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471146828", "createdAt": "2020-08-16T19:03:37Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -607,6 +633,57 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n           }\n         }\n       }\n+      if (onCommitMerges != null) { // only relevant if we do merge on getReader\n+        boolean replaceReaderSuccess = false;\n+        try {\n+          mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+          onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+          assert openingSegmentInfos != null;\n+          synchronized (this) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTAxNzQwNA=="}, "originalCommit": {"oid": "6d8c601789501d5880847cef13d7cacc402fcd0c"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODUzNzczOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxOTozMTo1OFrOHB4Fqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxOTozMTo1OFrOHB4Fqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyOTU3OA==", "bodyText": "s/do/to?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471729578", "createdAt": "2020-08-17T19:31:58Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -545,18 +546,54 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     // obtained during this flush are pooled, the first time\n     // this method is called:\n     readerPool.enableReaderPooling();\n-    DirectoryReader r = null;\n+    StandardDirectoryReader r = null;\n     doBeforeFlush();\n-    boolean anyChanges = false;\n+    boolean anyChanges;\n     /*\n      * for releasing a NRT reader we must ensure that \n      * DW doesn't add any segments or deletes until we are\n      * done with creating the NRT DirectoryReader. \n      * We release the two stage full flush after we are done opening the\n      * directory reader!\n      */\n+    MergePolicy.MergeSpecification onGetReaderMerges = null;\n+    AtomicBoolean hasTimedOut = new AtomicBoolean(false);\n+    Map<String, SegmentReader> mergedReaders = new HashMap<>();\n+    Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();\n+    // this function is used to control which SR are opened in order to keep track of them\n+    // and to reuse them in the case we wait for merges in this getReader call.\n+    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory = sci -> {\n+      final ReadersAndUpdates rld = getPooledInstance(sci, true);\n+      try {\n+        assert Thread.holdsLock(IndexWriter.this);\n+        SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);\n+        openedReadOnlyClones.put(sci.info.name, segmentReader);\n+        return segmentReader;\n+      } finally {\n+        release(rld);\n+      }\n+    };\n+    SegmentInfos openingSegmentInfos = null;\n+    final long maxFullFlushMergeWaitMillis = config.getMaxFullFlushMergeWaitMillis();\n     boolean success2 = false;\n     try {\n+      /* this is the essential part of the getReader method. We need to take care of the following things:\n+       *  - flush all currently in-memory DWPTs to disk\n+       *  - apply all deletes & updates to new and to the existing DWPTs\n+       *  - prevent flushes and applying deletes of concurrently indexing DWPTs to be applied\n+       *  - open a SDR on the updated SIS\n+       *\n+       * in order do prevent concurrent flushes we call DocumentsWriter#flushAllThreads that swaps out the deleteQueue", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0bbf1a15236d587e5e84c5eb112752594659c77"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODUzODQ1OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxOTozMjoxN1rOHB4GMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowNDo1NlrOHCGFPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyOTcxMw==", "bodyText": "Thank you for these awesome details!!", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471729713", "createdAt": "2020-08-17T19:32:17Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -545,18 +546,54 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     // obtained during this flush are pooled, the first time\n     // this method is called:\n     readerPool.enableReaderPooling();\n-    DirectoryReader r = null;\n+    StandardDirectoryReader r = null;\n     doBeforeFlush();\n-    boolean anyChanges = false;\n+    boolean anyChanges;\n     /*\n      * for releasing a NRT reader we must ensure that \n      * DW doesn't add any segments or deletes until we are\n      * done with creating the NRT DirectoryReader. \n      * We release the two stage full flush after we are done opening the\n      * directory reader!\n      */\n+    MergePolicy.MergeSpecification onGetReaderMerges = null;\n+    AtomicBoolean hasTimedOut = new AtomicBoolean(false);\n+    Map<String, SegmentReader> mergedReaders = new HashMap<>();\n+    Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();\n+    // this function is used to control which SR are opened in order to keep track of them\n+    // and to reuse them in the case we wait for merges in this getReader call.\n+    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory = sci -> {\n+      final ReadersAndUpdates rld = getPooledInstance(sci, true);\n+      try {\n+        assert Thread.holdsLock(IndexWriter.this);\n+        SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);\n+        openedReadOnlyClones.put(sci.info.name, segmentReader);\n+        return segmentReader;\n+      } finally {\n+        release(rld);\n+      }\n+    };\n+    SegmentInfos openingSegmentInfos = null;\n+    final long maxFullFlushMergeWaitMillis = config.getMaxFullFlushMergeWaitMillis();\n     boolean success2 = false;\n     try {\n+      /* this is the essential part of the getReader method. We need to take care of the following things:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0bbf1a15236d587e5e84c5eb112752594659c77"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk1ODg0Nw==", "bodyText": "yeah I think it makes sense to have these details in these complex methods", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471958847", "createdAt": "2020-08-18T07:04:56Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -545,18 +546,54 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     // obtained during this flush are pooled, the first time\n     // this method is called:\n     readerPool.enableReaderPooling();\n-    DirectoryReader r = null;\n+    StandardDirectoryReader r = null;\n     doBeforeFlush();\n-    boolean anyChanges = false;\n+    boolean anyChanges;\n     /*\n      * for releasing a NRT reader we must ensure that \n      * DW doesn't add any segments or deletes until we are\n      * done with creating the NRT DirectoryReader. \n      * We release the two stage full flush after we are done opening the\n      * directory reader!\n      */\n+    MergePolicy.MergeSpecification onGetReaderMerges = null;\n+    AtomicBoolean hasTimedOut = new AtomicBoolean(false);\n+    Map<String, SegmentReader> mergedReaders = new HashMap<>();\n+    Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();\n+    // this function is used to control which SR are opened in order to keep track of them\n+    // and to reuse them in the case we wait for merges in this getReader call.\n+    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory = sci -> {\n+      final ReadersAndUpdates rld = getPooledInstance(sci, true);\n+      try {\n+        assert Thread.holdsLock(IndexWriter.this);\n+        SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);\n+        openedReadOnlyClones.put(sci.info.name, segmentReader);\n+        return segmentReader;\n+      } finally {\n+        release(rld);\n+      }\n+    };\n+    SegmentInfos openingSegmentInfos = null;\n+    final long maxFullFlushMergeWaitMillis = config.getMaxFullFlushMergeWaitMillis();\n     boolean success2 = false;\n     try {\n+      /* this is the essential part of the getReader method. We need to take care of the following things:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcyOTcxMw=="}, "originalCommit": {"oid": "a0bbf1a15236d587e5e84c5eb112752594659c77"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODkxMDczOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMTozNDoxOVrOHB7okQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMTozNDoxOVrOHB7okQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc4NzY2NQ==", "bodyText": "Maybe move this assert to top of method?", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471787665", "createdAt": "2020-08-17T21:34:19Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -630,6 +694,64 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     return r;\n   }\n \n+  private StandardDirectoryReader finishGetReaderMerge(AtomicBoolean hasTimedOut, Map<String, SegmentReader> mergedReaders,\n+                                                       Map<String, SegmentReader> openedReadOnlyClones, SegmentInfos openingSegmentInfos,\n+                                                       boolean applyAllDeletes, boolean writeAllDeletes,\n+                                                       MergePolicy.MergeSpecification onCommitMerges, long maxCommitMergeWaitMillis) throws IOException {\n+    boolean replaceReaderSuccess = false;\n+    try {\n+      mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+      onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+      assert openingSegmentInfos != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0bbf1a15236d587e5e84c5eb112752594659c77"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0ODkxMzY2OnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMTozNToyMlrOHB7qWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowNzo1NlrOHCGK1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc4ODEyMw==", "bodyText": "Hmm, what if the merges finished before the timeout?  The await would return early, and return true if it did not timeout (I think?).  Maybe we do not need/care to distinguish that?  In which case maybe renamed hasTimedOut to something else (mergesFinished?).", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471788123", "createdAt": "2020-08-17T21:35:22Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -630,6 +694,64 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     return r;\n   }\n \n+  private StandardDirectoryReader finishGetReaderMerge(AtomicBoolean hasTimedOut, Map<String, SegmentReader> mergedReaders,\n+                                                       Map<String, SegmentReader> openedReadOnlyClones, SegmentInfos openingSegmentInfos,\n+                                                       boolean applyAllDeletes, boolean writeAllDeletes,\n+                                                       MergePolicy.MergeSpecification onCommitMerges, long maxCommitMergeWaitMillis) throws IOException {\n+    boolean replaceReaderSuccess = false;\n+    try {\n+      mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+      onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+      assert openingSegmentInfos != null;\n+      synchronized (this) {\n+        hasTimedOut.set(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0bbf1a15236d587e5e84c5eb112752594659c77"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk2MDI3Ng==", "bodyText": "I renamed it to stopCollectingMergedReaders", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r471960276", "createdAt": "2020-08-18T07:07:56Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -630,6 +694,64 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     return r;\n   }\n \n+  private StandardDirectoryReader finishGetReaderMerge(AtomicBoolean hasTimedOut, Map<String, SegmentReader> mergedReaders,\n+                                                       Map<String, SegmentReader> openedReadOnlyClones, SegmentInfos openingSegmentInfos,\n+                                                       boolean applyAllDeletes, boolean writeAllDeletes,\n+                                                       MergePolicy.MergeSpecification onCommitMerges, long maxCommitMergeWaitMillis) throws IOException {\n+    boolean replaceReaderSuccess = false;\n+    try {\n+      mergeScheduler.merge(mergeSource, MergeTrigger.GET_READER);\n+      onCommitMerges.await(maxCommitMergeWaitMillis, TimeUnit.MILLISECONDS);\n+      assert openingSegmentInfos != null;\n+      synchronized (this) {\n+        hasTimedOut.set(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTc4ODEyMw=="}, "originalCommit": {"oid": "a0bbf1a15236d587e5e84c5eb112752594659c77"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MTMyMDEwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/ReaderPool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMjo0NjozM1rOHCSCOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjoxMToyN1rOHCbyWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE1NDY4Mg==", "bodyText": "I love seeing diffs like this one, adding a String message to an otherwise cryptic assert!  It makes me realize you must have had a hellacious debugging session!", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472154682", "createdAt": "2020-08-18T12:46:33Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/ReaderPool.java", "diffHunk": "@@ -404,7 +404,7 @@ private PendingDeletes newPendingDeletes(SegmentReader reader, SegmentCommitInfo\n   private boolean noDups() {\n     Set<String> seen = new HashSet<>();\n     for(SegmentCommitInfo info : readerMap.keySet()) {\n-      assert !seen.contains(info.info.name);\n+      assert !seen.contains(info.info.name) : \"seen twice: \" + info.info.name ;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ade49d5fb05cb811404f025053b8c5bc1d5a55f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMxNDQ1OA==", "bodyText": "many fun issues in this PR to be honest. IW is tricky as hell in some places like we are incRefing files in StandardDirectoryReader but not in IW for NRT readers is mindblowing :D", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472314458", "createdAt": "2020-08-18T16:11:27Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/ReaderPool.java", "diffHunk": "@@ -404,7 +404,7 @@ private PendingDeletes newPendingDeletes(SegmentReader reader, SegmentCommitInfo\n   private boolean noDups() {\n     Set<String> seen = new HashSet<>();\n     for(SegmentCommitInfo info : readerMap.keySet()) {\n-      assert !seen.contains(info.info.name);\n+      assert !seen.contains(info.info.name) : \"seen twice: \" + info.info.name ;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE1NDY4Mg=="}, "originalCommit": {"oid": "0ade49d5fb05cb811404f025053b8c5bc1d5a55f"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MjEzMTkxOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTozMDowM1rOHCaDvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0ODowNFrOHCdNrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4NjE0MQ==", "bodyText": "Should we keep track the clones iff maxFullFlushMergeWaitMillis is positive? I know this is not expensive.", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472286141", "createdAt": "2020-08-18T15:30:03Z", "author": {"login": "dnhatn"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -545,18 +546,54 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     // obtained during this flush are pooled, the first time\n     // this method is called:\n     readerPool.enableReaderPooling();\n-    DirectoryReader r = null;\n+    StandardDirectoryReader r = null;\n     doBeforeFlush();\n-    boolean anyChanges = false;\n+    boolean anyChanges;\n     /*\n      * for releasing a NRT reader we must ensure that \n      * DW doesn't add any segments or deletes until we are\n      * done with creating the NRT DirectoryReader. \n      * We release the two stage full flush after we are done opening the\n      * directory reader!\n      */\n+    MergePolicy.MergeSpecification onGetReaderMerges = null;\n+    AtomicBoolean stopCollectingMergedReaders = new AtomicBoolean(false);\n+    Map<String, SegmentReader> mergedReaders = new HashMap<>();\n+    Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();\n+    // this function is used to control which SR are opened in order to keep track of them\n+    // and to reuse them in the case we wait for merges in this getReader call.\n+    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory = sci -> {\n+      final ReadersAndUpdates rld = getPooledInstance(sci, true);\n+      try {\n+        assert Thread.holdsLock(IndexWriter.this);\n+        SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);\n+        openedReadOnlyClones.put(sci.info.name, segmentReader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ade49d5fb05cb811404f025053b8c5bc1d5a55f"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNzgzOQ==", "bodyText": "++", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472337839", "createdAt": "2020-08-18T16:48:04Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -545,18 +546,54 @@ DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) thro\n     // obtained during this flush are pooled, the first time\n     // this method is called:\n     readerPool.enableReaderPooling();\n-    DirectoryReader r = null;\n+    StandardDirectoryReader r = null;\n     doBeforeFlush();\n-    boolean anyChanges = false;\n+    boolean anyChanges;\n     /*\n      * for releasing a NRT reader we must ensure that \n      * DW doesn't add any segments or deletes until we are\n      * done with creating the NRT DirectoryReader. \n      * We release the two stage full flush after we are done opening the\n      * directory reader!\n      */\n+    MergePolicy.MergeSpecification onGetReaderMerges = null;\n+    AtomicBoolean stopCollectingMergedReaders = new AtomicBoolean(false);\n+    Map<String, SegmentReader> mergedReaders = new HashMap<>();\n+    Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();\n+    // this function is used to control which SR are opened in order to keep track of them\n+    // and to reuse them in the case we wait for merges in this getReader call.\n+    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory = sci -> {\n+      final ReadersAndUpdates rld = getPooledInstance(sci, true);\n+      try {\n+        assert Thread.holdsLock(IndexWriter.this);\n+        SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);\n+        openedReadOnlyClones.put(sci.info.name, segmentReader);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4NjE0MQ=="}, "originalCommit": {"oid": "0ade49d5fb05cb811404f025053b8c5bc1d5a55f"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MjQyMDYwOnYy", "diffSide": "RIGHT", "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjozOTo0MlrOHCc5fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNjo0NTo0NlrOHCdISA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzMjY3MQ==", "bodyText": "Should we rename this to stopCollectingMergedReaders? I find that name better.", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472332671", "createdAt": "2020-08-18T16:39:42Z", "author": {"login": "dnhatn"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3179,9 +3317,9 @@ private long prepareCommitInternal() throws IOException {\n       SegmentInfos toCommit = null;\n       boolean anyChanges = false;\n       long seqNo;\n-      MergePolicy.MergeSpecification onCommitMerges = null;\n-      AtomicBoolean includeInCommit = new AtomicBoolean(true);\n-      final long maxCommitMergeWaitMillis = config.getMaxCommitMergeWaitMillis();\n+      MergePolicy.MergeSpecification pointInTimeMerges = null;\n+      AtomicBoolean hasTimedOut = new AtomicBoolean(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83d666eb9702949f11b5195a4a7ce81145750495"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzNjQ1Ng==", "bodyText": "++ I change it to a better name", "url": "https://github.com/apache/lucene-solr/pull/1623#discussion_r472336456", "createdAt": "2020-08-18T16:45:46Z", "author": {"login": "s1monw"}, "path": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java", "diffHunk": "@@ -3179,9 +3317,9 @@ private long prepareCommitInternal() throws IOException {\n       SegmentInfos toCommit = null;\n       boolean anyChanges = false;\n       long seqNo;\n-      MergePolicy.MergeSpecification onCommitMerges = null;\n-      AtomicBoolean includeInCommit = new AtomicBoolean(true);\n-      final long maxCommitMergeWaitMillis = config.getMaxCommitMergeWaitMillis();\n+      MergePolicy.MergeSpecification pointInTimeMerges = null;\n+      AtomicBoolean hasTimedOut = new AtomicBoolean(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjMzMjY3MQ=="}, "originalCommit": {"oid": "83d666eb9702949f11b5195a4a7ce81145750495"}, "originalPosition": 224}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1349, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}