{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNzQxODU5", "number": 1573, "title": "Cleanup TermsHashPerField", "bodyText": "Several classes within the IndexWriter indexing chain haven't been touched for several years. Most of these classes expose their internals through public members and are difficult to construct in tests since they depend on many other classes. This change tries to clean up TermsHashPerField and adds a dedicated standalone test for it to make it more accessible for other developers since it's simpler to understand. There are also attempts to make documentation better as a result of this refactoring.", "createdAt": "2020-06-12T15:18:21Z", "url": "https://github.com/apache/lucene-solr/pull/1573", "merged": true, "mergeCommit": {"oid": "c083e5414e232e154d70bbb446d03cef0684dbe6"}, "closed": true, "closedAt": "2020-06-16T12:45:46Z", "author": {"login": "s1monw"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcnr4TugH2gAyNDMzNzQxODU5OmExN2Q5YzA5OGQ2Y2YzNzA5ZWE0MzU1MzZhZjhhNDlkMDA4ZmRhZGI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcr0ctjgH2gAyNDMzNzQxODU5OjVlYmJmYTFkMDY4MTQwZjAwNmYxYzgxMDNhMmJiNTQxZDUwMWEzOWM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a17d9c098d6cf3709ea435536af8a49d008fdadb", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/a17d9c098d6cf3709ea435536af8a49d008fdadb", "committedDate": "2020-06-03T16:16:33Z", "message": "first cut"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26d204b23f3072dfd8b82e1b6be1127594f2ecc5", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/26d204b23f3072dfd8b82e1b6be1127594f2ecc5", "committedDate": "2020-06-10T14:52:00Z", "message": "fo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "103740fceb32f5f697f945127a2f9369c4f31510", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/103740fceb32f5f697f945127a2f9369c4f31510", "committedDate": "2020-06-12T14:55:52Z", "message": " test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b2b7c9df1ac6fc9f97077d979d40e07afd30399", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/4b2b7c9df1ac6fc9f97077d979d40e07afd30399", "committedDate": "2020-06-12T15:11:27Z", "message": "foo\n\n:"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2810b5b02c07cb0f9a94fa02c9d89166c7534483", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/2810b5b02c07cb0f9a94fa02c9d89166c7534483", "committedDate": "2020-06-12T15:12:18Z", "message": "Merge branch 'master' into cleanup_terms_hash_per_field"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9930367ddef17f17b379caba92dee68ff2323f1b", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/9930367ddef17f17b379caba92dee68ff2323f1b", "committedDate": "2020-06-13T12:07:37Z", "message": "Merge branch 'master' into cleanup_terms_hash_per_field"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57d00439a91e79dc22803908267cbb44ef0ec825", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/57d00439a91e79dc22803908267cbb44ef0ec825", "committedDate": "2020-06-13T12:23:52Z", "message": "remove unnecessary member"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3d20dc60c8f3c502810c8867cdb30642e851cd8", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/d3d20dc60c8f3c502810c8867cdb30642e851cd8", "committedDate": "2020-06-13T13:39:51Z", "message": "add more docs and rename vars to improve readability"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/acacaabcab06cf2d92057c0f8ced57431eb3964b", "committedDate": "2020-06-13T13:45:12Z", "message": "fix typo"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMTUwMzU4", "url": "https://github.com/apache/lucene-solr/pull/1573#pullrequestreview-430150358", "createdAt": "2020-06-13T16:31:50Z", "commit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxNjozMTo1MFrOGjYSsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QxODowNjo0N1rOGjYqzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM0Nw==", "bodyText": "Hmm why not keep this assertion (to confirm that if the field is not somehow indexed we are not accidentally/incorrectly running this code)?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751347", "createdAt": "2020-06-13T16:31:50Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -207,8 +202,6 @@ public void newPostingsArray() {\n \n   @Override\n   ParallelPostingsArray createPostingsArray(int size) {\n-    IndexOptions indexOptions = fieldInfo.getIndexOptions();\n-    assert indexOptions != IndexOptions.NONE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTM3Ng==", "bodyText": "s/terms text/terms's text", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751376", "createdAt": "2020-06-13T16:32:21Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "diffHunk": "@@ -22,14 +22,14 @@\n   final static int BYTES_PER_POSTING = 3 * Integer.BYTES;\n \n   final int size;\n-  final int[] textStarts;\n-  final int[] intStarts;\n-  final int[] byteStarts;\n+  final int[] textStarts; // maps term ID to the terms text start in the bytesHash", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQxMQ==", "bodyText": "+1 for this renaming!", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751411", "createdAt": "2020-06-13T16:32:45Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/ParallelPostingsArray.java", "diffHunk": "@@ -22,14 +22,14 @@\n   final static int BYTES_PER_POSTING = 3 * Integer.BYTES;\n \n   final int size;\n-  final int[] textStarts;\n-  final int[] intStarts;\n-  final int[] byteStarts;\n+  final int[] textStarts; // maps term ID to the terms text start in the bytesHash\n+  final int[] addressOffset; // maps term ID to current stream address", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTQzNg==", "bodyText": "Hmm, did these aggregations move somewhere else?  Oh, they look entirely removed?  Were they redundant (computed elsewhere) and these ones were unused?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751436", "createdAt": "2020-06-13T16:33:30Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java", "diffHunk": "@@ -56,12 +56,6 @@ public FreqProxTermsWriterPerField(FieldInvertState invertState, TermsHash terms\n   @Override\n   void finish() throws IOException {\n     super.finish();\n-    sumDocFreq += fieldState.uniqueTermCount;\n-    sumTotalTermFreq += fieldState.length;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTYwNw==", "bodyText": "Hmm docID is unused in this method?  But I guess the other impl (normal postings) needs it?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751607", "createdAt": "2020-06-13T16:36:04Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java", "diffHunk": "@@ -222,7 +234,7 @@ void writeProx(TermVectorsPostingsArray postings, int termID) {\n   }\n \n   @Override\n-  void newTerm(final int termID) {\n+  void newTerm(final int termID, final int docID) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTcxMg==", "bodyText": "Thank you for the javadocs/comments for such cryptic and ancient code ;)\nMaybe just stores instead of allows to store?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751712", "createdAt": "2020-06-13T16:37:43Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1MTc3OA==", "bodyText": "Ahh I see, we just moved the assertion to a better place, awesome.", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439751778", "createdAt": "2020-06-13T16:39:14Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTc1NzUxNg==", "bodyText": "s/allocated/allocate", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439757516", "createdAt": "2020-06-13T18:06:47Z", "author": {"login": "mikemccand"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class allows to store streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;\n+    this.indexOptions = indexOptions;\n     PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);\n     bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);\n   }\n \n   void reset() {\n     bytesHash.clear(false);\n+    sortedTermIDs = null;\n     if (nextPerField != null) {\n       nextPerField.reset();\n     }\n   }\n \n-  public void initReader(ByteSliceReader reader, int termID, int stream) {\n+  final void initReader(ByteSliceReader reader, int termID, int stream) {\n     assert stream < streamCount;\n-    int intStart = postingsArray.intStarts[termID];\n-    final int[] ints = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n-    final int upto = intStart & IntBlockPool.INT_BLOCK_MASK;\n+    int streamStartOffset = postingsArray.addressOffset[termID];\n+    final int[] streamAddressBuffer = intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];\n+    final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;\n     reader.init(bytePool,\n                 postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,\n-                ints[upto+stream]);\n+                streamAddressBuffer[offsetInAddressBuffer+stream]);\n   }\n \n-  int[] sortedTermIDs;\n+  private int[] sortedTermIDs;\n \n   /** Collapse the hash table and sort in-place; also sets\n-   * this.sortedTermIDs to the results */\n-  public int[] sortPostings() {\n+   * this.sortedTermIDs to the results\n+   * This method should not be called twice unless {@link #reset()}\n+   * or {@link #reinitHash()} was called. */\n+  final void sortTerms() {\n+    assert sortedTermIDs == null;\n     sortedTermIDs = bytesHash.sort();\n+  }\n+\n+  /**\n+   * Returns the sorted term IDs. {@link #sortTerms()} must be called before\n+   */\n+  final int[] getSortedTermIDs() {\n+    assert sortedTermIDs != null;\n     return sortedTermIDs;\n   }\n \n+  final void reinitHash() {\n+    sortedTermIDs = null;\n+    bytesHash.reinit();\n+  }\n+\n   private boolean doNextCall;\n \n   // Secondary entry point (for 2nd & subsequent TermsHash),\n   // because token text has already been \"interned\" into\n   // textStart, so we hash by textStart.  term vectors use\n   // this API.\n-  public void add(int textStart) throws IOException {\n+  private void add(int textStart, final int docID) throws IOException {\n     int termID = bytesHash.addByPoolOffset(textStart);\n     if (termID >= 0) {      // New posting\n       // First time we are seeing this token since we last\n       // flushed the hash.\n-      // Init stream slices\n-      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n-        intPool.nextBuffer();\n-      }\n-\n-      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n-        bytePool.nextBuffer();\n-      }\n+      initStreamSlices(termID, docID);\n+    } else {\n+      positionStreamSlice(termID, docID);\n+    }\n+  }\n \n-      intUptos = intPool.buffer;\n-      intUptoStart = intPool.intUpto;\n-      intPool.intUpto += streamCount;\n+  private void initStreamSlices(int termID, int docID) throws IOException {\n+    // Init stream slices\n+    // TODO: figure out why this is 2*streamCount here. streamCount should be enough?\n+    if ((2*streamCount) + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n+      // can we fit all the streams in the current buffer?\n+      intPool.nextBuffer();\n+    }\n \n-      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n+    if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < (2*streamCount) * ByteBlockPool.FIRST_LEVEL_SIZE) {\n+      // can we fit at least one byte per stream in the current buffer, if not allocated a new one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acacaabcab06cf2d92057c0f8ced57431eb3964b"}, "originalPosition": 178}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/6b02c1b8aaf5b46f59a046b76b4852a6cec176dc", "committedDate": "2020-06-14T08:21:01Z", "message": "apply feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMjQyODkw", "url": "https://github.com/apache/lucene-solr/pull/1573#pullrequestreview-430242890", "createdAt": "2020-06-14T18:55:42Z", "commit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NTo0MlrOGjezAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NTo0MlrOGjezAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1NzkyMQ==", "bodyText": "this the?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439857921", "createdAt": "2020-06-14T18:55:42Z", "author": {"login": "dweiss"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class stores streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMjQyOTk2", "url": "https://github.com/apache/lucene-solr/pull/1573#pullrequestreview-430242996", "createdAt": "2020-06-14T18:57:14Z", "commit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1NzoxNFrOGjezfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxODo1ODo1MlrOGjez_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1ODA0Ng==", "bodyText": "should not -> must not perhaps?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439858046", "createdAt": "2020-06-14T18:57:14Z", "author": {"login": "dweiss"}, "path": "lucene/core/src/java/org/apache/lucene/index/TermsHashPerField.java", "diffHunk": "@@ -19,203 +19,207 @@\n \n import java.io.IOException;\n \n-import org.apache.lucene.analysis.tokenattributes.TermFrequencyAttribute;\n-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.BytesRefHash.BytesStartArray;\n import org.apache.lucene.util.BytesRefHash;\n import org.apache.lucene.util.Counter;\n import org.apache.lucene.util.IntBlockPool;\n \n+/**\n+ * This class stores streams of information per term without knowing\n+ * the size of the stream ahead of time. Each stream typically encodes one level\n+ * of information like term frequency per document or term proximity. Internally\n+ * this class allocates a linked list of slices that can be read by a {@link ByteSliceReader}\n+ * for each term. Terms are first deduplicated in a {@link BytesRefHash} once this is done\n+ * internal data-structures point to the current offset of each stream that can be written to.\n+ */\n abstract class TermsHashPerField implements Comparable<TermsHashPerField> {\n   private static final int HASH_INIT_SIZE = 4;\n \n-  final TermsHash termsHash;\n-\n-  final TermsHashPerField nextPerField;\n-  protected final DocumentsWriterPerThread.DocState docState;\n-  protected final FieldInvertState fieldState;\n-  TermToBytesRefAttribute termAtt;\n-  protected TermFrequencyAttribute termFreqAtt;\n-\n-  // Copied from our perThread\n-  final IntBlockPool intPool;\n+  private final TermsHashPerField nextPerField;\n+  private final IntBlockPool intPool;\n   final ByteBlockPool bytePool;\n-  final ByteBlockPool termBytePool;\n-\n-  final int streamCount;\n-  final int numPostingInt;\n-\n-  protected final FieldInfo fieldInfo;\n-\n-  final BytesRefHash bytesHash;\n+  // for each term we store an integer per stream that points into the bytePool above\n+  // the address is updated once data is written to the stream to point to the next free offset\n+  // this the terms stream. The start address for the stream is stored in postingsArray.byteStarts[termId]\n+  // This is initialized in the #addTerm method, either to a brand new per term stream if the term is new or\n+  // to the addresses where the term stream was written to when we saw it the last time.\n+  private int[] termStreamAddressBuffer;\n+  private int streamAddressOffset;\n+  private final int streamCount;\n+  private final String fieldName;\n+  final IndexOptions indexOptions;\n+  /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this\n+  * TermsHashPerField is hashing term vectors.*/\n+  private final BytesRefHash bytesHash;\n \n   ParallelPostingsArray postingsArray;\n-  private final Counter bytesUsed;\n+  private int lastDocID; // only with assert\n \n   /** streamCount: how many streams this field stores per term.\n    * E.g. doc(+freq) is 1 stream, prox+offset is a second. */\n-\n-  public TermsHashPerField(int streamCount, FieldInvertState fieldState, TermsHash termsHash, TermsHashPerField nextPerField, FieldInfo fieldInfo) {\n-    intPool = termsHash.intPool;\n-    bytePool = termsHash.bytePool;\n-    termBytePool = termsHash.termBytePool;\n-    docState = termsHash.docState;\n-    this.termsHash = termsHash;\n-    bytesUsed = termsHash.bytesUsed;\n-    this.fieldState = fieldState;\n+  TermsHashPerField(int streamCount, IntBlockPool intPool, ByteBlockPool bytePool, ByteBlockPool termBytePool,\n+                    Counter bytesUsed, TermsHashPerField nextPerField, String fieldName, IndexOptions indexOptions) {\n+    this.intPool = intPool;\n+    this.bytePool = bytePool;\n     this.streamCount = streamCount;\n-    numPostingInt = 2*streamCount;\n-    this.fieldInfo = fieldInfo;\n+    this.fieldName = fieldName;\n     this.nextPerField = nextPerField;\n+    assert indexOptions != IndexOptions.NONE;\n+    this.indexOptions = indexOptions;\n     PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);\n     bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);\n   }\n \n   void reset() {\n     bytesHash.clear(false);\n+    sortedTermIDs = null;\n     if (nextPerField != null) {\n       nextPerField.reset();\n     }\n   }\n \n-  public void initReader(ByteSliceReader reader, int termID, int stream) {\n+  final void initReader(ByteSliceReader reader, int termID, int stream) {\n     assert stream < streamCount;\n-    int intStart = postingsArray.intStarts[termID];\n-    final int[] ints = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n-    final int upto = intStart & IntBlockPool.INT_BLOCK_MASK;\n+    int streamStartOffset = postingsArray.addressOffset[termID];\n+    final int[] streamAddressBuffer = intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];\n+    final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;\n     reader.init(bytePool,\n                 postingsArray.byteStarts[termID]+stream*ByteBlockPool.FIRST_LEVEL_SIZE,\n-                ints[upto+stream]);\n+                streamAddressBuffer[offsetInAddressBuffer+stream]);\n   }\n \n-  int[] sortedTermIDs;\n+  private int[] sortedTermIDs;\n \n   /** Collapse the hash table and sort in-place; also sets\n-   * this.sortedTermIDs to the results */\n-  public int[] sortPostings() {\n+   * this.sortedTermIDs to the results\n+   * This method should not be called twice unless {@link #reset()}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1ODE3Mw==", "bodyText": "extra space?", "url": "https://github.com/apache/lucene-solr/pull/1573#discussion_r439858173", "createdAt": "2020-06-14T18:58:52Z", "author": {"login": "dweiss"}, "path": "lucene/core/src/test/org/apache/lucene/index/TestTermsHashPerField.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.lucene.index;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+\n+import com.carrotsearch.randomizedtesting.generators.RandomPicks;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import org.apache.lucene.util.ByteBlockPool;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.Counter;\n+import org.apache.lucene.util.IntBlockPool;\n+import org.apache.lucene.util.LuceneTestCase;\n+\n+public class TestTermsHashPerField extends LuceneTestCase  {\n+\n+  private static TermsHashPerField createNewHash(AtomicInteger newCalled, AtomicInteger addCalled) {\n+    IntBlockPool intBlockPool = new IntBlockPool();\n+    ByteBlockPool byteBlockPool = new ByteBlockPool(new ByteBlockPool.DirectAllocator());\n+    ByteBlockPool termBlockPool = new ByteBlockPool(new ByteBlockPool.DirectAllocator());\n+\n+    TermsHashPerField hash = new TermsHashPerField(1, intBlockPool, byteBlockPool, termBlockPool, Counter.newCounter(),\n+        null, \"testfield\", IndexOptions.DOCS_AND_FREQS) {\n+\n+      private FreqProxTermsWriterPerField.FreqProxPostingsArray freqProxPostingsArray;\n+\n+      @Override\n+      void newTerm(int termID, int docID) {\n+        newCalled.incrementAndGet();\n+        FreqProxTermsWriterPerField.FreqProxPostingsArray postings = freqProxPostingsArray;\n+        postings.lastDocIDs[termID] = docID;\n+        postings.lastDocCodes[termID] = docID << 1;\n+        postings.termFreqs[termID] = 1;\n+      }\n+\n+      @Override\n+      void addTerm(int termID, int docID) {\n+        addCalled.incrementAndGet();\n+        FreqProxTermsWriterPerField.FreqProxPostingsArray postings = freqProxPostingsArray;\n+        if (docID != postings.lastDocIDs[termID]) {\n+          if (1 == postings.termFreqs[termID]) {\n+            writeVInt(0, postings.lastDocCodes[termID]|1);\n+          } else {\n+            writeVInt(0, postings.lastDocCodes[termID]);\n+            writeVInt(0, postings.termFreqs[termID]);\n+          }\n+          postings.termFreqs[termID] = 1;\n+          postings.lastDocCodes[termID] = (docID - postings.lastDocIDs[termID]) << 1;\n+          postings.lastDocIDs[termID] = docID;\n+        } else {\n+          postings.termFreqs[termID] = Math.addExact(postings.termFreqs[termID], 1);\n+        }\n+      }\n+\n+      @Override\n+      void newPostingsArray() {\n+        freqProxPostingsArray = (FreqProxTermsWriterPerField.FreqProxPostingsArray) postingsArray;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b02c1b8aaf5b46f59a046b76b4852a6cec176dc"}, "originalPosition": 81}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ef7fcb44d51cfb0ab6563f2db05709d182b1764", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/4ef7fcb44d51cfb0ab6563f2db05709d182b1764", "committedDate": "2020-06-15T18:13:49Z", "message": "apply feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e04aa190757b12bd93b0c71b4f5b721997700320", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/e04aa190757b12bd93b0c71b4f5b721997700320", "committedDate": "2020-06-15T18:14:11Z", "message": "Merge branch 'master' into cleanup_terms_hash_per_field"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ebbfa1d068140f006f1c8103a2bb541d501a39c", "author": {"user": {"login": "s1monw", "name": "Simon Willnauer"}}, "url": "https://github.com/apache/lucene-solr/commit/5ebbfa1d068140f006f1c8103a2bb541d501a39c", "committedDate": "2020-06-16T12:31:15Z", "message": "Merge branch 'master' into cleanup_terms_hash_per_field"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2615, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}