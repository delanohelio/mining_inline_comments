{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5NjI4MTQw", "number": 491, "reviewThreads": {"totalCount": 38, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1MToyOVrOET3xJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1OToyNFrOEYK9NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MjcyMTAzOnYy", "diffSide": "RIGHT", "path": "index.html", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1MToyOVrOG52exQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDozOTo0NVrOG_x_Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNDYyOQ==", "bodyText": "@bbejeck should producer precede consumer (swap lines)?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463314629", "createdAt": "2020-07-30T22:51:29Z", "author": {"login": "ybyzek"}, "path": "index.html", "diffHunk": "@@ -41,6 +41,7 @@ <h2 class=\"subtitle\">Apache Kafka is a powerful, scalable, fault-tolerant distri\n             <li><a href=\"kafka-console-consumer-read-specific-offsets-partitions/kafka.html\">Console consumer reads from a specific offset and partition</a></li>\n             <li><a href=\"change-topic-partitions-replicas/ksql.html\">Change the number of partitions and replicas of a Kafka topic</a></li>\n             <li><a href=\"creating-first-apache-kafka-consumer-application/kafka.html\">Build your first Kafka consumer application</a></li>\n+            <li><a href=\"creating-first-apache-kafka-producer-application/kafka.html\">Build your first Kafka producer application</a></li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzMjUxMQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469532511", "createdAt": "2020-08-12T20:39:45Z", "author": {"login": "bbejeck"}, "path": "index.html", "diffHunk": "@@ -41,6 +41,7 @@ <h2 class=\"subtitle\">Apache Kafka is a powerful, scalable, fault-tolerant distri\n             <li><a href=\"kafka-console-consumer-read-specific-offsets-partitions/kafka.html\">Console consumer reads from a specific offset and partition</a></li>\n             <li><a href=\"change-topic-partitions-replicas/ksql.html\">Change the number of partitions and replicas of a Kafka topic</a></li>\n             <li><a href=\"creating-first-apache-kafka-consumer-application/kafka.html\">Build your first Kafka consumer application</a></li>\n+            <li><a href=\"creating-first-apache-kafka-producer-application/kafka.html\">Build your first Kafka producer application</a></li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNDYyOQ=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MjcyNDIwOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1MzowM1rOG52gqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxODo1MTowNlrOG_uNNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTExNQ==", "bodyText": "Should this be materialized on a new line (see generated output -- it's on the same line as above)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463315115", "createdAt": "2020-07-30T22:53:03Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ3MDUxOA==", "bodyText": "Good catch, it's supposed to, I'm not sure why it's rendering that way.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469470518", "createdAt": "2020-08-12T18:51:06Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTExNQ=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MjcyNzY0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1NDo0MlrOG52isQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1NDo1NVrOG_0NAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTYzMw==", "bodyText": "Can you please elaborate, of all the producer configuration parameters, why mention just acks?\nIdentify the default value for acks\nConsider giving users a link to read up on other producer configuration parameters", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463315633", "createdAt": "2020-07-30T22:54:42Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.\n+`acks` - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, or `all`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ1MTY1NA==", "bodyText": "all the producer configuration parameters\n\nDo you mean those listed or all available configuration parameters?\nI think going through all configs would be too much here, probably better to provide a link.  The reason why I only covered acks is that one IMHO is the most important one to consider for the data durability and throughput trade-off.  The delivery.timeout is essential as well, so I'll update to add that one.\nEDIT: I'm thinking this is enough for this tutorial, but maybe we can have another more advanced one where we go into more configuration settings.\nWDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469451654", "createdAt": "2020-08-12T18:17:38Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.\n+`acks` - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, or `all`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTYzMw=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzNzU5Nw==", "bodyText": "Overall it's a balance -- share the most useful ones without overwhelming the user with all options.  Basically I just wanted to check how intentional it was that the KT shows just acks and the serializers.\nFYC whichever you decide to include, perhaps link to another resource to explore the other parameters.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469537597", "createdAt": "2020-08-12T20:48:17Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.\n+`acks` - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, or `all`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTYzMw=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2ODc3MA==", "bodyText": "Sounds good.  I updated it to include links to the config parameters discussed.  I also added a link for the full producer docs.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469568770", "createdAt": "2020-08-12T21:54:55Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.\n+`acks` - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, or `all`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTYzMw=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MjczMzI4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1NzowNFrOG52lzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo0MDozNFrOG_yDdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNjQzMA==", "bodyText": "The # make it a bit hard to read. For consideration, use , or : instead", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463316430", "createdAt": "2020-07-30T22:57:04Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc", "diffHunk": "@@ -0,0 +1,16 @@\n+////\n+   Example content file for how to include a console produer(s) in the tutorial.\n+   Usually you'll include a line referencing the script to run the console producer and also include some content\n+   describing how to input data as shown below.\n+\n+   Again modify this file as you need for your tutorial, as this is just sample content.  You also may have more than one\n+   console producer to run depending on how you structure your tutorial\n+\n+////\n+\n+Create the following file `input.txt` in the base directory of the tutorial.  The numbers before the `#` will be the key and the part after will be the value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzMzU1OQ==", "bodyText": "ack, updated to use a - instead", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469533559", "createdAt": "2020-08-12T20:40:34Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc", "diffHunk": "@@ -0,0 +1,16 @@\n+////\n+   Example content file for how to include a console produer(s) in the tutorial.\n+   Usually you'll include a line referencing the script to run the console producer and also include some content\n+   describing how to input data as shown below.\n+\n+   Again modify this file as you need for your tutorial, as this is just sample content.  You also may have more than one\n+   console producer to run depending on how you structure your tutorial\n+\n+////\n+\n+Create the following file `input.txt` in the base directory of the tutorial.  The numbers before the `#` will be the key and the part after will be the value.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNjQzMA=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MjczODM2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1OTo0OVrOG52pIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1Nzo0MFrOHAXRew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg==", "bodyText": "Why is this implemented with the user being prompted to provide path name, instead of just an argument appended to java -jar build/libs/kafka-producer-application-standalone-0.0.1.jar configuration/dev.properties?  FYC, the latter has better UX", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463317282", "createdAt": "2020-07-30T22:59:49Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n+           } catch (InterruptedException | ExecutionException e) {\n+               if (e instanceof  InterruptedException) {\n+                  Thread.currentThread().interrupt();\n+               }\n+           }\n+         });\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 1) {\n+      throw new IllegalArgumentException(\n+          \"This program takes one argument: the path to an environment configuration file.\");\n+    }\n+\n+    final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+    final String topic = props.getProperty(\"output.topic.name\");\n+    final Producer<String, String> producer = new KafkaProducer<>(props);\n+    final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+\n+    try (final BufferedReader stdinReader = new BufferedReader(new InputStreamReader(System.in))) {\n+      String filePath;\n+      System.out.println(\"Enter the file path to publish records > \");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzNDM2OQ==", "bodyText": "I thought about the same thing, but I wanted the user to be able to experiment and process additional files if desired.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469534369", "createdAt": "2020-08-12T20:41:51Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n+           } catch (InterruptedException | ExecutionException e) {\n+               if (e instanceof  InterruptedException) {\n+                  Thread.currentThread().interrupt();\n+               }\n+           }\n+         });\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 1) {\n+      throw new IllegalArgumentException(\n+          \"This program takes one argument: the path to an environment configuration file.\");\n+    }\n+\n+    final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+    final String topic = props.getProperty(\"output.topic.name\");\n+    final Producer<String, String> producer = new KafkaProducer<>(props);\n+    final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+\n+    try (final BufferedReader stdinReader = new BufferedReader(new InputStreamReader(System.in))) {\n+      String filePath;\n+      System.out.println(\"Enter the file path to publish records > \");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MzM1NQ==", "bodyText": "If a user wants to process additional files, could they just run it again?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470143355", "createdAt": "2020-08-13T17:57:40Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n+           } catch (InterruptedException | ExecutionException e) {\n+               if (e instanceof  InterruptedException) {\n+                  Thread.currentThread().interrupt();\n+               }\n+           }\n+         });\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 1) {\n+      throw new IllegalArgumentException(\n+          \"This program takes one argument: the path to an environment configuration file.\");\n+    }\n+\n+    final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+    final String topic = props.getProperty(\"output.topic.name\");\n+    final Producer<String, String> producer = new KafkaProducer<>(props);\n+    final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+\n+    try (final BufferedReader stdinReader = new BufferedReader(new InputStreamReader(System.in))) {\n+      String filePath;\n+      System.out.println(\"Enter the file path to publish records > \");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjc0NDA2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMzowMjo0NVrOG52spw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo0MjowM1rOG_yHBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxODE4Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                         System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n          \n          \n            \n                         System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" and timestamp \" + recordMetadata.timestamp());", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463318183", "createdAt": "2020-07-30T23:02:45Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzNDQ3MQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469534471", "createdAt": "2020-08-12T20:42:03Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxODE4Mw=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk1NTMzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1MzozNVrOG54nOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMTo1NlrOG_zmpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTU2Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n          \n          \n            \n            Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463349563", "createdAt": "2020-07-31T00:53:35Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1ODk0OA==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469558948", "createdAt": "2020-08-12T21:31:56Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTU2Mw=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk1NzQzOnYy", "diffSide": "RIGHT", "path": "_data/harnesses/kafka-producer-application/kafka.yml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NDo0MlrOG54oYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1NToxMlrOG_0Ngg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTg1Ng==", "bodyText": "FYC would this be appropriate to name as Write a unit test (note term unit is added)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463349856", "createdAt": "2020-07-31T00:54:42Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-producer-application/kafka.yml", "diffHunk": "@@ -0,0 +1,166 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Create a topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/harness-create-topic.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc\n+\n+    - title: Create the KafkaProducer application\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-src-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-src-dir.adoc\n+            \n+        - action: make_file\n+          file: src/main/java/io/confluent/developer/KafkaProducerApplication.java\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc \n+\n+    - title: Create data to produce to Kafka\n+      content:\n+        - action: make_file\n+          file: input.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc        \n+\n+    - title: Compile and run the KafkaProducer application\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/build-uberjar.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/build-uberjar.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/run-dev-app.sh\n+          stdin: tutorial-steps/dev/input-harness-filename.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/run-dev-app.adoc\n+\n+    - title: Confirm records sent by consuming from topic\n+      content:\n+        - action: execute_async\n+          file: tutorial-steps/dev/harness-console-consumer.sh\n+          stdout: tutorial-steps/dev/outputs/actual-output.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/run-consumer.adoc\n+\n+        - name: wait for the consumer to read the messages\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+test:\n+  steps:\n+    - title: Create a test configuration file\n+      content:\n+        - action: make_file\n+          file: configuration/test.properties\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/test/make-test-file.adoc\n+\n+    - title: Write a test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2ODg5OA==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469568898", "createdAt": "2020-08-12T21:55:12Z", "author": {"login": "bbejeck"}, "path": "_data/harnesses/kafka-producer-application/kafka.yml", "diffHunk": "@@ -0,0 +1,166 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Create a topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/harness-create-topic.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc\n+\n+    - title: Create the KafkaProducer application\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-src-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-src-dir.adoc\n+            \n+        - action: make_file\n+          file: src/main/java/io/confluent/developer/KafkaProducerApplication.java\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc \n+\n+    - title: Create data to produce to Kafka\n+      content:\n+        - action: make_file\n+          file: input.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc        \n+\n+    - title: Compile and run the KafkaProducer application\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/build-uberjar.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/build-uberjar.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/run-dev-app.sh\n+          stdin: tutorial-steps/dev/input-harness-filename.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/run-dev-app.adoc\n+\n+    - title: Confirm records sent by consuming from topic\n+      content:\n+        - action: execute_async\n+          file: tutorial-steps/dev/harness-console-consumer.sh\n+          stdout: tutorial-steps/dev/outputs/actual-output.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/run-consumer.adoc\n+\n+        - name: wait for the consumer to read the messages\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+test:\n+  steps:\n+    - title: Create a test configuration file\n+      content:\n+        - action: make_file\n+          file: configuration/test.properties\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/test/make-test-file.adoc\n+\n+    - title: Write a test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTg1Ng=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk1ODkzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NToyOVrOG54pNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMjowN1rOG_zm6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDA2OA==", "bodyText": "FYC: should all these be changed from *-ing to just *, e.g. Calling -> Call", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350068", "createdAt": "2020-07-31T00:55:29Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTAxNg==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469559016", "createdAt": "2020-08-12T21:32:07Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDA2OA=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk1OTU5OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NTo1OFrOG54pnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMzoyMlrOG_zo7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDE3Mg==", "bodyText": "What precisely is \"validated\"?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350172", "createdAt": "2020-07-31T00:55:58Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method\n+<2> Building the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to validate the records sent to the producer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTUzMw==", "bodyText": "@ybyzek I clarified this, let me know what you think", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469559533", "createdAt": "2020-08-12T21:33:22Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method\n+<2> Building the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to validate the records sent to the producer", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDE3Mg=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk2MDcxOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/code/configuration/prod.properties", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NjozOFrOG54qTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMzozMlrOG_zpMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDM0OQ==", "bodyText": "Is input topic required here, like the other stages?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350349", "createdAt": "2020-07-31T00:56:38Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/configuration/prod.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+bootstrap.servers=<FILL ME IN>\n+\n+key.serializer=org.apache.kafka.common.serialization.StringSerializer\n+value.serializer=org.apache.kafka.common.serialization.StringSerializer\n+acks=all\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTYwMg==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469559602", "createdAt": "2020-08-12T21:33:32Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/configuration/prod.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+bootstrap.servers=<FILL ME IN>\n+\n+key.serializer=org.apache.kafka.common.serialization.StringSerializer\n+value.serializer=org.apache.kafka.common.serialization.StringSerializer\n+acks=all\n+\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDM0OQ=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk2MjgyOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1Nzo1N1rOG54rfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1NTozMFrOG_0OGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDY1Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now create the following file at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.\n          \n          \n            \n            Including the code snippet explained above, now create the complete test code at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350652", "createdAt": "2020-07-31T00:57:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method\n+<2> Building the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to validate the records sent to the producer\n+\n+Now create the following file at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTA1MA==", "bodyText": "I didn't include this (or the other similar comment) as the users copy all the code when clicking on the \"copy\" icon on the right. Any other wording suggestions?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469569050", "createdAt": "2020-08-12T21:55:30Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method\n+<2> Building the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to validate the records sent to the producer\n+\n+Now create the following file at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDY1Mg=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk2NDY0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/code/input.txt", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTowMFrOG54sgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTowMFrOG54sgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDkxNQ==", "bodyText": "It's a simple question of weight ratios!", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350915", "createdAt": "2020-07-31T00:59:00Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/input.txt", "diffHunk": "@@ -0,0 +1,13 @@\n+1#value\n+2#words\n+3#All Streams\n+4#Lead to\n+5#Kafka\n+6#Go to\n+7#Kafka Summit\n+8#How can\n+9#a 10 ounce\n+10#bird carry a\n+11#5lb coconut", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk2NjI2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTo1M1rOG54tgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0OTozN1rOG_0ErQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTE2OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now go ahead and create the following file at `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.\n          \n          \n            \n            Including the code snippet explained above, now create the complete application code at\n          \n          \n            \n            `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463351168", "createdAt": "2020-07-31T00:59:53Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. The `RecordMetadata` contains metadata about a particular record after it has been acknowledged.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.\n+\n+\n+Now go ahead and create the following file at `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NjYzNw==", "bodyText": "Same as above", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469566637", "createdAt": "2020-08-12T21:49:37Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. The `RecordMetadata` contains metadata about a particular record after it has been acknowledged.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.\n+\n+\n+Now go ahead and create the following file at `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTE2OA=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk2ODExOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMTowNlrOG54unw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMDo0NTo0MlrOG_yOPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTQ1NQ==", "bodyText": "What's the value of injecting Spring into this tutorial?  SEO? ;)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463351455", "createdAt": "2020-07-31T01:01:06Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzMjI2Nw==", "bodyText": "I didn't think of it that way.  I added it as I think it reflects what goes on in practice.  I have a similar statement in the consumer tutorial. Do you think we should remove it?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469532267", "createdAt": "2020-08-12T20:39:33Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTQ1NQ=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzNjMxOQ==", "bodyText": "@bbejeck no strong leaning one way or the other, feel free to leave it in if it reflects what happens in practice.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469536319", "createdAt": "2020-08-12T20:45:42Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTQ1NQ=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk2OTQwOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMTo0OFrOG54vUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozNDowNFrOG_zqQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTYzMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n          \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n          \n      \n    \n    \n  \n\nTrimming, it's redundant to the statement at the top, and then explained later in testing section", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463351633", "createdAt": "2020-07-31T01:01:48Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTg3NQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469559875", "createdAt": "2020-08-12T21:34:04Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTYzMw=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjk3NDU1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowNDo1NVrOG54yag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozNDoxN1rOG_zqvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MjQyNg==", "bodyText": "FYC: add note...something like...\"Once the broker acknowledges that the record has been appended to its log, the broker returns  to the producer, which the application receives as RecordMetadata\u2014information about the committed message.\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463352426", "createdAt": "2020-07-31T01:04:55Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. The `RecordMetadata` contains metadata about a particular record after it has been acknowledged.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTk5OA==", "bodyText": "good call, updated.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469559998", "createdAt": "2020-08-12T21:34:17Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. The `RecordMetadata` contains metadata about a particular record after it has been acknowledged.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MjQyNg=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDM2NzczOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDoxNDoxMFrOG_3Hkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNTozMTowNlrOHAQ74A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYxNjUzMQ==", "bodyText": "Overall the description of the various acks settings seems overly verbose and potentially too much to throw at a new user.  FYC make a pruned down version and link to the docs for more information.  A pruned down version could look something like this (just as an example):\n- `acks=0`: \"fire and forget\", once the producer sends the record batch it is considered successful\n- `acks=1`: leader broker added the records to its local log but didn't wait for any acknowledgement from the followers\n- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgement of replication from all in-sync replicas", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469616531", "createdAt": "2020-08-13T00:14:10Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n+\n+\n+- Setting `acks` to `0` could be considered \"fire and forget\" as once the producer sends the record batch it is considered succesful even though the records may not reach the broker at all. So record loss is possible.  The offsets for records are always set to `-1`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAzOTUyMA==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470039520", "createdAt": "2020-08-13T15:31:06Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n+\n+\n+- Setting `acks` to `0` could be considered \"fire and forget\" as once the producer sends the record batch it is considered succesful even though the records may not reach the broker at all. So record loss is possible.  The offsets for records are always set to `-1`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYxNjUzMQ=="}, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDQ1NTAyOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDo0ODoyN1rOG_36tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNTozMTozOVrOHAQ9SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyOTYyMQ==", "bodyText": "For consistency, should this be Use instead of Using?  (Or change the other ones)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469629621", "createdAt": "2020-08-13T00:48:27Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Call the `produce` method\n+<2> Build the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAzOTg4MA==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470039880", "createdAt": "2020-08-13T15:31:39Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Call the `produce` method\n+<2> Build the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyOTYyMQ=="}, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDQ1NTg1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDo0OTowMlrOG_37NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNTozMjo0NFrOHARACA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyOTc0OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones\n          \n          \n            \n            <3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones sent\n          \n      \n    \n    \n  \n\nIIUC does sent belong at the end of the sentence?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469629749", "createdAt": "2020-08-13T00:49:02Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Call the `produce` method\n+<2> Build the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDA0MDU4NA==", "bodyText": "Yep, good catch", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470040584", "createdAt": "2020-08-13T15:32:44Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Call the `produce` method\n+<2> Build the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyOTc0OQ=="}, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzczNDAxOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0MzoyMVrOHAWqXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0MzoyMVrOHAWqXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzMzM0Mw==", "bodyText": "It reads strangely to say we're going to do something but then do something else.  So FYC, remove this line.  Then see comments below", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470133343", "createdAt": "2020-08-13T17:43:21Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzczNjM4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0Mzo1MFrOHAWsBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0Mzo1MFrOHAWsBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzMzc2NA==", "bodyText": "FYC, Delete this line", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470133764", "createdAt": "2020-08-13T17:43:50Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzczOTI0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NDoyMFrOHAWtwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NDoyMFrOHAWtwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNDIxMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Open a new terminal and window then run this command:\n          \n          \n            \n            Open a new terminal window and then run this command to open a shell on the broker docker container", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470134210", "createdAt": "2020-08-13T17:44:20Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.\n+\n+Open a new terminal and window then run this command:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc0MTQ2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NDo0NFrOHAWvLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NDo0NFrOHAWvLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNDU3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now use the following command to create the topic for the producer to write to\n          \n          \n            \n            Next, create the topic that the producer to write to", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470134572", "createdAt": "2020-08-13T17:44:44Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.\n+\n+Open a new terminal and window then run this command:\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/open-docker-shell.sh %}</code></pre>\n++++++\n+\n+Now use the following command to create the topic for the producer to write to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc0NTQ2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NToyM1rOHAWxrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NToyM1rOHAWxrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNTIxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Keep this terminal window open as you'll need to run a console conumer to verify your producer application in a few steps.\n          \n          \n            \n            Keep this terminal window open for later use when you run a console consumer to verify your producer application.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470135213", "createdAt": "2020-08-13T17:45:23Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.\n+\n+Open a new terminal and window then run this command:\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/open-docker-shell.sh %}</code></pre>\n++++++\n+\n+Now use the following command to create the topic for the producer to write to\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/create-topic.sh %}</code></pre>\n++++++\n+\n+Keep this terminal window open as you'll need to run a console conumer to verify your producer application in a few steps.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc0Nzc1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NTo1N1rOHAWzCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NTo1N1rOHAWzCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNTU2Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create the following Gradle build file, named `build.gradle` for the project:\n          \n          \n            \n            Create the following Gradle build file for the project, named `build.gradle`:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470135563", "createdAt": "2020-08-13T17:45:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Create the following Gradle build file, named `build.gradle` for the project:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc0OTEwOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NjoxOVrOHAWz3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NjoxOVrOHAWz3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNTc3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And be sure to run the following command to obtain the Gradle wrapper:\n          \n          \n            \n            Run the following command to obtain the Gradle wrapper:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470135774", "createdAt": "2020-08-13T17:46:19Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+And be sure to run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc1NDU1OnYy", "diffSide": "RIGHT", "path": "_data/harnesses/kafka-producer-application/kafka.yml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NzoyOFrOHAW26Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0NzoyOFrOHAW26Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNjU1Mw==", "bodyText": "FYC: this should be a separate title because it's not for the project, it's about producer properties?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470136553", "createdAt": "2020-08-13T17:47:28Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-producer-application/kafka.yml", "diffHunk": "@@ -0,0 +1,166 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Create a topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/harness-create-topic.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc1NjIyOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0Nzo1M1rOHAW34Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0Nzo1M1rOHAW34Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNjgwMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Let's do a quick walkthrough of some of the properties.\n          \n          \n            \n            Let's do a quick walkthrough of some of the producer properties.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470136801", "createdAt": "2020-08-13T17:47:53Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc2MDMzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0ODo1NVrOHAW6XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0ODo1NVrOHAW6XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNzQzNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n          \n          \n            \n            https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable values for `acks` are: `0`, `1` (the default), `-1`, or `all`.  Setting `acks` to `-1` is the same as setting it to `all`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470137436", "createdAt": "2020-08-13T17:48:55Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc2NDA1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0OTo1N1rOHAW8lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0OTo1N1rOHAW8lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzODAwNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            I've only covered a small sub-set of producer confgurations, the full list can be found in the https://kafka.apache.org/documentation/#producerconfigs[producer configs section] in the Apache Kafka documentation.\n          \n          \n            \n            This is only a small sub-set of producer configuration parameters. The full list of producer configuration parameters can be found in the https://kafka.apache.org/documentation/#producerconfigs[Apache Kafka documentation].", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470138004", "createdAt": "2020-08-13T17:49:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n+\n+\n+- `acks=0`: \"fire and forget\", once the producer sends the record batch it is considered successful\n+- `acks=1`: leader broker added the records to its local log but didn't wait for any acknowledgement from the followers\n+- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgement of replication from all in-sync replicas When using `aks=all`, it's stongly reccomended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.\n+\n+\n+I've only covered a small sub-set of producer confgurations, the full list can be found in the https://kafka.apache.org/documentation/#producerconfigs[producer configs section] in the Apache Kafka documentation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc3NDg1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1MjoxMVrOHAXCkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1MjoxMVrOHAXCkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTUzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n          \n          \n            \n            In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method.\n          \n          \n            \n            Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n          \n          \n            \n            \n          \n          \n            \n            (In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470139539", "createdAt": "2020-08-13T17:52:11Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc3NTg2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1MjoyMlrOHAXDKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1MjoyMlrOHAXDKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTY4OQ==", "bodyText": "If you accept the change above, you can delete this", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470139689", "createdAt": "2020-08-13T17:52:22Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc3NzY3OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1Mjo0OFrOHAXEOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1Mjo0OFrOHAXEOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTk2MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n          \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470139961", "createdAt": "2020-08-13T17:52:48Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc4MDY3OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1MzoyMFrOHAXFzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1MzoyMFrOHAXFzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MDM2NA==", "bodyText": "Should we delete this space?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470140364", "createdAt": "2020-08-13T17:53:20Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzc4NDc1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1NDoxNFrOHAXIJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1NDoxNFrOHAXIJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MDk2Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.\n          \n          \n            \n            The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  Note that calling `Future.get()` for any record will block until the produce request completes.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470140966", "createdAt": "2020-08-13T17:54:14Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzgwNDQzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/run-consumer.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1ODozNVrOHAXTpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1ODozNVrOHAXTpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MzkwOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now let's run a console consumer on the output topic to confirm you're application parsed the file correctly and published the expected records.\n          \n          \n            \n            Now run a console consumer that will read topics from the output topic to confirm your application published the expected records.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470143909", "createdAt": "2020-08-13T17:58:35Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/run-consumer.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+////\n+  This is a sample content file for how to include a console consumer to the tutorial, probably a good idea so the end user can watch the results\n+  of the tutorial.  Change the text as needed.\n+\n+////\n+\n+Now let's run a console consumer on the output topic to confirm you're application parsed the file correctly and published the expected records.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNzgwNzg4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/console-consumer.sh", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1OToyNFrOHAXVpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1OToyNFrOHAXVpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0NDQyMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            kafka-console-consumer --topic output-topic --bootstrap-server broker:9092 \\\n          \n          \n            \n            kafka-console-consumer --topic output-topic \\\n          \n          \n            \n             --bootstrap-server broker:9092 \\", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470144421", "createdAt": "2020-08-13T17:59:24Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/console-consumer.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+kafka-console-consumer --topic output-topic --bootstrap-server broker:9092 \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 1}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3976, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}