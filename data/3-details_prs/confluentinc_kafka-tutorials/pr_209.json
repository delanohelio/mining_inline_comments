{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1OTMyNzkx", "number": 209, "title": "DEVX-1479 Adds a Kafka Streams aggregate Min/Max tutorial", "bodyText": "Adds a tutorial which aggregates a min & max value using Kafka Streams.  Companion ksql tutorial: https://kafka-tutorials.confluent.io/create-stateful-aggregation-minmax/ksql.html\nResolves #15", "createdAt": "2020-02-17T04:27:25Z", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209", "merged": true, "mergeCommit": {"oid": "5f813097ab26d5965a10922d85406ad4ccad25e0"}, "closed": true, "closedAt": "2020-03-05T20:10:53Z", "author": {"login": "rspurgeon"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcFFfULAH2gAyMzc1OTMyNzkxOmZlODMwMjVmMWEwNDU4MzM4N2JiZWIxMjQyODY5MGEzNjU1NjAxZWY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcKxQr_AFqTM2OTg3ODA3NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "fe83025f1a04583387bbeb12428690a3655601ef", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/fe83025f1a04583387bbeb12428690a3655601ef", "committedDate": "2020-02-17T04:19:26Z", "message": "DEVX-1479 Adds a Kafka Streams aggregate Min/Max tutorial"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNjYyMDQw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#pullrequestreview-360662040", "createdAt": "2020-02-18T20:49:51Z", "commit": {"oid": "fe83025f1a04583387bbeb12428690a3655601ef"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMDo0OTo1MVrOFrR6Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzoyNDoyNlrOFrWGag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDkyNjU0Nw==", "bodyText": "minor: with 5.4 released should we update to use latest available versions?  It's a bit subjective so I'm fine if you left it as is.  The same for the other cp dependencies in this file.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#discussion_r380926547", "createdAt": "2020-02-18T20:49:51Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-minmax/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,50 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.3.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe83025f1a04583387bbeb12428690a3655601ef"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4NzczOA==", "bodyText": "We might want to mention something like\nSince you are changing the key, Kafka Streams will re-partition the data.   But ths is handled automatically for you, so it's a seamless operation.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#discussion_r380987738", "createdAt": "2020-02-18T23:02:05Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-minmax/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,18 @@\n+Then create the following file at `src/main/java/io/confluent/developer/AggregatingMinMax.java`. \n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/aggregating-minmax/kstreams/code/src/main/java/io/confluent/developer/AggregatingMinMax.java %}</code></pre>\n++++++\n+\n+Let's take a close look at the `buildTopology()` function, which uses the Kafka Streams DSL.\n+\n+Using the `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/StreamsBuilder.html[StreamsBuilder]` parameter, which is the helper object that lets us build our topology, we can apply the following sequence of stages:\n+\n+1. Call the `stream()` function which creates a `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KStream.html[KStream]<String, MovieTicketSales>` object based on the stream of records from the `inputTopic` Kafka topic.\n+\n+2. Our use case requires we calculate minimum and maximum movie revenue _by year_.  The `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KStream.html#groupBy-org.apache.kafka.streams.kstream.KeyValueMapper-[groupBy]` function creates a `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html[KGroupedStream]` object.  `KGroupedStream` represents a 'grouped record stream' which allows us to apply aggregations over the records, grouped by a the key.  Here, we are specifying the movie's year of release as the record value on which to group. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe83025f1a04583387bbeb12428690a3655601ef"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MzUwMg==", "bodyText": "The final parameter to aggregate is an ...Serdes\n\nIMHO we should change this to something like this\nThe final parameter to `aggregate` is a `Materialized` object which contains the `Serdes` required for (de)serializing records for the state store backing the aggregation", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#discussion_r380993502", "createdAt": "2020-02-18T23:18:56Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-minmax/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,18 @@\n+Then create the following file at `src/main/java/io/confluent/developer/AggregatingMinMax.java`. \n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/aggregating-minmax/kstreams/code/src/main/java/io/confluent/developer/AggregatingMinMax.java %}</code></pre>\n++++++\n+\n+Let's take a close look at the `buildTopology()` function, which uses the Kafka Streams DSL.\n+\n+Using the `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/StreamsBuilder.html[StreamsBuilder]` parameter, which is the helper object that lets us build our topology, we can apply the following sequence of stages:\n+\n+1. Call the `stream()` function which creates a `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KStream.html[KStream]<String, MovieTicketSales>` object based on the stream of records from the `inputTopic` Kafka topic.\n+\n+2. Our use case requires we calculate minimum and maximum movie revenue _by year_.  The `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KStream.html#groupBy-org.apache.kafka.streams.kstream.KeyValueMapper-[groupBy]` function creates a `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html[KGroupedStream]` object.  `KGroupedStream` represents a 'grouped record stream' which allows us to apply aggregations over the records, grouped by a the key.  Here, we are specifying the movie's year of release as the record value on which to group. \n+\n+3. Next we apply the `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html#aggregate-org.apache.kafka.streams.kstream.Initializer-org.apache.kafka.streams.kstream.Aggregator-org.apache.kafka.streams.kstream.Materialized-[aggregate]` function which allows us to combine record values over time as well as change the type of the result records from the type of the input records.  In our example we are aggregating `MovieTicketSales` records into the `YearlyMovieFigures` type by calculating a minimum and maximum value, grouped by `release_year`.  The first parameter given to `aggregate` is an `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/Initializer.html[Initializer]` which is used for creating the initial value used in the first aggregation invocation.  In our case, we are providing an instance of `YearlyMovieFigures` initialized with values that will make calculating minimum and maximums easy.  The second parameter to the function is the aggregation logic.  Here we calculate new minimum and maximums by comparing the incoming new `MovieTicketSales` record with the most recent aggregate value and we return a `YearlyMovieFigures` instance.  This `YearlyMovieFigures` instance is the new aggregated value which will propogate downstream as well as be the value returned to us in the next invocation of `aggregate`.  The final parameter to `aggregate` is an `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/common/serialization/Serdes.html[Serdes]` which instruct the library how to serialize and deserialize records at this stage.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe83025f1a04583387bbeb12428690a3655601ef"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5NTE3OA==", "bodyText": "nit: an output records is emitted. -> an output record is emitted.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#discussion_r380995178", "createdAt": "2020-02-18T23:24:26Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-minmax/kstreams/markup/dev/run-consumer.adoc", "diffHunk": "@@ -0,0 +1,13 @@\n+Leaving your original terminal running, open another to consume the events that have been aggregated by your application:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/aggregating-minmax/kstreams/code/tutorial-steps/dev/console-consumer.sh %}</code></pre>\n++++++\n+\n+After the consumer starts, you should see the following messages. Note that for every input record an output records is emitted. Each record represents an update to the aggregated values which is sent on every movie event specifically because caching is disabled in the code with `StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG` set to `0`. Read more on `https://docs.confluent.io/current/streams/developer-guide/memory-mgmt.html#record-caches-in-the-dsl[Record caches in the DSL]`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe83025f1a04583387bbeb12428690a3655601ef"}, "originalPosition": 7}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "32cafa5d7c2da1c6bcd5e5664021fda7da43e075", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/32cafa5d7c2da1c6bcd5e5664021fda7da43e075", "committedDate": "2020-02-19T01:48:06Z", "message": "DEVX-1479 Fixes tests for MinMax aggregate kstreams recipe"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2161dbaad63c09b1a0dc58f62788f311e3959173", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/2161dbaad63c09b1a0dc58f62788f311e3959173", "committedDate": "2020-02-19T01:53:46Z", "message": "DEVX-1479 Upgrades aggregate minmax to 5.4.0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48ba743015e6873b1789ed5ba7290ee33506a927", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/48ba743015e6873b1789ed5ba7290ee33506a927", "committedDate": "2020-02-19T02:13:02Z", "message": "DEVX-1479 Addresses some PR comments for aggregate minmax"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "826ecd465fd0128ae677fda2270a0419d464f22a", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/826ecd465fd0128ae677fda2270a0419d464f22a", "committedDate": "2020-02-19T02:15:10Z", "message": "DEVX-1479 Updates ak docs to 2.4 for aggregating minmax recipe"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwODA1MDAw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#pullrequestreview-360805000", "createdAt": "2020-02-19T02:24:55Z", "commit": {"oid": "826ecd465fd0128ae677fda2270a0419d464f22a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQwMjoyNDo1NVrOFrZJzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQwMjoyNDo1NVrOFrZJzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTA0NTE5Ng==", "bodyText": "@colinhicks @bbejeck I tried changing this to 2.4 to make my AK docs links render to 2.4 but it doesn't seem to work locally... but maybe will in a staging site?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#discussion_r381045196", "createdAt": "2020-02-19T02:24:55Z", "author": {"login": "rspurgeon"}, "path": "tutorials/aggregating-minmax/kstreams.html", "diffHunk": "@@ -3,5 +3,5 @@\n permalink: /create-stateful-aggregation-minmax/kstreams\n stack: kstreams\n static_data: aggregating-minmax\n-help_wanted: true\n+ak_javadoc_version: 2.4", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "826ecd465fd0128ae677fda2270a0419d464f22a"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1064c5c96051a89cc43c7cba64ad030e0ad0fa2d", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/1064c5c96051a89cc43c7cba64ad030e0ad0fa2d", "committedDate": "2020-02-19T02:45:37Z", "message": "DEVX-1479 Adds aggregate minmax to semaphore"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53edc66073e00679382fbc689e746e822824fa0d", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/53edc66073e00679382fbc689e746e822824fa0d", "committedDate": "2020-02-19T14:59:22Z", "message": "DEVX-1479 For aggregation minmax uses page front matter to set 2.4 ak docs site"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c12c437fb0d5c0e709c8dbcd6a8e976387ffc516", "author": {"user": {"login": "rspurgeon", "name": "Rick Spurgeon"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/c12c437fb0d5c0e709c8dbcd6a8e976387ffc516", "committedDate": "2020-03-05T17:55:54Z", "message": "DEVX-1479 removes path to netcat\n\nso sempaphore pulls it from path"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY5ODc4MDc1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/209#pullrequestreview-369878075", "createdAt": "2020-03-05T20:08:54Z", "commit": {"oid": "c12c437fb0d5c0e709c8dbcd6a8e976387ffc516"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 251, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}