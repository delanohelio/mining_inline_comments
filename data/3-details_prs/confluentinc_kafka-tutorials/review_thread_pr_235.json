{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzMjUxOTg2", "number": 235, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNDoyMTo1OVrODovehw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjo1MDowNVrODsw1vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0MDQ3NDk1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNDoyMjowMFrOF3eewQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNjo1MjoxNVrOF3lMrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzcxNTM5Mw==", "bodyText": "io.confluent.developer.RunningAverage.java should just be io.confluent.developer.RunningAverage .  I had an error in the gen_project script that I've fixed now.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r393715393", "createdAt": "2020-03-17T14:22:00Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,64 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+    maven { url 'http://packages.confluent.io/maven/' }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    compile \"org.apache.avro:avro:1.8.2\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.26\"\n+    implementation \"org.apache.kafka:kafka-streams:2.4.0\"\n+    implementation \"io.confluent:kafka-streams-avro-serde:5.4.0\"\n+\n+    compile group: 'com.typesafe', name: 'config', version: '1.4.0'\n+    testCompile \"org.apache.kafka:kafka-streams-test-utils:2.4.0\"\n+    testCompile \"junit:junit:4.12\"\n+    testImplementation 'org.hamcrest:hamcrest:2.2'\n+\n+    testCompileOnly \"org.projectlombok:lombok:1.18.12\"\n+    testAnnotationProcessor \"org.projectlombok:lombok:1.18.12\"\n+}\n+\n+test {\n+    testLogging {\n+        outputs.upToDateWhen { false }\n+        showStandardStreams = true\n+        exceptionFormat = \"full\"\n+    }\n+}\n+\n+jar {\n+  manifest {\n+    attributes(\n+      \"Class-Path\": configurations.compile.collect { it.getName() }.join(\" \"),\n+      \"Main-Class\": \"io.confluent.developer.RunningAverage.java\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdde1ebc7073d0d0deebb1f32de5935e1ddfe8f8"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzgyNTQ1NQ==", "bodyText": "Thanks!", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r393825455", "createdAt": "2020-03-17T16:52:15Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,64 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+    maven { url 'http://packages.confluent.io/maven/' }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    compile \"org.apache.avro:avro:1.8.2\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.26\"\n+    implementation \"org.apache.kafka:kafka-streams:2.4.0\"\n+    implementation \"io.confluent:kafka-streams-avro-serde:5.4.0\"\n+\n+    compile group: 'com.typesafe', name: 'config', version: '1.4.0'\n+    testCompile \"org.apache.kafka:kafka-streams-test-utils:2.4.0\"\n+    testCompile \"junit:junit:4.12\"\n+    testImplementation 'org.hamcrest:hamcrest:2.2'\n+\n+    testCompileOnly \"org.projectlombok:lombok:1.18.12\"\n+    testAnnotationProcessor \"org.projectlombok:lombok:1.18.12\"\n+}\n+\n+test {\n+    testLogging {\n+        outputs.upToDateWhen { false }\n+        showStandardStreams = true\n+        exceptionFormat = \"full\"\n+    }\n+}\n+\n+jar {\n+  manifest {\n+    attributes(\n+      \"Class-Path\": configurations.compile.collect { it.getName() }.join(\" \"),\n+      \"Main-Class\": \"io.confluent.developer.RunningAverage.java\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzcxNTM5Mw=="}, "originalCommit": {"oid": "fdde1ebc7073d0d0deebb1f32de5935e1ddfe8f8"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTY5MjM2OnYy", "diffSide": "RIGHT", "path": "_data/tutorials.yaml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowODo1MVrOF9W7Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowODo1MVrOF9W7Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzA0Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              title: \"How to compute average?\"\n          \n          \n            \n              title: \"Compute an average aggregation\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883042", "createdAt": "2020-03-30T01:08:51Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTY5MjgyOnYy", "diffSide": "RIGHT", "path": "_data/tutorials.yaml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowOTowN1rOF9W7WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowOTowN1rOF9W7WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzA5Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              meta-description: \"How to compute average?\"\n          \n          \n            \n              meta-description: \"Compute an average aggregation\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883097", "createdAt": "2020-03-30T01:09:07Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"\n+  meta-description: \"How to compute average?\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTY5Mjg4OnYy", "diffSide": "RIGHT", "path": "_data/tutorials.yaml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowOToxM1rOF9W7Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowOToxM1rOF9W7Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzEwNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              problem: \"Kafka Streams DSL aggregation natively support so-called incremental\\\" or \\\"cumulative\\\" aggregation functions like count, sum, min, max. However, average function can be composed of distributive functions, namely count and sum. In this tutorial we will write a program.\"\n          \n          \n            \n              problem: \"Kafka Streams natively supports \\\"incremental\\\" aggregation functions, in which the aggregation result is updated based on the values captured by each window. Incremental functions include count, sum, min, and max. An average aggregation cannot be computed incrementally. However, as this tutorial shows, it can be implemented by composing incremental functions, namely count and sum.\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883106", "createdAt": "2020-03-30T01:09:13Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"\n+  meta-description: \"How to compute average?\"\n+  slug: \"/aggregating-average\"\n+  problem: \"Kafka Streams DSL aggregation natively support so-called incremental\\\" or \\\"cumulative\\\" aggregation functions like count, sum, min, max. However, average function can be composed of distributive functions, namely count and sum. In this tutorial we will write a program.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTY5MzkyOnYy", "diffSide": "RIGHT", "path": "_data/tutorials.yaml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMToxMDoyMFrOF9W8Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMToxMDoyMFrOF9W8Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzI3MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains running average of ratings a movie received.\"\n          \n          \n            \n              introduction: \"Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains a running average rating for each movie.\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883271", "createdAt": "2020-03-30T01:10:20Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"\n+  meta-description: \"How to compute average?\"\n+  slug: \"/aggregating-average\"\n+  problem: \"Kafka Streams DSL aggregation natively support so-called incremental\\\" or \\\"cumulative\\\" aggregation functions like count, sum, min, max. However, average function can be composed of distributive functions, namely count and sum. In this tutorial we will write a program.\"\n+  introduction: \"Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains running average of ratings a movie received.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTY5ODg1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMToxNDoyNFrOF9W-sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMToxNDoyNFrOF9W-sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4Mzk1Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: We're going to use this pair to store intermediate results. \n          \n          \n            \n            Note: We're going to use this record to store intermediate results.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883953", "createdAt": "2020-03-30T01:14:24Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc", "diffHunk": "@@ -0,0 +1,8 @@\n+Next, create an Avro schema file at `src/main/avro/countsum.avsc` for the pair of counts and sums:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"avro\">{% include_raw tutorials/aggregating-average/kstreams/code/src/main/avro/countsum.avsc %}</code></pre>\n++++++\n+\n+NOTE: We're going to use this pair to store intermediate results. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTcwMjA4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMToxNzozMVrOF9XAiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMToxNzozMVrOF9XAiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NDQyNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Then create the following file at `src/main/java/io/confluent/developer/<MAIN-CLASS-NAME>.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.\n          \n          \n            \n            Then create the following file at `src/main/java/io/confluent/developer/RunningAverage.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399884427", "createdAt": "2020-03-30T01:17:31Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,12 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Then create the following file at `src/main/java/io/confluent/developer/<MAIN-CLASS-NAME>.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTcxODAyOnYy", "diffSide": "RIGHT", "path": "_data/harnesses/aggregating-average/kstreams.yml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTozMTowNlrOF9XJkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNDozMzowNlrOF9ZS4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NjczNw==", "bodyText": "run-consumer.adoc states\n\nBefore you start producing ratings, it\u2019s a good idea to set up the consumer on the output topic.\n\nBut the consume step is currently after the produce step above.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399886737", "createdAt": "2020-03-30T01:31:06Z", "author": {"login": "colinhicks"}, "path": "_data/harnesses/aggregating-average/kstreams.yml", "diffHunk": "@@ -0,0 +1,171 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-dev-file.adoc\n+            \n+    - title: Create a schema for the model obect\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-avro-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-dir.adoc\n+\n+        - action: make_file\n+          file: src/main/avro/rating.avsc\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-rating.adoc\n+        \n+        - action: make_file\n+          file: src/main/avro/countsum.avsc\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/build-project.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/build-project.adoc\n+\n+    - title: Create the Kafka Streams topology\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-src-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-src-dir.adoc\n+            \n+        - action: make_file\n+          file: src/main/java/io/confluent/developer/RunningAverage.java\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc \n+\n+    - title: Compile and run the Kafka Streams program\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/build-uberjar.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/build-uberjar.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/run-dev-app.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-dev-app.adoc\n+\n+    - title: Produce sample data to the input topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/console-producer.sh\n+          stdin: tutorial-steps/dev/input-ratings.json\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc\n+\n+    - title: Consume data from the output topic\n+      content:\n+        - action: execute_async\n+          file: tutorial-steps/dev/console-consumer.sh\n+          stdout: tutorial-steps/dev/outputs/actual-outputs.json\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-consumer.adoc", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTkyMTg5MA==", "bodyText": "hm, makes total sense. it may be the reason that I'm getting an empty file with consumer results.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399921890", "createdAt": "2020-03-30T04:33:06Z", "author": {"login": "gAmUssA"}, "path": "_data/harnesses/aggregating-average/kstreams.yml", "diffHunk": "@@ -0,0 +1,171 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-dev-file.adoc\n+            \n+    - title: Create a schema for the model obect\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-avro-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-dir.adoc\n+\n+        - action: make_file\n+          file: src/main/avro/rating.avsc\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-rating.adoc\n+        \n+        - action: make_file\n+          file: src/main/avro/countsum.avsc\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/build-project.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/build-project.adoc\n+\n+    - title: Create the Kafka Streams topology\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-src-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-src-dir.adoc\n+            \n+        - action: make_file\n+          file: src/main/java/io/confluent/developer/RunningAverage.java\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc \n+\n+    - title: Compile and run the Kafka Streams program\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/build-uberjar.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/build-uberjar.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/run-dev-app.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-dev-app.adoc\n+\n+    - title: Produce sample data to the input topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/console-producer.sh\n+          stdin: tutorial-steps/dev/input-ratings.json\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc\n+\n+    - title: Consume data from the output topic\n+      content:\n+        - action: execute_async\n+          file: tutorial-steps/dev/console-consumer.sh\n+          stdout: tutorial-steps/dev/outputs/actual-outputs.json\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-consumer.adoc", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NjczNw=="}, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTg1MzQzOnYy", "diffSide": "RIGHT", "path": ".semaphore/semaphore.yml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowMjozN1rOF9rRPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowMjozN1rOF9rRPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxNjM4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    - name: KStreams KStreams Running Average tests tests\n          \n          \n            \n                    - name: KStreams Running Average tests", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400216382", "createdAt": "2020-03-30T14:02:37Z", "author": {"login": "colinhicks"}, "path": ".semaphore/semaphore.yml", "diffHunk": "@@ -177,3 +177,7 @@ blocks:\n         - name: KStreams aggregation MIN/MAX tests\n           commands:\n             - make -C _includes/tutorials/aggregating-minmax/kstreams/code tutorial\n+\n+        - name: KStreams KStreams Running Average tests tests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTg2ODU1OnYy", "diffSide": "RIGHT", "path": "index.html", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowNjoyMlrOF9rbMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowNjoyMlrOF9rbMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxODkyOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        <li><a href=\"aggregating-average/kstreams.html\">Calculate running average</a></li>\n          \n          \n            \n                        <li><a href=\"aggregating-average/kstreams.html\">Calculate a running average</a></li>", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400218929", "createdAt": "2020-03-30T14:06:22Z", "author": {"login": "colinhicks"}, "path": "index.html", "diffHunk": "@@ -56,6 +56,7 @@ <h2 class=\"subtitle\">Apache Kafka is a powerful, scalable, fault-tolerant distri\n             <li><a href=\"create-stateful-aggregation-count/ksql.html\">Count a stream of events</a></li>\n             <li><a href=\"create-stateful-aggregation-sum/ksql.html\">Sum a stream of events</a></li>\n             <li><a href=\"create-stateful-aggregation-minmax/ksql.html\">Find the min/max in a stream of events</a></li>\n+            <li><a href=\"aggregating-average/kstreams.html\">Calculate running average</a></li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTg3NTA0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowNzozOVrOF9rfUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTo0Mjo0M1rOF9wDqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxOTk4Ng==", "bodyText": "Can this be version 2 like the other tutorials?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400219986", "createdAt": "2020-03-30T14:07:39Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,60 @@\n+---\n+version: '3.5'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5NDgyNQ==", "bodyText": "no, networks section isn't supported in ver 2", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400294825", "createdAt": "2020-03-30T15:42:43Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,60 @@\n+---\n+version: '3.5'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxOTk4Ng=="}, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTg3ODI1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowODoyMVrOF9rhbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzo1NzoxNlrOF91lCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIyMDUyNg==", "bodyText": "We can go ahead and bump CP versions to 5.4.1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400220526", "createdAt": "2020-03-30T14:08:21Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,60 @@\n+---\n+version: '3.5'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM4NTI4OA==", "bodyText": "done", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400385288", "createdAt": "2020-03-30T17:57:16Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,60 @@\n+---\n+version: '3.5'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIyMDUyNg=="}, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTg5NjUxOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDoxMjoyMVrOF9rtLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDoxMjoyMVrOF9rtLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIyMzUzMw==", "bodyText": "This paragraph has some out of date info, it's also not really necessary. I suggest deleting it altogether.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When the console producer starts, it will log some messages and hang, waiting for your input. Copy and paste one line at a time and press enter to send it. Note that these lines contain hard tabs between the key and the value, so retyping them without the tab will not work.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400223533", "createdAt": "2020-03-30T14:12:21Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc", "diffHunk": "@@ -0,0 +1,23 @@\n+////\n+   Example content file for how to include a console produer(s) in the tutorial.\n+   Usually you'll include a line referencing the script to run the console producer and also include some content\n+   describing how to input data as shown below.\n+\n+   Again modify this file as you need for your tutorial, as this is just sample content.  You also may have more than one\n+   console producer to run depending on how you structure your tutorial\n+\n+////\n+\n+In a new terminal, run:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/aggregating-average/kstreams/code/tutorial-steps/dev/console-producer.sh %}</code></pre>\n++++++\n+\n+When the console producer starts, it will log some messages and hang, waiting for your input. Copy and paste one line at a time and press enter to send it. Note that these lines contain hard tabs between the key and the value, so retyping them without the tab will not work.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjU3NTY1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjozNDo0M1rOF9yUlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzo1NzoxMFrOF91kyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzMTkyNQ==", "bodyText": "can we update to 2.4.1? Same for the streams-test-utils", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400331925", "createdAt": "2020-03-30T16:34:43Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,75 @@\n+buildscript {\n+  repositories {\n+    jcenter()\n+  }\n+  dependencies {\n+    classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+    classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+  }\n+}\n+\n+plugins {\n+  id \"java\"\n+  id \"application\"\n+  id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+  id \"idea\"\n+  id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+mainClassName = \"io.confluent.developer.RunningAverage\"\n+\n+repositories {\n+  jcenter()\n+  maven { url 'https://packages.confluent.io/maven/' }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+  implementation \"org.apache.avro:avro:1.8.2\"\n+  implementation \"org.slf4j:slf4j-simple:1.7.26\"\n+  implementation \"org.apache.kafka:kafka-streams:2.4.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM4NTIyNQ==", "bodyText": "done", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400385225", "createdAt": "2020-03-30T17:57:10Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,75 @@\n+buildscript {\n+  repositories {\n+    jcenter()\n+  }\n+  dependencies {\n+    classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+    classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+  }\n+}\n+\n+plugins {\n+  id \"java\"\n+  id \"application\"\n+  id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+  id \"idea\"\n+  id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+mainClassName = \"io.confluent.developer.RunningAverage\"\n+\n+repositories {\n+  jcenter()\n+  maven { url 'https://packages.confluent.io/maven/' }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+  implementation \"org.apache.avro:avro:1.8.2\"\n+  implementation \"org.slf4j:slf4j-simple:1.7.26\"\n+  implementation \"org.apache.kafka:kafka-streams:2.4.0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzMTkyNQ=="}, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjU5OTI2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjo0MDoyMVrOF9yjgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzo1NzowNVrOF91kmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNTc0NQ==", "bodyText": "running the test harness locally the streams app wouldn't shut down changing streams.close() to streams.close(Duration.ofSeconds(5)); fixes that.  Otherwise, the main thread will block forever, waiting for streams to stop.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400335745", "createdAt": "2020-03-30T16:40:21Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM3NDUzMA==", "bodyText": "this one nice @bbejeck\nThanks", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400374530", "createdAt": "2020-03-30T17:40:11Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNTc0NQ=="}, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM4NTE3OQ==", "bodyText": "done", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400385179", "createdAt": "2020-03-30T17:57:05Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNTc0NQ=="}, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjYwNjY5OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjo0MjowN1rOF9yoJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzo1NzowMFrOF91kbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNjkzNQ==", "bodyText": "running the test-harness locally failed for me, but adding   streams.cleanUp();  before streams.start()  fixes the issue.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400336935", "createdAt": "2020-03-30T16:42:07Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();\n+        latch.countDown();\n+      }\n+    });\n+\n+    try {\n+      streams.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM1NDY1MQ==", "bodyText": "snap. I added this but for some reasons, this change isn't there. I agree I will fix it", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400354651", "createdAt": "2020-03-30T17:09:20Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();\n+        latch.countDown();\n+      }\n+    });\n+\n+    try {\n+      streams.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNjkzNQ=="}, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM4NTEzNA==", "bodyText": "done", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400385134", "createdAt": "2020-03-30T17:57:00Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();\n+        latch.countDown();\n+      }\n+    });\n+\n+    try {\n+      streams.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNjkzNQ=="}, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjY0MTI3OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjo1MDowNVrOF9y9cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzo1Njo1MFrOF91kEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM0MjM4Nw==", "bodyText": "To help the reader get the main point of the tutorial may consider adding something like this\nwith a brief description of the code\nHere's how you'll calculate the running average:\n+++++\n<pre class=\"snippet\"><code class=\"java\">\n   final KTable<Long, CountAndSum> ratingCountAndSum =\n        ratingsById.aggregate(() -> new CountAndSum(0L, 0.0),\n                              (key, value, aggregate) -> {\n                                aggregate.setCount(aggregate.getCount() + 1);\n                                aggregate.setSum(aggregate.getSum() + value);\n                                return aggregate;\n                              },\n                              Materialized.with(Long(), countAndSumSerde));\n</code></pre>\n+++++\n\nJust a suggestion though", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400342387", "createdAt": "2020-03-30T16:50:05Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,12 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Then create the following file at `src/main/java/io/confluent/developer/RunningAverage.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.\n+\n+// Full topology description goes here\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM4NTA0MA==", "bodyText": "done \ud83d\udc4d", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400385040", "createdAt": "2020-03-30T17:56:50Z", "author": {"login": "gAmUssA"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,12 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Then create the following file at `src/main/java/io/confluent/developer/RunningAverage.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.\n+\n+// Full topology description goes here\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM0MjM4Nw=="}, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 9}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4052, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}