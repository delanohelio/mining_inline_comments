{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ0OTM4MDA0", "number": 465, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDowMjoxMVrOEMF4ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTozOTo0MlrOEQYXrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxMTE0NzMwOnYy", "diffSide": "RIGHT", "path": "_data/tutorials.yaml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDowMjoxMVrOGuAByQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDozMDo0MlrOGuBWeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg4ODEzNw==", "bodyText": "Would a link to the datagen repository work here?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450888137", "createdAt": "2020-07-07T14:02:11Z", "author": {"login": "rspurgeon"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -302,3 +302,14 @@ console-consumer-read-specific-offsets-partition:\n     ksql: disabled\n     kstreams: disabled\n     kafka: enabled\n+\n+kafka-connect-datagen-local:\n+  title: \"How to generate mock data to a local Kafka topic using the Kafka Connect Datagen\"\n+  meta-description: \"How to generate mock data to a local Kafka topic using the Kafka Connect Datagen\"\n+  slug: \"/kafka-connect-datagen-local\"\n+  problem: \"you want to test your Kafka applications but need mock data produced to Kafka topics.\"\n+  introduction: \"You will run a local instance of the kafka-connect-datagen connector to produce mock data to a local Kafka cluster\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwOTgxOA==", "bodyText": "Yes, added", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450909818", "createdAt": "2020-07-07T14:30:42Z", "author": {"login": "ybyzek"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -302,3 +302,14 @@ console-consumer-read-specific-offsets-partition:\n     ksql: disabled\n     kstreams: disabled\n     kafka: enabled\n+\n+kafka-connect-datagen-local:\n+  title: \"How to generate mock data to a local Kafka topic using the Kafka Connect Datagen\"\n+  meta-description: \"How to generate mock data to a local Kafka topic using the Kafka Connect Datagen\"\n+  slug: \"/kafka-connect-datagen-local\"\n+  problem: \"you want to test your Kafka applications but need mock data produced to Kafka topics.\"\n+  introduction: \"You will run a local instance of the kafka-connect-datagen connector to produce mock data to a local Kafka cluster\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg4ODEzNw=="}, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxMTE5MzY1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-connect-datagen-local/kafka/code/Dockerfile", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDoxMjozNlrOGuAffg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDoyMDowNlrOGuA1yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg5NTc0Mg==", "bodyText": "FYC Use an explicit version to prevent any unexpected compatibility issues down the line for users", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450895742", "createdAt": "2020-07-07T14:12:36Z", "author": {"login": "rspurgeon"}, "path": "_includes/tutorials/kafka-connect-datagen-local/kafka/code/Dockerfile", "diffHunk": "@@ -0,0 +1,5 @@\n+FROM confluentinc/cp-kafka-connect-base:5.5.1\n+\n+ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\"\n+\n+RUN confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:latest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwMTQ1MQ==", "bodyText": "I'll pin this to 0.3.2", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450901451", "createdAt": "2020-07-07T14:20:06Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-connect-datagen-local/kafka/code/Dockerfile", "diffHunk": "@@ -0,0 +1,5 @@\n+FROM confluentinc/cp-kafka-connect-base:5.5.1\n+\n+ENV CONNECT_PLUGIN_PATH=\"/usr/share/java,/usr/share/confluent-hub-components\"\n+\n+RUN confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:latest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg5NTc0Mg=="}, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxMTE5OTgwOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-connect-datagen-local/kafka/code/docker-compose.yml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDoxNDowNFrOGuAjew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDoxOToxMVrOGuAzCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg5Njc2Mw==", "bodyText": "We're using cp-kafka-connect-base:5.5.1 for the connector but 5.5.0 for the services.  Is that intentional?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450896763", "createdAt": "2020-07-07T14:14:04Z", "author": {"login": "rspurgeon"}, "path": "_includes/tutorials/kafka-connect-datagen-local/kafka/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,71 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwMDc0Ng==", "bodyText": "I'll change this to 5.5.0 for consistency.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450900746", "createdAt": "2020-07-07T14:19:11Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-connect-datagen-local/kafka/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,71 @@\n+---\n+version: '2'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.5.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg5Njc2Mw=="}, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxMTIyMzcxOnYy", "diffSide": "RIGHT", "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxNDoxOTowNFrOGuAyrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMjoxMTozNlrOG0hDng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwMDY1NQ==", "bodyText": "Is it possible to leave the prod steps out entirely as they aren't relevant for this tutorial?   The Cleanup Docker step seems like a \"Dev\" step IMO", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450900655", "createdAt": "2020-07-07T14:19:04Z", "author": {"login": "rspurgeon"}, "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "diffHunk": "@@ -0,0 +1,79 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: Dockerfile\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-dockerfile.adoc\n+\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+            \n+    - title: Create the connector\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/create-connector.sh\n+          stdout: tutorial-steps/dev/outputs/create-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/create-connector.adoc\n+\n+        - name: give Kafka Connect chance to create the connector\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+        - action: execute\n+          file: tutorial-steps/dev/check-connector.sh\n+          stdout: tutorial-steps/dev/outputs/check-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/check-connector.adoc\n+\n+        - name: give Kafka Connect further chance to get the data to the topic\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+    - title: Consume events from the Kafka topic\n+      content:\n+      - action: execute_async\n+        file: tutorial-steps/dev/harness-console-consumer-keys.sh\n+        stdout: tutorial-steps/dev/outputs/consume-topic.log\n+        render:\n+          file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/consume-topic-key-value.adoc\n+\n+      - name: wait for consumer to read records\n+        action: sleep\n+        ms: 10000\n+        render:\n+          skip: true\n+\n+prod:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkxMTcxOQ==", "bodyText": "It's setup this way to allow users to do this: https://github.com/confluentinc/kafka-tutorials#run-a-tutorial\nmake SEQUENCE=\"dev, test\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r450911719", "createdAt": "2020-07-07T14:33:13Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "diffHunk": "@@ -0,0 +1,79 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: Dockerfile\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-dockerfile.adoc\n+\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+            \n+    - title: Create the connector\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/create-connector.sh\n+          stdout: tutorial-steps/dev/outputs/create-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/create-connector.adoc\n+\n+        - name: give Kafka Connect chance to create the connector\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+        - action: execute\n+          file: tutorial-steps/dev/check-connector.sh\n+          stdout: tutorial-steps/dev/outputs/check-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/check-connector.adoc\n+\n+        - name: give Kafka Connect further chance to get the data to the topic\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+    - title: Consume events from the Kafka topic\n+      content:\n+      - action: execute_async\n+        file: tutorial-steps/dev/harness-console-consumer-keys.sh\n+        stdout: tutorial-steps/dev/outputs/consume-topic.log\n+        render:\n+          file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/consume-topic-key-value.adoc\n+\n+      - name: wait for consumer to read records\n+        action: sleep\n+        ms: 10000\n+        render:\n+          skip: true\n+\n+prod:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwMDY1NQ=="}, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcwOTc4Nw==", "bodyText": "FYC add a quick note to clean-up.adoc that the prod step is only for clean-up.  Or maybe we can find a way to have some sort of flag to set that is skipped when running make SEQUENCE=\"dev, test\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r457709787", "createdAt": "2020-07-20T21:45:54Z", "author": {"login": "bbejeck"}, "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "diffHunk": "@@ -0,0 +1,79 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: Dockerfile\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-dockerfile.adoc\n+\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+            \n+    - title: Create the connector\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/create-connector.sh\n+          stdout: tutorial-steps/dev/outputs/create-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/create-connector.adoc\n+\n+        - name: give Kafka Connect chance to create the connector\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+        - action: execute\n+          file: tutorial-steps/dev/check-connector.sh\n+          stdout: tutorial-steps/dev/outputs/check-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/check-connector.adoc\n+\n+        - name: give Kafka Connect further chance to get the data to the topic\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+    - title: Consume events from the Kafka topic\n+      content:\n+      - action: execute_async\n+        file: tutorial-steps/dev/harness-console-consumer-keys.sh\n+        stdout: tutorial-steps/dev/outputs/consume-topic.log\n+        render:\n+          file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/consume-topic-key-value.adoc\n+\n+      - name: wait for consumer to read records\n+        action: sleep\n+        ms: 10000\n+        render:\n+          skip: true\n+\n+prod:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwMDY1NQ=="}, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyMDczNA==", "bodyText": "In lieu of changing the framework to support a flag to skip cleanup, I went the \"easy\" route of adding the note to clean-up.adoc", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r457720734", "createdAt": "2020-07-20T22:11:36Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "diffHunk": "@@ -0,0 +1,79 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: Dockerfile\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-dockerfile.adoc\n+\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+            \n+    - title: Create the connector\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/create-connector.sh\n+          stdout: tutorial-steps/dev/outputs/create-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/create-connector.adoc\n+\n+        - name: give Kafka Connect chance to create the connector\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+        - action: execute\n+          file: tutorial-steps/dev/check-connector.sh\n+          stdout: tutorial-steps/dev/outputs/check-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/check-connector.adoc\n+\n+        - name: give Kafka Connect further chance to get the data to the topic\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+    - title: Consume events from the Kafka topic\n+      content:\n+      - action: execute_async\n+        file: tutorial-steps/dev/harness-console-consumer-keys.sh\n+        stdout: tutorial-steps/dev/outputs/consume-topic.log\n+        render:\n+          file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/consume-topic-key-value.adoc\n+\n+      - name: wait for consumer to read records\n+        action: sleep\n+        ms: 10000\n+        render:\n+          skip: true\n+\n+prod:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDkwMDY1NQ=="}, "originalCommit": {"oid": "09d222ec1324837a821bafa82de40fd1537159cc"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NjExOTUwOnYy", "diffSide": "RIGHT", "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTozOTo0MlrOG0gOOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMjowODoyM1rOG0g-Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcwNzA2NQ==", "bodyText": "The clean-up step isn't displayed when running through the tutorial.  The skip: true part needs to be replaced with file: <path>\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        skip: true\n          \n          \n            \n                        tutorials/kafka-connect-datagen-local/kafka/markup/dev/clean-up.adoc", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r457707065", "createdAt": "2020-07-20T21:39:42Z", "author": {"login": "bbejeck"}, "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "diffHunk": "@@ -0,0 +1,79 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: Dockerfile\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-dockerfile.adoc\n+\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+            \n+    - title: Create the connector\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/create-connector.sh\n+          stdout: tutorial-steps/dev/outputs/create-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/create-connector.adoc\n+\n+        - name: give Kafka Connect chance to create the connector\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+        - action: execute\n+          file: tutorial-steps/dev/check-connector.sh\n+          stdout: tutorial-steps/dev/outputs/check-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/check-connector.adoc\n+\n+        - name: give Kafka Connect further chance to get the data to the topic\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+    - title: Consume events from the Kafka topic\n+      content:\n+      - action: execute_async\n+        file: tutorial-steps/dev/harness-console-consumer-keys.sh\n+        stdout: tutorial-steps/dev/outputs/consume-topic.log\n+        render:\n+          file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/consume-topic-key-value.adoc\n+\n+      - name: wait for consumer to read records\n+        action: sleep\n+        ms: 10000\n+        render:\n+          skip: true\n+\n+prod:\n+  steps:\n+    - title: Cleanup Docker containers\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/clean-up.sh\n+          render:\n+            skip: true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e5485221c5687ca64ae1b5a1f1ea827ef607e21"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxOTM3MA==", "bodyText": "@bbejeck ack, thank you.  I've implemented this in the latest commit (it needed file: addition.)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/465#discussion_r457719370", "createdAt": "2020-07-20T22:08:23Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-connect-datagen-local/kafka.yml", "diffHunk": "@@ -0,0 +1,79 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: Dockerfile\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-dockerfile.adoc\n+\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+            \n+    - title: Create the connector\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/create-connector.sh\n+          stdout: tutorial-steps/dev/outputs/create-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/create-connector.adoc\n+\n+        - name: give Kafka Connect chance to create the connector\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+        - action: execute\n+          file: tutorial-steps/dev/check-connector.sh\n+          stdout: tutorial-steps/dev/outputs/check-connector.log\n+          render:\n+            file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/check-connector.adoc\n+\n+        - name: give Kafka Connect further chance to get the data to the topic\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+    - title: Consume events from the Kafka topic\n+      content:\n+      - action: execute_async\n+        file: tutorial-steps/dev/harness-console-consumer-keys.sh\n+        stdout: tutorial-steps/dev/outputs/consume-topic.log\n+        render:\n+          file: tutorials/kafka-connect-datagen-local/kafka/markup/dev/consume-topic-key-value.adoc\n+\n+      - name: wait for consumer to read records\n+        action: sleep\n+        ms: 10000\n+        render:\n+          skip: true\n+\n+prod:\n+  steps:\n+    - title: Cleanup Docker containers\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/clean-up.sh\n+          render:\n+            skip: true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcwNzA2NQ=="}, "originalCommit": {"oid": "0e5485221c5687ca64ae1b5a1f1ea827ef607e21"}, "originalPosition": 79}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3959, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}