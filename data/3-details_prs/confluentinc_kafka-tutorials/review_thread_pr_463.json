{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxNjk5Mzg2", "number": 463, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNTo1NjoxNFrOELvz9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNjo0MjoxMlrOELw6NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNzUzMTQwOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/create-topic.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNTo1NjoxNFrOGtdXsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNjowNjowM1rOG1pcCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMyMDMwNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            But your first, you're going to open a shell on the docker broker contaniner.\n          \n          \n            \n            But first, you're going to open a shell on the broker docker container.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r450320307", "createdAt": "2020-07-06T15:56:14Z", "author": {"login": "rspurgeon"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But your first, you're going to open a shell on the docker broker contaniner.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkwNjYzNA==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r458906634", "createdAt": "2020-07-22T16:06:03Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But your first, you're going to open a shell on the docker broker contaniner.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMyMDMwNw=="}, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNzU1Mjg4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-dev-file.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNjowMTo0NFrOGtdk1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNjowODo1OVrOG1pj5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMyMzY2OQ==", "bodyText": "The mix of the Header (Deserializers) and the formatted text as header looks a little odd in my opinion.  Maybe you could remove the headers from each and just reference the properties in each paragraph.  Possibly add bullet points if that would look good.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r450323669", "createdAt": "2020-07-06T16:01:44Z", "author": {"login": "rspurgeon"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+Then create a development configuration file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-consumer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick overview of some of the more important properties here:\n+\n+Deserilizers:: The `key.deserializer` and `value.deserializer` properties provide a class implementing the `Deserializer` interface for converting `byte` arrays into the expected object type of the key and value respectively.\n+\n+`max.poll.interval.ms`:: The `max.poll.interval.ms` is the maximum amount of time a consumer may take between calls to `Consumer.poll()`.  If a consumer instance takes longer than the specified time, it's considered non-reponsive and removed from the consumer-group triggering a rebalance.\n+\n+`enable.auto.commit`:: Setting this configuration to `true` enables the Kafka consumer to handle commiting offsets automatically for you.  The default setting is `true`, but it's included here to make it explicit.  When you enable auto commit, you need to ensure you've processed all records _**before**_ the consumer calls `poll` again.  Once there is a subsequent call to `poll`, all the records returned from the previous call are considered processed and the consumer commits the offsets.\n+\n+`auto.offset.reset`::  If a consumer instance can't locate any offsets for its topic-partition assignment(s), it will resume processing from the _**earliest**_ available offset.\n+\n+`group.id`:: Kafka uses the concept of a consumer-group which is used to represent a logical single group.  A consumer-group can be made up of mulitple members all sharing the same `group.id` configuration.  As members leave or join the consumer-group, the group-coordinator triggres a rebalance which causes topic-partion reassignment among active members of the group.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkwODY0Ng==", "bodyText": "Yep will do.  Good call.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r458908646", "createdAt": "2020-07-22T16:08:59Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+Then create a development configuration file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-consumer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick overview of some of the more important properties here:\n+\n+Deserilizers:: The `key.deserializer` and `value.deserializer` properties provide a class implementing the `Deserializer` interface for converting `byte` arrays into the expected object type of the key and value respectively.\n+\n+`max.poll.interval.ms`:: The `max.poll.interval.ms` is the maximum amount of time a consumer may take between calls to `Consumer.poll()`.  If a consumer instance takes longer than the specified time, it's considered non-reponsive and removed from the consumer-group triggering a rebalance.\n+\n+`enable.auto.commit`:: Setting this configuration to `true` enables the Kafka consumer to handle commiting offsets automatically for you.  The default setting is `true`, but it's included here to make it explicit.  When you enable auto commit, you need to ensure you've processed all records _**before**_ the consumer calls `poll` again.  Once there is a subsequent call to `poll`, all the records returned from the previous call are considered processed and the consumer commits the offsets.\n+\n+`auto.offset.reset`::  If a consumer instance can't locate any offsets for its topic-partition assignment(s), it will resume processing from the _**earliest**_ available offset.\n+\n+`group.id`:: Kafka uses the concept of a consumer-group which is used to represent a logical single group.  A consumer-group can be made up of mulitple members all sharing the same `group.id` configuration.  As members leave or join the consumer-group, the group-coordinator triggres a rebalance which causes topic-partion reassignment among active members of the group.\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMyMzY2OQ=="}, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNzY1OTA0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-consumer-app.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNjoyODo0MlrOGtemUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNjoxNDo1OVrOG1p0Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM0MDQzMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this tutorial you'll inject the dependencies in the `KafkaConsumerApplication.main()` method, but in practice you'll may use some depnendency injection code such as the  https://spring.io/projects/spring-framework[Spring Framework].\n          \n          \n            \n            In this tutorial you'll inject the dependencies in the `KafkaConsumerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the https://spring.io/projects/spring-framework[Spring Framework].", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r450340432", "createdAt": "2020-07-06T16:28:42Z", "author": {"login": "rspurgeon"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-consumer-app.adoc", "diffHunk": "@@ -0,0 +1,62 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+To complete this tutorial, you'll build a main application class and a helper class\n+\n+\n+First, you'll create the main application,`KafkaConsumerApplication`, which is the focal point of this tutorial; consuming records from a Kafka topic.\n+\n+\n+Let's go over some of the key parts of the `KafkaConsumerApplication` starting with the constructor:\n+\n+[source, java]\n+.KafkaConsumerApplication constructor\n+----\n+public KafkaConsumerApplication(final Consumer<String, String> consumer,\n+                                final ConsumerRecordsHandler<String, String> recordsHandler) { // <1>\n+    this.consumer = consumer;\n+    this.recordsHandler = recordsHandler;\n+}\n+----\n+\n+<1> Here you're suppling instances of the `Consumer` and `ConsumerRecordsHandler` via constructor parameters.\n+\n+By using interfaces vs. concreate implentations you can more easily test the `KafkaConsumerApplication` class by swapping in a `MockConsumer` for the test.  We'll cover testing in an upcoming section.  Also, interfaces make it simple to change `ConsumerRecord` handling at run-time.\n+\n+In this tutorial you'll inject the dependencies in the `KafkaConsumerApplication.main()` method, but in practice you'll may use some depnendency injection code such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkxMjgxOQ==", "bodyText": "ack", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r458912819", "createdAt": "2020-07-22T16:14:59Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/make-consumer-app.adoc", "diffHunk": "@@ -0,0 +1,62 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+To complete this tutorial, you'll build a main application class and a helper class\n+\n+\n+First, you'll create the main application,`KafkaConsumerApplication`, which is the focal point of this tutorial; consuming records from a Kafka topic.\n+\n+\n+Let's go over some of the key parts of the `KafkaConsumerApplication` starting with the constructor:\n+\n+[source, java]\n+.KafkaConsumerApplication constructor\n+----\n+public KafkaConsumerApplication(final Consumer<String, String> consumer,\n+                                final ConsumerRecordsHandler<String, String> recordsHandler) { // <1>\n+    this.consumer = consumer;\n+    this.recordsHandler = recordsHandler;\n+}\n+----\n+\n+<1> Here you're suppling instances of the `Consumer` and `ConsumerRecordsHandler` via constructor parameters.\n+\n+By using interfaces vs. concreate implentations you can more easily test the `KafkaConsumerApplication` class by swapping in a `MockConsumer` for the test.  We'll cover testing in an upcoming section.  Also, interfaces make it simple to change `ConsumerRecord` handling at run-time.\n+\n+In this tutorial you'll inject the dependencies in the `KafkaConsumerApplication.main()` method, but in practice you'll may use some depnendency injection code such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM0MDQzMg=="}, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNzcxMTI1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/print-consumer-file-results.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNjo0MjoxMlrOGtfGgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxNjo0MjoxMlrOGtfGgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM0ODY3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Run this command to print the results to the console:\n          \n          \n            \n            In a new terminal, run this command to print the results to the console:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/463#discussion_r450348672", "createdAt": "2020-07-06T16:42:12Z", "author": {"login": "rspurgeon"}, "path": "_includes/tutorials/kafka-consumer-application/kafka/markup/dev/print-consumer-file-results.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+////\n+  This is a sample content file for how to include a console consumer to the tutorial, probably a good idea so the end user can watch the results\n+  of the tutorial.  Change the text as needed.\n+\n+////\n+\n+Your consumer application should have consumed all the records sent and written them out to a file.\n+\n+Run this command to print the results to the console:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aca116bd56c063a547786e68da31c8c78b554fa3"}, "originalPosition": 9}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3956, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}