{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5NjI4MTQw", "number": 491, "title": "DEVX-1942: Create first producer application", "bodyText": "Covers #371", "createdAt": "2020-07-30T21:58:33Z", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491", "merged": true, "mergeCommit": {"oid": "799d663012fbf8ad28b24984ecd2e43baa4f32c8"}, "closed": true, "closedAt": "2020-08-14T14:46:52Z", "author": {"login": "bbejeck"}, "timelineItems": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc6H3oDgFqTQ1ODgxNTkxNQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-p9ZCgFqTQ2NzI0Njg1MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODE1OTE1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458815915", "createdAt": "2020-07-30T22:51:29Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMjo1MToyOVrOG52exQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMzowMjo0NVrOG52spw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNDYyOQ==", "bodyText": "@bbejeck should producer precede consumer (swap lines)?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463314629", "createdAt": "2020-07-30T22:51:29Z", "author": {"login": "ybyzek"}, "path": "index.html", "diffHunk": "@@ -41,6 +41,7 @@ <h2 class=\"subtitle\">Apache Kafka is a powerful, scalable, fault-tolerant distri\n             <li><a href=\"kafka-console-consumer-read-specific-offsets-partitions/kafka.html\">Console consumer reads from a specific offset and partition</a></li>\n             <li><a href=\"change-topic-partitions-replicas/ksql.html\">Change the number of partitions and replicas of a Kafka topic</a></li>\n             <li><a href=\"creating-first-apache-kafka-consumer-application/kafka.html\">Build your first Kafka consumer application</a></li>\n+            <li><a href=\"creating-first-apache-kafka-producer-application/kafka.html\">Build your first Kafka producer application</a></li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTExNQ==", "bodyText": "Should this be materialized on a new line (see generated output -- it's on the same line as above)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463315115", "createdAt": "2020-07-30T22:53:03Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNTYzMw==", "bodyText": "Can you please elaborate, of all the producer configuration parameters, why mention just acks?\nIdentify the default value for acks\nConsider giving users a link to read up on other producer configuration parameters", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463315633", "createdAt": "2020-07-30T22:54:42Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,15 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+`key.serializer` - The serializer the `KafkaProducer` will use to serialize the key.\n+`value.serializer` - The serializer the `KafkaProducer` will use to serialize the value.\n+`acks` - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, or `all`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNjQzMA==", "bodyText": "The # make it a bit hard to read. For consideration, use , or : instead", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463316430", "createdAt": "2020-07-30T22:57:04Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc", "diffHunk": "@@ -0,0 +1,16 @@\n+////\n+   Example content file for how to include a console produer(s) in the tutorial.\n+   Usually you'll include a line referencing the script to run the console producer and also include some content\n+   describing how to input data as shown below.\n+\n+   Again modify this file as you need for your tutorial, as this is just sample content.  You also may have more than one\n+   console producer to run depending on how you structure your tutorial\n+\n+////\n+\n+Create the following file `input.txt` in the base directory of the tutorial.  The numbers before the `#` will be the key and the part after will be the value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg==", "bodyText": "Why is this implemented with the user being prompted to provide path name, instead of just an argument appended to java -jar build/libs/kafka-producer-application-standalone-0.0.1.jar configuration/dev.properties?  FYC, the latter has better UX", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463317282", "createdAt": "2020-07-30T22:59:49Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n+           } catch (InterruptedException | ExecutionException e) {\n+               if (e instanceof  InterruptedException) {\n+                  Thread.currentThread().interrupt();\n+               }\n+           }\n+         });\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 1) {\n+      throw new IllegalArgumentException(\n+          \"This program takes one argument: the path to an environment configuration file.\");\n+    }\n+\n+    final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+    final String topic = props.getProperty(\"output.topic.name\");\n+    final Producer<String, String> producer = new KafkaProducer<>(props);\n+    final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+\n+    try (final BufferedReader stdinReader = new BufferedReader(new InputStreamReader(System.in))) {\n+      String filePath;\n+      System.out.println(\"Enter the file path to publish records > \");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxODE4Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                         System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n          \n          \n            \n                         System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" and timestamp \" + recordMetadata.timestamp());", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463318183", "createdAt": "2020-07-30T23:02:45Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU0Nzk0", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458854794", "createdAt": "2020-07-31T00:53:35Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1MzozNVrOG54nOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1MzozNVrOG54nOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTU2Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n          \n          \n            \n            Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463349563", "createdAt": "2020-07-31T00:53:35Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU1MTIx", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458855121", "createdAt": "2020-07-31T00:54:42Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NDo0MlrOG54oYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NDo0MlrOG54oYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0OTg1Ng==", "bodyText": "FYC would this be appropriate to name as Write a unit test (note term unit is added)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463349856", "createdAt": "2020-07-31T00:54:42Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-producer-application/kafka.yml", "diffHunk": "@@ -0,0 +1,166 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Create a topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/harness-create-topic.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc\n+\n+    - title: Create the KafkaProducer application\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-src-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-src-dir.adoc\n+            \n+        - action: make_file\n+          file: src/main/java/io/confluent/developer/KafkaProducerApplication.java\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc \n+\n+    - title: Create data to produce to Kafka\n+      content:\n+        - action: make_file\n+          file: input.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-data-file.adoc        \n+\n+    - title: Compile and run the KafkaProducer application\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/build-uberjar.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/build-uberjar.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/run-dev-app.sh\n+          stdin: tutorial-steps/dev/input-harness-filename.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/run-dev-app.adoc\n+\n+    - title: Confirm records sent by consuming from topic\n+      content:\n+        - action: execute_async\n+          file: tutorial-steps/dev/harness-console-consumer.sh\n+          stdout: tutorial-steps/dev/outputs/actual-output.txt\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/run-consumer.adoc\n+\n+        - name: wait for the consumer to read the messages\n+          action: sleep\n+          ms: 5000\n+          render:\n+            skip: true\n+\n+test:\n+  steps:\n+    - title: Create a test configuration file\n+      content:\n+        - action: make_file\n+          file: configuration/test.properties\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/test/make-test-file.adoc\n+\n+    - title: Write a test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU1MzUw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458855350", "createdAt": "2020-07-31T00:55:29Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NToyOVrOG54pNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NToyOVrOG54pNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDA2OA==", "bodyText": "FYC: should all these be changed from *-ing to just *, e.g. Calling -> Call", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350068", "createdAt": "2020-07-31T00:55:29Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU1NDgw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458855480", "createdAt": "2020-07-31T00:55:58Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NTo1OFrOG54pnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NTo1OFrOG54pnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDE3Mg==", "bodyText": "What precisely is \"validated\"?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350172", "createdAt": "2020-07-31T00:55:58Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method\n+<2> Building the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to validate the records sent to the producer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU1Njk0", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458855694", "createdAt": "2020-07-31T00:56:38Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NjozOFrOG54qTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1NjozOFrOG54qTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDM0OQ==", "bodyText": "Is input topic required here, like the other stages?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350349", "createdAt": "2020-07-31T00:56:38Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/configuration/prod.properties", "diffHunk": "@@ -0,0 +1,8 @@\n+bootstrap.servers=<FILL ME IN>\n+\n+key.serializer=org.apache.kafka.common.serialization.StringSerializer\n+value.serializer=org.apache.kafka.common.serialization.StringSerializer\n+acks=all\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU2MDI1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458856025", "createdAt": "2020-07-31T00:57:56Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1Nzo1N1rOG54rfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1Nzo1N1rOG54rfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDY1Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now create the following file at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.\n          \n          \n            \n            Including the code snippet explained above, now create the complete test code at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350652", "createdAt": "2020-07-31T00:57:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Earlier in this tutorial, I mentioned that using a thin wrapper class around the `KafkaProducer` makes testing easier.  The key enabler for testing is that the `KafkaProducerApplication` accepts an instnace of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface leaves us free to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Calling the `produce` method\n+<2> Building the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to validate the records sent to the producer\n+\n+Now create the following file at `src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU2MzI1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458856325", "createdAt": "2020-07-31T00:59:00Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTowMFrOG54sgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTowMFrOG54sgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MDkxNQ==", "bodyText": "It's a simple question of weight ratios!", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463350915", "createdAt": "2020-07-31T00:59:00Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/input.txt", "diffHunk": "@@ -0,0 +1,13 @@\n+1#value\n+2#words\n+3#All Streams\n+4#Lead to\n+5#Kafka\n+6#Go to\n+7#Kafka Summit\n+8#How can\n+9#a 10 ounce\n+10#bird carry a\n+11#5lb coconut", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU2NTgy", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458856582", "createdAt": "2020-07-31T00:59:53Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTo1M1rOG54tgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDo1OTo1M1rOG54tgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTE2OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now go ahead and create the following file at `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.\n          \n          \n            \n            Including the code snippet explained above, now create the complete application code at\n          \n          \n            \n            `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463351168", "createdAt": "2020-07-31T00:59:53Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. The `RecordMetadata` contains metadata about a particular record after it has been acknowledged.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.\n+\n+\n+Now go ahead and create the following file at `src/main/java/io/confluent/developer/KafkaProducerApplication.java`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU2OTM5", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458856939", "createdAt": "2020-07-31T01:01:05Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMTowNlrOG54unw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMTowNlrOG54unw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTQ1NQ==", "bodyText": "What's the value of injecting Spring into this tutorial?  SEO? ;)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463351455", "createdAt": "2020-07-31T01:01:06Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU3MTMy", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458857132", "createdAt": "2020-07-31T01:01:48Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMTo0OFrOG54vUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowMTo0OFrOG54vUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTYzMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n          \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n          \n      \n    \n    \n  \n\nTrimming, it's redundant to the statement at the top, and then explained later in testing section", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463351633", "createdAt": "2020-07-31T01:01:48Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU4MDU0", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458858054", "createdAt": "2020-07-31T01:04:55Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowNDo1NVrOG54yag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMTowNDo1NVrOG54yag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MjQyNg==", "bodyText": "FYC: add note...something like...\"Once the broker acknowledges that the record has been appended to its log, the broker returns  to the producer, which the application receives as RecordMetadata\u2014information about the committed message.\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463352426", "createdAt": "2020-07-31T01:04:55Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.  Again by having the processing encapsulated in a method we can easily test this code.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. The `RecordMetadata` contains metadata about a particular record after it has been acknowledged.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4ODU4Mzgx", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-458858381", "createdAt": "2020-07-31T01:06:12Z", "commit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/b0db1b51adc885823de57d076c40f40fb33171c3", "committedDate": "2020-07-30T22:04:20Z", "message": "fix semphore entry"}, "afterCommit": {"oid": "3f3bcbfa803e72dbb74de32c9aa0130de4534419", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/3f3bcbfa803e72dbb74de32c9aa0130de4534419", "committedDate": "2020-08-12T20:38:34Z", "message": "Updates for comments; rebased for merge conflicts"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3f3bcbfa803e72dbb74de32c9aa0130de4534419", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/3f3bcbfa803e72dbb74de32c9aa0130de4534419", "committedDate": "2020-08-12T20:38:34Z", "message": "Updates for comments; rebased for merge conflicts"}, "afterCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/7c9bdc89e6c17d6be39a96f001cc551c1162cefe", "committedDate": "2020-08-12T22:13:52Z", "message": "Rebased with master, added new tutorial to settings.gradle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MzU4MjQy", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-466358242", "createdAt": "2020-08-13T00:14:10Z", "commit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDoxNDoxMFrOG_3Hkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDoxNDoxMFrOG_3Hkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYxNjUzMQ==", "bodyText": "Overall the description of the various acks settings seems overly verbose and potentially too much to throw at a new user.  FYC make a pruned down version and link to the docs for more information.  A pruned down version could look something like this (just as an example):\n- `acks=0`: \"fire and forget\", once the producer sends the record batch it is considered successful\n- `acks=1`: leader broker added the records to its local log but didn't wait for any acknowledgement from the followers\n- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgement of replication from all in-sync replicas", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469616531", "createdAt": "2020-08-13T00:14:10Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n+\n+\n+- Setting `acks` to `0` could be considered \"fire and forget\" as once the producer sends the record batch it is considered succesful even though the records may not reach the broker at all. So record loss is possible.  The offsets for records are always set to `-1`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MzcxNDQ3", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-466371447", "createdAt": "2020-08-13T00:48:27Z", "commit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDo0ODoyN1rOG_36tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMDo0OTowMlrOG_37NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyOTYyMQ==", "bodyText": "For consistency, should this be Use instead of Using?  (Or change the other ones)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469629621", "createdAt": "2020-08-13T00:48:27Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Call the `produce` method\n+<2> Build the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyOTc0OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones\n          \n          \n            \n            <3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones sent\n          \n      \n    \n    \n  \n\nIIUC does sent belong at the end of the sentence?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469629749", "createdAt": "2020-08-13T00:49:02Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/test/make-application-test.adoc", "diffHunk": "@@ -0,0 +1,38 @@\n+////\n+  This content file is used to describe how to add test code you developed in this tutorial.  You'll need to update the\n+  text to suit your test code.\n+\n+\n+////\n+\n+Next we will see why the thin wrapper class around the `KafkaProducer` makes testing easier.  The `KafkaProducerApplication` accepts an instance of the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/Producer.html[Producer] **_interface_**.  The use of the interface allows us to inject any concrete type we want, including a https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/MockProducer.html[Mock Producer] for testing.\n+\n+We use a `MockProducer` because you only want to test your own code.  So you really only need to test that the producer recieves the expected records and in the expected format.  Plus since there is no broker, the tests run very fast, which becomes an important factor as the number of tests increase.\n+\n+\n+\n+\n+There is only one method in `KafkaProducerApplicationTest` annotated with `@Test`, and that is `testProduce()`.  Before you create the test, let's go over a few of the key points of the test\n+\n+[source, java]\n+----\n+final List<String> records = Arrays.asList(\"foo#bar\", \"bar#foo\", \"baz#bar\", \"great-weather\");\n+\n+records.forEach(producerApp::produce); <1>\n+\n+final List<KeyValue<String, String>> expectedList = Arrays.asList(KeyValue.pair(\"foo\", \"bar\"),\n+            KeyValue.pair(\"bar\", \"foo\"),\n+            KeyValue.pair(\"baz\", \"bar\"),\n+            KeyValue.pair(\"NO-KEY\",\"great-weather\")); <2>\n+final List<KeyValue<String, String>> actualList = mockProducer.history().stream().map(this::toKeyValue).collect(Collectors.toList()); <3>\n+\n+----\n+\n+<1> Call the `produce` method\n+<2> Build the expected list of records the producer should recieve\n+<3> Using the `MockProducer.history()` method to get the records sent to the producer so the test can assert the expected records match the actual ones", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2OTk0MjIw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-466994220", "createdAt": "2020-08-13T17:43:21Z", "commit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "state": "COMMENTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo0MzoyMVrOHAWqXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1Nzo0MFrOHAXRew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzMzM0Mw==", "bodyText": "It reads strangely to say we're going to do something but then do something else.  So FYC, remove this line.  Then see comments below", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470133343", "createdAt": "2020-08-13T17:43:21Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzMzc2NA==", "bodyText": "FYC, Delete this line", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470133764", "createdAt": "2020-08-13T17:43:50Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNDIxMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Open a new terminal and window then run this command:\n          \n          \n            \n            Open a new terminal window and then run this command to open a shell on the broker docker container", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470134210", "createdAt": "2020-08-13T17:44:20Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.\n+\n+Open a new terminal and window then run this command:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNDU3Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now use the following command to create the topic for the producer to write to\n          \n          \n            \n            Next, create the topic that the producer to write to", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470134572", "createdAt": "2020-08-13T17:44:44Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.\n+\n+Open a new terminal and window then run this command:\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/open-docker-shell.sh %}</code></pre>\n++++++\n+\n+Now use the following command to create the topic for the producer to write to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNTIxMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Keep this terminal window open as you'll need to run a console conumer to verify your producer application in a few steps.\n          \n          \n            \n            Keep this terminal window open for later use when you run a console consumer to verify your producer application.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470135213", "createdAt": "2020-08-13T17:45:23Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+\n+In this step we're going to create a topic for use during this tutorial.\n+\n+But first, you're going to open a shell on the broker docker container.\n+\n+Open a new terminal and window then run this command:\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/open-docker-shell.sh %}</code></pre>\n++++++\n+\n+Now use the following command to create the topic for the producer to write to\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/create-topic.sh %}</code></pre>\n++++++\n+\n+Keep this terminal window open as you'll need to run a console conumer to verify your producer application in a few steps.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNTU2Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Create the following Gradle build file, named `build.gradle` for the project:\n          \n          \n            \n            Create the following Gradle build file for the project, named `build.gradle`:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470135563", "createdAt": "2020-08-13T17:45:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Create the following Gradle build file, named `build.gradle` for the project:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNTc3NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            And be sure to run the following command to obtain the Gradle wrapper:\n          \n          \n            \n            Run the following command to obtain the Gradle wrapper:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470135774", "createdAt": "2020-08-13T17:46:19Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+And be sure to run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNjU1Mw==", "bodyText": "FYC: this should be a separate title because it's not for the project, it's about producer properties?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470136553", "createdAt": "2020-08-13T17:47:28Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/kafka-producer-application/kafka.yml", "diffHunk": "@@ -0,0 +1,166 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Create a topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/harness-create-topic.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/create-topic.adoc\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/kafka-producer-application/kafka/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNjgwMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Let's do a quick walkthrough of some of the properties.\n          \n          \n            \n            Let's do a quick walkthrough of some of the producer properties.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470136801", "createdAt": "2020-08-13T17:47:53Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzNzQzNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n          \n          \n            \n            https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable values for `acks` are: `0`, `1` (the default), `-1`, or `all`.  Setting `acks` to `-1` is the same as setting it to `all`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470137436", "createdAt": "2020-08-13T17:48:55Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzODAwNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            I've only covered a small sub-set of producer confgurations, the full list can be found in the https://kafka.apache.org/documentation/#producerconfigs[producer configs section] in the Apache Kafka documentation.\n          \n          \n            \n            This is only a small sub-set of producer configuration parameters. The full list of producer configuration parameters can be found in the https://kafka.apache.org/documentation/#producerconfigs[Apache Kafka documentation].", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470138004", "createdAt": "2020-08-13T17:49:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-dev-file.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+Then create a development file at `configuration/dev.properties`:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/kafka-producer-application/kafka/code/configuration/dev.properties %}</code></pre>\n++++++\n+\n+Let's do a quick walkthrough of some of the properties.\n+\n+https://kafka.apache.org/documentation/#key.serializer[key.serializer] - The serializer the `KafkaProducer` will use to serialize the key.\n+\n+https://kafka.apache.org/documentation/#value.serializer[value.serializer] - The serializer the `KafkaProducer` will use to serialize the value.\n+\n+https://kafka.apache.org/documentation/#acks[acks] - The `KafkaProducer` uses the `acks` configuration to tell the lead broker how many acknowledements to wait for to consider a produce request complete. Acceptable settings are `0`, `1`, `-1`, or all`.  Setting `acks` to `-1` is the same as setting it to `all`.  The default setting is `1`.\n+\n+\n+- `acks=0`: \"fire and forget\", once the producer sends the record batch it is considered successful\n+- `acks=1`: leader broker added the records to its local log but didn't wait for any acknowledgement from the followers\n+- `acks=all`: highest data durability guarantee, the leader broker persisted the record to its log and received acknowledgement of replication from all in-sync replicas When using `aks=all`, it's stongly reccomended to update https://kafka.apache.org/documentation/#min.insync.replicas[min.insync.replicas] as well.\n+\n+\n+I've only covered a small sub-set of producer confgurations, the full list can be found in the https://kafka.apache.org/documentation/#producerconfigs[producer configs section] in the Apache Kafka documentation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTUzOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n          \n          \n            \n            In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method.\n          \n          \n            \n            Having this thin wrapper class around a `Producer` is not required, but it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n          \n          \n            \n            \n          \n          \n            \n            (In practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework]).", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470139539", "createdAt": "2020-08-13T17:52:11Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTY4OQ==", "bodyText": "If you accept the change above, you can delete this", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470139689", "createdAt": "2020-08-13T17:52:22Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDEzOTk2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n          \n          \n            \n            The `KafkaProducerApplication.produce` method does some processing on a `String`, and then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470139961", "createdAt": "2020-08-13T17:52:48Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MDM2NA==", "bodyText": "Should we delete this space?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470140364", "createdAt": "2020-08-13T17:53:20Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MDk2Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.\n          \n          \n            \n            The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  Note that calling `Future.get()` for any record will block until the produce request completes.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470140966", "createdAt": "2020-08-13T17:54:14Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,70 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+Before you create your application file, let's look at some of the key points of this program:\n+\n+[source, java]\n+.KafkaProducerApplication constructor\n+----\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,  <1>\n+                                  final String topic) {                     <2>\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+----\n+\n+<1> Passing in the `Producer` instance as a constructor parameter.\n+<2> The topic to write records to\n+\n+\n+In this tutorial you'll inject the dependencies in the `KafkaProducerApplication.main()` method, but in practice you may want to use a dependency injection framework library, such as the  https://spring.io/projects/spring-framework[Spring Framework].\n+\n+Having this thin wrapper class around a `Producer` is not required.  But it does help with making our code easier to test.  We'll go into more details in the testing section of the tutorial.\n+\n+\n+Next let's take a look at the `KafkaProducerApplication.produce` method\n+[source, java]\n+.KafkaProducerApplication.produce\n+----\n+public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"-\");  <1>\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);  <2>\n+    return producer.send(producerRecord);                 <3>\n+  }\n+\n+----\n+\n+<1> Processing the String for sending message\n+<2> Creating the `ProducerRecord`\n+<3> Sending the record to the broker\n+\n+The `KafkaProducerApplication.produce` method does some processing on a `String` then sends the https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html[`ProducerRecord`].  While this code is a trivial example, it's enough to show the example of using a `KafkaProducer`.\n+\n+Notice that https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html#send-org.apache.kafka.clients.producer.ProducerRecord-[`KafkaProducer.send`] returns a https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/Future.html[Future] with a type of https://kafka.apache.org/25/javadoc/org/apache/kafka/clients/producer/RecordMetadata.html[RecordMetadata].\n+\n+The `KafkaProducer.send` method is asynchronous and returns as soon as the provided record is placed in the buffer of records to be sent to the broker. Once the broker acknowledges that the record has been appended to its log, the broker completes the produce request, which the application receives as `RecordMetadata`\u2014information about the committed message.  This tutorial prints the `timestamp` and `offset` for each record sent using the `RecordMetadata` object.  You should note that calling `Future.get()` for any record will block until the produce request completes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MzM1NQ==", "bodyText": "If a user wants to process additional files, could they just run it again?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470143355", "createdAt": "2020-08-13T17:57:40Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n+           } catch (InterruptedException | ExecutionException e) {\n+               if (e instanceof  InterruptedException) {\n+                  Thread.currentThread().interrupt();\n+               }\n+           }\n+         });\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 1) {\n+      throw new IllegalArgumentException(\n+          \"This program takes one argument: the path to an environment configuration file.\");\n+    }\n+\n+    final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+    final String topic = props.getProperty(\"output.topic.name\");\n+    final Producer<String, String> producer = new KafkaProducer<>(props);\n+    final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+\n+    try (final BufferedReader stdinReader = new BufferedReader(new InputStreamReader(System.in))) {\n+      String filePath;\n+      System.out.println(\"Enter the file path to publish records > \");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg=="}, "originalCommit": {"oid": "b0db1b51adc885823de57d076c40f40fb33171c3"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY3MDA1MjMw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-467005230", "createdAt": "2020-08-13T17:58:35Z", "commit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1ODozNVrOHAXTpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxNzo1OToyNFrOHAXVpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MzkwOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now let's run a console consumer on the output topic to confirm you're application parsed the file correctly and published the expected records.\n          \n          \n            \n            Now run a console consumer that will read topics from the output topic to confirm your application published the expected records.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470143909", "createdAt": "2020-08-13T17:58:35Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/markup/dev/run-consumer.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+////\n+  This is a sample content file for how to include a console consumer to the tutorial, probably a good idea so the end user can watch the results\n+  of the tutorial.  Change the text as needed.\n+\n+////\n+\n+Now let's run a console consumer on the output topic to confirm you're application parsed the file correctly and published the expected records.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0NDQyMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            kafka-console-consumer --topic output-topic --bootstrap-server broker:9092 \\\n          \n          \n            \n            kafka-console-consumer --topic output-topic \\\n          \n          \n            \n             --bootstrap-server broker:9092 \\", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470144421", "createdAt": "2020-08-13T17:59:24Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/kafka-producer-application/kafka/code/tutorial-steps/dev/console-consumer.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+kafka-console-consumer --topic output-topic --bootstrap-server broker:9092 \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f44e1abbe853853e673a6b7cbb01c694edb55397", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/f44e1abbe853853e673a6b7cbb01c694edb55397", "committedDate": "2020-08-13T19:23:35Z", "message": "initial commit for producer application"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b586f7c575088f0931c06b2148230ffc8e816ad0", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/b586f7c575088f0931c06b2148230ffc8e816ad0", "committedDate": "2020-08-13T19:23:35Z", "message": "added dev steps for producer application"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e25406254728ce0424f5e6f6d61db7af532d5ebe", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/e25406254728ce0424f5e6f6d61db7af532d5ebe", "committedDate": "2020-08-13T19:23:35Z", "message": "tutorial running from end-to-end"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "947122d3eeb76f531f8350d9075dd3c1fdce1032", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/947122d3eeb76f531f8350d9075dd3c1fdce1032", "committedDate": "2020-08-13T19:23:36Z", "message": "fix semphore entry"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c685747a25cc1beaddce512e92c35bf2748243f", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/8c685747a25cc1beaddce512e92c35bf2748243f", "committedDate": "2020-08-13T19:23:36Z", "message": "Updates for comments; rebased for merge conflicts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12f8ae84fff809c6d0cb570286eaa13341394104", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/12f8ae84fff809c6d0cb570286eaa13341394104", "committedDate": "2020-08-13T19:23:36Z", "message": "Rebased with master, added new tutorial to settings.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b91ec35fc12f730fd5f935bdabd989d791667f70", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/b91ec35fc12f730fd5f935bdabd989d791667f70", "committedDate": "2020-08-13T19:23:36Z", "message": "updates for comments from @ybyzek"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dce77ec559dce21beb98b20f7672ff7192c4735a", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/dce77ec559dce21beb98b20f7672ff7192c4735a", "committedDate": "2020-08-13T21:21:31Z", "message": "further updates per comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "98340011f27127a9c6cd7b1c22a176983023bb9b", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/98340011f27127a9c6cd7b1c22a176983023bb9b", "committedDate": "2020-08-13T15:35:12Z", "message": "updates for comments from @ybyzek"}, "afterCommit": {"oid": "dce77ec559dce21beb98b20f7672ff7192c4735a", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/dce77ec559dce21beb98b20f7672ff7192c4735a", "committedDate": "2020-08-13T21:21:31Z", "message": "further updates per comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY3MjQ2ODUx", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#pullrequestreview-467246851", "createdAt": "2020-08-14T01:02:33Z", "commit": {"oid": "dce77ec559dce21beb98b20f7672ff7192c4735a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 206, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}