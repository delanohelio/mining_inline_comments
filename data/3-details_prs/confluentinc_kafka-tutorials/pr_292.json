{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkxMDY5Njk5", "number": 292, "title": "Use ValueTransformerWithKey instead of Transformer", "bodyText": "Hello,\nthe current version of the tutorial uses Transformer. However, since Transformer may change the key, its usage will lead to redundant repartitioning when grouping operations are used aftewards.\nI propose to rewrite this example using ValueTransformer[WithKey], which will not cause the unwanted repartitioning.\nBy the way, it seems to me that we have a bug in transformValues: see the filter((k, v) -> v != null) after it. Without this filter, null values appear downstream, contrary to what is said in JavaDoc:\n\nIf the return value of ValueTransformerWithKey#transform(Object, Object)  is null, no records are emitted.", "createdAt": "2020-03-19T15:26:15Z", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292", "merged": true, "mergeCommit": {"oid": "7845c7a06b98b89f2f30f0f067ac47a5b326c591"}, "closed": true, "closedAt": "2020-04-08T22:47:17Z", "author": {"login": "inponomarev"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcPNdrsAH2gAyMzkxMDY5Njk5OjhiZWZkNDZkZTY3ZmI5MzIzNjhlMmEyZmMyMmYwM2ZmYWUyMzk0YmU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcVv0xKAFqTM5MDM4MDM0OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8befd46de67fb932368e2a2fc22f03ffae2394be", "author": {"user": {"login": "inponomarev", "name": "Ivan Ponomarev"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/8befd46de67fb932368e2a2fc22f03ffae2394be", "committedDate": "2020-03-19T15:16:08Z", "message": "use ValueTransformerWithKey instead of Transformer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57793a4fa13ccfe22d4037c4df462fcf062c31ab", "author": {"user": {"login": "inponomarev", "name": "Ivan Ponomarev"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/57793a4fa13ccfe22d4037c4df462fcf062c31ab", "committedDate": "2020-04-07T12:05:18Z", "message": "update the tutoral"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5MjI4NDI5", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#pullrequestreview-389228429", "createdAt": "2020-04-07T15:24:09Z", "commit": {"oid": "57793a4fa13ccfe22d4037c4df462fcf062c31ab"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyNDowOVrOGCI_bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNToyNDowOVrOGCI_bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDg5NzY0NA==", "bodyText": "Unfortunately, call-outs don't render correctly at the moment.  Can you remove the NOTE: text?  Maybe you could do something like **_Note that we are using_** instead.  We are working on adding support for call-outs.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#discussion_r404897644", "createdAt": "2020-04-07T15:24:09Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/finding-distinct/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -1,6 +1,9 @@\n Then create the following file at `src/main/java/io/confluent/developer/FindDistinctEvents.java`\n \n-Focusing on the `buildTopology` method, note how the Kafka Streams topology relies on a `https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/Transformer.html[Transformer]` and a `https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/state/WindowStore.html[Window Store]` to filter out the duplicate IP addresses.   Events are de-duped within a 2 minute window, and unique clicks are produced to a new topic named `distinct-clicks`.\n+Focusing on the `buildTopology` method, note how the Kafka Streams topology relies on a `https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/ValueTransformerWithKey.html[ValueTransformerWithKey]` and a `https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/state/WindowStore.html[Window Store]` to filter out the duplicate IP addresses.   Events are de-duped within a 2 minute window, and unique clicks are produced to a new topic named `distinct-clicks`.\n+\n+NOTE: Note that we are using `https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/ValueTransformerWithKey.html[ValueTransformerWithKey]` here instead of `https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/Transformer.html[Transformer]` since we need keys to transform data, but there is no need to re-key the stream. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57793a4fa13ccfe22d4037c4df462fcf062c31ab"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5Mjk0MTky", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#pullrequestreview-389294192", "createdAt": "2020-04-07T16:35:25Z", "commit": {"oid": "57793a4fa13ccfe22d4037c4df462fcf062c31ab"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNjozNToyNVrOGCMQAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNjozNToyNVrOGCMQAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk1MTA0Mg==", "bodyText": "Suggestion - I know this is pre-existing code, but can we update close() to close(Duration.ofSeconds(5))", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#discussion_r404951042", "createdAt": "2020-04-07T16:35:25Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/finding-distinct/kstreams/code/src/main/java/io/confluent/developer/FindDistinctEvents.java", "diffHunk": "@@ -30,224 +30,227 @@\n \n public class FindDistinctEvents {\n \n-  private static final String storeName = \"eventId-store\";\n-\n-  /**\n-   * Discards duplicate click events from the input stream by ip address\n-   * <p>\n-   * Duplicate records are detected based on ip address\n-   * The transformer remembers known ip addresses within an associated window state\n-   * store, which automatically purges/expires IPs from the store after a certain amount of\n-   * time has passed to prevent the store from growing indefinitely.\n-   * <p>\n-   * Note: This code is for demonstration purposes and was not tested for production usage.\n-   */\n-  private static class DeduplicationTransformer<K, V, E> implements Transformer<K, V, KeyValue<K, V>> {\n-\n-    private ProcessorContext context;\n+    private static final String storeName = \"eventId-store\";\n \n     /**\n-     * Key: ip address\n-     * Value: timestamp (event-time) of the corresponding event when the event ID was seen for the\n-     * first time\n+     * Discards duplicate click events from the input stream by ip address\n+     * <p>\n+     * Duplicate records are detected based on ip address\n+     * The transformer remembers known ip addresses within an associated window state\n+     * store, which automatically purges/expires IPs from the store after a certain amount of\n+     * time has passed to prevent the store from growing indefinitely.\n+     * <p>\n+     * Note: This code is for demonstration purposes and was not tested for production usage.\n      */\n-    private WindowStore<E, Long> eventIdStore;\n+    private static class DeduplicationTransformer<K, V, E> implements ValueTransformerWithKey<K, V, V> {\n+\n+        private ProcessorContext context;\n+\n+        /**\n+         * Key: ip address\n+         * Value: timestamp (event-time) of the corresponding event when the event ID was seen for the\n+         * first time\n+         */\n+        private WindowStore<E, Long> eventIdStore;\n+\n+        private final long leftDurationMs;\n+        private final long rightDurationMs;\n+\n+        private final KeyValueMapper<K, V, E> idExtractor;\n+\n+        /**\n+         * @param maintainDurationPerEventInMs how long to \"remember\" a known ip address\n+         *                                     during the time of which any incoming duplicates\n+         *                                     will be dropped, thereby de-duplicating the\n+         *                                     input.\n+         * @param idExtractor                  extracts a unique identifier from a record by which we de-duplicate input\n+         *                                     records; if it returns null, the record will not be considered for\n+         *                                     de-duping but forwarded as-is.\n+         */\n+        DeduplicationTransformer(final long maintainDurationPerEventInMs, final KeyValueMapper<K, V, E> idExtractor) {\n+            if (maintainDurationPerEventInMs < 1) {\n+                throw new IllegalArgumentException(\"maintain duration per event must be >= 1\");\n+            }\n+            leftDurationMs = maintainDurationPerEventInMs / 2;\n+            rightDurationMs = maintainDurationPerEventInMs - leftDurationMs;\n+            this.idExtractor = idExtractor;\n+        }\n \n-    private final long leftDurationMs;\n-    private final long rightDurationMs;\n+        @Override\n+        @SuppressWarnings(\"unchecked\")\n+        public void init(final ProcessorContext context) {\n+            this.context = context;\n+            eventIdStore = (WindowStore<E, Long>) context.getStateStore(storeName);\n+        }\n \n-    private final KeyValueMapper<K, V, E> idExtractor;\n+        @Override\n+        public V transform(final K key, final V value) {\n+            final E eventId = idExtractor.apply(key, value);\n+            if (eventId == null) {\n+                return value;\n+            } else {\n+                final V output;\n+                if (isDuplicate(eventId)) {\n+                    output = null;\n+                    updateTimestampOfExistingEventToPreventExpiry(eventId, context.timestamp());\n+                } else {\n+                    output = value;\n+                    rememberNewEvent(eventId, context.timestamp());\n+                }\n+                return output;\n+            }\n+        }\n \n-    /**\n-     * @param maintainDurationPerEventInMs how long to \"remember\" a known ip address\n-     *                                     during the time of which any incoming duplicates\n-     *                                     will be dropped, thereby de-duplicating the\n-     *                                     input.\n-     * @param idExtractor                  extracts a unique identifier from a record by which we de-duplicate input\n-     *                                     records; if it returns null, the record will not be considered for\n-     *                                     de-duping but forwarded as-is.\n-     */\n-    DeduplicationTransformer(final long maintainDurationPerEventInMs, final KeyValueMapper<K, V, E> idExtractor) {\n-      if (maintainDurationPerEventInMs < 1) {\n-        throw new IllegalArgumentException(\"maintain duration per event must be >= 1\");\n-      }\n-      leftDurationMs = maintainDurationPerEventInMs / 2;\n-      rightDurationMs = maintainDurationPerEventInMs - leftDurationMs;\n-      this.idExtractor = idExtractor;\n-    }\n+        private boolean isDuplicate(final E eventId) {\n+            final long eventTime = context.timestamp();\n+            final WindowStoreIterator<Long> timeIterator = eventIdStore.fetch(\n+                    eventId,\n+                    eventTime - leftDurationMs,\n+                    eventTime + rightDurationMs);\n+            final boolean isDuplicate = timeIterator.hasNext();\n+            timeIterator.close();\n+            return isDuplicate;\n+        }\n \n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void init(final ProcessorContext context) {\n-      this.context = context;\n-      eventIdStore = (WindowStore<E, Long>) context.getStateStore(storeName);\n-    }\n+        private void updateTimestampOfExistingEventToPreventExpiry(final E eventId, final long newTimestamp) {\n+            eventIdStore.put(eventId, newTimestamp, newTimestamp);\n+        }\n \n-    public KeyValue<K, V> transform(final K key, final V value) {\n-      final E eventId = idExtractor.apply(key, value);\n-      if (eventId == null) {\n-        return KeyValue.pair(key, value);\n-      } else {\n-        final KeyValue<K, V> output;\n-        if (isDuplicate(eventId)) {\n-          output = null;\n-          updateTimestampOfExistingEventToPreventExpiry(eventId, context.timestamp());\n-        } else {\n-          output = KeyValue.pair(key, value);\n-          rememberNewEvent(eventId, context.timestamp());\n+        private void rememberNewEvent(final E eventId, final long timestamp) {\n+            eventIdStore.put(eventId, timestamp, timestamp);\n         }\n-        return output;\n-      }\n+\n+        @Override\n+        public void close() {\n+            // Note: The store should NOT be closed manually here via `eventIdStore.close()`!\n+            // The Kafka Streams API will automatically close stores when necessary.\n+        }\n+\n     }\n \n-    private boolean isDuplicate(final E eventId) {\n-      final long eventTime = context.timestamp();\n-      final WindowStoreIterator<Long> timeIterator = eventIdStore.fetch(\n-              eventId,\n-              eventTime - leftDurationMs,\n-              eventTime + rightDurationMs);\n-      final boolean isDuplicate = timeIterator.hasNext();\n-      timeIterator.close();\n-      return isDuplicate;\n+    private SpecificAvroSerde<Click> buildClicksSerde(final Properties envProps) {\n+        final SpecificAvroSerde<Click> serde = new SpecificAvroSerde<>();\n+        Map<String, String> config = new HashMap<>();\n+        config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+        serde.configure(config, false);\n+        return serde;\n     }\n \n-    private void updateTimestampOfExistingEventToPreventExpiry(final E eventId, final long newTimestamp) {\n-      eventIdStore.put(eventId, newTimestamp, newTimestamp);\n+    public Topology buildTopology(Properties envProps,\n+                                  final SpecificAvroSerde<Click> clicksSerde) {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final String inputTopic = envProps.getProperty(\"input.topic.name\");\n+        final String outputTopic = envProps.getProperty(\"output.topic.name\");\n+\n+        // How long we \"remember\" an event.  During this time, any incoming duplicates of the event\n+        // will be, well, dropped, thereby de-duplicating the input data.\n+        //\n+        // The actual value depends on your use case.  To reduce memory and disk usage, you could\n+        // decrease the size to purge old windows more frequently at the cost of potentially missing out\n+        // on de-duplicating late-arriving records.\n+        final Duration windowSize = Duration.ofMinutes(2);\n+\n+        // retention period must be at least window size -- for this use case, we don't need a longer retention period\n+        // and thus just use the window size as retention time\n+        final Duration retentionPeriod = windowSize;\n+\n+        final StoreBuilder<WindowStore<String, Long>> dedupStoreBuilder = Stores.windowStoreBuilder(\n+                Stores.persistentWindowStore(storeName,\n+                        retentionPeriod,\n+                        windowSize,\n+                        false\n+                ),\n+                Serdes.String(),\n+                Serdes.Long());\n+\n+        builder.addStateStore(dedupStoreBuilder);\n+\n+        builder\n+                .stream(inputTopic, Consumed.with(Serdes.String(), clicksSerde))\n+                .transformValues(() -> new DeduplicationTransformer<>(windowSize.toMillis(), (key, value) -> value.getIp()), storeName)\n+                .filter((k, v) -> v != null)\n+                .to(outputTopic, Produced.with(Serdes.String(), clicksSerde));\n+\n+        return builder.build();\n     }\n \n-    private void rememberNewEvent(final E eventId, final long timestamp) {\n-      eventIdStore.put(eventId, timestamp, timestamp);\n+    public void createTopics(Properties envProps) {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+        AdminClient client = AdminClient.create(config);\n+\n+        List<NewTopic> topics = new ArrayList<>();\n+        topics.add(new NewTopic(\n+                envProps.getProperty(\"input.topic.name\"),\n+                Integer.parseInt(envProps.getProperty(\"input.topic.partitions\")),\n+                Short.parseShort(envProps.getProperty(\"input.topic.replication.factor\"))));\n+        topics.add(new NewTopic(\n+                envProps.getProperty(\"output.topic.name\"),\n+                Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n+                Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n+\n+        client.createTopics(topics);\n+        client.close();\n     }\n \n-    @Override\n-    public void close() {\n-      // Note: The store should NOT be closed manually here via `eventIdStore.close()`!\n-      // The Kafka Streams API will automatically close stores when necessary.\n+    public static Properties buildStreamsProperties(Properties envProps) {\n+        Properties props = new Properties();\n+\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+        return props;\n     }\n \n-  }\n-\n-  private SpecificAvroSerde<Click> buildClicksSerde(final Properties envProps) {\n-    final SpecificAvroSerde<Click> serde = new SpecificAvroSerde<>();\n-    Map<String, String> config = new HashMap<>();\n-    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-    serde.configure(config, false);\n-    return serde;\n-  }\n-\n-  public Topology buildTopology(Properties envProps,\n-                                final SpecificAvroSerde<Click> clicksSerde) {\n-    final StreamsBuilder builder = new StreamsBuilder();\n-\n-    final String inputTopic = envProps.getProperty(\"input.topic.name\");\n-    final String outputTopic = envProps.getProperty(\"output.topic.name\");\n-\n-    // How long we \"remember\" an event.  During this time, any incoming duplicates of the event\n-    // will be, well, dropped, thereby de-duplicating the input data.\n-    //\n-    // The actual value depends on your use case.  To reduce memory and disk usage, you could\n-    // decrease the size to purge old windows more frequently at the cost of potentially missing out\n-    // on de-duplicating late-arriving records.\n-    final Duration windowSize = Duration.ofMinutes(2);\n-\n-    // retention period must be at least window size -- for this use case, we don't need a longer retention period\n-    // and thus just use the window size as retention time\n-    final Duration retentionPeriod = windowSize;\n-\n-    final StoreBuilder<WindowStore<String, Long>> dedupStoreBuilder = Stores.windowStoreBuilder(\n-            Stores.persistentWindowStore(storeName,\n-                    retentionPeriod,\n-                    windowSize,\n-                    false\n-            ),\n-            Serdes.String(),\n-            Serdes.Long());\n-\n-    builder.addStateStore(dedupStoreBuilder);\n-\n-    builder\n-      .stream(inputTopic, Consumed.with(Serdes.String(), clicksSerde))\n-      .transform( () -> new DeduplicationTransformer<>(windowSize.toMillis(), (key, value) -> value.getIp()), storeName)\n-      .to(outputTopic, Produced.with(Serdes.String(), clicksSerde));\n-\n-    return builder.build();\n-  }\n-\n-  public void createTopics(Properties envProps) {\n-    Map<String, Object> config = new HashMap<>();\n-    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n-    AdminClient client = AdminClient.create(config);\n-\n-    List<NewTopic> topics = new ArrayList<>();\n-    topics.add(new NewTopic(\n-        envProps.getProperty(\"input.topic.name\"),\n-        Integer.parseInt(envProps.getProperty(\"input.topic.partitions\")),\n-        Short.parseShort(envProps.getProperty(\"input.topic.replication.factor\"))));\n-    topics.add(new NewTopic(\n-        envProps.getProperty(\"output.topic.name\"),\n-        Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n-        Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n-\n-    client.createTopics(topics);\n-    client.close();\n-  }\n-\n-  public static Properties buildStreamsProperties(Properties envProps) {\n-    Properties props = new Properties();\n-\n-    props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n-    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n-    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n-    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n-    props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n-\n-    return props;\n-  }\n-  public static Properties loadEnvProperties(String fileName) throws IOException {\n-    Properties envProps = new Properties();\n-    FileInputStream input = new FileInputStream(fileName);\n-    envProps.load(input);\n-    input.close();\n-\n-    return envProps;\n-  }\n-\n-  public static void main(String[] args) throws IOException {\n-    if (args.length < 1) {\n-      throw new IllegalArgumentException(\n-          \"This program takes one argument: the path to an environment configuration file.\");\n+    public static Properties loadEnvProperties(String fileName) throws IOException {\n+        Properties envProps = new Properties();\n+        FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n     }\n \n-    new FindDistinctEvents().runRecipe(args[0]);\n-  }\n-\n-  private void runRecipe(final String configPath) throws IOException {\n-    Properties envProps = this.loadEnvProperties(configPath);\n-    Properties streamProps = this.buildStreamsProperties(envProps);\n-\n-    Topology topology = this.buildTopology(envProps, this.buildClicksSerde(envProps));\n-    this.createTopics(envProps);\n-\n-    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n-    final CountDownLatch latch = new CountDownLatch(1);\n-\n-    // Attach shutdown handler to catch Control-C.\n-    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n-      @Override\n-      public void run() {\n-        streams.close();\n-        latch.countDown();\n-      }\n-    });\n-\n-    try {\n-      streams.start();\n-      latch.await();\n-    } catch (Throwable e) {\n-      System.exit(1);\n+    public static void main(String[] args) throws IOException {\n+        if (args.length < 1) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes one argument: the path to an environment configuration file.\");\n+        }\n+\n+        new FindDistinctEvents().runRecipe(args[0]);\n     }\n-    System.exit(0);\n \n-  }\n+    private void runRecipe(final String configPath) throws IOException {\n+        Properties envProps = this.loadEnvProperties(configPath);\n+        Properties streamProps = this.buildStreamsProperties(envProps);\n+\n+        Topology topology = this.buildTopology(envProps, this.buildClicksSerde(envProps));\n+        this.createTopics(envProps);\n+\n+        final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+        final CountDownLatch latch = new CountDownLatch(1);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+            @Override\n+            public void run() {\n+                streams.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57793a4fa13ccfe22d4037c4df462fcf062c31ab"}, "originalPosition": 419}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7db2a3b6b55f941b4e60b2417cb41f1fbd99baed", "author": {"user": {"login": "inponomarev", "name": "Ivan Ponomarev"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/7db2a3b6b55f941b4e60b2417cb41f1fbd99baed", "committedDate": "2020-04-07T17:15:45Z", "message": "code review corrections"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwMTQ3ODE3", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#pullrequestreview-390147817", "createdAt": "2020-04-08T16:41:50Z", "commit": {"oid": "7db2a3b6b55f941b4e60b2417cb41f1fbd99baed"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjo0MTo1MFrOGC3uVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNjo0MTo1MFrOGC3uVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY2MzMxNg==", "bodyText": "With this change I get the following when trying to run the app at step 6\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/kafka/clients/admin/Admin\n\tat org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:557)\n\tat io.confluent.developer.FindDistinctEvents.runRecipe(FindDistinctEvents.java:235)\n\tat io.confluent.developer.FindDistinctEvents.main(FindDistinctEvents.java:225)\nCaused by: java.lang.ClassNotFoundException: org.apache.kafka.clients.admin.Admin\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\t... 3 more\n\nMaybe just revert to 2.3.0 for now?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#discussion_r405663316", "createdAt": "2020-04-08T16:41:50Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/finding-distinct/kstreams/code/build.gradle", "diffHunk": "@@ -32,9 +32,9 @@ apply plugin: \"com.github.johnrengelman.shadow\"\n dependencies {\n     compile \"org.apache.avro:avro:1.8.2\"\n     implementation \"org.slf4j:slf4j-simple:1.7.26\"\n-    implementation \"org.apache.kafka:kafka-streams:2.3.0\"\n+    implementation \"org.apache.kafka:kafka-streams:2.4.1\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7db2a3b6b55f941b4e60b2417cb41f1fbd99baed"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd52f0f8393aea0611201cfa567dda3f7516cc39", "author": {"user": {"login": "inponomarev", "name": "Ivan Ponomarev"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/cd52f0f8393aea0611201cfa567dda3f7516cc39", "committedDate": "2020-04-08T21:13:47Z", "message": "revert to 2.3.0"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwMzgwMzQ5", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#pullrequestreview-390380349", "createdAt": "2020-04-08T22:41:40Z", "commit": {"oid": "cd52f0f8393aea0611201cfa567dda3f7516cc39"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 278, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}