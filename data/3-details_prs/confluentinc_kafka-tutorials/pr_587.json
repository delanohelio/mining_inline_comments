{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxMjI4MTU0", "number": 587, "title": "GH #570 : How to maintain message order and avoid message duplication with the idempotent producer", "bodyText": "This Kafka Tutorial addresses #570\nStaging:\nhttp://kafka-tutorials-staging.s3-website-us-west-2.amazonaws.com/GH-570/\nhttp://kafka-tutorials-staging.s3-website-us-west-2.amazonaws.com/GH-570/message-ordering/kafka.html", "createdAt": "2020-10-12T00:02:14Z", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587", "merged": true, "mergeCommit": {"oid": "a65fc20fe3677021ae8c7e5f18c923ec61cd3afc"}, "closed": true, "closedAt": "2020-10-28T20:10:11Z", "author": {"login": "ybyzek"}, "timelineItems": {"totalCount": 48, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdPZtVzgH2gAyNTAxMjI4MTU0OjYyZjMyYjU5NjQxZDkxY2YzOTEyODk4OTRkMjNjYTVmY2Q3MjZjZGE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdW9o7cgH2gAyNTAxMjI4MTU0OjAzZjA3ZmUxN2Q3ZGU5Zjk4OGIyZDBjMWVmMWYzMTk2MmE3YTEwMzE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "62f32b59641d91cf391289894d23ca5fcd726cda", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/62f32b59641d91cf391289894d23ca5fcd726cda", "committedDate": "2020-10-05T01:43:15Z", "message": "GH-570: DEVX-2108: initial commit for message ordering"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40fc6b8ada53342386b7a2733b7dc5241ce0fa01", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/40fc6b8ada53342386b7a2733b7dc5241ce0fa01", "committedDate": "2020-10-05T01:49:04Z", "message": "Rename ksql folder to kafka folder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79b413c8eb336b5811a1114ea087dd66ea4af6f9", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/79b413c8eb336b5811a1114ea087dd66ea4af6f9", "committedDate": "2020-10-05T01:52:28Z", "message": "Create original topic with 2 partitions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abf99e610c8d103ce477bfc03667eb46077412da", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/abf99e610c8d103ce477bfc03667eb46077412da", "committedDate": "2020-10-05T01:59:02Z", "message": "Clean up introduction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59faa2b74fb78893c12b7ad44859459b81543a41", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/59faa2b74fb78893c12b7ad44859459b81543a41", "committedDate": "2020-10-05T02:16:46Z", "message": "Remove unnecessary files (part 1)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7994fad79e44df32b623f63c04e8f342e1de8232", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/7994fad79e44df32b623f63c04e8f342e1de8232", "committedDate": "2020-10-05T02:31:17Z", "message": "acks=all"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ecd04b8c8c0d79b9773915c00898517a480ff8d", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/6ecd04b8c8c0d79b9773915c00898517a480ff8d", "committedDate": "2020-10-05T02:33:55Z", "message": "Remove max.in.flight.requests.per.connection"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0a1a006a8590f67c631fcf28f2eb277883e4c1f", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/c0a1a006a8590f67c631fcf28f2eb277883e4c1f", "committedDate": "2020-10-09T13:37:31Z", "message": "Add note about max.in.flight.requests.per.connection"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e8df1cb3c7b9f5611f4da66e8189ae71e6c0a76", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/4e8df1cb3c7b9f5611f4da66e8189ae71e6c0a76", "committedDate": "2020-10-09T13:47:53Z", "message": "Tweak"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "993467159c190c8e66023047d17c7e7cce038034", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/993467159c190c8e66023047d17c7e7cce038034", "committedDate": "2020-10-09T19:01:11Z", "message": "Add new steps"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2967d9a544ca3fea0aa04f3c795ee5ea1c756ede", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/2967d9a544ca3fea0aa04f3c795ee5ea1c756ede", "committedDate": "2020-10-10T00:21:55Z", "message": "Checkpoint"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39efd72bea941f3aab923bf8c67b6fbe3e935802", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/39efd72bea941f3aab923bf8c67b6fbe3e935802", "committedDate": "2020-10-10T00:47:33Z", "message": "Rename topic from topic1 to myTopic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc729babfe05c77008d1569757d29aaca5feaf9f", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/bc729babfe05c77008d1569757d29aaca5feaf9f", "committedDate": "2020-10-10T01:26:55Z", "message": "Cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "530f3cf8121dda260bae77df4075b5efd6429fc5", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/530f3cf8121dda260bae77df4075b5efd6429fc5", "committedDate": "2020-10-10T01:37:32Z", "message": "Tweak"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "321355f80d6eaf7fde592980e24b2487ebc1c2ae", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/321355f80d6eaf7fde592980e24b2487ebc1c2ae", "committedDate": "2020-10-10T02:24:37Z", "message": "Accurate outputs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01449cc056c9a62f97f4514fb7d9aa9a5a6981b0", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/01449cc056c9a62f97f4514fb7d9aa9a5a6981b0", "committedDate": "2020-10-10T02:45:43Z", "message": "Markup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "766d8674d2c4dea3b0c8930b8acc50983cbd52cc", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/766d8674d2c4dea3b0c8930b8acc50983cbd52cc", "committedDate": "2020-10-10T03:10:38Z", "message": "Change container criteria"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54f3cde716e829995894c63ba4610218c2ec930d", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/54f3cde716e829995894c63ba4610218c2ec930d", "committedDate": "2020-10-10T12:46:02Z", "message": "Minor tweaks; add file for answer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9901b61c6a8a3bf122df8a4f5e6fd90e4f3a0dc", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/d9901b61c6a8a3bf122df8a4f5e6fd90e4f3a0dc", "committedDate": "2020-10-11T23:10:59Z", "message": "Working on a clean make"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "445be4c82ab50c8abb6be792afcb4e9d9e035e0c", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/445be4c82ab50c8abb6be792afcb4e9d9e035e0c", "committedDate": "2020-10-12T00:01:02Z", "message": "website and make succeess"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c803f45d4ab021f0f9f793924358056407895717", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/c803f45d4ab021f0f9f793924358056407895717", "committedDate": "2020-10-12T00:02:41Z", "message": "Merge branch 'master' into GH-570"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31fa6d59c731ad8f4282d3a16f16f65465c5ecdd", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/31fa6d59c731ad8f4282d3a16f16f65465c5ecdd", "committedDate": "2020-10-12T00:09:06Z", "message": "Update .semaphore/semaphore.yml KSQL -> Kafka"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a804e847e31454bc432f3edc10e8376b9384b415", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/a804e847e31454bc432f3edc10e8376b9384b415", "committedDate": "2020-10-12T00:11:37Z", "message": "Add KT to settings.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78f7a60fa79c2b31e6399c1f747b020a474747b4", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/78f7a60fa79c2b31e6399c1f747b020a474747b4", "committedDate": "2020-10-12T00:15:57Z", "message": "Retitle card"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d91f85eb54eda05dc74a1f2e6b8cbfae934ff539", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/d91f85eb54eda05dc74a1f2e6b8cbfae934ff539", "committedDate": "2020-10-12T00:18:26Z", "message": "Tweak"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "committedDate": "2020-10-12T00:20:46Z", "message": "Tweak"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2Mjc5NjY2", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#pullrequestreview-506279666", "createdAt": "2020-10-12T04:05:47Z", "commit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDowNTo0N1rOHfuZOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDozNTo0MlrOHfuw5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyODAyNA==", "bodyText": "Should a note be added to this tutorial that the user needs to have Gradle installed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503028024", "createdAt": "2020-10-12T04:05:47Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDAwNg==", "bodyText": "KafkaProducer will automatically set acks=all if enable.idempotence=true:\n[main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=myApp] Overriding the default acks to all since idempotence is enabled.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030006", "createdAt": "2020-10-12T04:15:48Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,26 @@\n+Before you create your full application code, let's highlight the most important `ProducerConfig` configuration parameters:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/dev/answer-code.java %}</code></pre>\n++++++\n+\n+The first configuration sets the producer parameter https://kafka.apache.org/documentation/#enable.idempotence[enable.idempotence].\n+When set to `true`, it enables an idempotent producer which ensures that exactly one copy of each message is written to the brokers, and in order.\n+The default value is `enable.idempotence=false`, so you must explicitly set this to `enable.idempotence=true`.\n+For further reading, please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP98ExactlyOnceDeliveryandTransactionalMessaging-IdempotentProducerGuarantees[KIP-98].\n+\n+The second configuration sets the producer parameter https://kafka.apache.org/documentation/#acks[acks].\n+The `KafkaProducer` uses the `acks` configuration to tell the leader broker how many acknowledgments to wait for to consider a produce request complete.\n+You must set `acks=all` in order to use the idempotent producer, otherwise the producer cannot guarantee idempotence.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA==", "bodyText": "Maybe it's intentional, but NO-KEY is a key, and will result in all records being sent to the same partition (vs null, which would result in using the uniform sticky partitioner instead)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030288", "createdAt": "2020-10-12T04:17:22Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA==", "bodyText": "Not required, but doesn't hurt to explicitly set", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030400", "createdAt": "2020-10-12T04:17:56Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA==", "bodyText": "Since a shutdown hook is configured, this additional call to shutdown isn't needed.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030488", "createdAt": "2020-10-12T04:18:25Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw==", "bodyText": "It's probably more common to provide a callback to asynchronously handle acks, instead of handling the Future returned by send directly", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031233", "createdAt": "2020-10-12T04:21:57Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTQ2NA==", "bodyText": "Is this needed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031464", "createdAt": "2020-10-12T04:23:03Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTQ4OA==", "bodyText": "Is this needed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031488", "createdAt": "2020-10-12T04:23:11Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTU0MA==", "bodyText": "Is this needed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031540", "createdAt": "2020-10-12T04:23:28Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTYzNQ==", "bodyText": "Perhaps use kafka-clients instead of all these Kafka Streams dependencies, since this KT doesn't use Streams?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031635", "createdAt": "2020-10-12T04:24:02Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.30\"\n+    implementation \"org.apache.kafka:kafka-streams:2.6.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMjQ1Nw==", "bodyText": "Instead of kafka-run-class kafka.tools.DumpLogSegments, there's a script kafka-dump-log which can be used.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503032457", "createdAt": "2020-10-12T04:28:02Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-0.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+docker-compose exec broker kafka-run-class kafka.tools.DumpLogSegments \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMjU0Ng==", "bodyText": "Instead of kafka-run-class kafka.tools.DumpLogSegments, there's a script kafka-dump-log which can be used.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503032546", "createdAt": "2020-10-12T04:28:29Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+docker-compose exec broker kafka-run-class kafka.tools.DumpLogSegments \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw==", "bodyText": "This could be simplified to:\nSystem.err.printf(\"Error reading file %s due to %s%n\", filePath, e);", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503033223", "createdAt": "2020-10-12T04:31:37Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ==", "bodyText": "Is this intended to show up somewhere in the tutorial? I didn't see it on staging. Maybe it's only for automated internal testing with Semaphore?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503033539", "createdAt": "2020-10-12T04:33:12Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java", "diffHunk": "@@ -0,0 +1,47 @@\n+package io.confluent.developer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDA2NA==", "bodyText": "typo \"detects ensures\". Also, retries must be > 0 if enable.idempotence=true", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503034064", "createdAt": "2020-10-12T04:35:34Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-0.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+Now let's look at the Kafka broker's log segment files using the `DumpLogSegments` utility.\n+First, examine partition 0, indicated by the `0` in `myTopic-0`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-0.sh %}</code></pre>\n++++++\n+\n+You should see:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"text\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/expected-log-segments-myTopic-partition-0.txt %}</code></pre>\n++++++\n+\n+Note the familiar `producerId: 0`, which corresponds to the earlier log output when we ran the producer application.\n+(If the producer were not idempotent, you would see `producerId: -1`.)\n+\n+Also observe that each message has a unique sequence number, starting with `sequence: 0` through `sequence: 8`.\n+This sequence number is how the broker detects ensures idempotency per partition (if there are retries) and ordered messages (if sequences numbers are sent out of sequence).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDA4Ng==", "bodyText": "typo \"detects ensures\". Also, retries must be > 0 if enable.idempotence=true", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503034086", "createdAt": "2020-10-12T04:35:42Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-1.adoc", "diffHunk": "@@ -0,0 +1,18 @@\n+Use the `DumpLogSegments` utility again to examine partition 1, indicated by the `1` in `myTopic-1`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh %}</code></pre>\n++++++\n+\n+You should see:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"text\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/expected-log-segments-myTopic-partition-1.txt %}</code></pre>\n++++++\n+\n+Note the familiar `producerId: 0`, which corresponds to the earlier log output when we ran the producer application.\n+(If the producer were not idempotent, you would see `producerId: -1`.)\n+\n+Also observe that each message has a unique sequence number, starting with `sequence: 0` through `sequence: 2`.\n+This sequence number is how the broker detects ensures idempotency per partition (if there are retries) and ordered messages (if sequences numbers are sent out of sequence).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c41a2f7d46e417f51e81f1a7623959f1ba446302", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/c41a2f7d46e417f51e81f1a7623959f1ba446302", "committedDate": "2020-10-12T12:23:18Z", "message": "Replace kafka-run-class kafka.tools.DumpLogSegments with kafka-dump-log"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0bf4585c706dbe2f3f54692a1b3d05903278d7b3", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/0bf4585c706dbe2f3f54692a1b3d05903278d7b3", "committedDate": "2020-10-12T12:51:48Z", "message": "Mention retries; remove additional reference to DumpLogSegments; fix grammar"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b8744b67920f3de5ec1345cc8a61a5fb1a035f2", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/2b8744b67920f3de5ec1345cc8a61a5fb1a035f2", "committedDate": "2020-10-12T12:55:45Z", "message": "Tweak"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a1845b6d2febd7bfc2afb693a599cefdee14f91b", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/a1845b6d2febd7bfc2afb693a599cefdee14f91b", "committedDate": "2020-10-12T13:14:51Z", "message": "Update answer; update description of parameters"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2NjQyMDA1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#pullrequestreview-506642005", "createdAt": "2020-10-12T13:55:40Z", "commit": {"oid": "a1845b6d2febd7bfc2afb693a599cefdee14f91b"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3ce318705e67a17902ebecfe205e8539d338180", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/f3ce318705e67a17902ebecfe205e8539d338180", "committedDate": "2020-10-12T14:13:11Z", "message": "Write real null key if none is specified"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eac90b90b32c61fd673ebc2a1979e368f8d4489", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/1eac90b90b32c61fd673ebc2a1979e368f8d4489", "committedDate": "2020-10-12T14:38:22Z", "message": "Shout out to transactions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12e0b49c39a1035b172e96a1c1835347b5900090", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/12e0b49c39a1035b172e96a1c1835347b5900090", "committedDate": "2020-10-12T15:21:56Z", "message": "Remove unnecessary lines from build.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f03c5e4dc979d99bed65cc0477d45ecff9e99b72", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/f03c5e4dc979d99bed65cc0477d45ecff9e99b72", "committedDate": "2020-10-12T15:40:25Z", "message": "Use kafka-clients and explicitly add jackson"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cd2c4be3492fc5e29cfe14624b7feb0b214dfeb", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/8cd2c4be3492fc5e29cfe14624b7feb0b214dfeb", "committedDate": "2020-10-12T17:38:46Z", "message": "Tweak introduction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83cf59fb1a67cca371c2bbce1f0d4ca3fa77c686", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/83cf59fb1a67cca371c2bbce1f0d4ca3fa77c686", "committedDate": "2020-10-13T00:03:35Z", "message": "Change back to kafka-streams to avoid error in test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe8636c038bd624991f94950fbc066bd55a92b4c", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/fe8636c038bd624991f94950fbc066bd55a92b4c", "committedDate": "2020-10-13T00:05:49Z", "message": "Unit test passes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA5ODA2NzQx", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#pullrequestreview-509806741", "createdAt": "2020-10-15T21:02:55Z", "commit": {"oid": "fe8636c038bd624991f94950fbc066bd55a92b4c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9429d36e8bc4f19e60f600168da3421dec2b338e", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/9429d36e8bc4f19e60f600168da3421dec2b338e", "committedDate": "2020-10-22T13:28:56Z", "message": "Merge branch 'master' into GH-570"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44f2b420df1bc802781c0dc7e9c8fde322691f3b", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/44f2b420df1bc802781c0dc7e9c8fde322691f3b", "committedDate": "2020-10-22T13:35:32Z", "message": "Remove addShutdownHook; update error message to System.err.printf"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4214a91f008525b00d78f89479dcca72cfd89fde", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/4214a91f008525b00d78f89479dcca72cfd89fde", "committedDate": "2020-10-25T12:16:43Z", "message": "Implement Callback() and update output"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead", "committedDate": "2020-10-25T21:50:54Z", "message": "Grammar fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MDA1MDU2", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#pullrequestreview-518005056", "createdAt": "2020-10-27T18:10:58Z", "commit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxODoxMDo1OVrOHpKNaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxODoxMDo1OVrOHpKNaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw==", "bodyText": "I'd probably write this as a lambda instead of an anonymous inner class (more concise, and implicitly gives you access to the producerRecord inside the callback), but not critical.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512920937", "createdAt": "2020-10-27T18:10:59Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public void produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = null;\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        producer.send(producerRecord,\n+            new Callback() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MDE4Njkw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#pullrequestreview-518018690", "createdAt": "2020-10-27T18:27:53Z", "commit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d8572ba985096b03078355bb948487e8837bb47", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/1d8572ba985096b03078355bb948487e8837bb47", "committedDate": "2020-10-27T18:51:56Z", "message": "Convert callback to lambda"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03f07fe17d7de9f988b2d0c1ef1f31962a7a1031", "author": {"user": {"login": "ybyzek", "name": "Yeva Byzek"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/03f07fe17d7de9f988b2d0c1ef1f31962a7a1031", "committedDate": "2020-10-28T13:32:29Z", "message": "Tweak"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 78, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}