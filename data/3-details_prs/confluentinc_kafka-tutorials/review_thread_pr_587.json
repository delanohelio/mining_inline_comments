{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxMjI4MTU0", "number": 587, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDowNTo0N1rOEsdxQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxODoxMDo1OVrOEyfPng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYwNTQ3OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-gradle-wrapper.adoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDowNTo0N1rOHfuZOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMjoyMDo0MVrOHgPCbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyODAyNA==", "bodyText": "Should a note be added to this tutorial that the user needs to have Gradle installed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503028024", "createdAt": "2020-10-12T04:05:47Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1MzM3Mw==", "bodyText": "Many tutorials use Gradle and I don't think any mention it as a pre-req. In fact, I don't think any KT identifies required local installs to run (in contrast the README mentions pre-reqs for local testing).  Maybe we file a GH issue to track separately?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503253373", "createdAt": "2020-10-12T12:13:53Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyODAyNA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMDY0MA==", "bodyText": "A separate issue sounds fine to me if any changes are needed at all here.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503310640", "createdAt": "2020-10-12T13:51:25Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyODAyNA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU2Mjg2Mw==", "bodyText": "Filed #591 for tracking", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503562863", "createdAt": "2020-10-12T22:20:41Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-gradle-wrapper.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Run the following command to obtain the Gradle wrapper:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyODAyNA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYxOTMwOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-application.adoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoxNTo0OFrOHfug9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMzo1MjowOVrOHf_rFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDAwNg==", "bodyText": "KafkaProducer will automatically set acks=all if enable.idempotence=true:\n[main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=myApp] Overriding the default acks to all since idempotence is enabled.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030006", "createdAt": "2020-10-12T04:15:48Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,26 @@\n+Before you create your full application code, let's highlight the most important `ProducerConfig` configuration parameters:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/dev/answer-code.java %}</code></pre>\n++++++\n+\n+The first configuration sets the producer parameter https://kafka.apache.org/documentation/#enable.idempotence[enable.idempotence].\n+When set to `true`, it enables an idempotent producer which ensures that exactly one copy of each message is written to the brokers, and in order.\n+The default value is `enable.idempotence=false`, so you must explicitly set this to `enable.idempotence=true`.\n+For further reading, please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP98ExactlyOnceDeliveryandTransactionalMessaging-IdempotentProducerGuarantees[KIP-98].\n+\n+The second configuration sets the producer parameter https://kafka.apache.org/documentation/#acks[acks].\n+The `KafkaProducer` uses the `acks` configuration to tell the leader broker how many acknowledgments to wait for to consider a produce request complete.\n+You must set `acks=all` in order to use the idempotent producer, otherwise the producer cannot guarantee idempotence.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1MjUxOQ==", "bodyText": "I was aware that setting enable.idempotence=true overrides some other producer settings. I was on the fence about whether to include them or not, and I decided to include them for a few reasons:\n\nKT can teach users about those settings\nSometimes being explicit is better practice because it prevents unintended behavior (user might miss the log message)...\nFuture proof: what if that behavior defaults change in Kafka, etc.\n\nMaybe KT could say that acks=all is required for idempotency, so either the user must set it explicitly or whatever setting is there will get overridden by the app.  WDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503252519", "createdAt": "2020-10-12T12:12:06Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,26 @@\n+Before you create your full application code, let's highlight the most important `ProducerConfig` configuration parameters:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/dev/answer-code.java %}</code></pre>\n++++++\n+\n+The first configuration sets the producer parameter https://kafka.apache.org/documentation/#enable.idempotence[enable.idempotence].\n+When set to `true`, it enables an idempotent producer which ensures that exactly one copy of each message is written to the brokers, and in order.\n+The default value is `enable.idempotence=false`, so you must explicitly set this to `enable.idempotence=true`.\n+For further reading, please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP98ExactlyOnceDeliveryandTransactionalMessaging-IdempotentProducerGuarantees[KIP-98].\n+\n+The second configuration sets the producer parameter https://kafka.apache.org/documentation/#acks[acks].\n+The `KafkaProducer` uses the `acks` configuration to tell the leader broker how many acknowledgments to wait for to consider a produce request complete.\n+You must set `acks=all` in order to use the idempotent producer, otherwise the producer cannot guarantee idempotence.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDAwNg=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4ODI0OQ==", "bodyText": "@mikebin revised copy available, WDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503288249", "createdAt": "2020-10-12T13:15:25Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,26 @@\n+Before you create your full application code, let's highlight the most important `ProducerConfig` configuration parameters:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/dev/answer-code.java %}</code></pre>\n++++++\n+\n+The first configuration sets the producer parameter https://kafka.apache.org/documentation/#enable.idempotence[enable.idempotence].\n+When set to `true`, it enables an idempotent producer which ensures that exactly one copy of each message is written to the brokers, and in order.\n+The default value is `enable.idempotence=false`, so you must explicitly set this to `enable.idempotence=true`.\n+For further reading, please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP98ExactlyOnceDeliveryandTransactionalMessaging-IdempotentProducerGuarantees[KIP-98].\n+\n+The second configuration sets the producer parameter https://kafka.apache.org/documentation/#acks[acks].\n+The `KafkaProducer` uses the `acks` configuration to tell the leader broker how many acknowledgments to wait for to consider a produce request complete.\n+You must set `acks=all` in order to use the idempotent producer, otherwise the producer cannot guarantee idempotence.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDAwNg=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMTEyNA==", "bodyText": "LGTM - thanks!", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503311124", "createdAt": "2020-10-12T13:52:09Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/dev/make-application.adoc", "diffHunk": "@@ -0,0 +1,26 @@\n+Before you create your full application code, let's highlight the most important `ProducerConfig` configuration parameters:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"java\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/dev/answer-code.java %}</code></pre>\n++++++\n+\n+The first configuration sets the producer parameter https://kafka.apache.org/documentation/#enable.idempotence[enable.idempotence].\n+When set to `true`, it enables an idempotent producer which ensures that exactly one copy of each message is written to the brokers, and in order.\n+The default value is `enable.idempotence=false`, so you must explicitly set this to `enable.idempotence=true`.\n+For further reading, please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP98ExactlyOnceDeliveryandTransactionalMessaging-IdempotentProducerGuarantees[KIP-98].\n+\n+The second configuration sets the producer parameter https://kafka.apache.org/documentation/#acks[acks].\n+The `KafkaProducer` uses the `acks` configuration to tell the leader broker how many acknowledgments to wait for to consider a produce request complete.\n+You must set `acks=all` in order to use the idempotent producer, otherwise the producer cannot guarantee idempotence.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDAwNg=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyMTIzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoxNzoyMlrOHfuiEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMDo1Mzo0OFrOHiaCnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA==", "bodyText": "Maybe it's intentional, but NO-KEY is a key, and will result in all records being sent to the same partition (vs null, which would result in using the uniform sticky partitioner instead)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030288", "createdAt": "2020-10-12T04:17:22Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0Nzk2OA==", "bodyText": "@mikebin good point.  Not intentional in that this code is duplicated from kafka-producer-application (cc: @bbejeck ) but in that case, IIUC the partitioner didn't matter.\nSo stepping back, what is the desirable behavior for this tutorial if there is no key?  I think a true null and round robin to use both partitions would better demonstrate ordering on a per-partition basis.  If you agree, I'll modify the code accordingly.  WDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503247968", "createdAt": "2020-10-12T12:02:57Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMjIxMg==", "bodyText": "null makes sense if no key is provided, although in this specific tutorial, keys are provided in the input data, so only if the user experiments with their own data would that code be relevant.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503312212", "createdAt": "2020-10-12T13:53:47Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMyMjg4Mg==", "bodyText": "I've tested this and it seems that the default partitioner behavior is, as described in https://github.com/apache/kafka/blob/2.6/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java#L30:\n\nIf no partition or key is present choose the sticky partition that changes when the batch is full\n\nNote that it changes when the batch is full, and as it turns out, with the current application all the null-keyed messages get put into the same batch so they all land in the same partition anyway.  If we wanted to force a different behavior, we could set batch.size=0 to get more batches and get some kind of distribution, but that conflates different subjects in one KT and could lead to bad copy/paste.\nSo I'm thinking to change the code to allow no keys such that a real null key gets written, but we just need to understand that it doesn't necessarily mean they will round robin because they will likely land in the same batch.\nWDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503322882", "createdAt": "2020-10-12T14:10:40Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMzMDI1OA==", "bodyText": "SGTM", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503330258", "createdAt": "2020-10-12T14:22:12Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0MDI4NA==", "bodyText": "The \"NO-KEY\" was intentional to provide a default key if a record didn't have one after splitting the line.  Probably poorly worded.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505840284", "createdAt": "2020-10-15T20:53:48Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyMjAzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoxNzo1NlrOHfuigA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMzo1NDo0MlrOHf_x2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA==", "bodyText": "Not required, but doesn't hurt to explicitly set", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030400", "createdAt": "2020-10-12T04:17:56Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4ODM5NA==", "bodyText": "@mikebin revised copy available, WDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503288394", "createdAt": "2020-10-12T13:15:40Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMjg1Nw==", "bodyText": "LGTM", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503312857", "createdAt": "2020-10-12T13:54:42Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyMjU2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoxODoyNVrOHfui2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxMzozNjowNVrOHmiHPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA==", "bodyText": "Since a shutdown hook is configured, this additional call to shutdown isn't needed.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030488", "createdAt": "2020-10-12T04:18:25Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1MDExOA==", "bodyText": "@mikebin , ok, I can validate and remove the call to shutdown().\n@bbejeck similar to the previous comment, this code is also present in other KTs. Assuming you agree with this removal, do you have a preference whether this PR addresses those KTs or should I file a separate GH issue?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503250118", "createdAt": "2020-10-12T12:07:11Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTgzMTE4NQ==", "bodyText": "@mikebin  good catch\n\n@bbejeck similar to the previous comment, this code is also present in other KTs. Assuming you agree with this removal, do you have a preference whether this PR addresses those KTs or should I file a separate GH issue?\n\nthat was an oversight on my part during refactoring, I'll clean it up in a separate PR\nEDIT: When I originally developed the tutorial it would continue running until the user did a CTRL+C hence the shutdown hook.  But later I refactored to produce the records in the file and shutdown.\nSo I'd say put the producerApp.shutdown() back in and remove the shutdown hook, I've done the same in the corresponding tutorial.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505831185", "createdAt": "2020-10-15T20:43:08Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2Mzk4Mw==", "bodyText": "AI for self: mirror changes that @bbejeck did in #594", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r509663983", "createdAt": "2020-10-21T20:23:24Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE2Njg0NQ==", "bodyText": "Addressed by 44f2b42", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r510166845", "createdAt": "2020-10-22T13:36:05Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyNzQ4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyMTo1N1rOHfulwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMjoxNzoyNFrOHn47BA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw==", "bodyText": "It's probably more common to provide a callback to asynchronously handle acks, instead of handling the Future returned by send directly", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031233", "createdAt": "2020-10-12T04:21:57Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM2NTk3NA==", "bodyText": "cc: @bbejeck also borrowed from the other KT.  Any leaning on how to fix and where?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503365974", "createdAt": "2020-10-12T15:17:38Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE2Nzc4NQ==", "bodyText": "@bbejeck did you have a leaning on whether to leave this as-is, or whether to implement Callback?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r510167785", "createdAt": "2020-10-22T13:37:20Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA0MDg3Mw==", "bodyText": "Thanks for discussing @bbejeck , as we agreed, I will add Callbacks just to this KT and no impact to other KTs", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r511040873", "createdAt": "2020-10-23T17:44:18Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU4OTEyNA==", "bodyText": "@bbejeck @mikebin 4214a91 implements the callback", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r511589124", "createdAt": "2020-10-25T12:17:24Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyODk1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyMzowM1rOHfumqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNToyMjozNFrOHgDM3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTQ2NA==", "bodyText": "Is this needed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031464", "createdAt": "2020-10-12T04:23:03Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM2ODkyNg==", "bodyText": "@mikebin this line is removed in 12e0b49", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503368926", "createdAt": "2020-10-12T15:22:34Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTQ2NA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyOTA0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyMzoxMVrOHfumwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNToyMjo0MVrOHgDNHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTQ4OA==", "bodyText": "Is this needed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031488", "createdAt": "2020-10-12T04:23:11Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM2ODk5MQ==", "bodyText": "@mikebin this line is removed in 12e0b49", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503368991", "createdAt": "2020-10-12T15:22:41Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTQ4OA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyOTMyOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyMzoyOFrOHfum9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNToyMjo0N1rOHgDNaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTU0MA==", "bodyText": "Is this needed?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031540", "createdAt": "2020-10-12T04:23:28Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM2OTA2NA==", "bodyText": "@mikebin this line is removed in 12e0b49", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503369064", "createdAt": "2020-10-12T15:22:47Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTU0MA=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYyOTg5OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyNDowMlrOHfunUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNTo0Mjo0N1rOHgD8AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTYzNQ==", "bodyText": "Perhaps use kafka-clients instead of all these Kafka Streams dependencies, since this KT doesn't use Streams?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031635", "createdAt": "2020-10-12T04:24:02Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.30\"\n+    implementation \"org.apache.kafka:kafka-streams:2.6.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM3MDA5Nw==", "bodyText": "@mikebin I did not replace this line in 12e0b49 because when I did, it resulted in\njava.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/JsonNode\n\tat org.apache.kafka.clients.producer.internals.TransactionManager.bumpIdempotentEpochAndResetIdIfNeeded(TransactionManager.java:585)\n\tat org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:311)\n\tat org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:240)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.JsonNode\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\t... 4 more\n\n@gAmUssA had a message on this in Slack, and sounds like it was fixed after 2.6.0 was released.  So I will leave it as-is for now.  I do not want to downgrade to 2.5.0 because it results in different behavior that relevant for this KT:\n[main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=myApp] Overriding the default retries config to the recommended value of 2147483647 since the idempotent producer is enabled.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503370097", "createdAt": "2020-10-12T15:24:31Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.30\"\n+    implementation \"org.apache.kafka:kafka-streams:2.6.0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTYzNQ=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MDM2OA==", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10378", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503380368", "createdAt": "2020-10-12T15:41:40Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.30\"\n+    implementation \"org.apache.kafka:kafka-streams:2.6.0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTYzNQ=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MDk5Mw==", "bodyText": "@mikebin thanks!  @gAmUssA thanks for the workaround.  f03c5e4 should address this now", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503380993", "createdAt": "2020-10-12T15:42:47Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/build.gradle", "diffHunk": "@@ -0,0 +1,63 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.21.0\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+\n+    maven {\n+        url \"https://packages.confluent.io/maven\"\n+    }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    implementation \"org.apache.avro:avro:1.10.0\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.30\"\n+    implementation \"org.apache.kafka:kafka-streams:2.6.0\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTYzNQ=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYzNTg1OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-0.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyODowMlrOHfuqiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMjowODoxMVrOHf7-xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMjQ1Nw==", "bodyText": "Instead of kafka-run-class kafka.tools.DumpLogSegments, there's a script kafka-dump-log which can be used.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503032457", "createdAt": "2020-10-12T04:28:02Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-0.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+docker-compose exec broker kafka-run-class kafka.tools.DumpLogSegments \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1MDYyOA==", "bodyText": "@mikebin I was not aware of that, thank you!  I will validate and use kafka-dump-log.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503250628", "createdAt": "2020-10-12T12:08:11Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-0.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+docker-compose exec broker kafka-run-class kafka.tools.DumpLogSegments \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMjQ1Nw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDYzNjQ2OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyODoyOVrOHfuq4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDoyODoyOVrOHfuq4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMjU0Ng==", "bodyText": "Instead of kafka-run-class kafka.tools.DumpLogSegments, there's a script kafka-dump-log which can be used.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503032546", "createdAt": "2020-10-12T04:28:29Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh", "diffHunk": "@@ -0,0 +1,4 @@\n+docker-compose exec broker kafka-run-class kafka.tools.DumpLogSegments \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDY0MTM5OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDozMTozN1rOHfuthw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxMzozNTo1NlrOHmiG1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw==", "bodyText": "This could be simplified to:\nSystem.err.printf(\"Error reading file %s due to %s%n\", filePath, e);", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503033223", "createdAt": "2020-10-12T04:31:37Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0OTM2NA==", "bodyText": "@mikebin , sure, I can make this simplification.\n@bbejeck this code is also present in kafka-producer-application KT and kafka-producer-application-callback KT.  Assuming you agree with this simplification, do you have a preference whether this PR addresses those KTs or should I file a separate GH issue?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503249364", "createdAt": "2020-10-12T12:05:45Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTgzMTk4Nw==", "bodyText": "same as above I'll put it in the same PR", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505831987", "createdAt": "2020-10-15T20:44:05Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2NDI1Mw==", "bodyText": "AI for self: mirror changes that @bbejeck did in #594", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r509664253", "createdAt": "2020-10-21T20:23:44Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE2Njc0MQ==", "bodyText": "Addressed by 44f2b42", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r510166741", "createdAt": "2020-10-22T13:35:56Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDY0MzYzOnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDozMzoxMlrOHfuuww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQyMDo1NzoxM1rOHiaN5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ==", "bodyText": "Is this intended to show up somewhere in the tutorial? I didn't see it on staging. Maybe it's only for automated internal testing with Semaphore?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503033539", "createdAt": "2020-10-12T04:33:12Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java", "diffHunk": "@@ -0,0 +1,47 @@\n+package io.confluent.developer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM3MTIzNQ==", "bodyText": "Hmmm, not sure about this one.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503371235", "createdAt": "2020-10-12T15:26:18Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java", "diffHunk": "@@ -0,0 +1,47 @@\n+package io.confluent.developer;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0MzE3NQ==", "bodyText": "The project this KT was cloned from has a different test section that uses a unit test.   Part of the build runs only the unit tests via gradle and not the harness runner, so it might be a good idea to leave it in.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505843175", "createdAt": "2020-10-15T20:57:13Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java", "diffHunk": "@@ -0,0 +1,47 @@\n+package io.confluent.developer;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDY0NzM0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-0.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDozNTozNFrOHfuw0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDozNTozNFrOHfuw0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDA2NA==", "bodyText": "typo \"detects ensures\". Also, retries must be > 0 if enable.idempotence=true", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503034064", "createdAt": "2020-10-12T04:35:34Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-0.adoc", "diffHunk": "@@ -0,0 +1,19 @@\n+Now let's look at the Kafka broker's log segment files using the `DumpLogSegments` utility.\n+First, examine partition 0, indicated by the `0` in `myTopic-0`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-0.sh %}</code></pre>\n++++++\n+\n+You should see:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"text\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/expected-log-segments-myTopic-partition-0.txt %}</code></pre>\n++++++\n+\n+Note the familiar `producerId: 0`, which corresponds to the earlier log output when we ran the producer application.\n+(If the producer were not idempotent, you would see `producerId: -1`.)\n+\n+Also observe that each message has a unique sequence number, starting with `sequence: 0` through `sequence: 8`.\n+This sequence number is how the broker detects ensures idempotency per partition (if there are retries) and ordered messages (if sequences numbers are sent out of sequence).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MDY0NzU0OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-1.adoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNDozNTo0MlrOHfuw5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMzo1NjowN1rOHf_1iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDA4Ng==", "bodyText": "typo \"detects ensures\". Also, retries must be > 0 if enable.idempotence=true", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503034086", "createdAt": "2020-10-12T04:35:42Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-1.adoc", "diffHunk": "@@ -0,0 +1,18 @@\n+Use the `DumpLogSegments` utility again to examine partition 1, indicated by the `1` in `myTopic-1`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh %}</code></pre>\n++++++\n+\n+You should see:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"text\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/expected-log-segments-myTopic-partition-1.txt %}</code></pre>\n++++++\n+\n+Note the familiar `producerId: 0`, which corresponds to the earlier log output when we ran the producer application.\n+(If the producer were not idempotent, you would see `producerId: -1`.)\n+\n+Also observe that each message has a unique sequence number, starting with `sequence: 0` through `sequence: 2`.\n+This sequence number is how the broker detects ensures idempotency per partition (if there are retries) and ordered messages (if sequences numbers are sent out of sequence).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4ODQ4NQ==", "bodyText": "@mikebin revised copy available, WDYT?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503288485", "createdAt": "2020-10-12T13:15:49Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-1.adoc", "diffHunk": "@@ -0,0 +1,18 @@\n+Use the `DumpLogSegments` utility again to examine partition 1, indicated by the `1` in `myTopic-1`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh %}</code></pre>\n++++++\n+\n+You should see:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"text\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/expected-log-segments-myTopic-partition-1.txt %}</code></pre>\n++++++\n+\n+Note the familiar `producerId: 0`, which corresponds to the earlier log output when we ran the producer application.\n+(If the producer were not idempotent, you would see `producerId: -1`.)\n+\n+Also observe that each message has a unique sequence number, starting with `sequence: 0` through `sequence: 2`.\n+This sequence number is how the broker detects ensures idempotency per partition (if there are retries) and ordered messages (if sequences numbers are sent out of sequence).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDA4Ng=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMzgwMQ==", "bodyText": "LGTM", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503313801", "createdAt": "2020-10-12T13:56:07Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/markup/test/consume-log-segments-myTopic-partition-1.adoc", "diffHunk": "@@ -0,0 +1,18 @@\n+Use the `DumpLogSegments` utility again to examine partition 1, indicated by the `1` in `myTopic-1`.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/dump-log-segments-1.sh %}</code></pre>\n++++++\n+\n+You should see:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"text\">{% include_raw tutorials/message-ordering/kafka/code/tutorial-steps/test/expected-log-segments-myTopic-partition-1.txt %}</code></pre>\n++++++\n+\n+Note the familiar `producerId: 0`, which corresponds to the earlier log output when we ran the producer application.\n+(If the producer were not idempotent, you would see `producerId: -1`.)\n+\n+Also observe that each message has a unique sequence number, starting with `sequence: 0` through `sequence: 2`.\n+This sequence number is how the broker detects ensures idempotency per partition (if there are retries) and ordered messages (if sequences numbers are sent out of sequence).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzNDA4Ng=="}, "originalCommit": {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMzc2MTU4OnYy", "diffSide": "RIGHT", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxODoxMDo1OVrOHpKNaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxODo1OToyOFrOHpMExw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw==", "bodyText": "I'd probably write this as a lambda instead of an anonymous inner class (more concise, and implicitly gives you access to the producerRecord inside the callback), but not critical.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512920937", "createdAt": "2020-10-27T18:10:59Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public void produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = null;\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        producer.send(producerRecord,\n+            new Callback() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk0Nzk3MQ==", "bodyText": "@mikebin very cool suggestion .\n@mikebin  and @bbejeck WDYT about the implementation 1d8572b ?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512947971", "createdAt": "2020-10-27T18:53:26Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public void produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = null;\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        producer.send(producerRecord,\n+            new Callback() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw=="}, "originalCommit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk1MTQ5NQ==", "bodyText": "LGTM!", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512951495", "createdAt": "2020-10-27T18:59:28Z", "author": {"login": "mikebin"}, "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public void produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = null;\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        producer.send(producerRecord,\n+            new Callback() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw=="}, "originalCommit": {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead"}, "originalPosition": 46}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3827, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}