{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzMjUxOTg2", "number": 235, "title": "DEVX-1560: Running Average KStreams tutorial", "bodyText": "Implementation of Average\nFIXES #205", "createdAt": "2020-03-03T23:40:03Z", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235", "merged": true, "mergeCommit": {"oid": "07b53282e1ff39a7811ea47dec1abfa1f6a2352a"}, "closed": true, "closedAt": "2020-03-30T20:13:06Z", "author": {"login": "gAmUssA"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcOjhldgFqTM3NjA4MDkzMw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcSzPcogFqTM4NDE0MjgyNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc2MDgwOTMz", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#pullrequestreview-376080933", "createdAt": "2020-03-17T14:21:59Z", "commit": {"oid": "fdde1ebc7073d0d0deebb1f32de5935e1ddfe8f8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNDoyMjowMFrOF3eewQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxNDoyMjowMFrOF3eewQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzcxNTM5Mw==", "bodyText": "io.confluent.developer.RunningAverage.java should just be io.confluent.developer.RunningAverage .  I had an error in the gen_project script that I've fixed now.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r393715393", "createdAt": "2020-03-17T14:22:00Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,64 @@\n+buildscript {\n+    repositories {\n+        jcenter()\n+    }\n+    dependencies {\n+        classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+        classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+    }\n+}\n+\n+plugins {\n+    id \"java\"\n+    id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+    id \"idea\"\n+    id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+\n+repositories {\n+    jcenter()\n+    maven { url 'http://packages.confluent.io/maven/' }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+    compile \"org.apache.avro:avro:1.8.2\"\n+    implementation \"org.slf4j:slf4j-simple:1.7.26\"\n+    implementation \"org.apache.kafka:kafka-streams:2.4.0\"\n+    implementation \"io.confluent:kafka-streams-avro-serde:5.4.0\"\n+\n+    compile group: 'com.typesafe', name: 'config', version: '1.4.0'\n+    testCompile \"org.apache.kafka:kafka-streams-test-utils:2.4.0\"\n+    testCompile \"junit:junit:4.12\"\n+    testImplementation 'org.hamcrest:hamcrest:2.2'\n+\n+    testCompileOnly \"org.projectlombok:lombok:1.18.12\"\n+    testAnnotationProcessor \"org.projectlombok:lombok:1.18.12\"\n+}\n+\n+test {\n+    testLogging {\n+        outputs.upToDateWhen { false }\n+        showStandardStreams = true\n+        exceptionFormat = \"full\"\n+    }\n+}\n+\n+jar {\n+  manifest {\n+    attributes(\n+      \"Class-Path\": configurations.compile.collect { it.getName() }.join(\" \"),\n+      \"Main-Class\": \"io.confluent.developer.RunningAverage.java\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdde1ebc7073d0d0deebb1f32de5935e1ddfe8f8"}, "originalPosition": 57}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fdde1ebc7073d0d0deebb1f32de5935e1ddfe8f8", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/fdde1ebc7073d0d0deebb1f32de5935e1ddfe8f8", "committedDate": "2020-03-05T15:45:09Z", "message": "Code cleanup"}, "afterCommit": {"oid": "e95cba8cef9fd4966dd20ef089f3de37e2e9b8dd", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/e95cba8cef9fd4966dd20ef089f3de37e2e9b8dd", "committedDate": "2020-03-17T18:36:22Z", "message": "Implemented the rest of harness"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e95cba8cef9fd4966dd20ef089f3de37e2e9b8dd", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/e95cba8cef9fd4966dd20ef089f3de37e2e9b8dd", "committedDate": "2020-03-17T18:36:22Z", "message": "Implemented the rest of harness"}, "afterCommit": {"oid": "585c100b5d5c90f14c8649ac225fc23b7e671a6f", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/585c100b5d5c90f14c8649ac225fc23b7e671a6f", "committedDate": "2020-03-17T18:55:43Z", "message": "Implemented the rest of harness"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35de66bfbf93c8a3a1e15e99bebca7b50b1a3cae", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/35de66bfbf93c8a3a1e15e99bebca7b50b1a3cae", "committedDate": "2020-03-23T20:01:22Z", "message": "Initial commit of running average example\n\nReverted old stuff and committing only that matters"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "585c100b5d5c90f14c8649ac225fc23b7e671a6f", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/585c100b5d5c90f14c8649ac225fc23b7e671a6f", "committedDate": "2020-03-17T18:55:43Z", "message": "Implemented the rest of harness"}, "afterCommit": {"oid": "35de66bfbf93c8a3a1e15e99bebca7b50b1a3cae", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/35de66bfbf93c8a3a1e15e99bebca7b50b1a3cae", "committedDate": "2020-03-23T20:01:22Z", "message": "Initial commit of running average example\n\nReverted old stuff and committing only that matters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "061ba02d2dde23b5691b99a56c2851ad58a57d54", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/061ba02d2dde23b5691b99a56c2851ad58a57d54", "committedDate": "2020-03-24T20:12:28Z", "message": "Fixing output to file and runner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5be242d57b9dc572af60587e8dd0e3a3451a2796", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/5be242d57b9dc572af60587e8dd0e3a3451a2796", "committedDate": "2020-03-27T15:56:43Z", "message": "Minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "180092aea810519ec634a85f9927b453ba410171", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/180092aea810519ec634a85f9927b453ba410171", "committedDate": "2020-03-27T22:29:30Z", "message": "Tweaking production configuration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzNDgxNTU1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#pullrequestreview-383481555", "createdAt": "2020-03-30T01:08:51Z", "commit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTowODo1MVrOF9W7Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTozMTowNlrOF9XJkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzA0Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              title: \"How to compute average?\"\n          \n          \n            \n              title: \"Compute an average aggregation\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883042", "createdAt": "2020-03-30T01:08:51Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzA5Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              meta-description: \"How to compute average?\"\n          \n          \n            \n              meta-description: \"Compute an average aggregation\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883097", "createdAt": "2020-03-30T01:09:07Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"\n+  meta-description: \"How to compute average?\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzEwNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              problem: \"Kafka Streams DSL aggregation natively support so-called incremental\\\" or \\\"cumulative\\\" aggregation functions like count, sum, min, max. However, average function can be composed of distributive functions, namely count and sum. In this tutorial we will write a program.\"\n          \n          \n            \n              problem: \"Kafka Streams natively supports \\\"incremental\\\" aggregation functions, in which the aggregation result is updated based on the values captured by each window. Incremental functions include count, sum, min, and max. An average aggregation cannot be computed incrementally. However, as this tutorial shows, it can be implemented by composing incremental functions, namely count and sum.\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883106", "createdAt": "2020-03-30T01:09:13Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"\n+  meta-description: \"How to compute average?\"\n+  slug: \"/aggregating-average\"\n+  problem: \"Kafka Streams DSL aggregation natively support so-called incremental\\\" or \\\"cumulative\\\" aggregation functions like count, sum, min, max. However, average function can be composed of distributive functions, namely count and sum. In this tutorial we will write a program.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4MzI3MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains running average of ratings a movie received.\"\n          \n          \n            \n              introduction: \"Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains a running average rating for each movie.\"", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883271", "createdAt": "2020-03-30T01:10:20Z", "author": {"login": "colinhicks"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -214,3 +214,14 @@ flatten-nested-data:\n     ksql: enabled\n     kstreams: disabled\n     kafka: disabled\n+\n+aggregating-average:\n+  title: \"How to compute average?\"\n+  meta-description: \"How to compute average?\"\n+  slug: \"/aggregating-average\"\n+  problem: \"Kafka Streams DSL aggregation natively support so-called incremental\\\" or \\\"cumulative\\\" aggregation functions like count, sum, min, max. However, average function can be composed of distributive functions, namely count and sum. In this tutorial we will write a program.\"\n+  introduction: \"Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains running average of ratings a movie received.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4Mzk1Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: We're going to use this pair to store intermediate results. \n          \n          \n            \n            Note: We're going to use this record to store intermediate results.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399883953", "createdAt": "2020-03-30T01:14:24Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc", "diffHunk": "@@ -0,0 +1,8 @@\n+Next, create an Avro schema file at `src/main/avro/countsum.avsc` for the pair of counts and sums:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"avro\">{% include_raw tutorials/aggregating-average/kstreams/code/src/main/avro/countsum.avsc %}</code></pre>\n++++++\n+\n+NOTE: We're going to use this pair to store intermediate results. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NDQyNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Then create the following file at `src/main/java/io/confluent/developer/<MAIN-CLASS-NAME>.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.\n          \n          \n            \n            Then create the following file at `src/main/java/io/confluent/developer/RunningAverage.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399884427", "createdAt": "2020-03-30T01:17:31Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,12 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Then create the following file at `src/main/java/io/confluent/developer/<MAIN-CLASS-NAME>.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NjczNw==", "bodyText": "run-consumer.adoc states\n\nBefore you start producing ratings, it\u2019s a good idea to set up the consumer on the output topic.\n\nBut the consume step is currently after the produce step above.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r399886737", "createdAt": "2020-03-30T01:31:06Z", "author": {"login": "colinhicks"}, "path": "_data/harnesses/aggregating-average/kstreams.yml", "diffHunk": "@@ -0,0 +1,171 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/init.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Configure the project\n+      content:\n+        - action: make_file\n+          file: build.gradle\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-build-file.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/gradle-wrapper.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-gradle-wrapper.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/make-configuration-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-config-dir.adoc\n+\n+        - action: make_file\n+          file: configuration/dev.properties\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-dev-file.adoc\n+            \n+    - title: Create a schema for the model obect\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-avro-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-dir.adoc\n+\n+        - action: make_file\n+          file: src/main/avro/rating.avsc\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-rating.adoc\n+        \n+        - action: make_file\n+          file: src/main/avro/countsum.avsc\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/build-project.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/build-project.adoc\n+\n+    - title: Create the Kafka Streams topology\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/make-src-dir.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-src-dir.adoc\n+            \n+        - action: make_file\n+          file: src/main/java/io/confluent/developer/RunningAverage.java\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc \n+\n+    - title: Compile and run the Kafka Streams program\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/build-uberjar.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/build-uberjar.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/run-dev-app.sh\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-dev-app.adoc\n+\n+    - title: Produce sample data to the input topic\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/console-producer.sh\n+          stdin: tutorial-steps/dev/input-ratings.json\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc\n+\n+    - title: Consume data from the output topic\n+      content:\n+        - action: execute_async\n+          file: tutorial-steps/dev/console-consumer.sh\n+          stdout: tutorial-steps/dev/outputs/actual-outputs.json\n+          render:\n+            file: tutorials/aggregating-average/kstreams/markup/dev/run-consumer.adoc", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "180092aea810519ec634a85f9927b453ba410171"}, "originalPosition": 109}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9629b2a89be6f6a7a35322900c4a6acc6bad477a", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/9629b2a89be6f6a7a35322900c4a6acc6bad477a", "committedDate": "2020-03-30T04:31:32Z", "message": "Update _data/tutorials.yaml\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d4bcd3755d34f4becc03077d049b67b070a67b4", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/5d4bcd3755d34f4becc03077d049b67b070a67b4", "committedDate": "2020-03-30T04:31:49Z", "message": "Update _includes/tutorials/aggregating-average/kstreams/markup/dev/make-avro-schema-countsum.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a809e1eaa081e153845fa614f632088c9cb09abb", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/a809e1eaa081e153845fa614f632088c9cb09abb", "committedDate": "2020-03-30T04:32:02Z", "message": "Update _includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e531df7fb1bf211efd3d727337dc2a96548281f", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/5e531df7fb1bf211efd3d727337dc2a96548281f", "committedDate": "2020-03-30T04:39:30Z", "message": "Update _data/tutorials.yaml\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3e14022d0604a24c890c2ed458169a8baa7c949", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/c3e14022d0604a24c890c2ed458169a8baa7c949", "committedDate": "2020-03-30T04:39:49Z", "message": "Update _data/tutorials.yaml\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "778848f464e2b39a879d0a8d5141054f015c1f12", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/778848f464e2b39a879d0a8d5141054f015c1f12", "committedDate": "2020-03-30T04:40:04Z", "message": "Update _data/tutorials.yaml\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97623ee262176b25408fc461008764a5280365a1", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/97623ee262176b25408fc461008764a5280365a1", "committedDate": "2020-03-30T04:50:13Z", "message": "Changing order of execution of producer and consumer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzODg1ODgw", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#pullrequestreview-383885880", "createdAt": "2020-03-30T14:02:37Z", "commit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDowMjozN1rOF9rRPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNDoxMjoyMVrOF9rtLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxNjM4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    - name: KStreams KStreams Running Average tests tests\n          \n          \n            \n                    - name: KStreams Running Average tests", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400216382", "createdAt": "2020-03-30T14:02:37Z", "author": {"login": "colinhicks"}, "path": ".semaphore/semaphore.yml", "diffHunk": "@@ -177,3 +177,7 @@ blocks:\n         - name: KStreams aggregation MIN/MAX tests\n           commands:\n             - make -C _includes/tutorials/aggregating-minmax/kstreams/code tutorial\n+\n+        - name: KStreams KStreams Running Average tests tests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxODkyOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        <li><a href=\"aggregating-average/kstreams.html\">Calculate running average</a></li>\n          \n          \n            \n                        <li><a href=\"aggregating-average/kstreams.html\">Calculate a running average</a></li>", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400218929", "createdAt": "2020-03-30T14:06:22Z", "author": {"login": "colinhicks"}, "path": "index.html", "diffHunk": "@@ -56,6 +56,7 @@ <h2 class=\"subtitle\">Apache Kafka is a powerful, scalable, fault-tolerant distri\n             <li><a href=\"create-stateful-aggregation-count/ksql.html\">Count a stream of events</a></li>\n             <li><a href=\"create-stateful-aggregation-sum/ksql.html\">Sum a stream of events</a></li>\n             <li><a href=\"create-stateful-aggregation-minmax/ksql.html\">Find the min/max in a stream of events</a></li>\n+            <li><a href=\"aggregating-average/kstreams.html\">Calculate running average</a></li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIxOTk4Ng==", "bodyText": "Can this be version 2 like the other tutorials?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400219986", "createdAt": "2020-03-30T14:07:39Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,60 @@\n+---\n+version: '3.5'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIyMDUyNg==", "bodyText": "We can go ahead and bump CP versions to 5.4.1", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400220526", "createdAt": "2020-03-30T14:08:21Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/docker-compose.yml", "diffHunk": "@@ -0,0 +1,60 @@\n+---\n+version: '3.5'\n+\n+services:\n+  zookeeper:\n+    image: confluentinc/cp-zookeeper:5.4.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDIyMzUzMw==", "bodyText": "This paragraph has some out of date info, it's also not really necessary. I suggest deleting it altogether.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When the console producer starts, it will log some messages and hang, waiting for your input. Copy and paste one line at a time and press enter to send it. Note that these lines contain hard tabs between the key and the value, so retyping them without the tab will not work.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400223533", "createdAt": "2020-03-30T14:12:21Z", "author": {"login": "colinhicks"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc", "diffHunk": "@@ -0,0 +1,23 @@\n+////\n+   Example content file for how to include a console produer(s) in the tutorial.\n+   Usually you'll include a line referencing the script to run the console producer and also include some content\n+   describing how to input data as shown below.\n+\n+   Again modify this file as you need for your tutorial, as this is just sample content.  You also may have more than one\n+   console producer to run depending on how you structure your tutorial\n+\n+////\n+\n+In a new terminal, run:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"shell\">{% include_raw tutorials/aggregating-average/kstreams/code/tutorial-steps/dev/console-producer.sh %}</code></pre>\n++++++\n+\n+When the console producer starts, it will log some messages and hang, waiting for your input. Copy and paste one line at a time and press enter to send it. Note that these lines contain hard tabs between the key and the value, so retyping them without the tab will not work.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97623ee262176b25408fc461008764a5280365a1"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cda03dad6218e4f247726dffaf6549d2f5d4db2a", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/cda03dad6218e4f247726dffaf6549d2f5d4db2a", "committedDate": "2020-03-30T15:43:03Z", "message": "Update _includes/tutorials/aggregating-average/kstreams/markup/dev/run-producer.adoc\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/17359cc713eae044da0b12c652a7c7ee5ef38497", "committedDate": "2020-03-30T15:43:21Z", "message": "Update .semaphore/semaphore.yml\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg0MDMyMjQz", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#pullrequestreview-384032243", "createdAt": "2020-03-30T16:34:42Z", "commit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjozNDo0M1rOF9yUlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjo1MDowNVrOF9y9cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzMTkyNQ==", "bodyText": "can we update to 2.4.1? Same for the streams-test-utils", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400331925", "createdAt": "2020-03-30T16:34:43Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/build.gradle", "diffHunk": "@@ -0,0 +1,75 @@\n+buildscript {\n+  repositories {\n+    jcenter()\n+  }\n+  dependencies {\n+    classpath \"com.commercehub.gradle.plugin:gradle-avro-plugin:0.15.1\"\n+    classpath \"com.github.jengelman.gradle.plugins:shadow:4.0.2\"\n+  }\n+}\n+\n+plugins {\n+  id \"java\"\n+  id \"application\"\n+  id \"com.google.cloud.tools.jib\" version \"1.1.1\"\n+  id \"idea\"\n+  id \"eclipse\"\n+}\n+\n+sourceCompatibility = \"1.8\"\n+targetCompatibility = \"1.8\"\n+version = \"0.0.1\"\n+mainClassName = \"io.confluent.developer.RunningAverage\"\n+\n+repositories {\n+  jcenter()\n+  maven { url 'https://packages.confluent.io/maven/' }\n+}\n+\n+apply plugin: \"com.commercehub.gradle.plugin.avro\"\n+apply plugin: \"com.github.johnrengelman.shadow\"\n+\n+dependencies {\n+  implementation \"org.apache.avro:avro:1.8.2\"\n+  implementation \"org.slf4j:slf4j-simple:1.7.26\"\n+  implementation \"org.apache.kafka:kafka-streams:2.4.0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNTc0NQ==", "bodyText": "running the test harness locally the streams app wouldn't shut down changing streams.close() to streams.close(Duration.ofSeconds(5)); fixes that.  Otherwise, the main thread will block forever, waiting for streams to stop.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400335745", "createdAt": "2020-03-30T16:40:21Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMzNjkzNQ==", "bodyText": "running the test-harness locally failed for me, but adding   streams.cleanUp();  before streams.start()  fixes the issue.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400336935", "createdAt": "2020-03-30T16:42:07Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/code/src/main/java/io/confluent/developer/RunningAverage.java", "diffHunk": "@@ -0,0 +1,211 @@\n+package io.confluent.developer;\n+\n+import com.typesafe.config.Config;\n+import com.typesafe.config.ConfigFactory;\n+\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.stream.Stream;\n+\n+import io.confluent.demo.CountAndSum;\n+import io.confluent.demo.Rating;\n+import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;\n+import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\n+\n+import static io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.Short.parseShort;\n+import static java.util.Optional.ofNullable;\n+import static java.util.stream.Collectors.toMap;\n+import static org.apache.kafka.common.serialization.Serdes.Double;\n+import static org.apache.kafka.common.serialization.Serdes.Long;\n+import static org.apache.kafka.streams.StreamsConfig.APPLICATION_ID_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.BOOTSTRAP_SERVERS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG;\n+import static org.apache.kafka.streams.StreamsConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.streams.kstream.Grouped.with;\n+\n+public class RunningAverage {\n+\n+  //region buildStreamsProperties\n+  protected Properties buildStreamsProperties(Properties envProps) {\n+    Properties config = new Properties();\n+    config.putAll(envProps);\n+\n+    config.put(APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+    config.put(BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+    config.put(DEFAULT_KEY_SERDE_CLASS_CONFIG, Long().getClass());\n+    config.put(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Double().getClass());\n+    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+\n+    config.put(REPLICATION_FACTOR_CONFIG, envProps.getProperty(\"default.topic.replication.factor\"));\n+    config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, envProps.getProperty(\"offset.reset.policy\"));\n+\n+    config.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+    return config;\n+  }\n+  //endregion\n+\n+  //region createTopics\n+\n+  /**\n+   * Create topics using AdminClient API\n+   */\n+  private void createTopics(Properties envProps) {\n+    Map<String, Object> config = new HashMap<>();\n+\n+    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+    AdminClient client = AdminClient.create(config);\n+\n+    List<NewTopic> topics = new ArrayList<>();\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"input.ratings.topic.name\"),\n+        parseInt(envProps.getProperty(\"input.ratings.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"input.ratings.topic.replication.factor\"))));\n+\n+    topics.add(new NewTopic(\n+        envProps.getProperty(\"output.rating-averages.topic.name\"),\n+        parseInt(envProps.getProperty(\"output.rating-averages.topic.partitions\")),\n+        parseShort(envProps.getProperty(\"output.rating-averages.topic.replication.factor\"))));\n+\n+    client.createTopics(topics);\n+    client.close();\n+\n+  }\n+  //endregion\n+\n+  private void run() {\n+\n+    Properties envProps = this.loadEnvProperties();\n+    Properties streamProps = this.buildStreamsProperties(envProps);\n+    Topology topology = this.buildTopology(new StreamsBuilder(), envProps);\n+\n+    this.createTopics(envProps);\n+\n+    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+    final CountDownLatch latch = new CountDownLatch(1);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+      @Override\n+      public void run() {\n+        streams.close();\n+        latch.countDown();\n+      }\n+    });\n+\n+    try {\n+      streams.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM0MjM4Nw==", "bodyText": "To help the reader get the main point of the tutorial may consider adding something like this\nwith a brief description of the code\nHere's how you'll calculate the running average:\n+++++\n<pre class=\"snippet\"><code class=\"java\">\n   final KTable<Long, CountAndSum> ratingCountAndSum =\n        ratingsById.aggregate(() -> new CountAndSum(0L, 0.0),\n                              (key, value, aggregate) -> {\n                                aggregate.setCount(aggregate.getCount() + 1);\n                                aggregate.setSum(aggregate.getSum() + value);\n                                return aggregate;\n                              },\n                              Materialized.with(Long(), countAndSumSerde));\n</code></pre>\n+++++\n\nJust a suggestion though", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#discussion_r400342387", "createdAt": "2020-03-30T16:50:05Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/aggregating-average/kstreams/markup/dev/make-topology.adoc", "diffHunk": "@@ -0,0 +1,12 @@\n+////\n+In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.\n+The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.\n+////\n+\n+Then create the following file at `src/main/java/io/confluent/developer/RunningAverage.java`. Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL.\n+\n+// Full topology description goes here\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17359cc713eae044da0b12c652a7c7ee5ef38497"}, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53edd2d8e0d97aa19d03b98c9fee18ed6a28eb4e", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/53edd2d8e0d97aa19d03b98c9fee18ed6a28eb4e", "committedDate": "2020-03-30T17:33:55Z", "message": "Update index.html\n\nCo-Authored-By: Colin Hicks <colin.hicks@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "340c7ee4e393ef866d2502366b14871ead81b251", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/340c7ee4e393ef866d2502366b14871ead81b251", "committedDate": "2020-03-30T17:56:16Z", "message": "Bump cp version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "815e9c860ab92fd1efaca1704b16a164e807349e", "author": {"user": null}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/815e9c860ab92fd1efaca1704b16a164e807349e", "committedDate": "2020-03-30T17:56:16Z", "message": "Small fixed to make test reliable"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg0MTM4NDQz", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#pullrequestreview-384138443", "createdAt": "2020-03-30T18:52:57Z", "commit": {"oid": "815e9c860ab92fd1efaca1704b16a164e807349e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg0MTQyODI3", "url": "https://github.com/confluentinc/kafka-tutorials/pull/235#pullrequestreview-384142827", "createdAt": "2020-03-30T18:58:45Z", "commit": {"oid": "815e9c860ab92fd1efaca1704b16a164e807349e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 272, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}