{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5Mjg0ODAx", "number": 604, "title": "DEVX-2184: \"Detecting Abnormal Transactions\" recipe migration", "bodyText": "Jira Ticket\nThis PR proposes migrating an original streams processing \"recipe\" for detecting abnormal transactions to Kafka Tutorials. The semaphore test added for this KT is passing, no other KTs were altered in this PR.\nThe original recipe was not as straight forward as some of the other recipes so I did augment/alter the original content: I added explanations for the queries, used timestamps in the messages to help with testing assertions, generated fake data.\nI did have issues with getting my test/input.json to persist through to my stream created via an inner join but do not have problems when using INSERT INTO <data> in the src/statements.sql. I'd prefer to have my test data split out of src/statements.sql. Please let me know if you have thoughts on how to resolve this. Left as is, I think I need to alter the \"Take it to Production\" section due to the test data in the src/statements.sql.", "createdAt": "2020-10-23T23:48:02Z", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604", "merged": true, "mergeCommit": {"oid": "56b24b9cfd32d1612f3bbb727d5ea5f65b9dfb20"}, "closed": true, "closedAt": "2020-10-28T22:47:56Z", "author": {"login": "awalther28"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdS5vtFAH2gAyNTA5Mjg0ODAxOjhiN2YzZWEyYTU0NDM0MGJkNjU4ZGI4NmJkMDE3ODYzOWE0MzYyNGE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdXEaqPAH2gAyNTA5Mjg0ODAxOjY0Yzg1MzJhOTJhNTY2Yjg3MzI2MzBiODRlYTZjNzU2N2ZmODIzZjc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8b7f3ea2a544340bd658db86bd0178639a43624a", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/8b7f3ea2a544340bd658db86bd0178639a43624a", "committedDate": "2020-10-15T22:44:34Z", "message": "DEVX-2184; start migration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "027c95a3e31b2f2d79d53b3156c6dfb1ab5125cd", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/027c95a3e31b2f2d79d53b3156c6dfb1ab5125cd", "committedDate": "2020-10-21T16:37:22Z", "message": "DEVX-2184; first pass at outline"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b9c153c31008f5e7e62957a46246db31f898f1e", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/9b9c153c31008f5e7e62957a46246db31f898f1e", "committedDate": "2020-10-22T20:08:24Z", "message": "DEVX-2184; provide clearer explainations and fix steps"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c49c1f567182f5184bf1074321536c007b7ed59", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/3c49c1f567182f5184bf1074321536c007b7ed59", "committedDate": "2020-10-22T23:02:38Z", "message": "DEVX-2184; working on testing"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b80999f1185c696f7810b218da6cf69fe3c4cdac", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/b80999f1185c696f7810b218da6cf69fe3c4cdac", "committedDate": "2020-10-23T20:26:40Z", "message": "DEVX-2184; fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0791bc08e20c8939f919d1d59738d42a77d940ce", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/0791bc08e20c8939f919d1d59738d42a77d940ce", "committedDate": "2020-10-23T23:32:12Z", "message": "DEVX-2184; change title/heading"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/e73b70cc2a2957e4e004f6e786eb79a9fe411727", "committedDate": "2020-10-23T23:49:50Z", "message": "Merge branch 'master' into DEVX-2184"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MzU4ODgy", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#pullrequestreview-516358882", "createdAt": "2020-10-25T13:24:58Z", "commit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzoyNDo1OFrOHn5Zrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxMzo1NDowOFrOHn5mUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5Njk3NA==", "bodyText": "@awalther28 is it possible that \"abnormal events\" might be too domain specific?  If so, maybe add a clause here with a few more words that generalizes this, because \"abnormal events\" might not be well understood or descriptive enough?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511596974", "createdAt": "2020-10-25T13:24:58Z", "author": {"login": "ybyzek"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -484,3 +483,24 @@ masking-data:\n     ruby: disabled\n     scala: disabled\n     swift: disabled\n+\n+anomaly-detection:\n+  title: \"Detecting abnormal events\"\n+  meta-description: \"Detecting abnormal events\"\n+  slug: \"/anomaly-detection\"\n+  question: \"How do I find abnormal events?\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzI2OA==", "bodyText": "Feel free to wordsmith...\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              introduction: \"Consider a topic with events containing information about financial transactions and a table with known suspicious names. A common pattern for fraudsters is to disguise transactions under the name of a popular company, the idea being that the chances of them being recognized is very low. For example, transactions labeled Verizon, Citibank, USPS, etc., are likely to look similar and blend in.\n          \n          \n            \n              introduction: \"A common pattern for fraudsters is to disguise transactions under the name of a popular company, the idea being that the chances of them being recognized is very low. For example, transactions labeled Verizon, Citibank, USPS, etc., are likely to look similar and blend in.  This tutorial shows you how to identify this pattern of behavior by detecting 'abnormal' transactions that occur within a window of time", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511597268", "createdAt": "2020-10-25T13:27:44Z", "author": {"login": "ybyzek"}, "path": "_data/tutorials.yaml", "diffHunk": "@@ -484,3 +483,24 @@ masking-data:\n     ruby: disabled\n     scala: disabled\n     swift: disabled\n+\n+anomaly-detection:\n+  title: \"Detecting abnormal events\"\n+  meta-description: \"Detecting abnormal events\"\n+  slug: \"/anomaly-detection\"\n+  question: \"How do I find abnormal events?\"\n+  introduction: \"Consider a topic with events containing information about financial transactions and a table with known suspicious names. A common pattern for fraudsters is to disguise transactions under the name of a popular company, the idea being that the chances of them being recognized is very low. For example, transactions labeled Verizon, Citibank, USPS, etc., are likely to look similar and blend in.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5NzUwNg==", "bodyText": "Would it be appropriate to leverage the new \"Short Answer\" capabilities, a la https://github.com/confluentinc/kafka-tutorials/blob/master/_data/harnesses/aggregating-count/ksql.yml#L1-L7", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511597506", "createdAt": "2020-10-25T13:29:44Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/anomaly-detection/ksql.yml", "diffHunk": "@@ -0,0 +1,125 @@\n+dev:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5ODAzOQ==", "bodyText": "Other workflows sometimes presume pre-existence of the Kafka topic before creating the ksqlDB stream or table.  Would it help to add a note about what happens in this case where the Kafka topic doesn't already exist?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511598039", "createdAt": "2020-10-25T13:34:27Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-suspicious-names-table.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+First, you will need to create a ksqlDB table and Kafka topic to represent the suspicious names data. A table is more fitting for this suspicious names data because it is a mutable collection that changes over time. We may want to add company names to this table or remove them in the future.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5ODY3Mg==", "bodyText": "Given that there is no data in the topic yet, is this required?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511598672", "createdAt": "2020-10-25T13:40:00Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/set-properties.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Set ksqlDB to process data from the beginning of each Kafka topic:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5ODgyNQ==", "bodyText": "Consistency check: what should be in single quotes, backticks, double quotes?  Not sure we have an established style guide, WDYT?  cc: @bbejeck", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511598825", "createdAt": "2020-10-25T13:41:28Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-suspicious-transactions-stream.adoc", "diffHunk": "@@ -0,0 +1,14 @@\n+Using the table of suspicious names and stream of transactions, create a new stream of events containing transactions that were sent to an account name contained in the 'suspicious_names' table. We can do this by performing an `INNER JOIN`. In this case the `INNER JOIN` will couple events in the transaction stream where the \"recipient\" is the same as \"company_name\" in the suspicious_names table. The stream created below will continuously be populated by the coupled events created by the query.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5ODk0Mg==", "bodyText": "\"we flagged\" -- did we flag them or is it just that their names were in the table?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511598942", "createdAt": "2020-10-25T13:42:29Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/find-suspicious-transactions.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Inspect the new stream.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/anomaly-detection/ksql/code/tutorial-steps/dev/find-suspicious-transactions.sql %}</code></pre>\n++++++\n+\n+Note that some of the transactions we inserted earlier were to companies that we flagged as possibly suspicious.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5OTA3Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is alarming. ksqlDB gives us the ability to see if alarming activity is/was present for a particular user with the following query.\n          \n          \n            \n            For this use case, let's say that a single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is an anomaly. ksqlDB gives us the ability to see if any anomalies are present for a particular user with the following query.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511599073", "createdAt": "2020-10-25T13:43:54Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-accounts-to-monitor-table.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+A single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is alarming. ksqlDB gives us the ability to see if alarming activity is/was present for a particular user with the following query.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5OTE3MA==", "bodyText": "Not sure if there's a convention or not, but most of the meat in this tutorial is in this section.  Is there value in breaking this up?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511599170", "createdAt": "2020-10-25T13:44:40Z", "author": {"login": "ybyzek"}, "path": "_data/harnesses/anomaly-detection/ksql.yml", "diffHunk": "@@ -0,0 +1,125 @@\n+dev:\n+  steps:\n+    - title: Initialize the project\n+      content:\n+        - action: execute\n+          file: tutorial-steps/dev/init.sh\n+          render:\n+            file: tutorials/anomaly-detection/ksql/markup/dev/init.adoc\n+\n+        - change_directory: anomaly-detection\n+          action: execute\n+          file: tutorial-steps/dev/make-dirs.sh\n+          render:\n+            file: tutorials/anomaly-detection/ksql/markup/dev/make-dirs.adoc\n+\n+    - title: Get Confluent Platform\n+      content:\n+        - action: make_file\n+          file: docker-compose.yml\n+          render:\n+            file: tutorials/anomaly-detection/ksql/markup/dev/make-docker-compose.adoc\n+\n+        - action: execute_async\n+          file: tutorial-steps/dev/docker-compose-up.sh\n+          render:\n+            file: tutorials/anomaly-detection/ksql/markup/dev/start-compose.adoc\n+\n+        - action: execute\n+          file: tutorial-steps/dev/wait-for-containers.sh\n+          render:\n+            skip: true\n+\n+    - title: Write the program interactively using the CLI", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5OTYzOQ==", "bodyText": "This is a lot of information that is essentially buried in a \"footnote\".  Readers often skip this as a result.  Can you move most of this out of the footnote and into the text area?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511599639", "createdAt": "2020-10-25T13:49:06Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-accounts-to-monitor-table.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+A single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is alarming. ksqlDB gives us the ability to see if alarming activity is/was present for a particular user with the following query.\n+\n+[source,sql]\n+----\n+CREATE TABLE accounts_to_monitor\n+    WITH (kafka_topic='accounts_to_monitor', partitions=1, value_format='JSON') AS\n+    SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, // <1>\n+           TIMESTAMPTOSTRING(WINDOWEND, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_END,\n+           USERNAME\n+    FROM suspicious_transactions\n+    WINDOW TUMBLING (SIZE 24 HOURS) // <2>\n+    GROUP BY USERNAME\n+    HAVING COUNT(*) > 3; // <3>\n+----\n+<1> The timestamps here are important, they tell us what interval of time suspicious activity occurred.\n+<2> The `WINDOW TUMBLING` part of the query allows us to do an aggregation with distinct time boundaries. In this case our window is fixed at a length of 24 hours, does not allow gaps, and does not allow overlapping. Other types of windows are explained in the \"Collect data over time\" section of Kafka-Tutorials. If further explanation is need, checkout the link:https://docs.ksqldb.io/en/latest/concepts/time-and-windows-in-ksqldb-queries/#windows-in-sql-queries[*ksqlDB documentation about windows*]. It contains in depth descriptions and visualizations.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU5OTg5Mg==", "bodyText": "\"This Kafka topic\" -- which Kafka topic?\nDoes the topic drive, or maybe the events drive?  (a la \"event-driven\" language)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511599892", "createdAt": "2020-10-25T13:50:49Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/print-accounts-to-monitor.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+The ksqlDB table and thus Kafka topic contain a list of accounts against which more than three suspicious transactions have taken place in a 24-hour window.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/anomaly-detection/ksql/code/tutorial-steps/dev/print-accounts-to-monitor.sql %}</code></pre>\n++++++\n+\n+The output should look like the following:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/anomaly-detection/ksql/code/tutorial-steps/dev/print-accounts-to-monitor.log %}</code></pre>\n++++++\n+\n+Note that if you were to alter the `LIMIT` of results to something greater than 1, you would not see any other accounts flagged even though Victor von Frankenstein had a transaction that was flagged as suspicious. If you decided to rerun the query with a new limit, use `CTRL+D` to terminate the query. +\n+\n+\n+This Kafka topic can be used to drive monitoring and alerting applications that could take action such as placing a hold on the account, notifying the card holder, etc. +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMDAzMw==", "bodyText": "Depending on what you are trying to achieve, maybe the INSERT statements don't below here.  Just the queries.  Like, if a user were to take this to production, they just want the table/stream/queries, but not the fake data gen pieces?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511600033", "createdAt": "2020-10-25T13:52:01Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/make-src-file.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Now that you have a series of statements that's doing the right thing, the last step is to put them into a file so that they can be used outside the CLI session. Create a file at `src/statements.sql` with the following content:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMDA1NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Similarly, create a file at `test/output.json` with the expected outputs. ksqlDB joins its grouping key with the window boundaries, we need to use a bit of extra expression to describe what to expect. We leverage the window key to describe the start and end boundaries that the key represents. Checkout our tutorial on link:https://kafka-tutorials.confluent.io/create-tumbling-windows/ksql.html[*tumbling windows*] for a more comprehensive explanation.\n          \n          \n            \n            Create a file at `test/output.json` with the expected outputs. ksqlDB joins its grouping key with the window boundaries, we need to use a bit of extra expression to describe what to expect. We leverage the window key to describe the start and end boundaries that the key represents. Checkout our tutorial on link:https://kafka-tutorials.confluent.io/create-tumbling-windows/ksql.html[*tumbling windows*] for a more comprehensive explanation.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511600055", "createdAt": "2020-10-25T13:52:22Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/test/make-test-output.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Similarly, create a file at `test/output.json` with the expected outputs. ksqlDB joins its grouping key with the window boundaries, we need to use a bit of extra expression to describe what to expect. We leverage the window key to describe the start and end boundaries that the key represents. Checkout our tutorial on link:https://kafka-tutorials.confluent.io/create-tumbling-windows/ksql.html[*tumbling windows*] for a more comprehensive explanation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMDE3OA==", "bodyText": "Output from what?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511600178", "createdAt": "2020-10-25T13:53:38Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/test/make-test-output.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Similarly, create a file at `test/output.json` with the expected outputs. ksqlDB joins its grouping key with the window boundaries, we need to use a bit of extra expression to describe what to expect. We leverage the window key to describe the start and end boundaries that the key represents. Checkout our tutorial on link:https://kafka-tutorials.confluent.io/create-tumbling-windows/ksql.html[*tumbling windows*] for a more comprehensive explanation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMDA1NQ=="}, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYwMDIxMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Lastly, invoke the tests using the test runner and the statements file that you created earlier:\n          \n          \n            \n            Invoke the tests using the ksqlDB test runner and the statements file that you created earlier:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r511600210", "createdAt": "2020-10-25T13:54:08Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/test/run-tests.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Lastly, invoke the tests using the test runner and the statements file that you created earlier:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e73b70cc2a2957e4e004f6e786eb79a9fe411727"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dddb2bad54e99e6b38211b83cd67d417d6627e18", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/dddb2bad54e99e6b38211b83cd67d417d6627e18", "committedDate": "2020-10-26T14:05:42Z", "message": "Apply suggestions from code review\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "543045219444f279f58f5d762d4369e63c9e0a20", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/543045219444f279f58f5d762d4369e63c9e0a20", "committedDate": "2020-10-26T16:56:52Z", "message": "DEVX-2184; address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b54a9c5e23a58b9006da3d9e56013270cff9b40", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/7b54a9c5e23a58b9006da3d9e56013270cff9b40", "committedDate": "2020-10-26T18:24:36Z", "message": "DEVX-2184; fix broken tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f7b9f3a8127feef28a95b9b857629d807ac615f", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/9f7b9f3a8127feef28a95b9b857629d807ac615f", "committedDate": "2020-10-26T19:37:58Z", "message": "DEVX-2184; remove 'flagging'"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b83ebc354682f61a0a3edb6a83c19705a951c8e1", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/b83ebc354682f61a0a3edb6a83c19705a951c8e1", "committedDate": "2020-10-26T19:54:55Z", "message": "DEVX-2184; add offset policy back"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3MTMxNTU3", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#pullrequestreview-517131557", "createdAt": "2020-10-26T20:02:25Z", "commit": {"oid": "b83ebc354682f61a0a3edb6a83c19705a951c8e1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMDowMjoyNVrOHogQHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMDowMjoyNVrOHogQHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjIzMzUwMQ==", "bodyText": "@awalther28 is the INNER JOIN critical to the short answer for detecting anomalies?  Is it possible that only the WINDOWING is the key piece of the solution?  (In that, INNER JOIN is only required if the data isn't already joined...).\nTwo benefits to consider if INNER JOIN were removed are (a) keeping the short answer short and (b) freeing up real estate on the site.\nI'm on the fence, just wanted to share this food for thought", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512233501", "createdAt": "2020-10-26T20:02:25Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/answer/join-and-windowing.adoc", "diffHunk": "@@ -0,0 +1,11 @@\n+Create new stream with INNER JOIN between the table and stream.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b83ebc354682f61a0a3edb6a83c19705a951c8e1"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/b2fb30b0ecb9639fd67a69e6414fad1571a41fe2", "committedDate": "2020-10-26T20:19:16Z", "message": "DEVX-2184; change short answer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3MTk2NDA5", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#pullrequestreview-517196409", "createdAt": "2020-10-26T21:39:46Z", "commit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "state": "APPROVED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMTozOTo0NlrOHojbeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMTo1MDo0M1rOHojvew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4NTU2Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Given that transaction events are joined with table reference data, use `WINDOWING` to group anomalous transactions.\n          \n          \n            \n            Assuming transaction events are joined with table reference data, use `WINDOWING` to group anomalous transactions.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512285562", "createdAt": "2020-10-26T21:39:46Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/answer/join-and-windowing.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Given that transaction events are joined with table reference data, use `WINDOWING` to group anomalous transactions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4Njc2MQ==", "bodyText": "Note: the order of fields in the description does not match the order of fields in the ksqlDB statement. For a user following along, can we update the fields or the query so they are in sync?", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512286761", "createdAt": "2020-10-26T21:42:08Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-transactions-stream.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Likewise, you'll need a ksqlDB stream and Kafka topic to represent transaction events. The transaction information includes the identifier, the user sending the money, the time of the transaction, and the name of the recipient. Since this data represents a historical sequence of events, a stream is most appropriate.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4NzA3MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Set KSQL to process data from the beginning of each Kafka topic:\n          \n          \n            \n            Set ksqlDB to process data from the beginning of each Kafka topic:", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512287070", "createdAt": "2020-10-26T21:42:50Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/set-properties.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Set KSQL to process data from the beginning of each Kafka topic:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4Nzk5Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Using the table of suspicious names and stream of transactions, create a new stream of events containing transactions that were sent to an account name contained in the 'suspicious_names' table. We can do this by performing an `INNER JOIN`. In this case the `INNER JOIN` will couple events in the transaction stream where the \"recipient\" is the same as \"company_name\" in the suspicious_names table. The stream created below will continuously be populated by the coupled events created by the query.\n          \n          \n            \n            Using the table of suspicious names and stream of transactions, create a new stream of events containing only those transactions that were sent to an account name contained in the 'suspicious_names' table. We can do this by performing an `INNER JOIN`. In this case the `INNER JOIN` will couple events in the transaction stream where the \"recipient\" is the same as \"company_name\" in the suspicious_names table. The stream created below will continuously be populated by the coupled events created by the query.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512287996", "createdAt": "2020-10-26T21:44:47Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-suspicious-transactions-stream.adoc", "diffHunk": "@@ -0,0 +1,14 @@\n+Using the table of suspicious names and stream of transactions, create a new stream of events containing transactions that were sent to an account name contained in the 'suspicious_names' table. We can do this by performing an `INNER JOIN`. In this case the `INNER JOIN` will couple events in the transaction stream where the \"recipient\" is the same as \"company_name\" in the suspicious_names table. The stream created below will continuously be populated by the coupled events created by the query.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4ODQ5MA==", "bodyText": "Awesome presentation @awalther28", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512288490", "createdAt": "2020-10-26T21:45:46Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-accounts-to-monitor-table.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+For this use case, let's say that a single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is an anomaly. ksqlDB gives us the ability to see if any anomalies are present for a particular user with the following query.\n+\n+[source,sql]\n+----\n+CREATE TABLE accounts_to_monitor\n+    WITH (kafka_topic='accounts_to_monitor', partitions=1, value_format='JSON') AS\n+    SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, // <1>\n+           TIMESTAMPTOSTRING(WINDOWEND, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_END,\n+           USERNAME\n+    FROM suspicious_transactions\n+    WINDOW TUMBLING (SIZE 24 HOURS) // <2>\n+    GROUP BY USERNAME\n+    HAVING COUNT(*) > 3; // <3>\n+----\n++++++\n+<div class=\"colist arabic\">\n+<p><i class=\"conum\" data-value=\"1\"></i><b>1</b> The timestamps here are important, they tell us what interval of time suspicious activity occurred.</p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4ODk4Nw==", "bodyText": "they tell us what interval of time suspicious activity occurred -- the timestamps themselves don't tell the interval.  Would it be more accurate to phrase, something along the lines of, they are used to calculate the interval of time between transactions (or however you see fit)", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512288987", "createdAt": "2020-10-26T21:47:00Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-accounts-to-monitor-table.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+For this use case, let's say that a single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is an anomaly. ksqlDB gives us the ability to see if any anomalies are present for a particular user with the following query.\n+\n+[source,sql]\n+----\n+CREATE TABLE accounts_to_monitor\n+    WITH (kafka_topic='accounts_to_monitor', partitions=1, value_format='JSON') AS\n+    SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, // <1>\n+           TIMESTAMPTOSTRING(WINDOWEND, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_END,\n+           USERNAME\n+    FROM suspicious_transactions\n+    WINDOW TUMBLING (SIZE 24 HOURS) // <2>\n+    GROUP BY USERNAME\n+    HAVING COUNT(*) > 3; // <3>\n+----\n++++++\n+<div class=\"colist arabic\">\n+<p><i class=\"conum\" data-value=\"1\"></i><b>1</b> The timestamps here are important, they tell us what interval of time suspicious activity occurred.</p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4OTM3Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <p><i class=\"conum\" data-value=\"2\"></i><b>2</b> The <code>WINDOW TUMBLING</code> part of the query allows us to do an aggregation with distinct time boundaries. In this case our window is fixed at a length of 24 hours, does not allow gaps, and does not allow overlapping. Other types of windows are explained in the \"Collect data over time\" section of Kafka-Tutorials. If further explanation is need, checkout the <a href=\"https://docs.ksqldb.io/en/latest/concepts/time-and-windows-in-ksqldb-queries/#windows-in-sql-queries\"><strong>ksqlDB documentation about windows</strong></a>. It contains in depth descriptions and visualizations.</p>\n          \n          \n            \n            <p><i class=\"conum\" data-value=\"2\"></i><b>2</b> The <code>WINDOW TUMBLING</code> part of the query allows us to do an aggregation with distinct time boundaries. In this case our window is fixed at a length of 24 hours, does not allow gaps, and does not allow overlapping. Other types of windows are explained in the \"Collect data over time\" section of Kafka-Tutorials. For more in-depth descriptions and visualizations, checkout the <a href=\"https://docs.ksqldb.io/en/latest/concepts/time-and-windows-in-ksqldb-queries/#windows-in-sql-queries\"><strong>ksqlDB documentation about windows</strong></a>. </p>", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512289376", "createdAt": "2020-10-26T21:47:52Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-accounts-to-monitor-table.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+For this use case, let's say that a single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is an anomaly. ksqlDB gives us the ability to see if any anomalies are present for a particular user with the following query.\n+\n+[source,sql]\n+----\n+CREATE TABLE accounts_to_monitor\n+    WITH (kafka_topic='accounts_to_monitor', partitions=1, value_format='JSON') AS\n+    SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, // <1>\n+           TIMESTAMPTOSTRING(WINDOWEND, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_END,\n+           USERNAME\n+    FROM suspicious_transactions\n+    WINDOW TUMBLING (SIZE 24 HOURS) // <2>\n+    GROUP BY USERNAME\n+    HAVING COUNT(*) > 3; // <3>\n+----\n++++++\n+<div class=\"colist arabic\">\n+<p><i class=\"conum\" data-value=\"1\"></i><b>1</b> The timestamps here are important, they tell us what interval of time suspicious activity occurred.</p>\n+<p><i class=\"conum\" data-value=\"2\"></i><b>2</b> The <code>WINDOW TUMBLING</code> part of the query allows us to do an aggregation with distinct time boundaries. In this case our window is fixed at a length of 24 hours, does not allow gaps, and does not allow overlapping. Other types of windows are explained in the \"Collect data over time\" section of Kafka-Tutorials. If further explanation is need, checkout the <a href=\"https://docs.ksqldb.io/en/latest/concepts/time-and-windows-in-ksqldb-queries/#windows-in-sql-queries\"><strong>ksqlDB documentation about windows</strong></a>. It contains in depth descriptions and visualizations.</p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI4OTg4OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The ksqlDB table and thus Kafka topic contain a list of accounts against which more than three suspicious transactions have taken place in a 24-hour window.\n          \n          \n            \n            The ksqlDB table, and the underlying Kafka topic backing this table, contain a list of accounts against which more than three suspicious transactions have taken place in a 24-hour window.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512289889", "createdAt": "2020-10-26T21:49:07Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/print-accounts-to-monitor.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+The ksqlDB table and thus Kafka topic contain a list of accounts against which more than three suspicious transactions have taken place in a 24-hour window.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI5MDE1Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Type 'exit' and hit enter to shutdown the ksqlDB cli.\n          \n          \n            \n            \n          \n          \n            \n            Type 'exit' and hit enter to shutdown the ksqlDB cli.", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512290157", "createdAt": "2020-10-26T21:49:39Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/print-accounts-to-monitor.adoc", "diffHunk": "@@ -0,0 +1,17 @@\n+The ksqlDB table and thus Kafka topic contain a list of accounts against which more than three suspicious transactions have taken place in a 24-hour window.\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/anomaly-detection/ksql/code/tutorial-steps/dev/print-accounts-to-monitor.sql %}</code></pre>\n++++++\n+\n+The output should look like the following:\n+\n++++++\n+<pre class=\"snippet\"><code class=\"sql\">{% include_raw tutorials/anomaly-detection/ksql/code/tutorial-steps/dev/print-accounts-to-monitor.log %}</code></pre>\n++++++\n+\n+Note that if you were to alter the `LIMIT` of results to something greater than 1, you would not see any other accounts flagged even though Victor von Frankenstein had a transaction that was flagged as suspicious. If you decided to rerun the query with a new limit, use `CTRL+D` to terminate the query. +\n+\n+\n+Events within the Kafka topic accounts_to_monitor can be used to drive monitoring and alerting applications that could take action such as placing a hold on the account, notifying the card holder, etc. +\n+Type 'exit' and hit enter to shutdown the ksqlDB cli.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI5MDY4Mw==", "bodyText": "Would it make sense to modify as:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that you have a series of statements that's doing the right thing, the last step is to put them into a file so that they can be used outside the CLI session. Create a file at `src/statements.sql` with the following content:\n          \n          \n            \n            Now that you have a series of statements that's doing the right thing, the last step is to put them into a file so that they can be used outside the CLI session. Create a file at `src/statements.sql` with the following content that represents the suspicious names (In production, you would likely use Kafka Connect to read the suspicious names from a database into a Kafka topic, and then create a ksqlDB stream for it).", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512290683", "createdAt": "2020-10-26T21:50:43Z", "author": {"login": "ybyzek"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/make-src-file.adoc", "diffHunk": "@@ -0,0 +1,5 @@\n+Now that you have a series of statements that's doing the right thing, the last step is to put them into a file so that they can be used outside the CLI session. Create a file at `src/statements.sql` with the following content:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2fb30b0ecb9639fd67a69e6414fad1571a41fe2"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "299cc28fbe067bf88901779e4db77d23eccc9c5e", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/299cc28fbe067bf88901779e4db77d23eccc9c5e", "committedDate": "2020-10-26T21:55:09Z", "message": "Apply suggestions from code review\n\nCo-authored-by: Yeva Byzek <ybyzek@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74d5eec36e6d3ff61bc89bb879c99930d6557c5a", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/74d5eec36e6d3ff61bc89bb879c99930d6557c5a", "committedDate": "2020-10-26T22:40:02Z", "message": "DEVX-2184; alter window annot and list of fields"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f244aead130964f725b6cc34cbefedee56543c4", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/2f244aead130964f725b6cc34cbefedee56543c4", "committedDate": "2020-10-26T23:37:12Z", "message": "DEVX-2184; create new statement file for Prod"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0eeede4a127ca8b3daca1c494a91707d4eeb0dc", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/d0eeede4a127ca8b3daca1c494a91707d4eeb0dc", "committedDate": "2020-10-27T13:47:52Z", "message": "DEVX-2184; change test sttmt file name"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3ODgwNTk4", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#pullrequestreview-517880598", "createdAt": "2020-10-27T16:06:16Z", "commit": {"oid": "d0eeede4a127ca8b3daca1c494a91707d4eeb0dc"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNjowNjoxNlrOHpEWBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNjowNjoxNlrOHpEWBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjgyNDgzOA==", "bodyText": "Super minor nit: It wasn't obvious to me at first to create this table. Maybe add a sentence at the end, telling the user explicitly to create the table.  But only if you want to. Again this is a minor subjective comment.\nEDIT: All the tests passed, so forget my previous comment!", "url": "https://github.com/confluentinc/kafka-tutorials/pull/604#discussion_r512824838", "createdAt": "2020-10-27T16:06:16Z", "author": {"login": "bbejeck"}, "path": "_includes/tutorials/anomaly-detection/ksql/markup/dev/create-accounts-to-monitor-table.adoc", "diffHunk": "@@ -0,0 +1,21 @@\n+For this use case, let's say that a single transaction to one of the companies in the suspicious_names table is probably okay, but multiple transactions to one or more of those companies in a 24-hour period is an anomaly. ksqlDB gives us the ability to see if any anomalies are present for a particular user with the following query.\n+\n+[source,sql]\n+----\n+CREATE TABLE accounts_to_monitor\n+    WITH (kafka_topic='accounts_to_monitor', partitions=1, value_format='JSON') AS\n+    SELECT TIMESTAMPTOSTRING(WINDOWSTART, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_START, // <1>\n+           TIMESTAMPTOSTRING(WINDOWEND, 'yyyy-MM-dd HH:mm:ss Z') AS WINDOW_END,\n+           USERNAME\n+    FROM suspicious_transactions\n+    WINDOW TUMBLING (SIZE 24 HOURS) // <2>\n+    GROUP BY USERNAME\n+    HAVING COUNT(*) > 3; // <3>\n+----\n++++++\n+<div class=\"colist arabic\">\n+<p><i class=\"conum\" data-value=\"1\"></i><b>1</b> The fields `WINDOW_START` and `WINDOW_END` tell us what interval of time suspicious activity occurred.</p>\n+<p><i class=\"conum\" data-value=\"2\"></i><b>2</b> The <code>WINDOW TUMBLING</code> part of the query allows us to do an aggregation with distinct time boundaries. In this case our window is fixed at a length of 24 hours, does not allow gaps, and does not allow overlapping. Other types of windows are explained in the \"Collect data over time\" section of Kafka-Tutorials. For more in-depth descriptions and visualizations, checkout the <a href=\"https://docs.ksqldb.io/en/latest/concepts/time-and-windows-in-ksqldb-queries/#windows-in-sql-queries\"><strong>ksqlDB documentation about windows</strong></a>. </p>\n+<p><i class=\"conum\" data-value=\"3\"></i><b>3</b> The last two lines of the query address how you would determine if a user had multiple suspicious transactions. This aspect of the query says, in essence, if any user has greater than 3 suspicious transactions during the window, emit an event to the accounts_to_monitor table. </p>\n+</div>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0eeede4a127ca8b3daca1c494a91707d4eeb0dc"}, "originalPosition": 20}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd6d979136e1cdfc3a203b148134e4a53516f251", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/fd6d979136e1cdfc3a203b148134e4a53516f251", "committedDate": "2020-10-28T21:09:27Z", "message": "DEVX-2184; direct table creation + other use cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "993713b15fc1c9a97fdeb522c4982fb64c976211", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/993713b15fc1c9a97fdeb522c4982fb64c976211", "committedDate": "2020-10-28T21:11:04Z", "message": "Merge branch 'master' into DEVX-2184"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a40aa8dc31e31a4aad84118ff5d10253bc4b6ee1", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/a40aa8dc31e31a4aad84118ff5d10253bc4b6ee1", "committedDate": "2020-10-28T21:17:16Z", "message": "DEVX-2184; fix typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64c8532a92a566b8732630b84ea6c7567ff823f7", "author": {"user": {"login": "awalther28", "name": "Allison Walther"}}, "url": "https://github.com/confluentinc/kafka-tutorials/commit/64c8532a92a566b8732630b84ea6c7567ff823f7", "committedDate": "2020-10-28T21:26:14Z", "message": "Merge branch 'DEVX-2184' of github.com:confluentinc/kafka-tutorials into DEVX-2184"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 91, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}