{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQwMjg5MDQ3", "number": 1498, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMjowOFrOFFthuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTo1Njo0MVrOFHPpUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTMzMTE1OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMjowOFrOIGW3Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMjowOFrOIGW3Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzNjkyNg==", "bodyText": "I think there's a word missing here\"\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            export a model as JSON and import to another cluster.\n          \n          \n            \n            export a model as JSON and import it to another cluster.", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543536926", "createdAt": "2020-12-15T17:22:08Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTMzNzEwOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMzoyMlrOIGW6kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMzoyMlrOIGW6kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzNzgwOQ==", "bodyText": "We try to avoid \"we\" unless it's a tutorial, which this isn't. Use \"you\" instead:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n          \n          \n            \n            1. Given a model _name_, find the model _ID_. You can use `curl` to call the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543537809", "createdAt": "2020-12-15T17:23:22Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTMzOTYzOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMzo1N1rOIGW8Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyMzo1N1rOIGW8Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzODIwMg==", "bodyText": "We typically don't capitalize link titles:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {ref}/get-trained-models.html[Get Trained Model API] to list all models with\n          \n          \n            \n            {ref}/get-trained-models.html[get trained model API] to list all models with", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543538202", "createdAt": "2020-12-15T17:23:57Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM0ODc0OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyNTo0N1rOIGXBag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyNTo0N1rOIGXBag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUzOTU2Mg==", "bodyText": "Not a big deal, but wish is not something we tend to use in the docs:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If you wish to show just the model IDs available, use `jq` to select a subset.\n          \n          \n            \n            If you want to show just the model IDs available, use `jq` to select a subset.", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543539562", "createdAt": "2020-12-15T17:25:47Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM1Mzg2OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyNjo1N1rOIGXEnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyNjo1N1rOIGXEnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MDM4MA==", "bodyText": "Per above, let's try replacing \"we\" with \"you\".  For example:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In our case, we would like to export the model with ID `flights1-1607953694065`.\n          \n          \n            \n            In this example, you are exporting the model with ID `flights1-1607953694065`.", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543540380", "createdAt": "2020-12-15T17:26:57Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM1NTc4OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyNzoyNVrOIGXFzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyNzoyNVrOIGXFzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MDY4NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n          \n          \n            \n            {ref}/get-trained-models.html[get trained models API] to export the entire model", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543540684", "createdAt": "2020-12-15T17:27:25Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM2MjI2OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyODo0MVrOIGXJjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyODo0MVrOIGXJjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MTY0Ng==", "bodyText": "I think the Console is well enough known that you don't need to link it here:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n          \n          \n            \n            the model over HTTP into a file. If you use the {kib} Console,", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543541646", "createdAt": "2020-12-15T17:28:41Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM2NDAxOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyOTowNVrOIGXKtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyOTowNVrOIGXKtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MTk0Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543541942", "createdAt": "2020-12-15T17:29:05Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM2NTkyOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyOToyNFrOIGXL0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzoyOToyNFrOIGXL0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MjIyNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            browser to become unresponsive due to the size of exported models.\n          \n          \n            \n            browser might be unresponsive due to the size of exported models.", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543542224", "createdAt": "2020-12-15T17:29:24Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM2OTA3OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMDowMFrOIGXNmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMDowMFrOIGXNmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MjY4Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Note the query parameters that are used during export. These are necessary to\n          \n          \n            \n            * Note the query parameters that are used during export. These parameters are necessary to", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543542683", "createdAt": "2020-12-15T17:30:00Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM3MzU5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMDo1N1rOIGXQYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMDo1N1rOIGXQYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MzM5NA==", "bodyText": "This is another paragraph where ideally \"we\" can be replaced with \"you\".  For example:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * We need to unnest the JSON object by one level to extract just the model\n          \n          \n            \n            * You must unnest the JSON object by one level to extract just the model", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543543394", "createdAt": "2020-12-15T17:30:57Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 194}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM3NTMyOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMToxOFrOIGXRgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMToxOFrOIGXRgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0MzY4MA==", "bodyText": "Ditto:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            definition. We also want to remove the existing model ID in order to not have\n          \n          \n            \n            definition. You must also remove the existing model ID in order to not have", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543543680", "createdAt": "2020-12-15T17:31:18Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 195}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM3ODMxOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMTo0N1rOIGXTLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMTo0N1rOIGXTLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NDEwOQ==", "bodyText": "Ditto:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ID collisions when we import again. We do these steps using `jq` inline but\n          \n          \n            \n            ID collisions when you import again. You can do these steps using `jq` inline or", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543544109", "createdAt": "2020-12-15T17:31:47Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM3OTgzOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMjoxMVrOIGXUMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMjoxMVrOIGXUMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NDM2OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            create-trained-models.html[Created Trained Model API]. When we specify the URL,\n          \n          \n            \n            create-trained-models.html[created trained model API]. When you specify the URL,", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543544368", "createdAt": "2020-12-15T17:32:11Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM4MTY5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMjoyN1rOIGXVNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMjoyN1rOIGXVNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NDYzMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            we can also set the model ID to something new using the last path part of the\n          \n          \n            \n            you can also set the model ID to something new using the last path part of the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543544631", "createdAt": "2020-12-15T17:32:27Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM4NjM3OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMzoyNVrOIGXYDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozMzoyNVrOIGXYDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NTM1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n          \n          \n            \n            * Models exported from the {ref}/get-trained-models.html[get trained models API]", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543545358", "createdAt": "2020-12-15T17:33:25Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 222}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM5MDA5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNDowOVrOIGXaLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxODoxODoxOVrOIGZVlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NTkwMA==", "bodyText": "Not sure why this link is fully qualified. I think it should use {ref} same as others...\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            are limited in size by the https://www.elastic.co/guide/en/elasticsearch/\n          \n          \n            \n            are limited in size by the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543545900", "createdAt": "2020-12-15T17:34:09Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n+are limited in size by the https://www.elastic.co/guide/en/elasticsearch/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU3NzQ5Mg==", "bodyText": "Sorry, I didn't have the right {ref}'s for some links.", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543577492", "createdAt": "2020-12-15T18:18:19Z", "author": {"login": "joshdevins"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n+are limited in size by the https://www.elastic.co/guide/en/elasticsearch/", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NTkwMA=="}, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 223}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM5MTUyOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNDoyM1rOIGXa_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNDoyM1rOIGXa_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NjExMQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            reference/current/modules-http.html#_http_settings[http.max_content_length]\n          \n          \n            \n            {ref}/modules-http.html#_http_settings[http.max_content_length]", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543546111", "createdAt": "2020-12-15T17:34:23Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n+are limited in size by the https://www.elastic.co/guide/en/elasticsearch/\n+reference/current/modules-http.html#_http_settings[http.max_content_length]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM5NDkwOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNTowNFrOIGXc-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNTowNFrOIGXc-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NjYxOA==", "bodyText": "Is ES the right word here or should it be cluster?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Elasticsearch is under load, or when model sizes are very large. Increasing\n          \n          \n            \n            cluster is under load, or when model sizes are very large. Increasing", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543546618", "createdAt": "2020-12-15T17:35:04Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n+are limited in size by the https://www.elastic.co/guide/en/elasticsearch/\n+reference/current/modules-http.html#_http_settings[http.max_content_length]\n+global configuration value in Elasticsearch. The default value is `100mb` and\n+may need to be increased depending on the size of model being exported.\n+\n+* Connection timeouts can occur when either the source or destination\n+Elasticsearch is under load, or when model sizes are very large. Increasing", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTM5OTk5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNjowOFrOIGXf7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNjowOFrOIGXf7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NzM3NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The example uses Python to train, save and import the model, however, you can\n          \n          \n            \n            The example uses Python to train, save and import the model. However, you can", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543547375", "createdAt": "2020-12-15T17:36:08Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n+are limited in size by the https://www.elastic.co/guide/en/elasticsearch/\n+reference/current/modules-http.html#_http_settings[http.max_content_length]\n+global configuration value in Elasticsearch. The default value is `100mb` and\n+may need to be increased depending on the size of model being exported.\n+\n+* Connection timeouts can occur when either the source or destination\n+Elasticsearch is under load, or when model sizes are very large. Increasing\n+https://ec.haxx.se/usingcurl/usingcurl-timeouts[timeout configurations] for\n+`curl` (e.g. `curl --max-time 600`) or your client of choice will help\n+alleviate the problem. In rare cases you may need to reduce load on the\n+Elasticsearch cluster, for example by adding nodes.\n+--\n \n \n [discrete]\n-[[move-trained-model-to-es]]\n-== Moving a model to the {stack}\n+[[import-external-model-to-es]]\n+== Importing an external model to the {stack}\n \n-It is possible to add a model to your {es} cluster even if the model is not \n+It is possible to import a model to your {es} cluster even if the model is not\n trained by Elastic {dfanalytics}.\n \n-You can find an example of training a model, then adding it to {es} by using \n-eland \n-https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[in eland docs].\n-The example uses Python to train and move the model, however, you can use any \n-Client as long as the format of your trained model meets \n-https://github.com/elastic/ml-json-schemas[the required schema].\n-\n+You can find an example of training a model, then importing it to {es} with\n+`eland` in the\n+https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[eland machine learning demo].\n+The example uses Python to train, save and import the model, however, you can", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxNTQwMTU5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNjozMVrOIGXg-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxNzozNjozMVrOIGXg-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NzY0Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            use any Client as long as the model conforms to the https://github.com/elastic/\n          \n          \n            \n            use any client as long as the model conforms to the https://github.com/elastic/", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r543547643", "createdAt": "2020-12-15T17:36:31Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,146 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. Using `curl` we can call the\n+{ref}/get-trained-models.html[Get Trained Model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you wish to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In our case, we would like to export the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[Get Trained Model API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u elastic:changeme \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. Using the https://www.elastic.co/guide/en/\n+kibana/current/console-kibana.html[Kibana Dev Tools console] will cause the\n+browser to become unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* We need to unnest the JSON object by one level to extract just the model\n+definition. We also want to remove the existing model ID in order to not have\n+ID collisions when we import again. We do these steps using `jq` inline but\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[Created Trained Model API]. When we specify the URL,\n+we can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u elastic:changeme \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[Get Trained Model API]\n+are limited in size by the https://www.elastic.co/guide/en/elasticsearch/\n+reference/current/modules-http.html#_http_settings[http.max_content_length]\n+global configuration value in Elasticsearch. The default value is `100mb` and\n+may need to be increased depending on the size of model being exported.\n+\n+* Connection timeouts can occur when either the source or destination\n+Elasticsearch is under load, or when model sizes are very large. Increasing\n+https://ec.haxx.se/usingcurl/usingcurl-timeouts[timeout configurations] for\n+`curl` (e.g. `curl --max-time 600`) or your client of choice will help\n+alleviate the problem. In rare cases you may need to reduce load on the\n+Elasticsearch cluster, for example by adding nodes.\n+--\n \n \n [discrete]\n-[[move-trained-model-to-es]]\n-== Moving a model to the {stack}\n+[[import-external-model-to-es]]\n+== Importing an external model to the {stack}\n \n-It is possible to add a model to your {es} cluster even if the model is not \n+It is possible to import a model to your {es} cluster even if the model is not\n trained by Elastic {dfanalytics}.\n \n-You can find an example of training a model, then adding it to {es} by using \n-eland \n-https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[in eland docs].\n-The example uses Python to train and move the model, however, you can use any \n-Client as long as the format of your trained model meets \n-https://github.com/elastic/ml-json-schemas[the required schema].\n-\n+You can find an example of training a model, then importing it to {es} with\n+`eland` in the\n+https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[eland machine learning demo].\n+The example uses Python to train, save and import the model, however, you can\n+use any Client as long as the model conforms to the https://github.com/elastic/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "727437b96c98fd423574441c9b44ac8bb46f728e"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTI4MzgwOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToyNjo0OVrOIInl6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToyNjo0OVrOIInl6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwODIwMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the model over HTTP into a file. If you use the {kib} Console, \n          \n          \n            \n            the model over HTTP into a file. If you use the {kib} Console, the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r545908202", "createdAt": "2020-12-18T15:26:49Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,145 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import it to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. You can use `curl` to call the\n+{ref}/get-trained-models.html[get trained model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you want to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In this example, you are exporting the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[get trained models API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. If you use the {kib} Console, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8443f5c178d6a98a75e13b5864a87fc8e87d8e1"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTI5MDA2OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToyODoyM1rOIInptA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToyODoyM1rOIInptA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwOTE3Mg==", "bodyText": "This URL is failing in the output, I think maybe it has to be all on one line:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n          \n          \n            \n            3. Import the saved model using `curl` to upload the JSON file to the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r545909172", "createdAt": "2020-12-18T15:28:23Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,145 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import it to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. You can use `curl` to call the\n+{ref}/get-trained-models.html[get trained model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you want to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In this example, you are exporting the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[get trained models API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. If you use the {kib} Console, \n+browser might be unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These parameters are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* You must unnest the JSON object by one level to extract just the model\n+definition. You must also remove the existing model ID in order to not have\n+ID collisions when you import again. You can do these steps using `jq` inline or\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8443f5c178d6a98a75e13b5864a87fc8e87d8e1"}, "originalPosition": 202}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTI5MDk3OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToyODo0MVrOIInqUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNToyODo0MVrOIInqUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwOTMyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            create-trained-models.html[created trained model API]. When you specify the URL,\n          \n          \n            \n            {ref}/create-trained-models.html[created trained model API]. When you specify the URL,", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r545909328", "createdAt": "2020-12-18T15:28:41Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,145 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import it to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. You can use `curl` to call the\n+{ref}/get-trained-models.html[get trained model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you want to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In this example, you are exporting the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[get trained models API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. If you use the {kib} Console, \n+browser might be unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These parameters are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* You must unnest the JSON object by one level to extract just the model\n+definition. You must also remove the existing model ID in order to not have\n+ID collisions when you import again. You can do these steps using `jq` inline or\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[created trained model API]. When you specify the URL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8443f5c178d6a98a75e13b5864a87fc8e87d8e1"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTI5ODY5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTozMDoyOFrOIInu4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTozMDoyOFrOIInu4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkxMDQ5OA==", "bodyText": "This URL also fails in the output, so I think it needs to be on one line too:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            use any client as long as the model conforms to the https://github.com/elastic/\n          \n          \n            \n            use any client as long as the model conforms to the", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r545910498", "createdAt": "2020-12-18T15:30:28Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,145 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import it to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. You can use `curl` to call the\n+{ref}/get-trained-models.html[get trained model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you want to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In this example, you are exporting the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[get trained models API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. If you use the {kib} Console, \n+browser might be unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These parameters are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* You must unnest the JSON object by one level to extract just the model\n+definition. You must also remove the existing model ID in order to not have\n+ID collisions when you import again. You can do these steps using `jq` inline or\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[created trained model API]. When you specify the URL,\n+you can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u username:password \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[get trained models API]\n+are limited in size by the\n+{ref}/modules-http.html#_http_settings[http.max_content_length]\n+global configuration value in Elasticsearch. The default value is `100mb` and\n+may need to be increased depending on the size of model being exported.\n+\n+* Connection timeouts can occur when either the source or destination\n+cluster is under load, or when model sizes are very large. Increasing\n+https://ec.haxx.se/usingcurl/usingcurl-timeouts[timeout configurations] for\n+`curl` (e.g. `curl --max-time 600`) or your client of choice will help\n+alleviate the problem. In rare cases you may need to reduce load on the\n+Elasticsearch cluster, for example by adding nodes.\n+--\n \n \n [discrete]\n-[[move-trained-model-to-es]]\n-== Moving a model to the {stack}\n+[[import-external-model-to-es]]\n+== Importing an external model to the {stack}\n \n-It is possible to add a model to your {es} cluster even if the model is not \n+It is possible to import a model to your {es} cluster even if the model is not\n trained by Elastic {dfanalytics}.\n \n-You can find an example of training a model, then adding it to {es} by using \n-eland \n-https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[in eland docs].\n-The example uses Python to train and move the model, however, you can use any \n-Client as long as the format of your trained model meets \n-https://github.com/elastic/ml-json-schemas[the required schema].\n-\n+You can find an example of training a model, then importing it to {es} with\n+`eland` in the\n+https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[eland machine learning demo].\n+The example uses Python to train, save and import the model. However, you can\n+use any client as long as the model conforms to the https://github.com/elastic/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8443f5c178d6a98a75e13b5864a87fc8e87d8e1"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTI5OTg2OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTozMDo1MVrOIInvqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTozMDo1MVrOIInvqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkxMDY5OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ml-json-schemas[schema].\n          \n          \n            \n            https://github.com/elastic/ml-json-schemas[schema].", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r545910699", "createdAt": "2020-12-18T15:30:51Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,145 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import it to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. You can use `curl` to call the\n+{ref}/get-trained-models.html[get trained model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you want to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In this example, you are exporting the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[get trained models API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. If you use the {kib} Console, \n+browser might be unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These parameters are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* You must unnest the JSON object by one level to extract just the model\n+definition. You must also remove the existing model ID in order to not have\n+ID collisions when you import again. You can do these steps using `jq` inline or\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the {ref}/\n+create-trained-models.html[created trained model API]. When you specify the URL,\n+you can also set the model ID to something new using the last path part of the\n+URL.\n++\n+--\n [source, bash]\n --------------------------------------------------\n-curl -H 'Content-Type: application/json' -XPUT \"$ES_ADDRESS/_ml/inference/$MODEL\" -d@$MODEL.json\n+curl -u username:password \\\n+  -H 'Content-Type: application/json' \\\n+  -X PUT \"http://localhost:9200/_ml/trained_models/flights1-imported\" \\\n+  --data-binary @flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n+--\n+\n+[NOTE]\n+--\n+* Models exported from the {ref}/get-trained-models.html[get trained models API]\n+are limited in size by the\n+{ref}/modules-http.html#_http_settings[http.max_content_length]\n+global configuration value in Elasticsearch. The default value is `100mb` and\n+may need to be increased depending on the size of model being exported.\n+\n+* Connection timeouts can occur when either the source or destination\n+cluster is under load, or when model sizes are very large. Increasing\n+https://ec.haxx.se/usingcurl/usingcurl-timeouts[timeout configurations] for\n+`curl` (e.g. `curl --max-time 600`) or your client of choice will help\n+alleviate the problem. In rare cases you may need to reduce load on the\n+Elasticsearch cluster, for example by adding nodes.\n+--\n \n \n [discrete]\n-[[move-trained-model-to-es]]\n-== Moving a model to the {stack}\n+[[import-external-model-to-es]]\n+== Importing an external model to the {stack}\n \n-It is possible to add a model to your {es} cluster even if the model is not \n+It is possible to import a model to your {es} cluster even if the model is not\n trained by Elastic {dfanalytics}.\n \n-You can find an example of training a model, then adding it to {es} by using \n-eland \n-https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[in eland docs].\n-The example uses Python to train and move the model, however, you can use any \n-Client as long as the format of your trained model meets \n-https://github.com/elastic/ml-json-schemas[the required schema].\n-\n+You can find an example of training a model, then importing it to {es} with\n+`eland` in the\n+https://eland.readthedocs.io/en/latest/examples/introduction_to_eland_webinar.html#Machine-Learning-Demo[eland machine learning demo].\n+The example uses Python to train, save and import the model. However, you can\n+use any client as long as the model conforms to the https://github.com/elastic/\n+ml-json-schemas[schema].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8443f5c178d6a98a75e13b5864a87fc8e87d8e1"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTQwNjkxOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTo1Njo0MVrOIIouQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTo1Njo0MVrOIIouQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkyNjcyMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {ref}/create-trained-models.html[created trained model API]. When you specify the URL,\n          \n          \n            \n            {ref}/put-trained-models.html[created trained model API]. When you specify the URL,", "url": "https://github.com/elastic/stack-docs/pull/1498#discussion_r545926720", "createdAt": "2020-12-18T15:56:41Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,168 +13,145 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate \n-https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n-these trained models in the {stack}, you must store them in {es} documents by \n+adhere to the appropriate\n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use\n+these trained models in the {stack}, you must store them in {es} documents by\n using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data\n Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like \n+Alternatively, you can use APIs like\n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n \n \n [discrete]\n-[[move-between-clusters]]\n-== Moving a trained model between clusters\n-\n-It is a common scenario that the {ml} models are trained in a development or \n-test environment and then used in a production environment. In this case, you \n-need to move your trained model from one cluster to another. The trained model \n-APIs enable you to move your trained model between clusters. The following \n-description shows you the process step by step.\n-\n-1. (Optional) In the cluster where you trained the model, make the call below by \n-using the console in **Dev Tools** to get the configuration information of your \n-trained models.\n+[[export-import]]\n+== Exporting and importing models\n+\n+Models trained in Elasticsearch are portable and can be transferred between\n+clusters. This is particularly useful when models are trained in isolation from\n+the cluster where they are used for inference. The following instructions show\n+how to use https://curl.se/[`curl`] and https://stedolan.github.io/jq/[`jq`] to\n+export a model as JSON and import it to another cluster.\n+\n+1. Given a model _name_, find the model _ID_. You can use `curl` to call the\n+{ref}/get-trained-models.html[get trained model API] to list all models with\n+their IDs.\n +\n --\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq . -C \\\n+    | more\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n+// NOTCONSOLE\n \n-The API response contains the `model_id` of the trained models. Check the \n-`model_id` of the trained model you want to move, you need to add it to the API \n-call in the next step.\n---\n+If you want to show just the model IDs available, use `jq` to select a subset.\n \n-2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n-trained model definition. You need to specify the following query parameters in \n-the call:\n-+\n---\n-* `for_export`: This parameter allows the model to be in an acceptable format to \n-be retrieved and then added to another cluster. Set it to `true`.\n-\n-* `include`: Set this to `definition` for the API to include the definition in \n-the response.\n-\n-* `decompress_definition`: It specifies in what format the included model \n-definition should be returned. Set it to `false` for getting a custom compressed \n-format. It is also valid to use the JSON format, but it is not optimal. As the \n-decompressed definition may be significantly larger, it is recommended to use \n-the compressed format.\n-   \n-The following call is an example to get the trained model definition. (Replace \n-`<model_id>` with the actual ID of the trained model.)\n-\n-[source,console]\n+[source, bash]\n --------------------------------------------------\n-GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+curl -s -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models\" \\\n+    | jq -C -r '.trained_model_configs[].model_id'\n --------------------------------------------------\n-// TEST[skip:setup kibana sample data]\n-\n-The API response returns a `trained_model_configs` array that contains a \n-`compressed_definition` object and the analytics and inference configuration \n-information.\n---\n-\n-3. Copy the content of `trained_model_configs`.\n-\n-4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n-cluster you want to move the trained model to. Paste the content of the \n-`trained_model_configs` to the request body of the API call. The API response \n-contains the model information with metadata.\n-\n-Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n-in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n-\n-[NOTE]\n---\n-The trained model definition can be so large that it may take a long time for a \n-computer clipboard to copy and paste it. It is recommended to do it \n-programmatically, for example via a bash script or via Client code. You can find \n-examples below.\n---\n-\n-The following Python snippet exports the trained model that you reference to a \n-JSON file:\n+// NOTCONSOLE\n \n-[source, py]\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n-ml_client = MlClient(es_client)\n-result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n-compressed_df = result['trained_model_configs'][0]\n-with open('model_filename.json', 'w') as handle:\n-    handle.write(json.dumps(compressed_df))\n+flights1-1607953694065\n+flights0-1607953585123\n+lang_ident_model_1\n --------------------------------------------------\n // NOTCONSOLE\n \n+In this example, you are exporting the model with ID `flights1-1607953694065`.\n+--\n \n-The following Python snippet imports the model that stored in the JSON file to \n-a cluster:\n-\n-[source, py]\n+2. Using `curl` from the command line, again use the\n+{ref}/get-trained-models.html[get trained models API] to export the entire model\n+definition and save it to a JSON file.\n++\n+--\n+[source, bash]\n --------------------------------------------------\n-import json\n-from elasticsearch import Elasticsearch\n-from elasticsearch.client.ml import MlClient\n-es_client = Elasticsearch(args.es, http_auth=(username, password), use_ssl=True, timeout=60)\n-ml_client = MlClient(es_client)\n-with open(filename, 'r') as handle:\n-  compressed_model = json.loads(handle.read())\n-for field in ('version', 'create_time', 'estimated_heap_memory_usage_bytes', 'estimated_operations', 'license_level', 'id','created_by'):\n-  if field in compressed_model:\n-    del compressed_model[field]\n-ml_client.put_trained_model(model_id=model_id, body=compressed_model)\n+curl -u username:password \\\n+  -X GET \"http://localhost:9200/_ml/trained_models/flights1-1607953694065?exclude_generated=true&include=definition&decompress_definition=false\" \\\n+    | jq '.trained_model_configs[0] | del(.model_id)' \\\n+    > flights1.json\n --------------------------------------------------\n // NOTCONSOLE\n \n+A few observations:\n \n-You can achieve the same by running a bash script. Populate the environment \n-variables:\n-\n-`ES_ADDRESS=\"https://username:password@elasticsearch-address\"`\n+* Exporting models requires using `curl` or a similar tool that can *stream*\n+the model over HTTP into a file. If you use the {kib} Console, the\n+browser might be unresponsive due to the size of exported models.\n \n-`MODEL=\"my_model_name\"`\n+* Note the query parameters that are used during export. These parameters are necessary to\n+export the model in a way that it can later be imported again and used for\n+inference.\n \n+* You must unnest the JSON object by one level to extract just the model\n+definition. You must also remove the existing model ID in order to not have\n+ID collisions when you import again. You can do these steps using `jq` inline or\n+alternatively it can be done to the resulting JSON file after downloading using\n+`jq` or other tools.\n+--\n \n-Then run the script:\n-\n+3. Import the saved model using `curl` to upload the JSON file to the\n+{ref}/create-trained-models.html[created trained model API]. When you specify the URL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "06a63d3d3301adf3b64fd2532e64677a473c56ed"}, "originalPosition": 203}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1348, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}