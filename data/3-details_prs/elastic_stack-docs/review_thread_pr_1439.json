{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2Njc2OTk0", "number": 1439, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjozMTo1NlrOE2J5TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMjoyMlrOFA0p_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjIwNjg0OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjozMTo1NlrOHu1BFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjozMTo1NlrOHu1BFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg2NTE3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            GET _ml/inference/\n          \n          \n            \n            GET _ml/inference/\n          \n      \n    \n    \n  \n\nI think this is deprecated and we should use the trained models API instead: https://www.elastic.co/guide/en/elasticsearch/reference/master/get-trained-models.html", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r518865174", "createdAt": "2020-11-06T16:31:56Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjIzODY4OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjo0MDoyNFrOHu1U6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwODoyMzoxM1rOHxC8Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg3MDI0OA==", "bodyText": "Looking at this example, it made me wonder which of these options were required to be the same as in the original cluster (other than compressed_definition).  Do they all have to be identical?  If not, I'm not sure if there's a lot of value to having this example and wonder if it should be removed, since the only option of importance is not shown fully anyway.", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r518870248", "createdAt": "2020-11-06T16:40:24Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-inference.html[GET trained model API] to get the trained \n+model definition. You need to specify the following query parameters in the \n+call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-inference.html[Create trained model API] in the cluster you \n+want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call.\n++\n+--\n+The following call is an example to create the trained model. The actual \n+`compressed_definition` value is replaced by a place holder.\n+\n+.Create trained model API example", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg4ODU0MQ==", "bodyText": "@lcawl AFAIK, you can change the metadata in the PUT body, but it does not affect your model. However, the model configuration data will be inaccurate in this case as the model hasn't been trained with that configuration. The compressed_definition is a really long (in this example it's almost 50.000 character-long) series of characters and it is not human-readable. This is why I would omit it. I'll ask Ben about his opinion when I open the PR for review.", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r518888541", "createdAt": "2020-11-06T17:11:42Z", "author": {"login": "szabosteve"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-inference.html[GET trained model API] to get the trained \n+model definition. You need to specify the following query parameters in the \n+call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-inference.html[Create trained model API] in the cluster you \n+want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call.\n++\n+--\n+The following call is an example to create the trained model. The actual \n+`compressed_definition` value is replaced by a place holder.\n+\n+.Create trained model API example", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg3MDI0OA=="}, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3OTc2NQ==", "bodyText": "I agree with not fully expanding the compressed_definition, but it might be useful to elide the rest of the config for brevity.", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r520779765", "createdAt": "2020-11-10T18:30:49Z", "author": {"login": "benwtrent"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-inference.html[GET trained model API] to get the trained \n+model definition. You need to specify the following query parameters in the \n+call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-inference.html[Create trained model API] in the cluster you \n+want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call.\n++\n+--\n+The following call is an example to create the trained model. The actual \n+`compressed_definition` value is replaced by a place holder.\n+\n+.Create trained model API example", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg3MDI0OA=="}, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTE5MDQwMg==", "bodyText": "I removed the API example in 106e319.", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r521190402", "createdAt": "2020-11-11T08:23:13Z", "author": {"login": "szabosteve"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-inference.html[GET trained model API] to get the trained \n+model definition. You need to specify the following query parameters in the \n+call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-inference.html[Create trained model API] in the cluster you \n+want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call.\n++\n+--\n+The following call is an example to create the trained model. The actual \n+`compressed_definition` value is replaced by a place holder.\n+\n+.Create trained model API example", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg3MDI0OA=="}, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjI0OTAxOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjo0MzoxMlrOHu1bVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjo0MzoxMlrOHu1bVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg3MTg5NQ==", "bodyText": "Is there reason this is specifically mentioning Elasticsearch? It seems like the section is about moving it into Elastic or the Stack in general:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            === Moving a model to {es}\n          \n          \n            \n            === Moving a model to the {stack}", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r518871895", "createdAt": "2020-11-06T16:43:12Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-inference.html[GET trained model API] to get the trained \n+model definition. You need to specify the following query parameters in the \n+call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-inference.html[Create trained model API] in the cluster you \n+want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call.\n++\n+--\n+The following call is an example to create the trained model. The actual \n+`compressed_definition` value is replaced by a place holder.\n+\n+.Create trained model API example\n+[%collapsible]\n+====\n+[source,console]\n+--------------------------------------------------\n+PUT _ml/inference/<my_model_id>\n+{\n+   \"compressed_definition\":\"<definition value of the trained model>\",\n+   \"tags\":[\n+      \"reg-trained-model\"\n+   ],\n+   \"metadata\":{\n+      \"analytics_config\":{\n+         \"max_num_threads\":1,\n+         \"model_memory_limit\":\"25mb\",\n+         \"create_time\":1604579862340,\n+         \"allow_lazy_start\":false,\n+         \"description\":\"\",\n+         \"analyzed_fields\":{\n+            \"excludes\":[\n+            ],\n+            \"includes\":[\n+               \"AvgTicketPrice\",\n+               \"Carrier\",\n+               \"Dest\",\n+               \"DestCityName\",\n+               \"DestCountry\",\n+               \"DestWeather\",\n+               \"DistanceKilometers\",\n+               \"DistanceMiles\",\n+               \"FlightDelay\",\n+               \"FlightDelayMin\",\n+               \"FlightTimeHour\",\n+               \"FlightTimeMin\",\n+               \"Origin\",\n+               \"OriginCityName\",\n+               \"OriginCountry\",\n+               \"OriginWeather\",\n+               \"dayOfWeek\"\n+            ]\n+         },\n+         \"id\":\"reg-trained-model\",\n+         \"source\":{\n+            \"query\":{\n+               \"match_all\":{\n+               }\n+            },\n+            \"index\":[\n+               \"kibana_sample_data_flights\"\n+            ]\n+         },\n+         \"dest\":{\n+            \"index\":\"reg-trained-model-ind\",\n+            \"results_field\":\"ml\"\n+         },\n+         \"analysis\":{\n+            \"regression\":{\n+               \"randomize_seed\":-5746203410061298773,\n+               \"dependent_variable\":\"FlightDelayMin\",\n+               \"training_percent\":10.0,\n+               \"loss_function\":\"mse\",\n+               \"num_top_feature_importance_values\":0,\n+               \"prediction_field_name\":\"FlightDelayMin_prediction\"\n+            }\n+         },\n+         \"version\":\"7.9.0\"\n+      }\n+   },\n+   \"input\":{\n+      \"field_names\":[\n+        \"AvgTicketPrice\",\n+        \"Carrier\",\n+        \"Dest\",\n+        \"DestCityName\",\n+        \"DestCountry\",\n+        \"DestWeather\",\n+        \"DistanceKilometers\",\n+        \"DistanceMiles\",\n+        \"FlightDelay\",\n+        \"FlightDelayMin\",\n+        \"FlightTimeHour\",\n+        \"FlightTimeMin\",\n+        \"Origin\",\n+        \"OriginCityName\",\n+        \"OriginCountry\",\n+        \"OriginWeather\",\n+        \"dayOfWeek\"\n+      ]\n+   },\n+   \"inference_config\":{\n+      \"regression\":{\n+         \"results_field\":\"FlightDelayMin_prediction\",\n+         \"num_top_feature_importance_values\":0\n+      }\n+   }\n+}\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+====\n+\n+The API response contains the model information with metadata.\n+--\n+\n+Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n+in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n+\n+[NOTE]\n+--\n+The trained model definition can be so large that it may take a long time for a \n+computer clipboard to copy and paste it. It is recommended to do it \n+programmatically, for example via a bash script or via Client code. You can find \n+examples below.\n+--\n+\n+// Bash and Python examples\n+\n+[discrete]\n+[[move-trained-model-to-es]]\n+=== Moving a model to {es}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjI1NjUzOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjo0NDo1M1rOHu1fng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjo0NDo1M1rOHu1fng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg3Mjk5MA==", "bodyText": "If I'm understanding correctly and this is about moving a model generated outside Elastic into Elastic, I think it might be clearer by adding this qualification:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            trained by {dfanalytics}.\n          \n          \n            \n            trained by Elastic {dfanalytics}.", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r518872990", "createdAt": "2020-11-06T16:44:53Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,223 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-inference.html[GET trained model API] to get the trained \n+model definition. You need to specify the following query parameters in the \n+call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/inference/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-inference.html[Create trained model API] in the cluster you \n+want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call.\n++\n+--\n+The following call is an example to create the trained model. The actual \n+`compressed_definition` value is replaced by a place holder.\n+\n+.Create trained model API example\n+[%collapsible]\n+====\n+[source,console]\n+--------------------------------------------------\n+PUT _ml/inference/<my_model_id>\n+{\n+   \"compressed_definition\":\"<definition value of the trained model>\",\n+   \"tags\":[\n+      \"reg-trained-model\"\n+   ],\n+   \"metadata\":{\n+      \"analytics_config\":{\n+         \"max_num_threads\":1,\n+         \"model_memory_limit\":\"25mb\",\n+         \"create_time\":1604579862340,\n+         \"allow_lazy_start\":false,\n+         \"description\":\"\",\n+         \"analyzed_fields\":{\n+            \"excludes\":[\n+            ],\n+            \"includes\":[\n+               \"AvgTicketPrice\",\n+               \"Carrier\",\n+               \"Dest\",\n+               \"DestCityName\",\n+               \"DestCountry\",\n+               \"DestWeather\",\n+               \"DistanceKilometers\",\n+               \"DistanceMiles\",\n+               \"FlightDelay\",\n+               \"FlightDelayMin\",\n+               \"FlightTimeHour\",\n+               \"FlightTimeMin\",\n+               \"Origin\",\n+               \"OriginCityName\",\n+               \"OriginCountry\",\n+               \"OriginWeather\",\n+               \"dayOfWeek\"\n+            ]\n+         },\n+         \"id\":\"reg-trained-model\",\n+         \"source\":{\n+            \"query\":{\n+               \"match_all\":{\n+               }\n+            },\n+            \"index\":[\n+               \"kibana_sample_data_flights\"\n+            ]\n+         },\n+         \"dest\":{\n+            \"index\":\"reg-trained-model-ind\",\n+            \"results_field\":\"ml\"\n+         },\n+         \"analysis\":{\n+            \"regression\":{\n+               \"randomize_seed\":-5746203410061298773,\n+               \"dependent_variable\":\"FlightDelayMin\",\n+               \"training_percent\":10.0,\n+               \"loss_function\":\"mse\",\n+               \"num_top_feature_importance_values\":0,\n+               \"prediction_field_name\":\"FlightDelayMin_prediction\"\n+            }\n+         },\n+         \"version\":\"7.9.0\"\n+      }\n+   },\n+   \"input\":{\n+      \"field_names\":[\n+        \"AvgTicketPrice\",\n+        \"Carrier\",\n+        \"Dest\",\n+        \"DestCityName\",\n+        \"DestCountry\",\n+        \"DestWeather\",\n+        \"DistanceKilometers\",\n+        \"DistanceMiles\",\n+        \"FlightDelay\",\n+        \"FlightDelayMin\",\n+        \"FlightTimeHour\",\n+        \"FlightTimeMin\",\n+        \"Origin\",\n+        \"OriginCityName\",\n+        \"OriginCountry\",\n+        \"OriginWeather\",\n+        \"dayOfWeek\"\n+      ]\n+   },\n+   \"inference_config\":{\n+      \"regression\":{\n+         \"results_field\":\"FlightDelayMin_prediction\",\n+         \"num_top_feature_importance_values\":0\n+      }\n+   }\n+}\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+====\n+\n+The API response contains the model information with metadata.\n+--\n+\n+Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n+in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n+\n+[NOTE]\n+--\n+The trained model definition can be so large that it may take a long time for a \n+computer clipboard to copy and paste it. It is recommended to do it \n+programmatically, for example via a bash script or via Client code. You can find \n+examples below.\n+--\n+\n+// Bash and Python examples\n+\n+[discrete]\n+[[move-trained-model-to-es]]\n+=== Moving a model to {es}\n+\n+It is possible to add a model to your {es} cluster even if the model is not \n+trained by {dfanalytics}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6bf815e07b8d9e8b08bbc0fd9e0622b6169b47f"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDc4ODYwOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoyNTowMlrOHwprBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoyNTowMlrOHwprBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3NjQ1NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It is a common scenario that the {ml} models are trained in a cluster that is \n          \n          \n            \n            used for development and test purposes, and they are used in another cluster \n          \n          \n            \n            which is the production environment. In this case, you need to move your trained \n          \n          \n            \n            It is a common scenario that the {ml} models are trained in a development\n          \n          \n            \n            or test environment and then used in a production environment. \n          \n          \n            \n            In this case, you need to move your trained", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r520776455", "createdAt": "2020-11-10T18:25:02Z", "author": {"login": "benwtrent"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,270 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e695d34b21ccbbf4792ae23063201394ec23190d"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDc5NTM5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoyNjo0N1rOHwpvBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoyNjo0N1rOHwpvBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3NzQ3OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * `include_model_definition`: It specifies whether the model definition is \n          \n          \n            \n            returned in the response. Set it to `true`.\n          \n          \n            \n            * `include`: Set this to `definition` for the API to include the definition in the response.\n          \n      \n    \n    \n  \n\ninclude_model_definition is deprecated.", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r520777479", "createdAt": "2020-11-10T18:26:47Z", "author": {"login": "benwtrent"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,270 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n+trained model definition. You need to specify the following query parameters in \n+the call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e695d34b21ccbbf4792ae23063201394ec23190d"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDc5ODYxOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoyNzozOVrOHwpxCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoyNzozOVrOHwpxCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3Nzk5NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            GET _ml/trained_models/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false\n          \n          \n            \n            GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r520777995", "createdAt": "2020-11-10T18:27:39Z", "author": {"login": "benwtrent"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,270 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a cluster that is \n+used for development and test purposes, and they are used in another cluster \n+which is the production environment. In this case, you need to move your trained \n+model from one cluster to another. The trained model APIs enable you to move \n+your trained model between clusters. The following description shows you the \n+process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n+trained model definition. You need to specify the following query parameters in \n+the call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include_model_definition`: It specifies whether the model definition is \n+returned in the response. Set it to `true`.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/<model_id>?for_export=true&include_model_definition=true&decompress_definition=false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e695d34b21ccbbf4792ae23063201394ec23190d"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2NDA1MzYyOnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoxODo0OFrOH_S3Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoxODo0OFrOH_S3Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMTMzNQ==", "bodyText": "Hi @szabosteve ! Can we add these two lines to the snippet\nimport json\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.client.ml import MlClient", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r536131335", "createdAt": "2020-12-04T14:18:48Z", "author": {"login": "Winterflower"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,163 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a development or \n+test environment and then used in a production environment. In this case, you \n+need to move your trained model from one cluster to another. The trained model \n+APIs enable you to move your trained model between clusters. The following \n+description shows you the process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n+trained model definition. You need to specify the following query parameters in \n+the call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include`: Set this to `definition` for the API to include the definition in \n+the response.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n+cluster you want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call. The API response \n+contains the model information with metadata.\n+\n+Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n+in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n+\n+[NOTE]\n+--\n+The trained model definition can be so large that it may take a long time for a \n+computer clipboard to copy and paste it. It is recommended to do it \n+programmatically, for example via a bash script or via Client code. You can find \n+examples below.\n+--\n+\n+The following Python snippet exports the trained model that you reference to a \n+JSON file:\n+\n+[source, py]\n+--------------------------------------------------", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcfc374977db9927183d1d8a26fb250457db279f"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2NDA3MDM3OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMjoyMlrOH_TAog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNDoyMjoyMlrOH_TAog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEzMzc5NA==", "bodyText": "Can we add these two lines to the top\nimport json\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.client.ml import MlClient", "url": "https://github.com/elastic/stack-docs/pull/1439#discussion_r536133794", "createdAt": "2020-12-04T14:22:22Z", "author": {"login": "Winterflower"}, "path": "docs/en/stack/ml/df-analytics/ml-trained-models.asciidoc", "diffHunk": "@@ -13,16 +13,163 @@ information about this process, see <<ml-supervised-workflow>> and\n <<ml-inference>>.\n \n You can also supply trained models that are not created by {dfanalytics-job} but\n-adhere to the appropriate https://github.com/elastic/ml-json-schemas[JSON schema].\n-If you want to use these trained models in the {stack}, you must store them in\n-{es} documents by using the {ref}/put-trained-models.html[create trained models API].\n+adhere to the appropriate \n+https://github.com/elastic/ml-json-schemas[JSON schema]. If you want to use \n+these trained models in the {stack}, you must store them in {es} documents by \n+using the {ref}/put-trained-models.html[create trained models API].\n \n-In {kib}, you can view and manage your trained models within\n-*{ml-app}* > *Data Frame Analytics*:\n+In {kib}, you can view and manage your trained models within *{ml-app}* > *Data \n+Frame Analytics*:\n \n [role=\"screenshot\"]\n image::images/trained-model-management.png[\"List of trained models in the {ml-app} app in {kib}\"]\n \n-Alternatively, you can use APIs like\n+Alternatively, you can use APIs like \n {ref}/get-trained-models.html[get trained models] and\n {ref}/delete-trained-models.html[delete trained models].\n+\n+\n+[discrete]\n+[[move-between-clusters]]\n+== Moving a trained model between clusters\n+\n+It is a common scenario that the {ml} models are trained in a development or \n+test environment and then used in a production environment. In this case, you \n+need to move your trained model from one cluster to another. The trained model \n+APIs enable you to move your trained model between clusters. The following \n+description shows you the process step by step.\n+\n+1. (Optional) In the cluster where you trained the model, make the call below by \n+using the console in **Dev Tools** to get the configuration information of your \n+trained models.\n++\n+--\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response contains the `model_id` of the trained models. Check the \n+`model_id` of the trained model you want to move, you need to add it to the API \n+call in the next step.\n+--\n+\n+2. Use the {ref}/get-trained-models.html[GET trained model API] to get the \n+trained model definition. You need to specify the following query parameters in \n+the call:\n++\n+--\n+* `for_export`: This parameter allows the model to be in an acceptable format to \n+be retrieved and then added to another cluster. Set it to `true`.\n+\n+* `include`: Set this to `definition` for the API to include the definition in \n+the response.\n+\n+* `decompress_definition`: It specifies in what format the included model \n+definition should be returned. Set it to `false` for getting a custom compressed \n+format. It is also valid to use the JSON format, but it is not optimal. As the \n+decompressed definition may be significantly larger, it is recommended to use \n+the compressed format.\n+   \n+The following call is an example to get the trained model definition. (Replace \n+`<model_id>` with the actual ID of the trained model.)\n+\n+[source,console]\n+--------------------------------------------------\n+GET _ml/trained_models/<model_id>?for_export=true&include=definition&decompress_definition=false\n+--------------------------------------------------\n+// TEST[skip:setup kibana sample data]\n+\n+The API response returns a `trained_model_configs` array that contains a \n+`compressed_definition` object and the analytics and inference configuration \n+information.\n+--\n+\n+3. Copy the content of `trained_model_configs`.\n+\n+4. Use the {ref}/put-trained-models.html[Create trained model API] in the \n+cluster you want to move the trained model to. Paste the content of the \n+`trained_model_configs` to the request body of the API call. The API response \n+contains the model information with metadata.\n+\n+Your trained model is ready to be used as a <<ml-inference-processor,processor>> \n+in an ingest pipeline or as an <<ml-inference-aggregation,aggregation>>.\n+\n+[NOTE]\n+--\n+The trained model definition can be so large that it may take a long time for a \n+computer clipboard to copy and paste it. It is recommended to do it \n+programmatically, for example via a bash script or via Client code. You can find \n+examples below.\n+--\n+\n+The following Python snippet exports the trained model that you reference to a \n+JSON file:\n+\n+[source, py]\n+--------------------------------------------------\n+es_client = Elasticsearch('URL to your ES instance', http_auth=(username, password), use_ssl=True)\n+ml_client = MlClient(es_client)\n+result = ml_client.get_trained_models(model_id='your-model-id', decompress_definition=False, include=definition)\n+compressed_df = result['trained_model_configs'][0]\n+with open('model_filename.json', 'w') as handle:\n+    handle.write(json.dumps(compressed_df))\n+--------------------------------------------------\n+// NOTCONSOLE\n+\n+\n+The following Python snippet imports the model that stored in the JSON file to \n+a cluster:\n+\n+[source, py]\n+--------------------------------------------------", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcfc374977db9927183d1d8a26fb250457db279f"}, "originalPosition": 121}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1320, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}