{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4MTU3ODMz", "number": 1268, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMDo1MDoxN1rOEODrnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMDo1MDoxN1rOEODrnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTc1ODM5OnYy", "diffSide": "RIGHT", "path": "docs/en/stack/ml/df-analytics/ml-inference.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMDo1MDoxN1rOGw_7TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMDo1MDoxN1rOGw_7TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzMjIwNA==", "bodyText": "This aggregation enables you to run ... set up a processor in the ingest pipeline.\n\nThis sentence felt long to me.  Not a mandatory change, but you might consider breaking it up. For example:\n\nThis aggregation ... at search time. If you want to perform .. small set of data, this aggregation enables you to generate predictions without the need to set up processors and pipelines.", "url": "https://github.com/elastic/stack-docs/pull/1268#discussion_r454032204", "createdAt": "2020-07-14T00:50:17Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/ml-inference.asciidoc", "diffHunk": "@@ -33,17 +34,35 @@ Alternatively, you can use a pre-trained language identification model to\n determine the language of text. {lang-ident-cap} supports 109 languages. For \n more information and configuration details, check the <<ml-lang-ident>> page.\n \n+\n [[ml-inference-processor]]\n === {infer-cap} processor\n \n-{infer-cap} is a processor specified in an {ref}/pipeline.html[ingest pipeline]. \n-It uses a stored {dfanalytics} model to infer against the data that is being \n-ingested in the pipeline. The model is used on the \n-{ref}/ingest.html[ingest node]. {infer-cap} pre-processes the data by using the \n-model and provides a prediction. After the process, the pipeline continues \n-executing (if there is any other processor in the pipeline), finally the new \n-data together with the results are indexed into the destination index.\n+{infer-cap} can be used as a processor specified in an \n+{ref}/pipeline.html[ingest pipeline]. It uses a stored {dfanalytics} model to \n+infer against the data that is being ingested in the pipeline. The model is used \n+on the {ref}/ingest.html[ingest node]. {infer-cap} pre-processes the data by \n+using the model and provides a prediction. After the process, the pipeline \n+continues executing (if there is any other processor in the pipeline), finally \n+the new data together with the results are indexed into the destination index.\n \n Check the {ref}/inference-processor.html[{infer} processor] and \n {ref}/ml-df-analytics-apis.html[the {ml} {dfanalytics} API documentation] to \n learn more about the feature.\n+\n+\n+[[ml-inference-aggregation]]\n+=== {infer-cap} aggregation\n+\n+{infer-cap} can also be used as a pipeline aggregation. You can reference a \n+pre-trained {dfanalytics} model in the aggregation to infer on the result field \n+of the parent bucket aggregation. The {infer} aggregation uses the model on the \n+results to provide a prediction. This aggregation enables you to run ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b7f240935aa1cacb534888b02ea5706721dceca"}, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1418, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}