{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE3MzE4NDk0", "number": 1062, "title": "[DOCS] Adds loss function description to regression conceptual docs", "bodyText": "Overview\nThis PR expands the regression conceptual documentation with a high-level explanation of loss functions.\nPreview\nRegression", "createdAt": "2020-05-13T12:08:54Z", "url": "https://github.com/elastic/stack-docs/pull/1062", "merged": true, "mergeCommit": {"oid": "c5c550d1af8634059b3fe49c9e5cff6c0f368838"}, "closed": true, "closedAt": "2020-05-28T06:52:12Z", "author": {"login": "szabosteve"}, "timelineItems": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcg3sWogH2gAyNDE3MzE4NDk0OjQzMjgyNDg0YmNhYzQyMjYxZGM3OTViOTM3OGNhNWQyYzNiNDVhMjg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABclUokzAH2gAyNDE3MzE4NDk0OjQ1OTY5N2Y4ZDMwZWY1YzE1NmMwOTk4ODZmNWFiMWM5NWYyN2EzMjI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "43282484bcac42261dc795b9378ca5d2c3b45a28", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/43282484bcac42261dc795b9378ca5d2c3b45a28", "committedDate": "2020-05-13T12:04:53Z", "message": "[DOCS] Adds loss function description to regression conceptual docs."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d57fc3a0e2f8b1dc9b0959150f84bc591cc5b325", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/d57fc3a0e2f8b1dc9b0959150f84bc591cc5b325", "committedDate": "2020-05-13T12:11:57Z", "message": "[DOCS] Changes section title."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50977bdd235e07b67c5effdb49cd628fd9aa586e", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/50977bdd235e07b67c5effdb49cd628fd9aa586e", "committedDate": "2020-05-13T12:26:52Z", "message": "[DOCS] Some editing."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb04e2f79bb7d96e7ff232d7b2de5948071c284d", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/bb04e2f79bb7d96e7ff232d7b2de5948071c284d", "committedDate": "2020-05-14T07:24:50Z", "message": "[DOCS] Removes [discrete] attributes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d07298cc064e9e3ecd0f7b396ebb0c2527779d98", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/d07298cc064e9e3ecd0f7b396ebb0c2527779d98", "committedDate": "2020-05-14T07:36:19Z", "message": "[DOCS] Moves loss function section under training the regression model section."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/40fd0fad5d2129e53917ff886d9fc9d38c3d23d4", "committedDate": "2020-05-15T09:26:37Z", "message": "[DOCS] Refines description."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3Njc1MTYy", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-417675162", "createdAt": "2020-05-25T12:23:39Z", "commit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMjoyMzozOVrOGZ_bwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxMjo0MjowMFrOGZ_5mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwNjg4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Loss function is for measuring how well a given {ml} model fits the specific \n          \n          \n            \n            data set. The loss function boils down all the different aspects of the model to \n          \n          \n            \n            a single number. The bigger the difference between the prediction and the ground \n          \n          \n            \n            Loss function measures how well a given {ml} model fits the specific \n          \n          \n            \n            data set. The loss function boils down all the different under- and overestimations of the model to \n          \n          \n            \n            a single number. The bigger the difference between the prediction and the ground", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429906882", "createdAt": "2020-05-25T12:23:39Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwNzU4Ng==", "bodyText": "Or something along those lines. Strictly speaking, loss function doesn't care about the properties/aspects of the model, it only takes the predictions and the ground truth into account.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429907586", "createdAt": "2020-05-25T12:25:16Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwNjg4Mg=="}, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwOTA1NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The various types of loss function calculate the model error differently. It \n          \n          \n            \n            The various types of loss function calculate the prediction error differently. It", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429909055", "createdAt": "2020-05-25T12:28:20Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground \n+truth, the higher the value of the loss function. Loss functions are used \n+automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]\n+* mean squared logarithmic error (`msle`; a variation of `mse`)\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]\n+\n+The various types of loss function calculate the model error differently. It ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkwOTIzNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the value distribution in your data set, the problem that you want to model, the \n          \n          \n            \n            the target distribution in your data set, the problem that you want to model, the", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429909237", "createdAt": "2020-05-25T12:28:46Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground \n+truth, the higher the value of the loss function. Loss functions are used \n+automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]\n+* mean squared logarithmic error (`msle`; a variation of `mse`)\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]\n+\n+The various types of loss function calculate the model error differently. It \n+means that which loss function is appropriate for your case highly depends on \n+the value distribution in your data set, the problem that you want to model, the ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkxMDkxOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            TIP: Unless you fully understand the purpose of the different loss functions, it \n          \n          \n            \n            is highly recommended that you leave it unset and use the default parameters.\n          \n          \n            \n            TIP: The default loss function parameter values work fine for most of the cases, so unless you fully understand the impact of the different loss function parameters, it is highly recommended that you use the default values.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429910919", "createdAt": "2020-05-25T12:32:54Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground \n+truth, the higher the value of the loss function. Loss functions are used \n+automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]\n+* mean squared logarithmic error (`msle`; a variation of `mse`)\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]\n+\n+The various types of loss function calculate the model error differently. It \n+means that which loss function is appropriate for your case highly depends on \n+the value distribution in your data set, the problem that you want to model, the \n+number of outliers in the data, and so on.\n+\n+You can determine which loss function to be used during {reganalysis} though the \n+{ref}/put-dfanalytics.html[create {dfanalytics-jobs} API]. The default is mean \n+squared error (`mse`). If you choose `msle` or `huber`, you can also set up a \n+parameter for the loss function which is `t` (defaults to `1.0`) for `msle` and \n+`\u1e9f` (defaults to `1.0`) for `huber`. With these parameters, you can further \n+refine the behavior of the chosen functions. There is no parameter to manually \n+set up for `mse`.\n+\n+TIP: Unless you fully understand the purpose of the different loss functions, it \n+is highly recommended that you leave it unset and use the default parameters.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkxMTE4MQ==", "bodyText": "Can we add formulas here? Otherwise, it is unclear what t and delta mean.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429911181", "createdAt": "2020-05-25T12:33:36Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground \n+truth, the higher the value of the loss function. Loss functions are used \n+automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]\n+* mean squared logarithmic error (`msle`; a variation of `mse`)\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkxNDUyMA==", "bodyText": "I would also like to add a sentence to the purpose of each loss function. Something like\nMSE: The default choice if no additional information about the data set is available.\nMSLE: If you know that the target values are all positive with a long tail distribution (e.g. prices, population, etc.)\nHuber: If you want to prevent that the model tries to fit the outliers at the cost of fitting regular data.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r429914520", "createdAt": "2020-05-25T12:42:00Z", "author": {"login": "valeriy42"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,41 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function is for measuring how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different aspects of the model to \n+a single number. The bigger the difference between the prediction and the ground \n+truth, the higher the value of the loss function. Loss functions are used \n+automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]\n+* mean squared logarithmic error (`msle`; a variation of `mse`)\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTkxMTE4MQ=="}, "originalCommit": {"oid": "40fd0fad5d2129e53917ff886d9fc9d38c3d23d4"}, "originalPosition": 45}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c5a6a751a1b0474bc99093fa6ed56d51b0c1bee", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/8c5a6a751a1b0474bc99093fa6ed56d51b0c1bee", "committedDate": "2020-05-25T13:42:54Z", "message": "Apply suggestions from code review\n\nCo-authored-by: Valeriy Khakhutskyy <1292899+valeriy42@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39176d3748315e1c325c0eed0cdfc8bce45907d3", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/39176d3748315e1c325c0eed0cdfc8bce45907d3", "committedDate": "2020-05-25T13:58:02Z", "message": "[DOCS] Addresses feedback."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e938363fec3e9f666f744d73c191818b95f27be", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/6e938363fec3e9f666f744d73c191818b95f27be", "committedDate": "2020-05-25T14:02:01Z", "message": "[DOCS] Modifies description."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95feca66854182fb1861801c2debfe3acc9c1c7f", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/95feca66854182fb1861801c2debfe3acc9c1c7f", "committedDate": "2020-05-25T14:05:41Z", "message": "[DOCS] Fixes line breaks."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "577fa9a6522a60fd3137530666b775aa10386a13", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/577fa9a6522a60fd3137530666b775aa10386a13", "committedDate": "2020-05-25T14:09:34Z", "message": "[DOCS] Fixes line breaks."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/44abff38e2ffb4998adedcc6e148a77aa44bcc48", "committedDate": "2020-05-25T14:30:40Z", "message": "[DOCS] Addresses feedback."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3NzQ5ODIy", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-417749822", "createdAt": "2020-05-25T14:33:28Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDE5MjY1", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418419265", "createdAt": "2020-05-26T15:31:43Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozMTo0M1rOGaj5dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozMTo0M1rOGaj5dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUwNDMwOA==", "bodyText": "I think the noun needs to match the verb here. For example:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Loss function measures how well a given {ml} model fits the specific \n          \n          \n            \n            A loss function measures how well a given {ml} model fits the specific", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430504308", "createdAt": "2020-05-26T15:31:43Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDIwNDU2", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418420456", "createdAt": "2020-05-26T15:32:59Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozMjo1OVrOGaj85g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozMjo1OVrOGaj85g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUwNTE5MA==", "bodyText": "Since the previous sentence started with the same phrase, I think you can shorten this one:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            data set. The loss function boils down all the different under- and \n          \n          \n            \n            data set. It boils down all the different under- and", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430505190", "createdAt": "2020-05-26T15:32:59Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDIyNzY2", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418422766", "createdAt": "2020-05-26T15:34:45Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozNDo0NVrOGakCBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozNDo0NVrOGakCBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUwNjUwMg==", "bodyText": "This sentence reads quite long, so you might consider shortening it like this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            trees to compare the various iterations of the model and decide which iteration \n          \n          \n            \n            trees to compare the performance of various iterations of the model.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430506502", "createdAt": "2020-05-26T15:34:45Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDI0MDA5", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418424009", "createdAt": "2020-05-26T15:35:30Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozNTozMFrOGakEFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozNTozMFrOGakEFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUwNzAyOQ==", "bodyText": "If you accept the suggestion for the first half of this sentence, then this bit is no longer required:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            performs better.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430507029", "createdAt": "2020-05-26T15:35:30Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDI3NTI2", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418427526", "createdAt": "2020-05-26T15:38:43Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozODo0M1rOGakRew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTozODo0M1rOGakRew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUxMDQ1OQ==", "bodyText": "I don't think it's necessary to repeat \"fit\" here:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            fit regular data.\n          \n          \n            \n            regular data.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430510459", "createdAt": "2020-05-26T15:38:43Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: \n+It is the default choice when no additional information about the data set is \n+available.\n+* mean squared logarithmic error (`msle`; a variation of `mse`): It is for \n+cases where the target values are all positive with a long tail distribution \n+(for example, prices or population).\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:\n+Use it when you want to prevent the model trying to fit the outliers instead of \n+fit regular data.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDMwMzQ4", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418430348", "createdAt": "2020-05-26T15:41:43Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0MTo0M1rOGakdrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0MTo0M1rOGakdrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUxMzU4MA==", "bodyText": "... calculate the prediction error differently...\n\nThis is the first time we've mentioned the term \"prediction error\". If this is the appropriate term for the number generated by loss functions, I think it should be introduced earlier. For example in the sentence \"The loss function boils down ... to a single number, known as the prediction error\".", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430513580", "createdAt": "2020-05-26T15:41:43Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: \n+It is the default choice when no additional information about the data set is \n+available.\n+* mean squared logarithmic error (`msle`; a variation of `mse`): It is for \n+cases where the target values are all positive with a long tail distribution \n+(for example, prices or population).\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:\n+Use it when you want to prevent the model trying to fit the outliers instead of \n+fit regular data.\n+\n+The various types of loss function calculate the prediction error differently. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDMyNjMy", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418432632", "createdAt": "2020-05-26T15:44:13Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0NDoxNFrOGakkxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0NDoxNFrOGakkxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUxNTM5OQ==", "bodyText": "IMO this can be shortened:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            It means that which loss function is appropriate for your case highly depends on \n          \n          \n            \n            The appropriate loss function for your use case depends on", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430515399", "createdAt": "2020-05-26T15:44:14Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: \n+It is the default choice when no additional information about the data set is \n+available.\n+* mean squared logarithmic error (`msle`; a variation of `mse`): It is for \n+cases where the target values are all positive with a long tail distribution \n+(for example, prices or population).\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:\n+Use it when you want to prevent the model trying to fit the outliers instead of \n+fit regular data.\n+\n+The various types of loss function calculate the prediction error differently. \n+It means that which loss function is appropriate for your case highly depends on ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDM0OTE4", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418434918", "createdAt": "2020-05-26T15:46:45Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0Njo0NVrOGakrcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0Njo0NVrOGakrcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUxNzEwNg==", "bodyText": "IMO \"determine\" usually means \"find out\" whereas here I think we want to set/specify:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You can determine which loss function to be used during {reganalysis} though the \n          \n          \n            \n            You can specify the loss function to be used during {reganalysis} though the", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430517106", "createdAt": "2020-05-26T15:46:45Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: \n+It is the default choice when no additional information about the data set is \n+available.\n+* mean squared logarithmic error (`msle`; a variation of `mse`): It is for \n+cases where the target values are all positive with a long tail distribution \n+(for example, prices or population).\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:\n+Use it when you want to prevent the model trying to fit the outliers instead of \n+fit regular data.\n+\n+The various types of loss function calculate the prediction error differently. \n+It means that which loss function is appropriate for your case highly depends on \n+the target distribution in your data set, the problem that you want to model, \n+the number of outliers in the data, and so on.\n+\n+You can determine which loss function to be used during {reganalysis} though the ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDM1OTY0", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418435964", "createdAt": "2020-05-26T15:47:55Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0Nzo1NlrOGakupg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo0Nzo1NlrOGakupg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUxNzkyNg==", "bodyText": "{ref}/put-dfanalytics.html[create {dfanalytics-jobs} API].\n\nWill it also be possible to set the loss function in the UI?  If yes, I\nthink it's better to just say that you set it when you create the job.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430517926", "createdAt": "2020-05-26T15:47:56Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: \n+It is the default choice when no additional information about the data set is \n+available.\n+* mean squared logarithmic error (`msle`; a variation of `mse`): It is for \n+cases where the target values are all positive with a long tail distribution \n+(for example, prices or population).\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:\n+Use it when you want to prevent the model trying to fit the outliers instead of \n+fit regular data.\n+\n+The various types of loss function calculate the prediction error differently. \n+It means that which loss function is appropriate for your case highly depends on \n+the target distribution in your data set, the problem that you want to model, \n+the number of outliers in the data, and so on.\n+\n+You can determine which loss function to be used during {reganalysis} though the \n+{ref}/put-dfanalytics.html[create {dfanalytics-jobs} API]. The default is mean ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NDM4MTYx", "url": "https://github.com/elastic/stack-docs/pull/1062#pullrequestreview-418438161", "createdAt": "2020-05-26T15:50:21Z", "commit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo1MDoyMlrOGak1QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNTo1MDoyMlrOGak1QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDUxOTYxNw==", "bodyText": "There is no parameter to manually...\n\nIMO this sentence is redundant given the way you worded the previous one.", "url": "https://github.com/elastic/stack-docs/pull/1062#discussion_r430519617", "createdAt": "2020-05-26T15:50:22Z", "author": {"login": "lcawl"}, "path": "docs/en/stack/ml/df-analytics/dfa-regression.asciidoc", "diffHunk": "@@ -88,7 +85,47 @@ gradient boosting methodologies.\n //end::regression-algorithms[]\n \n \n-[discrete]\n+[[dfa-regression-lossfunction]]\n+===== Loss functions for {regression} analyses\n+\n+Loss function measures how well a given {ml} model fits the specific \n+data set. The loss function boils down all the different under- and \n+overestimations of the model to a single number. The bigger the difference \n+between the prediction and the ground truth, the higher the value of the loss \n+function. Loss functions are used automatically in the background during \n+<<hyperparameters,hyperparameter optimization>> and when training the decision \n+trees to compare the various iterations of the model and decide which iteration \n+performs better. \n+\n+In the {stack}, there are three different types of loss function:\n+\n+* https://en.wikipedia.org/wiki/Mean_squared_error[mean squared error (`mse`)]: \n+It is the default choice when no additional information about the data set is \n+available.\n+* mean squared logarithmic error (`msle`; a variation of `mse`): It is for \n+cases where the target values are all positive with a long tail distribution \n+(for example, prices or population).\n+* https://en.wikipedia.org/wiki/Huber_loss#Pseudo-Huber_loss_function[Pseudo-Huber loss (`huber`)]:\n+Use it when you want to prevent the model trying to fit the outliers instead of \n+fit regular data.\n+\n+The various types of loss function calculate the prediction error differently. \n+It means that which loss function is appropriate for your case highly depends on \n+the target distribution in your data set, the problem that you want to model, \n+the number of outliers in the data, and so on.\n+\n+You can determine which loss function to be used during {reganalysis} though the \n+{ref}/put-dfanalytics.html[create {dfanalytics-jobs} API]. The default is mean \n+squared error (`mse`). If you choose `msle` or `huber`, you can also set up a \n+parameter for the loss function. With the parameter, you can further refine the \n+behavior of the chosen functions. There is no parameter to manually set up for ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44abff38e2ffb4998adedcc6e148a77aa44bcc48"}, "originalPosition": 62}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03f948325ea8451a7d10b3f8d9733862e7cd4349", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/03f948325ea8451a7d10b3f8d9733862e7cd4349", "committedDate": "2020-05-27T07:04:49Z", "message": "Apply suggestions from code review\n\nCo-authored-by: Lisa Cawley <lcawley@elastic.co>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "459697f8d30ef5c156c099886f5ab1c95f27a322", "author": {"user": {"login": "szabosteve", "name": "Istv\u00e1n Zolt\u00e1n Szab\u00f3"}}, "url": "https://github.com/elastic/stack-docs/commit/459697f8d30ef5c156c099886f5ab1c95f27a322", "committedDate": "2020-05-27T08:03:42Z", "message": "[DOCS] Addresses feedback."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4737, "cost": 1, "resetAt": "2021-11-01T14:20:25Z"}}}