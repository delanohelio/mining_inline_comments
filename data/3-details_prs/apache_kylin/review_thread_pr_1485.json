{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI1NzQ0ODU3", "number": 1485, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNjoyNzoyMVrOFB3cww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwNTowOTo0OVrOFDRlhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NTAxMzc5OnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNjoyNzoyMVrOIAvI2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNjoyNzoyMVrOIAvI2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzY0MzIyNQ==", "bodyText": "If we have 10 thousand cuboid to be statistics, we will call Long.toString 10 thousand times and create 10 thousand String objects. Is it the cause of bad performance ?", "url": "https://github.com/apache/kylin/pull/1485#discussion_r537643225", "createdAt": "2020-12-07T16:27:21Z", "author": {"login": "hit-lacus"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(String, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[String, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i.toString, AggInfo(i.toString)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }\n+      info(ids(idx).toString).cuboid.counter.addHashDirectly(value)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e13c8857700fd4d1c4e4daede6600562c62d494"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTk3OTk0OnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowMTowNVrOICTc_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOTowNDoyOFrOIC-hPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4Njc4MA==", "bodyText": "please add repartition operation : inputDs.rdd.repartition(inputDs.sparkSession.sparkContext.defaultParallelism)", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539286780", "createdAt": "2020-12-09T13:01:05Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5MjM4MQ==", "bodyText": "Thanks, it is reasonable.", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539992381", "createdAt": "2020-12-10T09:04:28Z", "author": {"login": "hit-lacus"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4Njc4MA=="}, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTk4ODk4OnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowMzowOVrOICTiUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowMzowOVrOICTiUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4ODE0NQ==", "bodyText": "convert 'List' to 'Array': seg.getAllLayout.map(x => x.getId).toArray", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539288145", "createdAt": "2020-12-09T13:03:09Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTk5NDczOnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowNDozM1rOICTl6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowNDozM1rOICTl6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4OTA2Ng==", "bodyText": "change ids: List[Long] to ids: Array[Long]", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539289066", "createdAt": "2020-12-09T13:04:33Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTk5ODI4OnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowNToxOFrOICTn7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOTowNTozNlrOIC-kOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4OTU4Mw==", "bodyText": "uses mutable.LongMap[AggInfo]() instead of mutable.Map[Long, AggInfo]() here", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539289583", "createdAt": "2020-12-09T13:05:18Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5MzE0NA==", "bodyText": "Great !", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539993144", "createdAt": "2020-12-10T09:05:36Z", "author": {"login": "hit-lacus"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI4OTU4Mw=="}, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NjAxNjYwOnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzowOTozMlrOICTy0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOToxNDoyMFrOIC-7Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI5MjM2OA==", "bodyText": "some suggestion:\nval currCuboidBitSet = allCuboidsBitSet(idx)\nval currCuboidLength = currCuboidBitSet.length\nwhile (position < currCuboidLength) {\n         value += rowHashCodesLong(currCuboidBitSet(position))\n         position += 1\n}\n\nreduce the time of calling 'allCuboidsBitSet(idx)'", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539292368", "createdAt": "2020-12-09T13:09:32Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5OTAzOQ==", "bodyText": "Good advice.", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539999039", "createdAt": "2020-12-10T09:14:20Z", "author": {"login": "hit-lacus"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI5MjM2OA=="}, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NjAyOTY3OnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzoxMjozMlrOICT6dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzoxMjozMlrOICT6dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTI5NDMyNQ==", "bodyText": "uses cuboidIds: Array[Long] instead of cuboidIds: List[Long]", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539294325", "createdAt": "2020-12-09T13:12:32Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"\n+      // add column ordinal to the hash value to distinguish between (a,b) and (b,a)\n+      rowHashCodesLong(idx) = hc.putUnencodedChars(colValue).hash().padToLong() + idx\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter1 += (endMills - startMills)\n+\n+\n+    startMills = System.currentTimeMillis()\n+    // use the row key column hash to get a consolidated hash for each cuboid\n+    val n = allCuboidsBitSet.length\n+    idx = 0\n+    while (idx < n) {\n+      var value: Long = 0\n+      var position = 0\n+      while (position < allCuboidsBitSet(idx).length) {\n+        value += rowHashCodesLong(allCuboidsBitSet(idx)(position))\n+        position += 1\n+      }\n+      info(ids(idx)).cuboid.counter.addHashDirectly(value)\n+      idx += 1\n+    }\n+    endMills = System.currentTimeMillis()\n+    meter2 += (endMills - startMills)\n+  }\n+\n+  def getCuboidBitSet(cuboidIds: List[Long], nRowKey: Int): Array[Array[Integer]] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4OTM4NzI2OnYy", "diffSide": "RIGHT", "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMjoyNjoxMFrOICyhzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMjoyNjoxMFrOICyhzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc5NTkxNw==", "bodyText": "If r.get(idx) == null, r.get(idx).toString will throw NPE, please change to :\nvar colValue = if (r.get(idx) == null) \"0\" else r.get(idx).toString", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539795917", "createdAt": "2020-12-10T02:26:10Z", "author": {"login": "zzcclp"}, "path": "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kylin.engine.spark.job\n+\n+\n+import org.apache.kylin.engine.spark.metadata.SegmentInfo\n+import org.apache.kylin.measure.hllc.HLLCounter\n+import org.apache.kylin.shaded.com.google.common.hash.{HashFunction, Hashing}\n+import org.apache.spark.sql.{Dataset, Row}\n+\n+import scala.collection.mutable\n+\n+/**\n+ * Calculate HLLCounter for each cuboid, to serve Cube Planner (to calculate cost and benefit of each cuboid).\n+ */\n+object CuboidStatisticsJob {\n+\n+  /**\n+   * @param inputDs Part of FlatTable which contains all normal dimensions\n+   * @return Cuboid level statistics data\n+   */\n+  def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {\n+\n+    val rkc = seg.allColumns.count(c => c.rowKey)\n+    // maybe we should use sample operation to reduce cost later\n+    val res = inputDs.rdd\n+      .mapPartitions(new CuboidStatisticsJob(seg.getAllLayout.map(x => x.getId), rkc).statisticsWithinPartition)\n+    val l = res.map(a => (a.key, a)).reduceByKey((a, b) => a.merge(b)).collect()\n+    //    l.foreach(x => println(x._1 + \" >>><<< \" + x._2.cuboid.counter.getCountEstimate))\n+    l\n+  }\n+}\n+\n+class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {\n+  private val info = mutable.Map[Long, AggInfo]()\n+  private var allCuboidsBitSet: Array[Array[Integer]] = Array()\n+  private val hf: HashFunction = Hashing.murmur3_128\n+  private val rowHashCodesLong = new Array[Long](rkc)\n+  private var idx = 0\n+  private var meter1 = 0L\n+  private var meter2 = 0L\n+  private var startMills = 0L\n+  private var endMills = 0L\n+\n+\n+  def statisticsWithinPartition(rows: Iterator[Row]): Iterator[AggInfo] = {\n+    init()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition1-\" + System.currentTimeMillis())\n+    rows.foreach(update)\n+    printStat()\n+    println(\"CuboidStatisticsJob-statisticsWithinPartition2-\" + System.currentTimeMillis())\n+    info.valuesIterator\n+  }\n+\n+  def init(): Unit = {\n+    println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())\n+    allCuboidsBitSet = getCuboidBitSet(ids, rkc)\n+    ids.foreach(i => info.put(i, AggInfo(i)))\n+    println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())\n+  }\n+\n+  def update(r: Row): Unit = {\n+    idx += 1\n+    if (idx <= 5)\n+      println(r)\n+    updateCuboid(r)\n+  }\n+\n+  def updateCuboid(r: Row): Unit = {\n+    // generate hash for each row key column\n+    startMills = System.currentTimeMillis()\n+    var idx = 0\n+    while (idx < rkc) {\n+      val hc = hf.newHasher\n+      var colValue = r.get(idx).toString\n+      if (colValue == null) colValue = \"0\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4OTc4MTgxOnYy", "diffSide": "RIGHT", "path": "core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwNTowOTo0OVrOIC1zNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwNTowOTo0OVrOIC1zNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTg0OTUyNA==", "bodyText": "What's the difference between '.seq' directory and '.json' directory? Do we need to delete which one after running job successfully?", "url": "https://github.com/apache/kylin/pull/1485#discussion_r539849524", "createdAt": "2020-12-10T05:09:49Z", "author": {"login": "zzcclp"}, "path": "core-cube/src/main/java/org/apache/kylin/cube/CubeSegment.java", "diffHunk": "@@ -530,11 +537,19 @@ public void setSnapshots(ConcurrentHashMap<String, String> snapshots) {\n     }\n \n     public String getStatisticsResourcePath() {\n-        return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid());\n+        return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid(), \".seq\");\n+    }\n+\n+    public String getPreciseStatisticsResourcePath() {\n+        return getStatisticsResourcePath(this.getCubeInstance().getName(), this.getUuid(), \".json\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22e96c7ad1f845644aad66ceb904f1bdaa90bce8"}, "originalPosition": 53}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1567, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}