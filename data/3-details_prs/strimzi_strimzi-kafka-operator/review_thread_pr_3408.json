{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4Mzc0Mjk3", "number": 3408, "reviewThreads": {"totalCount": 54, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozMTozN1rOETanmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoyMTo1OFrOEdMm6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4Nzk0NTIxOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozMTozN1rOG5JDCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQwODozNDoxOVrOG5ZcRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA==", "bodyText": "Wouldn't it be better to make some variable for this? Because you use it in AssertionError too.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570250", "createdAt": "2020-07-29T20:31:37Z", "author": {"login": "im-konge"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjgzODg1Mw==", "bodyText": "It was not a good approach on how to verify I have changed the it....", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462838853", "createdAt": "2020-07-30T08:34:19Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA=="}, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4Nzk0NzI3OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozMjoxM1rOG5JEVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozMjoxM1rOG5JEVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDU4Mg==", "bodyText": "To the variable I mentioned above (just as note)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570582", "createdAt": "2020-07-29T20:32:13Z", "author": {"login": "im-konge"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4Nzk2NjkwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozODoxM1rOG5JQsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMzoxOTo1N1rOG5iqxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng==", "bodyText": "Just thinking -> is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.\nAnyway the KafkaUtils is not needed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462573746", "createdAt": "2020-07-29T20:38:13Z", "author": {"login": "im-konge"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk5MDAyMQ==", "bodyText": "I have split these 2...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462990021", "createdAt": "2020-07-30T13:19:57Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng=="}, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTkwMzYyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyMToxNFrOG5uqOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyMToxNFrOG5uqOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NjQ4OQ==", "bodyText": "KafkaDynamicConfiguration? typo?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463186489", "createdAt": "2020-07-30T18:21:14Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTkxMTUwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyMzoyOVrOG5uvBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwODo0MDozNVrOG6w_kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA==", "bodyText": "What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463187718", "createdAt": "2020-07-30T18:23:29Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+    min_insync_replicas,\n+    unclean_leader_election_enable,\n+    message_max_bytes,\n+    metric_reporters,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_message_downconversion_enable,\n+    log_preallocate,\n+\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0MzM4OA==", "bodyText": "I think it's just sample from all configuration options which Kafka offers. but I am not sure.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443388", "createdAt": "2020-07-31T07:12:08Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+    min_insync_replicas,\n+    unclean_leader_election_enable,\n+    message_max_bytes,\n+    metric_reporters,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_message_downconversion_enable,\n+    log_preallocate,\n+\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3MzI5OQ==", "bodyText": "Yes this is all Kafka configuration, which can by changed without triggering rolling update.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464273299", "createdAt": "2020-08-03T08:40:35Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+    min_insync_replicas,\n+    unclean_leader_election_enable,\n+    message_max_bytes,\n+    metric_reporters,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_message_downconversion_enable,\n+    log_preallocate,\n+\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTkyMzU0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyNjo0NlrOG5u2Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyNjo0NlrOG5u2Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTU5OQ==", "bodyText": "The unclean.leader.election.enable should show up here as well. So maybe you can assert it too?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189599", "createdAt": "2020-07-30T18:26:46Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTkyNTk0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyNzoyOVrOG5u34w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwODo0MjoxMlrOG6xCyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw==", "bodyText": "Is this some copy paster left-over? Or what is the value of this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189987", "createdAt": "2020-07-30T18:27:29Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NDEyMg==", "bodyText": "These DynamicConfigurationIsolatedST are the Standa`s tests, which were located in the KafkaST. @stanlyDoge", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464274122", "createdAt": "2020-08-03T08:42:12Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTkzNDg0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMDowN1rOG5u9eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMDowN1rOG5u9eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MTQxOA==", "bodyText": "Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463191418", "createdAt": "2020-07-30T18:30:07Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTk0MDU5OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMTo0NFrOG5vBBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0NTozOFrOG7TN4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA==", "bodyText": "A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192324", "createdAt": "2020-07-30T18:31:44Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDI3NzU4NQ==", "bodyText": "I did not write these tests (just copy-paste) but I will split him if it will be possible.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464277585", "createdAt": "2020-08-03T08:48:46Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNDAxOA==", "bodyText": "The idea behind this test was to perform as many as possible changes in listeners. I agree the test is complicated and the environmental restrictions suck. Splitting test to smaller bits (one listener, one test) seem like a good idea.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464834018", "createdAt": "2020-08-04T06:45:38Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTk0MjU4OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMjoyN1rOG5vCRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMjoyN1rOG5vCRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjY0NA==", "bodyText": "You seem to be doing rolling updates. Please use persistent cluster.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192644", "createdAt": "2020-07-30T18:32:27Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTk0NDU0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMzoxMVrOG5vDiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozMzoxMVrOG5vDiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5Mjk3MQ==", "bodyText": "Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192971", "createdAt": "2020-07-30T18:33:11Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 255}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTk0ODkxOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozNDoyNlrOG5vGKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxMTo1NTozOFrOG7dGvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA==", "bodyText": "Are you saying that the change here doesn't trigger RU?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463193640", "createdAt": "2020-07-30T18:34:26Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 327}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk5NjAyOA==", "bodyText": "This should trigger a rolling update! Using method verifyThatRunningPodsAreStable is not a good practice here (again I did write these tests :D) and I have replaced it with the snapShot waitTillSsHasRolled.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464996028", "createdAt": "2020-08-04T11:55:38Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 327}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTk1NTQ4OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozNjoyNlrOG5vKXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxMDoyNDo0NFrOG7acKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw==", "bodyText": "Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463194717", "createdAt": "2020-07-30T18:36:26Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MjgzNg==", "bodyText": "Ye, I suggest to use https://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests or https://junit.org/junit5/docs/current/user-guide/#writing-tests-dynamic-tests", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463452836", "createdAt": "2020-07-31T07:36:07Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk1MjM2MQ==", "bodyText": "Done", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464952361", "createdAt": "2020-08-04T10:24:44Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MzU5MTAyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzoxMzozNVrOG5-XuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxMTozNDoxN1rOG7ccvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng==", "bodyText": "How the output of this log looks? It's formated to be easily readable ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443896", "createdAt": "2020-07-31T07:13:35Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4NTI3OQ==", "bodyText": "Yes :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464985279", "createdAt": "2020-08-04T11:34:17Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MzU5NDI2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzoxNTowMVrOG5-ZsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxMjoxMjowNFrOG7dm3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA==", "bodyText": "Maybe we should show dyn.configuration in case of error as well?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444400", "createdAt": "2020-07-31T07:15:01Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration.toString(),\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration.toString(),\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAwMTcxMQ==", "bodyText": "?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465001711", "createdAt": "2020-08-04T12:07:08Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration.toString(),\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration.toString(),\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAwNDI1Mg==", "bodyText": "If the error occurs then we show the Kafka Pod {}....", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465004252", "createdAt": "2020-08-04T12:12:04Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration.toString(),\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration.toString(),\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MzU5NTkxOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzoxNTozNFrOG5-arg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzoxNTozNFrOG5-arg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDY1NA==", "bodyText": "Why 2 brokers instead of 3?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444654", "createdAt": "2020-07-31T07:15:34Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MzY0MDQ3OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzozMjozM1rOG5-1Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzozMjozM1rOG5-1Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MTQ1MQ==", "bodyText": "Are you sure we don't have similar test like this? Maybe in ListenersST or in rollingupdate ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463451451", "createdAt": "2020-07-31T07:32:33Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 256}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MzY1MDU2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzozNjozMlrOG5-7Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxMTozODo0OVrOG7clYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ==", "bodyText": "Indent", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463453011", "createdAt": "2020-07-31T07:36:32Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1534,332 +1524,18 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-                .editKafka()\n-                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n-                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n-                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-                .endKafka()\n+            .editKafka()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4NzQ5MA==", "bodyText": "Always fun! :D", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r464987490", "createdAt": "2020-08-04T11:38:49Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1534,332 +1524,18 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-                .editKafka()\n-                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n-                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n-                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-                .endKafka()\n+            .editKafka()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNTk0NTgwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxNDowN1rOG7t5Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxNDowN1rOG7t5Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MTEzNQ==", "bodyText": "Indent?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465271135", "createdAt": "2020-08-04T19:14:07Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNTk1NTIwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxNzowMVrOG7t_RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxNzowMVrOG7t_RQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MjY0NQ==", "bodyText": "I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465272645", "createdAt": "2020-08-04T19:17:01Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNTk2MTQyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxOTowN1rOG7uDWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwOToxMToyOFrOG8BG-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ==", "bodyText": "This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like parametrizedTest? Maybe @samuel-hawker will be better for this \ud83d\ude04", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465273689", "createdAt": "2020-08-04T19:19:07Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+\n+        \"log.flush.interval.ms, \" + 20,\n+\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.cleaner.threads, \" + 1,\n+\n+        \"num.network.threads, \" + 2,\n+        \"testLogIndexLogMessageLogMessage, \" + 5,\n+        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n+        \"log.preallocate, \" + true,\n+\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048\n+    })\n+    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTU4NTkxNQ==", "bodyText": "Already change the naming )", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465585915", "createdAt": "2020-08-05T09:11:28Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+\n+        \"log.flush.interval.ms, \" + 20,\n+\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.cleaner.threads, \" + 1,\n+\n+        \"num.network.threads, \" + 2,\n+        \"testLogIndexLogMessageLogMessage, \" + 5,\n+        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n+        \"log.preallocate, \" + true,\n+\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048\n+    })\n+    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ=="}, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjc3MTY2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxMDoxOVrOG8vA5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjowMDo1OVrOG8wd2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw==", "bodyText": "What enum?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338023", "createdAt": "2020-08-06T11:10:19Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2MTgxOQ==", "bodyText": "old code....i have changed it.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466361819", "createdAt": "2020-08-06T12:00:59Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjc3NzQ4OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxMjoyMlrOG8vEcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxMjoyMlrOG8vEcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODkzMQ==", "bodyText": "For all these I would call it brokerConfigName or something. You don't care, in these methods whether the particular config supports dynamic update or not.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338931", "createdAt": "2020-08-06T11:12:22Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjc3OTc1OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxMzowN1rOG8vFzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxMzowN1rOG8vFzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTI3OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n          \n          \n            \n                 * Verifies that updated configuration was successfully changed inside Kafka pods", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339278", "createdAt": "2020-08-06T11:13:07Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjc4MjY1OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxNDoxMlrOG8vHjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjoxNTo0MFrOG8w6Tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw==", "bodyText": "Is there any reason for doing this via kafka-configs.sh and not just using an Admin client instance?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339727", "createdAt": "2020-08-06T11:14:12Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2OTEwMw==", "bodyText": "AdminClient has one disadvantage. If we use him we need som external listener, which is not a good for run the tests in all envinronments.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466369103", "createdAt": "2020-08-06T12:15:40Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjc5MDA0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxNjozMlrOG8vL7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjoyODoyNlrOG8xUdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA==", "bodyText": "Why are you calling both like this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466340844", "createdAt": "2020-08-06T11:16:32Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM2NzI0Mg==", "bodyText": "This is just refactored tests, which just update the Dyn. configuration of the broker. The point is to make as many changes as possible. If I understand correctly from @stanlyDoge.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466367242", "createdAt": "2020-08-06T12:12:02Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3NTc5OQ==", "bodyText": "Yeah, as I understand the test this two lines set the same property (unclean.leader.election.enable) to false and then to true. We are checking at the end of the test, whether kafka pods rolled.\nI would consider changing another property.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466375799", "createdAt": "2020-08-06T12:28:26Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjgwODI0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToyMjozOFrOG8vW6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTo1NjozMVrOG_n61g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA==", "bodyText": "Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?\nIt's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466343658", "createdAt": "2020-08-06T11:22:38Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MDgzOQ==", "bodyText": "It is an almost a complete list of Dyn. configuration. I picked randomly, which were not read-only and also which does not have this\n FORBIDDEN_PREFIXES = \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl., security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer., super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers;", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466370839", "createdAt": "2020-08-06T12:19:06Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MTY4Ng==", "bodyText": "Hmmm, I can create a .csv files, which will bind to specific version :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466371686", "createdAt": "2020-08-06T12:20:45Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NzUxMA==", "bodyText": "No. You could include columns in the CSV defining the version range between which to test with that config, skipping versions outside that range. But the key point here is that to get complete coverage we need to automate this, which means not relying on an annotation to drive the values, but that generated file I described.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469367510", "createdAt": "2020-08-12T15:56:31Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjgyNDMwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToyODowNVrOG8vgsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjoyMjozOFrOG8xINQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg==", "bodyText": "Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466346162", "createdAt": "2020-08-06T11:28:05Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3MjY2MQ==", "bodyText": "Everywhere, where the broker will not be triggered by change of dyn. configuration we should tag as DYNAMIC_CONFIGURATION", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466372661", "createdAt": "2020-08-06T12:22:38Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjgzOTgwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMTozMjo1M1rOG8vp0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjoyNjowNVrOG8xPSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA==", "bodyText": "Should we add a parameter String dynConfProperty to make this more general?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466348498", "createdAt": "2020-08-06T11:32:53Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3NDQ3NQ==", "bodyText": "Currently, I am using only the one specific property. So it would be useless from my POV to extend it for now.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466374475", "createdAt": "2020-08-06T12:26:05Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjg1MTk0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMTozNjo1OVrOG8vxQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjozNzowMVrOG8xnGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw==", "bodyText": "Just a nit. Should this be execute phase?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466350403", "createdAt": "2020-08-06T11:36:59Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,\n+    })\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM3ODI2Ng==", "bodyText": "No, it is exercise phase -> https://thoughtbot.com/blog/four-phase-test. I am changing the state of the SUT.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466378266", "createdAt": "2020-08-06T12:32:49Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,\n+    })\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4MDU3MQ==", "bodyText": "Ah, ok. Thanks.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466380571", "createdAt": "2020-08-06T12:37:01Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,\n+    })\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMzM1OTkyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMzo1NjoxNFrOG80sDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMzo1NjoxNFrOG80sDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQzMDk4OA==", "bodyText": "I think the indent is still same here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466430988", "createdAt": "2020-08-06T13:56:14Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY2Nzk1OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozMjo1N1rOG9BbhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzozMzo0NFrOG-EChA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA==", "bodyText": "I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639748", "createdAt": "2020-08-06T19:32:57Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA3Ng==", "bodyText": "Yes I will use TestKafkaVersion class to do it dynamically. :) Thanks", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467731076", "createdAt": "2020-08-10T07:33:44Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY2OTA1OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozMzoxOVrOG9BcNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNzozMzo0NlrOG-EClA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw==", "bodyText": "I'm not sure I understand why are we asserting these when we don't configure them anywhere.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639927", "createdAt": "2020-08-06T19:33:19Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzczMTA5Mg==", "bodyText": "If I understand you correctly...but I configure it in the @BeforeEach phase.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467731092", "createdAt": "2020-08-10T07:33:46Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY3MDIxOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozMzo0MVrOG9Bc6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwOTozNzo0OVrOG9TX6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA==", "bodyText": "Why should this be \"true\"?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640104", "createdAt": "2020-08-06T19:33:41Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MTg1Mw==", "bodyText": "This method (as you pointed) is setting unclean.leader.election.enable, which is dynamically changeable option. Default value is false so it is just changing its value.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466841853", "createdAt": "2020-08-07T06:14:52Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkzMzczNw==", "bodyText": "I think the code should be improved to make it easier to understand and read even without understanding some Kafka defaults.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466933737", "createdAt": "2020-08-07T09:37:49Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY3MDg2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozMzo1MlrOG9BdUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNjoxNDo1NVrOG9NxEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA==", "bodyText": "Why should this be \"true\"?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640208", "createdAt": "2020-08-06T19:33:52Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MTg3NA==", "bodyText": "And after the value is changed, it is verified whether it actually changed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466841874", "createdAt": "2020-08-07T06:14:55Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY3NzQyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozNTozMFrOG9BhBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODowNDoyMVrOG-E2CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ==", "bodyText": "This has generic name, but it is not generic method. It should ba then maybe named updateAndVerifyDynUncleanLeaderElectionConf", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641159", "createdAt": "2020-08-06T19:35:30Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NDI2NA==", "bodyText": "I have refactored that method to be more generic :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467744264", "createdAt": "2020-08-10T08:04:21Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY4MTAxOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozNjoyNFrOG9BjCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozNjoyNFrOG9BjCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTY3NQ==", "bodyText": "I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass kafkaConfig as parameter?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641675", "createdAt": "2020-08-06T19:36:24Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        // change dynamically changeable option\n+        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 265}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDY5OTMyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0MjowNlrOG9BuJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0MjowNlrOG9BuJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NDUxNw==", "bodyText": "Can we add some Javadoc explaining what this does?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466644517", "createdAt": "2020-08-06T19:42:06Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDcwNDYzOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0MzozMlrOG9BxUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODowNDo0MVrOG-E2nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ==", "bodyText": "Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466645329", "createdAt": "2020-08-06T19:43:32Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njg0MjU5OA==", "bodyText": "It is copy-paste from test I wrote. At the time we had dynamic updates of kafka configuration only, we wanted to be sure the changes of logging will trigger RU. For the reasons you mentioned, it should be deleted in this PR or in my PR after rebase.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466842598", "createdAt": "2020-08-07T06:17:09Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc0NDQxMw==", "bodyText": "Removed :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r467744413", "createdAt": "2020-08-10T08:04:41Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ=="}, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMjc3NTM2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTo1Mjo0OVrOG_nwaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTo1Mjo0OVrOG_nwaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NDg0MA==", "bodyText": "What does this tell you that the result of the update to the Kafka CR does not?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469364840", "createdAt": "2020-08-12T15:52:49Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +156,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "defb26bb06660cf9d8eb8f07c9b899cb920b4796"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzYzNDIyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowMTo1MlrOHBu_ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowMTo1MlrOHBu_ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MDU3OQ==", "bodyText": "I think it should be in kafka directory in systemtest package", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471580579", "createdAt": "2020-08-17T16:01:52Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "347dd238537142b016a9fa33e8fe7c077da7583c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NzY0MDE1OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowMzoyM1rOHBvDYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowMzoyM1rOHBvDYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MTUzOA==", "bodyText": "It should be in kafka directory of systemtest I think.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471581538", "createdAt": "2020-08-17T16:03:23Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "347dd238537142b016a9fa33e8fe7c077da7583c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MDg3NjIwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMDo0NDoyOFrOHCN0Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMzowNToyNFrOHGXImA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ==", "bodyText": "Shouldn't be this annotation from spotbugs?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472085511", "createdAt": "2020-08-18T10:44:28Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -4,6 +4,8 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQzMjUzNg==", "bodyText": "AFAIK everywhere we use SuppressFBWarnings. I know that FB were deprecated and replaced by SpotBugs but  from code a i cannot find any SuppressSBWarnings or something similar like this.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r476432536", "createdAt": "2020-08-25T13:05:24Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -4,6 +4,8 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ=="}, "originalCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MTAxNTI3OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMToyOToxMFrOHCPJmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMToyOToxMFrOHCPJmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNzQxNg==", "bodyText": "Maybe we should use prefixes from FORBIDDEN_PREFIXES ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472107416", "createdAt": "2020-08-18T11:29:10Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzE4NDUwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDoxMTo1MFrOHCkaLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDoxMTo1MFrOHCkaLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ1NTcyNA==", "bodyText": "I think that @im-konge is in other PR adding the tag ROLLING_UPDATE. I think we should use it here as well because these are closely related.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472455724", "createdAt": "2020-08-18T20:11:50Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzMzODIwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDo0Mjo0NFrOHCl9Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDo0Mjo0NFrOHCl9Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4MTA0Mg==", "bodyText": "How does this deal with the exceptions to the forbidden prefixes?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472481042", "createdAt": "2020-08-18T20:42:44Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzM0MzI0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyMzo1MVrOHEvTGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyMzo1MVrOHEvTGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMTI5MA==", "bodyText": "I think you mean @link.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474731290", "createdAt": "2020-08-21T14:23:51Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzM1ODY0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyNzo1OFrOHEvcqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyNzo1OFrOHEvcqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMzczOQ==", "bodyText": "You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n          \n          \n            \n                 * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474733739", "createdAt": "2020-08-21T14:27:58Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzM2MDY5OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyODozM1rOHEvd-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyODozM1rOHEvd-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczNDA3NQ==", "bodyText": "rethrow a RuntimeException", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474734075", "createdAt": "2020-08-21T14:28:33Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzM4NjE4OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDozNTowNFrOHEvuUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxNDoyMjoyM1rOHGaizg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng==", "bodyText": "You should be able to use the config-model module. This is what KafkaConfiguration does:\nprivate Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\n        String name = \"/kafka-\" + kafkaVersion.version() + \"-config-model.json\";\n        try {\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\n                    throw new RuntimeException(\"Incorrect version\");\n                }\n                return configModels.getConfigs();\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n        }\n    }\nThat will give you nicely typed ConfigModel to deal with, rather than JsonObject. Please try to refactor KafkaConfiguration to use a common implementation rather than copying the above code.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474738256", "createdAt": "2020-08-21T14:35:04Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjQ4ODM5OA==", "bodyText": "Done :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r476488398", "createdAt": "2020-08-25T14:22:23Z", "author": {"login": "see-quick"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng=="}, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzQwMjY0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDozOTozNFrOHEv4ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNToyMjozOVrOHExehA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA==", "bodyText": "Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:\ncmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474740874", "createdAt": "2020-08-21T14:39:34Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc2Njk4MA==", "bodyText": "Yeah, will do in another PR and also create an issue for that.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474766980", "createdAt": "2020-08-21T15:22:39Z", "author": {"login": "see-quick"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA=="}, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzQxODU5OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDo0NDowNFrOHEwCyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDo0NDowNFrOHEwCyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MzQ5Nw==", "bodyText": "I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about withExternalListeners, it's that changing an external listeners config causes a RU. So the name would be better as testUpdateToExternalListenerCausesRollingRestart", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474743497", "createdAt": "2020-08-21T14:44:04Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2NzQyMjIwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDo0NDo1N1rOHEwE9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDo0NDo1N1rOHEwE9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0NDA1NA==", "bodyText": "Same comment about javadoc or method name.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474744054", "createdAt": "2020-08-21T14:44:57Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MDUwNzI4OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoyMTo1OFrOHINzmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoyMTo1OFrOHINzmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3Njg1Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which process all supported configs by Kafka and filter all which are not dynamic\n          \n          \n            \n                 * Return dynamic Kafka configs supported by the the given version of Kafka.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r478376856", "createdAt": "2020-08-27T12:21:58Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -157,4 +170,152 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "308c3ecdb4ea1d3e0389c44e4da596f89bf08acd"}, "originalPosition": 137}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1564, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}