{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgxMTA2OTI1", "number": 3606, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwNzoxODo1OFrOEgzzog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxODoyMjozMVrOEhzOWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyODM4NjkwOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwNzoxODo1OFrOHNy9gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOToyNDoxNlrOHPq9lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDIyODQ4Mg==", "bodyText": "See point 4 in rejected alternatives:\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-412%3A+Extend+Admin+API+to+support+dynamic+application+log+levels#KIP412:ExtendAdminAPItosupportdynamicapplicationloglevels-RejectedAlternatives", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r484228482", "createdAt": "2020-09-07T07:18:58Z", "author": {"login": "sknot-rh"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -179,4 +104,47 @@ private static boolean isValidLoggerLevel(String level) {\n         return VALID_LOGGER_LEVELS.contains(level);\n     }\n \n+    static class LoggingLevelResolver {\n+\n+        Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();\n+            int b = e;\n+            while (b > -1) {\n+                b = name.lastIndexOf('.', e);\n+                if (b == -1) {\n+                    level = config.get(\"root\");\n+                } else {\n+                    level = config.get(name.substring(0, b));\n+                }\n+                if (level != null) {\n+                    return LoggingLevel.valueOf(level.split(\",\")[0]);\n+                }\n+                e = b - 1;\n+            }\n+            // still here? Not even root logger defined?\n+            return LoggingLevel.INFO;\n+        }\n+    }\n+\n+    enum LoggingLevel {\n+        ALL,\n+        FATAL,\n+        ERROR,\n+        WARN,\n+        INFO,\n+        DEBUG,\n+        TRACE,\n+        OFF", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edfcda99b1cc44d06c80cc98cc9e4f424908941c"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDI5MjcyNQ==", "bodyText": "One can type anything in Kafka CR logging / loggers, including otherwise valid levels like FINE and FINEST which are perfectly valid for java util logging, but not for Log4j. The question is how do we want to proceed when an unsupported logging level is specified. Currently FINE and FINEST probably just break things so we have to add some exception catching.\nWe can remove OFF from enum, and handle it the same way as FINE, FINEST.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r484292725", "createdAt": "2020-09-07T09:00:02Z", "author": {"login": "mstruk"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -179,4 +104,47 @@ private static boolean isValidLoggerLevel(String level) {\n         return VALID_LOGGER_LEVELS.contains(level);\n     }\n \n+    static class LoggingLevelResolver {\n+\n+        Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();\n+            int b = e;\n+            while (b > -1) {\n+                b = name.lastIndexOf('.', e);\n+                if (b == -1) {\n+                    level = config.get(\"root\");\n+                } else {\n+                    level = config.get(name.substring(0, b));\n+                }\n+                if (level != null) {\n+                    return LoggingLevel.valueOf(level.split(\",\")[0]);\n+                }\n+                e = b - 1;\n+            }\n+            // still here? Not even root logger defined?\n+            return LoggingLevel.INFO;\n+        }\n+    }\n+\n+    enum LoggingLevel {\n+        ALL,\n+        FATAL,\n+        ERROR,\n+        WARN,\n+        INFO,\n+        DEBUG,\n+        TRACE,\n+        OFF", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDIyODQ4Mg=="}, "originalCommit": {"oid": "edfcda99b1cc44d06c80cc98cc9e4f424908941c"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE5NDU4MQ==", "bodyText": "Actually the KIP-412 also does not seem to understand ALL level, so we should treat it as TRACE.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r486194581", "createdAt": "2020-09-10T09:24:16Z", "author": {"login": "mstruk"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -179,4 +104,47 @@ private static boolean isValidLoggerLevel(String level) {\n         return VALID_LOGGER_LEVELS.contains(level);\n     }\n \n+    static class LoggingLevelResolver {\n+\n+        Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();\n+            int b = e;\n+            while (b > -1) {\n+                b = name.lastIndexOf('.', e);\n+                if (b == -1) {\n+                    level = config.get(\"root\");\n+                } else {\n+                    level = config.get(name.substring(0, b));\n+                }\n+                if (level != null) {\n+                    return LoggingLevel.valueOf(level.split(\",\")[0]);\n+                }\n+                e = b - 1;\n+            }\n+            // still here? Not even root logger defined?\n+            return LoggingLevel.INFO;\n+        }\n+    }\n+\n+    enum LoggingLevel {\n+        ALL,\n+        FATAL,\n+        ERROR,\n+        WARN,\n+        INFO,\n+        DEBUG,\n+        TRACE,\n+        OFF", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDIyODQ4Mg=="}, "originalCommit": {"oid": "edfcda99b1cc44d06c80cc98cc9e4f424908941c"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyOTM4NTYzOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMTo0NjozNlrOHN8S3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjo0MjoyNFrOHN9_Lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4MTQwNA==", "bodyText": "Can you put there any doc?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r484381404", "createdAt": "2020-09-07T11:46:36Z", "author": {"login": "sknot-rh"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -179,4 +111,45 @@ private static boolean isValidLoggerLevel(String level) {\n         return VALID_LOGGER_LEVELS.contains(level);\n     }\n \n+    static class LoggingLevelResolver {\n+\n+        Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ebdda61a7b2e6c00d9081c8d09858b4f0c24cfb"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQwOTEzNQ==", "bodyText": "Not sure if you have something specific in mind for this section of the code, or do you mean more docs generally? Any specific questions you are looking clarifications for?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r484409135", "createdAt": "2020-09-07T12:42:24Z", "author": {"login": "mstruk"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -179,4 +111,45 @@ private static boolean isValidLoggerLevel(String level) {\n         return VALID_LOGGER_LEVELS.contains(level);\n     }\n \n+    static class LoggingLevelResolver {\n+\n+        Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4MTQwNA=="}, "originalCommit": {"oid": "8ebdda61a7b2e6c00d9081c8d09858b4f0c24cfb"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMjgwMzk2OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMjo1Njo1MVrOHObkaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNzo0Mzo1OFrOHO6wmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDg5MzgwMA==", "bodyText": "I don't understand how this is deprecated.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r484893800", "createdAt": "2020-09-08T12:56:51Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -68,115 +54,120 @@ public int getDiffSize() {\n         if (brokerConfigs == null || desired == null) {\n             return Collections.emptyList();\n         }\n-        Map<String, String> currentMap;\n+\n         Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n-        currentMap = brokerConfigs.entries().stream().collect(\n-            Collectors.toMap(\n-                ConfigEntry::name,\n-                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n \n         OrderedProperties orderedProperties = new OrderedProperties();\n         desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n         desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n         orderedProperties.addStringPairs(desired);\n         Map<String, String> desiredMap = orderedProperties.asMap();\n \n-        ObjectMapper orderedMapper = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n-\n-        JsonNode source = orderedMapper.valueToTree(currentMap);\n-        JsonNode target = orderedMapper.valueToTree(desiredMap);\n-        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n-\n-        for (JsonNode d : jsonDiff) {\n-            String pathValue = d.get(\"path\").asText();\n-            String pathValueWithoutSlash = pathValue.substring(1);\n+        LoggingLevelResolver levelResolver = new LoggingLevelResolver(desiredMap);\n \n-            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n-                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n-                    .findFirst();\n-\n-            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n-                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n-                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n-                }\n-            }\n-            String op = d.get(\"op\").asText();\n-            if (optEntry.isPresent()) {\n-                ConfigEntry entry = optEntry.get();\n-                if (\"remove\".equals(op)) {\n-                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n-                } else if (\"replace\".equals(op)) {\n-                    // entry is in the current, desired is updated value\n-                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n-                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n-                    }\n-                }\n-            } else {\n-                if (\"add\".equals(op)) {\n-                    // entry is not in the current, it is added\n-                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n-                }\n-            }\n-            if (\"remove\".equals(op)) {\n-                // there is a lot of properties set by default - not having them in desired causes very noisy log output\n-                log.trace(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.trace(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.trace(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n-            } else {\n-                log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        for (ConfigEntry entry: brokerConfigs.entries()) {\n+            LoggingLevel desiredLevel;\n+            try {\n+                desiredLevel = levelResolver.resolveLevel(entry.name());\n+            } catch (IllegalArgumentException e) {\n+                log.warn(\"Skipping {} - it is configured with an unsupported value (\\\"{}\\\")\", entry.name(), e.getMessage());\n+                continue;\n             }\n-        }\n-        return updatedCE;\n-    }\n \n-    private static String parseLogLevelFromAppenderCouple(String level) {\n-        int index = level.indexOf(\",\");\n-        if (index > 0) {\n-            return level.substring(0, index).trim();\n-        } else {\n-            return level.trim();\n+            if (!desiredLevel.name().equals(entry.value())) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(entry.name(), desiredLevel.name()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} has a deprecated value. Setting to {}\", entry.name(), desiredLevel.name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72840982cd84f847fa9ce19face0f747320b3471"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM4NTY0MA==", "bodyText": "Maybe it's a wrong word. Some alternatives: 'obsolete', 'old', 'outdated'.\nCompletely reworded as in 'The logging level for {} category has changed' ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r485385640", "createdAt": "2020-09-09T07:07:59Z", "author": {"login": "mstruk"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -68,115 +54,120 @@ public int getDiffSize() {\n         if (brokerConfigs == null || desired == null) {\n             return Collections.emptyList();\n         }\n-        Map<String, String> currentMap;\n+\n         Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n-        currentMap = brokerConfigs.entries().stream().collect(\n-            Collectors.toMap(\n-                ConfigEntry::name,\n-                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n \n         OrderedProperties orderedProperties = new OrderedProperties();\n         desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n         desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n         orderedProperties.addStringPairs(desired);\n         Map<String, String> desiredMap = orderedProperties.asMap();\n \n-        ObjectMapper orderedMapper = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n-\n-        JsonNode source = orderedMapper.valueToTree(currentMap);\n-        JsonNode target = orderedMapper.valueToTree(desiredMap);\n-        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n-\n-        for (JsonNode d : jsonDiff) {\n-            String pathValue = d.get(\"path\").asText();\n-            String pathValueWithoutSlash = pathValue.substring(1);\n+        LoggingLevelResolver levelResolver = new LoggingLevelResolver(desiredMap);\n \n-            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n-                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n-                    .findFirst();\n-\n-            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n-                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n-                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n-                }\n-            }\n-            String op = d.get(\"op\").asText();\n-            if (optEntry.isPresent()) {\n-                ConfigEntry entry = optEntry.get();\n-                if (\"remove\".equals(op)) {\n-                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n-                } else if (\"replace\".equals(op)) {\n-                    // entry is in the current, desired is updated value\n-                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n-                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n-                    }\n-                }\n-            } else {\n-                if (\"add\".equals(op)) {\n-                    // entry is not in the current, it is added\n-                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n-                }\n-            }\n-            if (\"remove\".equals(op)) {\n-                // there is a lot of properties set by default - not having them in desired causes very noisy log output\n-                log.trace(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.trace(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.trace(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n-            } else {\n-                log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        for (ConfigEntry entry: brokerConfigs.entries()) {\n+            LoggingLevel desiredLevel;\n+            try {\n+                desiredLevel = levelResolver.resolveLevel(entry.name());\n+            } catch (IllegalArgumentException e) {\n+                log.warn(\"Skipping {} - it is configured with an unsupported value (\\\"{}\\\")\", entry.name(), e.getMessage());\n+                continue;\n             }\n-        }\n-        return updatedCE;\n-    }\n \n-    private static String parseLogLevelFromAppenderCouple(String level) {\n-        int index = level.indexOf(\",\");\n-        if (index > 0) {\n-            return level.substring(0, index).trim();\n-        } else {\n-            return level.trim();\n+            if (!desiredLevel.name().equals(entry.value())) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(entry.name(), desiredLevel.name()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} has a deprecated value. Setting to {}\", entry.name(), desiredLevel.name());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDg5MzgwMA=="}, "originalCommit": {"oid": "72840982cd84f847fa9ce19face0f747320b3471"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQwNDgyNA==", "bodyText": "Ah, then 'outdated' would be fine.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r485404824", "createdAt": "2020-09-09T07:43:58Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -68,115 +54,120 @@ public int getDiffSize() {\n         if (brokerConfigs == null || desired == null) {\n             return Collections.emptyList();\n         }\n-        Map<String, String> currentMap;\n+\n         Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n-        currentMap = brokerConfigs.entries().stream().collect(\n-            Collectors.toMap(\n-                ConfigEntry::name,\n-                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n \n         OrderedProperties orderedProperties = new OrderedProperties();\n         desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n         desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n         orderedProperties.addStringPairs(desired);\n         Map<String, String> desiredMap = orderedProperties.asMap();\n \n-        ObjectMapper orderedMapper = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n-\n-        JsonNode source = orderedMapper.valueToTree(currentMap);\n-        JsonNode target = orderedMapper.valueToTree(desiredMap);\n-        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n-\n-        for (JsonNode d : jsonDiff) {\n-            String pathValue = d.get(\"path\").asText();\n-            String pathValueWithoutSlash = pathValue.substring(1);\n+        LoggingLevelResolver levelResolver = new LoggingLevelResolver(desiredMap);\n \n-            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n-                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n-                    .findFirst();\n-\n-            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n-                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n-                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n-                }\n-            }\n-            String op = d.get(\"op\").asText();\n-            if (optEntry.isPresent()) {\n-                ConfigEntry entry = optEntry.get();\n-                if (\"remove\".equals(op)) {\n-                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n-                } else if (\"replace\".equals(op)) {\n-                    // entry is in the current, desired is updated value\n-                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n-                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n-                    }\n-                }\n-            } else {\n-                if (\"add\".equals(op)) {\n-                    // entry is not in the current, it is added\n-                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n-                }\n-            }\n-            if (\"remove\".equals(op)) {\n-                // there is a lot of properties set by default - not having them in desired causes very noisy log output\n-                log.trace(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.trace(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.trace(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n-            } else {\n-                log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        for (ConfigEntry entry: brokerConfigs.entries()) {\n+            LoggingLevel desiredLevel;\n+            try {\n+                desiredLevel = levelResolver.resolveLevel(entry.name());\n+            } catch (IllegalArgumentException e) {\n+                log.warn(\"Skipping {} - it is configured with an unsupported value (\\\"{}\\\")\", entry.name(), e.getMessage());\n+                continue;\n             }\n-        }\n-        return updatedCE;\n-    }\n \n-    private static String parseLogLevelFromAppenderCouple(String level) {\n-        int index = level.indexOf(\",\");\n-        if (index > 0) {\n-            return level.substring(0, index).trim();\n-        } else {\n-            return level.trim();\n+            if (!desiredLevel.name().equals(entry.value())) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(entry.name(), desiredLevel.name()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} has a deprecated value. Setting to {}\", entry.name(), desiredLevel.name());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDg5MzgwMA=="}, "originalCommit": {"oid": "72840982cd84f847fa9ce19face0f747320b3471"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzODc3NzIxOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxODoyMjozMVrOHPUXLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOToxNDozNVrOHPqmag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNDMwMA==", "bodyText": "Wouldn't it better to use this name or am I missing some meaning of e? )\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        int e = name.length();\n          \n          \n            \n                        int loggingCategoryNameLength = name.length();", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r485824300", "createdAt": "2020-09-09T18:22:31Z", "author": {"login": "see-quick"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -68,115 +54,120 @@ public int getDiffSize() {\n         if (brokerConfigs == null || desired == null) {\n             return Collections.emptyList();\n         }\n-        Map<String, String> currentMap;\n+\n         Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n-        currentMap = brokerConfigs.entries().stream().collect(\n-            Collectors.toMap(\n-                ConfigEntry::name,\n-                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n \n         OrderedProperties orderedProperties = new OrderedProperties();\n         desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n         desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n         orderedProperties.addStringPairs(desired);\n         Map<String, String> desiredMap = orderedProperties.asMap();\n \n-        ObjectMapper orderedMapper = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n-\n-        JsonNode source = orderedMapper.valueToTree(currentMap);\n-        JsonNode target = orderedMapper.valueToTree(desiredMap);\n-        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n-\n-        for (JsonNode d : jsonDiff) {\n-            String pathValue = d.get(\"path\").asText();\n-            String pathValueWithoutSlash = pathValue.substring(1);\n+        LoggingLevelResolver levelResolver = new LoggingLevelResolver(desiredMap);\n \n-            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n-                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n-                    .findFirst();\n-\n-            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n-                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n-                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n-                }\n-            }\n-            String op = d.get(\"op\").asText();\n-            if (optEntry.isPresent()) {\n-                ConfigEntry entry = optEntry.get();\n-                if (\"remove\".equals(op)) {\n-                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n-                } else if (\"replace\".equals(op)) {\n-                    // entry is in the current, desired is updated value\n-                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n-                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n-                    }\n-                }\n-            } else {\n-                if (\"add\".equals(op)) {\n-                    // entry is not in the current, it is added\n-                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n-                }\n-            }\n-            if (\"remove\".equals(op)) {\n-                // there is a lot of properties set by default - not having them in desired causes very noisy log output\n-                log.trace(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.trace(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.trace(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n-            } else {\n-                log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        for (ConfigEntry entry: brokerConfigs.entries()) {\n+            LoggingLevel desiredLevel;\n+            try {\n+                desiredLevel = levelResolver.resolveLevel(entry.name());\n+            } catch (IllegalArgumentException e) {\n+                log.warn(\"Skipping {} - it is configured with an unsupported value (\\\"{}\\\")\", entry.name(), e.getMessage());\n+                continue;\n             }\n-        }\n-        return updatedCE;\n-    }\n \n-    private static String parseLogLevelFromAppenderCouple(String level) {\n-        int index = level.indexOf(\",\");\n-        if (index > 0) {\n-            return level.substring(0, index).trim();\n-        } else {\n-            return level.trim();\n+            if (!desiredLevel.name().equals(entry.value())) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(entry.name(), desiredLevel.name()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} has a deprecated value. Setting to {}\", entry.name(), desiredLevel.name());\n+            }\n         }\n-    }\n \n-    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n-        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n-            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n-            if (isValidLoggerLevel(level)) {\n-                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n-                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n-            } else {\n-                log.warn(\"Level {} is not valid logging level\", level);\n+        for (Map.Entry<String, String> ent: desiredMap.entrySet()) {\n+            String name = ent.getKey();\n+            if (name.startsWith(\"log4j.appender\")) {\n+                continue;\n+            }\n+            ConfigEntry configEntry = brokerConfigs.get(name);\n+            if (configEntry == null) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(name, ent.getValue()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set. Setting to {}\", name, ent.getValue());\n             }\n         }\n-    }\n \n-    /**\n-     * All loggers can be set dynamically. If the logger is not set in desire, set it to ERROR. Loggers with already set to ERROR should be skipped.\n-     * ERROR is set as inactive because log4j does not support OFF logger value.\n-     * We want to skip \"root\" logger as well to avoid duplicated key in alterConfigOps collection.\n-     * @param alterConfigOps collection of AlterConfigOp\n-     * @param pathValueWithoutSlash name of \"removed\" logger\n-     * @param entry entry to be removed (set to ERROR)\n-     */\n-    private static void removeProperty(Collection<AlterConfigOp> alterConfigOps, String pathValueWithoutSlash, ConfigEntry entry) {\n-        if (!pathValueWithoutSlash.contains(\"log4j.appender\") && !pathValueWithoutSlash.equals(\"root\") && !\"ERROR\".equals(entry.value())) {\n-            alterConfigOps.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, \"ERROR\"), AlterConfigOp.OpType.SET));\n-            log.trace(\"{} not set in desired, setting to ERROR\", entry.name());\n-        }\n+        return updatedCE;\n     }\n \n     /**\n      * @return whether the current config and the desired config are identical (thus, no update is necessary).\n      */\n     @Override\n     public boolean isEmpty() {\n-        return  diff.size() == 0;\n+        return diff.size() == 0;\n     }\n \n-    private static boolean isValidLoggerLevel(String level) {\n-        return VALID_LOGGER_LEVELS.contains(level);\n+    /**\n+     * This internal class calculates the logging level of an arbitrary category based on the logging configuration.\n+     *\n+     * It takes Log4j properties configuration in the form of a map of key:value pairs,\n+     * where key is the category name, and the value is whatever comes to the right of '=' sign in log4j.properties,\n+     * which is either a logging level, or a logging level followed by a comma, and followed by the appender name.\n+     */\n+    static class LoggingLevelResolver {\n+\n+        private final Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        /**\n+         * The method that returns the logging level of the category\n+         * based on logging configuration, taking inheritance into account.\n+         *\n+         * For example, if looking for a logging level for 'io.strimzi.kafka.oauth.server.OAuthKafkaPrincipalBuilder',\n+         * the following configuration lookups are performed until one is found:\n+         * <ul>\n+         *     <li>io.strimzi.kafka.oauth.server.OAuthKafkaPrincipalBuilder</li>\n+         *     <li>io.strimzi.kafka.oauth.server</li>\n+         *     <li>io.strimzi.kafka.oauth</li>\n+         *     <li>io.strimzi.kafka</li>\n+         *     <li>io.strimzi</li>\n+         *     <li>io</li>\n+         *     <li>root</li>\n+         * </ul>\n+         *\n+         * @param name The logging category name\n+         * @return The logging level\n+         */\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72840982cd84f847fa9ce19face0f747320b3471"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4ODY1MA==", "bodyText": "Not really. Long variables make code harder to read so they should be used judiciously. Here 'e' really stands for 'end index', but seems to me, that should be obvious.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3606#discussion_r486188650", "createdAt": "2020-09-10T09:14:35Z", "author": {"login": "mstruk"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -68,115 +54,120 @@ public int getDiffSize() {\n         if (brokerConfigs == null || desired == null) {\n             return Collections.emptyList();\n         }\n-        Map<String, String> currentMap;\n+\n         Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n-        currentMap = brokerConfigs.entries().stream().collect(\n-            Collectors.toMap(\n-                ConfigEntry::name,\n-                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n \n         OrderedProperties orderedProperties = new OrderedProperties();\n         desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n         desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n         orderedProperties.addStringPairs(desired);\n         Map<String, String> desiredMap = orderedProperties.asMap();\n \n-        ObjectMapper orderedMapper = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n-\n-        JsonNode source = orderedMapper.valueToTree(currentMap);\n-        JsonNode target = orderedMapper.valueToTree(desiredMap);\n-        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n-\n-        for (JsonNode d : jsonDiff) {\n-            String pathValue = d.get(\"path\").asText();\n-            String pathValueWithoutSlash = pathValue.substring(1);\n+        LoggingLevelResolver levelResolver = new LoggingLevelResolver(desiredMap);\n \n-            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n-                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n-                    .findFirst();\n-\n-            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n-                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n-                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n-                }\n-            }\n-            String op = d.get(\"op\").asText();\n-            if (optEntry.isPresent()) {\n-                ConfigEntry entry = optEntry.get();\n-                if (\"remove\".equals(op)) {\n-                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n-                } else if (\"replace\".equals(op)) {\n-                    // entry is in the current, desired is updated value\n-                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n-                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n-                    }\n-                }\n-            } else {\n-                if (\"add\".equals(op)) {\n-                    // entry is not in the current, it is added\n-                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n-                }\n-            }\n-            if (\"remove\".equals(op)) {\n-                // there is a lot of properties set by default - not having them in desired causes very noisy log output\n-                log.trace(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.trace(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.trace(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n-            } else {\n-                log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n-                log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n-                log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        for (ConfigEntry entry: brokerConfigs.entries()) {\n+            LoggingLevel desiredLevel;\n+            try {\n+                desiredLevel = levelResolver.resolveLevel(entry.name());\n+            } catch (IllegalArgumentException e) {\n+                log.warn(\"Skipping {} - it is configured with an unsupported value (\\\"{}\\\")\", entry.name(), e.getMessage());\n+                continue;\n             }\n-        }\n-        return updatedCE;\n-    }\n \n-    private static String parseLogLevelFromAppenderCouple(String level) {\n-        int index = level.indexOf(\",\");\n-        if (index > 0) {\n-            return level.substring(0, index).trim();\n-        } else {\n-            return level.trim();\n+            if (!desiredLevel.name().equals(entry.value())) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(entry.name(), desiredLevel.name()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} has a deprecated value. Setting to {}\", entry.name(), desiredLevel.name());\n+            }\n         }\n-    }\n \n-    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n-        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n-            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n-            if (isValidLoggerLevel(level)) {\n-                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n-                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n-            } else {\n-                log.warn(\"Level {} is not valid logging level\", level);\n+        for (Map.Entry<String, String> ent: desiredMap.entrySet()) {\n+            String name = ent.getKey();\n+            if (name.startsWith(\"log4j.appender\")) {\n+                continue;\n+            }\n+            ConfigEntry configEntry = brokerConfigs.get(name);\n+            if (configEntry == null) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(name, ent.getValue()), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set. Setting to {}\", name, ent.getValue());\n             }\n         }\n-    }\n \n-    /**\n-     * All loggers can be set dynamically. If the logger is not set in desire, set it to ERROR. Loggers with already set to ERROR should be skipped.\n-     * ERROR is set as inactive because log4j does not support OFF logger value.\n-     * We want to skip \"root\" logger as well to avoid duplicated key in alterConfigOps collection.\n-     * @param alterConfigOps collection of AlterConfigOp\n-     * @param pathValueWithoutSlash name of \"removed\" logger\n-     * @param entry entry to be removed (set to ERROR)\n-     */\n-    private static void removeProperty(Collection<AlterConfigOp> alterConfigOps, String pathValueWithoutSlash, ConfigEntry entry) {\n-        if (!pathValueWithoutSlash.contains(\"log4j.appender\") && !pathValueWithoutSlash.equals(\"root\") && !\"ERROR\".equals(entry.value())) {\n-            alterConfigOps.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, \"ERROR\"), AlterConfigOp.OpType.SET));\n-            log.trace(\"{} not set in desired, setting to ERROR\", entry.name());\n-        }\n+        return updatedCE;\n     }\n \n     /**\n      * @return whether the current config and the desired config are identical (thus, no update is necessary).\n      */\n     @Override\n     public boolean isEmpty() {\n-        return  diff.size() == 0;\n+        return diff.size() == 0;\n     }\n \n-    private static boolean isValidLoggerLevel(String level) {\n-        return VALID_LOGGER_LEVELS.contains(level);\n+    /**\n+     * This internal class calculates the logging level of an arbitrary category based on the logging configuration.\n+     *\n+     * It takes Log4j properties configuration in the form of a map of key:value pairs,\n+     * where key is the category name, and the value is whatever comes to the right of '=' sign in log4j.properties,\n+     * which is either a logging level, or a logging level followed by a comma, and followed by the appender name.\n+     */\n+    static class LoggingLevelResolver {\n+\n+        private final Map<String, String> config;\n+\n+        LoggingLevelResolver(Map<String, String> loggingConfig) {\n+            this.config = loggingConfig;\n+        }\n+\n+        /**\n+         * The method that returns the logging level of the category\n+         * based on logging configuration, taking inheritance into account.\n+         *\n+         * For example, if looking for a logging level for 'io.strimzi.kafka.oauth.server.OAuthKafkaPrincipalBuilder',\n+         * the following configuration lookups are performed until one is found:\n+         * <ul>\n+         *     <li>io.strimzi.kafka.oauth.server.OAuthKafkaPrincipalBuilder</li>\n+         *     <li>io.strimzi.kafka.oauth.server</li>\n+         *     <li>io.strimzi.kafka.oauth</li>\n+         *     <li>io.strimzi.kafka</li>\n+         *     <li>io.strimzi</li>\n+         *     <li>io</li>\n+         *     <li>root</li>\n+         * </ul>\n+         *\n+         * @param name The logging category name\n+         * @return The logging level\n+         */\n+        LoggingLevel resolveLevel(String name) {\n+            String level = config.get(name);\n+            if (level != null) {\n+                return LoggingLevel.valueOf(level.split(\",\")[0]);\n+            }\n+\n+            int e = name.length();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgyNDMwMA=="}, "originalCommit": {"oid": "72840982cd84f847fa9ce19face0f747320b3471"}, "originalPosition": 215}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1091, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}