{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4MjgwMjM1", "number": 3405, "reviewThreads": {"totalCount": 59, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTowMTo0OFrOETKapw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMTozNToyOFrOFLDsRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTI5MDYzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTowMTo0OFrOG4vWhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTo0OTozOFrOG4xEfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE0OTI1NA==", "bodyText": "Not something I am asking you to correct, but was there a reason you capitalized Streams, but not kafka?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462149254", "createdAt": "2020-07-29T09:01:48Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,17 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /** The store topic for the kafka Streams based TopicStore */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE3NzQwNQ==", "bodyText": "Typo, both should be capitalized.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462177405", "createdAt": "2020-07-29T09:49:38Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,17 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /** The store topic for the kafka Streams based TopicStore */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE0OTI1NA=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTI5NjY0OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTowMzoyM1rOG4vaJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTo0OTo1MVrOG4xFFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1MDE4Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            /**\n          \n          \n            \n             * Configure all things needed for KafkaStreamsTopicStore\n          \n          \n            \n             */\n          \n          \n            \n            /**\n          \n          \n            \n             * Configuration required for KafkaStreamsTopicStore\n          \n          \n            \n             */", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462150183", "createdAt": "2020-07-29T09:03:23Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE3NzU1OA==", "bodyText": "OK", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462177558", "createdAt": "2020-07-29T09:49:51Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1MDE4Mw=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTMwMzkzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTowNToyNVrOG4vexg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTowNToyNVrOG4vexg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1MTM2Ng==", "bodyText": "Conventionally, in the rest of the code, fields have been annotated like so:\n/* test */ KafkaStreams streams;\n\nAgain not a big deal, but it makes it slightly easier to navigate and understand when the repo is consistent", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462151366", "createdAt": "2020-07-29T09:05:25Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<Object> closeables = new ArrayList<>();\n+\n+    KafkaStreams streams; // exposed for test purposes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTM0MjY1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOToxNTozNlrOG4v2nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTo1MzowMlrOG4xM_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1NzQ2OQ==", "bodyText": "Why do we not just change this to only take Objects of type AutoCloseable ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462157469", "createdAt": "2020-07-29T09:15:36Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<Object> closeables = new ArrayList<>();\n+\n+    KafkaStreams streams; // exposed for test purposes\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+\n+            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+            closeables.add(serviceImpl);\n+\n+            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+            streams = new KafkaStreams(topology, kafkaProperties);\n+            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+            closeables.add(streams);\n+            streams.start();\n+\n+            String appServer = config.get(Config.APPLICATION_SERVER);\n+            String[] hostPort = appServer.split(\":\");\n+            log.info(\"Application server gRPC: '{}'\", appServer);\n+            HostInfo hostInfo = new HostInfo(hostPort[0], Integer.parseInt(hostPort[1]));\n+\n+            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n+\n+            ReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n+                    streams,\n+                    hostInfo,\n+                    storeName,\n+                    Serdes.String(),\n+                    new TopicSerde(),\n+                    new DefaultGrpcChannelProvider(),\n+                    true,\n+                    filter\n+            );\n+            closeables.add(store);\n+\n+            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                    kafkaProperties,\n+                    Serdes.String().serializer(),\n+                    new TopicCommandSerde()\n+            );\n+            closeables.add(producer);\n+\n+            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n+                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n+            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n+                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n+            );\n+            closeables.add(service);\n+\n+            // gRPC\n+\n+            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n+            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n+            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n+            server.start();\n+            AutoCloseable serverCloseable = server::stop;\n+            closeables.add(serverCloseable);\n+\n+            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n+            closeables.add(topicStore);\n+        } catch (Exception e) {\n+            stop(); // stop what we already started for any exception\n+            throw e;\n+        }\n+    }\n+\n+    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n+            KafkaStreams streams,\n+            String storeName,\n+            FilterPredicate<String, Topic> filterPredicate\n+    ) {\n+        return new KeyValueStoreGrpcImplLocalDispatcher(\n+                streams,\n+                KeyValueSerde\n+                        .newRegistry()\n+                        .register(\n+                                storeName,\n+                                Serdes.String(), new TopicSerde()\n+                        ),\n+                filterPredicate\n+        );\n+    }\n+\n+    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n+            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n+    ) {\n+        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n+    }\n+\n+    private Lifecycle streamsGrpcServer(\n+            HostInfo localHost,\n+            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n+            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n+    ) {\n+        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n+                new UnknownStatusDescriptionInterceptor(\n+                        Map.of(\n+                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n+                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n+                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n+                                Throwable.class, Status.INTERNAL\n+                        )\n+                );\n+\n+        Server server = ServerBuilder\n+                .forPort(localHost.port())\n+                .addService(\n+                        ServerInterceptors.intercept(\n+                                streamsStoreGrpcImpl,\n+                                unknownStatusDescriptionInterceptor\n+                        )\n+                )\n+                .addService(\n+                        ServerInterceptors.intercept(\n+                                streamsAsyncBiFunctionServiceGrpcImpl,\n+                                unknownStatusDescriptionInterceptor\n+                        )\n+                )\n+                .build();\n+\n+        return new Lifecycle() {\n+            @Override\n+            public void start() {\n+                try {\n+                    server.start();\n+                } catch (IOException e) {\n+                    throw new UncheckedIOException(e);\n+                }\n+            }\n+\n+            @Override\n+            public void stop() {\n+                ConcurrentUtil\n+                        .<Server>consumer(Server::awaitTermination)\n+                        .accept(server.shutdown());\n+            }\n+\n+            @Override\n+            public boolean isRunning() {\n+                return !(server.isShutdown() || server.isTerminated());\n+            }\n+        };\n+    }\n+\n+    public void stop() {\n+        Collections.reverse(closeables);\n+        closeables.forEach(KafkaStreamsConfiguration::close);\n+    }\n+\n+    public TopicStore getTopicStore() {\n+        return topicStore;\n+    }\n+\n+    private static void close(Object service) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE3OTU4Mw==", "bodyText": "I guess this was a copy/paste, where some proxies got auto-generated/added AutoClosable, but was not part of the interface itself.\nLet me check if this change would work here -- if all references are actually AutoCloseable.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462179583", "createdAt": "2020-07-29T09:53:02Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configure all things needed for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<Object> closeables = new ArrayList<>();\n+\n+    KafkaStreams streams; // exposed for test purposes\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+\n+            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+            closeables.add(serviceImpl);\n+\n+            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+            streams = new KafkaStreams(topology, kafkaProperties);\n+            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+            closeables.add(streams);\n+            streams.start();\n+\n+            String appServer = config.get(Config.APPLICATION_SERVER);\n+            String[] hostPort = appServer.split(\":\");\n+            log.info(\"Application server gRPC: '{}'\", appServer);\n+            HostInfo hostInfo = new HostInfo(hostPort[0], Integer.parseInt(hostPort[1]));\n+\n+            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n+\n+            ReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n+                    streams,\n+                    hostInfo,\n+                    storeName,\n+                    Serdes.String(),\n+                    new TopicSerde(),\n+                    new DefaultGrpcChannelProvider(),\n+                    true,\n+                    filter\n+            );\n+            closeables.add(store);\n+\n+            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                    kafkaProperties,\n+                    Serdes.String().serializer(),\n+                    new TopicCommandSerde()\n+            );\n+            closeables.add(producer);\n+\n+            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n+                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n+            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n+                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n+            );\n+            closeables.add(service);\n+\n+            // gRPC\n+\n+            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n+            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n+            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n+            server.start();\n+            AutoCloseable serverCloseable = server::stop;\n+            closeables.add(serverCloseable);\n+\n+            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n+            closeables.add(topicStore);\n+        } catch (Exception e) {\n+            stop(); // stop what we already started for any exception\n+            throw e;\n+        }\n+    }\n+\n+    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n+            KafkaStreams streams,\n+            String storeName,\n+            FilterPredicate<String, Topic> filterPredicate\n+    ) {\n+        return new KeyValueStoreGrpcImplLocalDispatcher(\n+                streams,\n+                KeyValueSerde\n+                        .newRegistry()\n+                        .register(\n+                                storeName,\n+                                Serdes.String(), new TopicSerde()\n+                        ),\n+                filterPredicate\n+        );\n+    }\n+\n+    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n+            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n+    ) {\n+        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n+    }\n+\n+    private Lifecycle streamsGrpcServer(\n+            HostInfo localHost,\n+            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n+            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n+    ) {\n+        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n+                new UnknownStatusDescriptionInterceptor(\n+                        Map.of(\n+                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n+                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n+                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n+                                Throwable.class, Status.INTERNAL\n+                        )\n+                );\n+\n+        Server server = ServerBuilder\n+                .forPort(localHost.port())\n+                .addService(\n+                        ServerInterceptors.intercept(\n+                                streamsStoreGrpcImpl,\n+                                unknownStatusDescriptionInterceptor\n+                        )\n+                )\n+                .addService(\n+                        ServerInterceptors.intercept(\n+                                streamsAsyncBiFunctionServiceGrpcImpl,\n+                                unknownStatusDescriptionInterceptor\n+                        )\n+                )\n+                .build();\n+\n+        return new Lifecycle() {\n+            @Override\n+            public void start() {\n+                try {\n+                    server.start();\n+                } catch (IOException e) {\n+                    throw new UncheckedIOException(e);\n+                }\n+            }\n+\n+            @Override\n+            public void stop() {\n+                ConcurrentUtil\n+                        .<Server>consumer(Server::awaitTermination)\n+                        .accept(server.shutdown());\n+            }\n+\n+            @Override\n+            public boolean isRunning() {\n+                return !(server.isShutdown() || server.isTerminated());\n+            }\n+        };\n+    }\n+\n+    public void stop() {\n+        Collections.reverse(closeables);\n+        closeables.forEach(KafkaStreamsConfiguration::close);\n+    }\n+\n+    public TopicStore getTopicStore() {\n+        return topicStore;\n+    }\n+\n+    private static void close(Object service) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE1NzQ2OQ=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTM2NTU1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreTestBase.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOToyMTo0NFrOG4wErQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOTo1NDoxMFrOG4xPjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTA2OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        .compose(v -> {\n          \n          \n            \n                            // update my_topic\n          \n          \n            \n                            return store.update(updatedTopic);\n          \n          \n            \n                        })\n          \n          \n            \n                         // update my_topic\n          \n          \n            \n                        .compose(v -> store.update(updatedTopic))", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462161069", "createdAt": "2020-07-29T09:21:44Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreTestBase.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public abstract class TopicStoreTestBase {\n+\n+    protected TopicStore store;\n+\n+    protected abstract boolean canRunTest();\n+\n+    @Test\n+    public void testCrud(VertxTestContext context) {\n+        Assumptions.assumeTrue(canRunTest());\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store.create(topic)\n+            .onComplete(context.succeeding())\n+\n+            // Read the topic\n+            .compose(v -> store.read(new TopicName(topicName)))\n+            .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                // assert topics equal\n+                assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+            })))\n+\n+            // try to create it again: assert an error\n+            .compose(v -> store.create(topic))\n+            .onComplete(context.failing(e -> context.verify(() -> {\n+                assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                failedCreateCompleted.complete();\n+            })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+            .compose(v -> {\n+                // update my_topic\n+                return store.update(updatedTopic);\n+            })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTQ3MQ==", "bodyText": "On another note these tests are really well written!! :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462161471", "createdAt": "2020-07-29T09:22:23Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreTestBase.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public abstract class TopicStoreTestBase {\n+\n+    protected TopicStore store;\n+\n+    protected abstract boolean canRunTest();\n+\n+    @Test\n+    public void testCrud(VertxTestContext context) {\n+        Assumptions.assumeTrue(canRunTest());\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store.create(topic)\n+            .onComplete(context.succeeding())\n+\n+            // Read the topic\n+            .compose(v -> store.read(new TopicName(topicName)))\n+            .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                // assert topics equal\n+                assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+            })))\n+\n+            // try to create it again: assert an error\n+            .compose(v -> store.create(topic))\n+            .onComplete(context.failing(e -> context.verify(() -> {\n+                assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                failedCreateCompleted.complete();\n+            })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+            .compose(v -> {\n+                // update my_topic\n+                return store.update(updatedTopic);\n+            })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTA2OQ=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MDIzOQ==", "bodyText": "This is an old test, I just abstracted it, so I can re-use it with my Kafka Streams TopicStore. :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462180239", "createdAt": "2020-07-29T09:54:10Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreTestBase.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public abstract class TopicStoreTestBase {\n+\n+    protected TopicStore store;\n+\n+    protected abstract boolean canRunTest();\n+\n+    @Test\n+    public void testCrud(VertxTestContext context) {\n+        Assumptions.assumeTrue(canRunTest());\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store.create(topic)\n+            .onComplete(context.succeeding())\n+\n+            // Read the topic\n+            .compose(v -> store.read(new TopicName(topicName)))\n+            .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                // assert topics equal\n+                assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+            })))\n+\n+            // try to create it again: assert an error\n+            .compose(v -> store.create(topic))\n+            .onComplete(context.failing(e -> context.verify(() -> {\n+                assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                failedCreateCompleted.complete();\n+            })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+            .compose(v -> {\n+                // update my_topic\n+                return store.update(updatedTopic);\n+            })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTA2OQ=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTM3MTA5OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwOToyMzoxM1rOG4wICA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMDoxODoxNlrOG4yDow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA==", "bodyText": "This teardown doesn't do anything.. am I missing something here?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462161928", "createdAt": "2020-07-29T09:23:13Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();\n+                })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+                .compose(v -> {\n+                    // update my_topic\n+                    return store2.update(updatedTopic);\n+                })\n+                .onComplete(context.succeeding())\n+\n+                // re-read it and assert equal\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(rereadTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(rereadTopic.getTopicName(), is(updatedTopic.getTopicName()));\n+                    assertThat(rereadTopic.getNumPartitions(), is(updatedTopic.getNumPartitions()));\n+                    assertThat(rereadTopic.getNumReplicas(), is(updatedTopic.getNumReplicas()));\n+                    assertThat(rereadTopic.getConfig(), is(updatedTopic.getConfig()));\n+                })))\n+\n+                // delete it\n+                .compose(v -> store2.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.succeeding())\n+\n+                // assert we can't read it again\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(deletedTopic -> context.verify(() ->\n+                        assertThat(deletedTopic, is(nullValue()))))\n+                )\n+\n+                // delete it again: assert an error\n+                .compose(v -> store1.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.NoSuchEntityExistsException.class));\n+                    async.flag();\n+                })));\n+    }\n+\n+    @AfterEach\n+    public void teardown(VertxTestContext context) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4MzAwNw==", "bodyText": "It does that Checkpoint::flag ... whatever that does , as I've copied this from previous test.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462183007", "createdAt": "2020-07-29T09:59:02Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();\n+                })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+                .compose(v -> {\n+                    // update my_topic\n+                    return store2.update(updatedTopic);\n+                })\n+                .onComplete(context.succeeding())\n+\n+                // re-read it and assert equal\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(rereadTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(rereadTopic.getTopicName(), is(updatedTopic.getTopicName()));\n+                    assertThat(rereadTopic.getNumPartitions(), is(updatedTopic.getNumPartitions()));\n+                    assertThat(rereadTopic.getNumReplicas(), is(updatedTopic.getNumReplicas()));\n+                    assertThat(rereadTopic.getConfig(), is(updatedTopic.getConfig()));\n+                })))\n+\n+                // delete it\n+                .compose(v -> store2.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.succeeding())\n+\n+                // assert we can't read it again\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(deletedTopic -> context.verify(() ->\n+                        assertThat(deletedTopic, is(nullValue()))))\n+                )\n+\n+                // delete it again: assert an error\n+                .compose(v -> store1.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.NoSuchEntityExistsException.class));\n+                    async.flag();\n+                })));\n+    }\n+\n+    @AfterEach\n+    public void teardown(VertxTestContext context) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE4NTc5OA==", "bodyText": "Ah, this is not needed anymore, the checkpont needs to be flagged or it delays a test from finishing, since you aren't waiting for any actual teardown, this is redundant.\nThis is sort of analogous to doing\nfuture = new Future()\nfuture.complete()\nIf you're waiting on nothing it is just not needed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462185798", "createdAt": "2020-07-29T10:03:43Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();\n+                })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+                .compose(v -> {\n+                    // update my_topic\n+                    return store2.update(updatedTopic);\n+                })\n+                .onComplete(context.succeeding())\n+\n+                // re-read it and assert equal\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(rereadTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(rereadTopic.getTopicName(), is(updatedTopic.getTopicName()));\n+                    assertThat(rereadTopic.getNumPartitions(), is(updatedTopic.getNumPartitions()));\n+                    assertThat(rereadTopic.getNumReplicas(), is(updatedTopic.getNumReplicas()));\n+                    assertThat(rereadTopic.getConfig(), is(updatedTopic.getConfig()));\n+                })))\n+\n+                // delete it\n+                .compose(v -> store2.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.succeeding())\n+\n+                // assert we can't read it again\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(deletedTopic -> context.verify(() ->\n+                        assertThat(deletedTopic, is(nullValue()))))\n+                )\n+\n+                // delete it again: assert an error\n+                .compose(v -> store1.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.NoSuchEntityExistsException.class));\n+                    async.flag();\n+                })));\n+    }\n+\n+    @AfterEach\n+    public void teardown(VertxTestContext context) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE5MTczNQ==", "bodyText": "So I can remove this as well in \"KafkaStreamsTopicStoreTest\" ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462191735", "createdAt": "2020-07-29T10:14:32Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();\n+                })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+                .compose(v -> {\n+                    // update my_topic\n+                    return store2.update(updatedTopic);\n+                })\n+                .onComplete(context.succeeding())\n+\n+                // re-read it and assert equal\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(rereadTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(rereadTopic.getTopicName(), is(updatedTopic.getTopicName()));\n+                    assertThat(rereadTopic.getNumPartitions(), is(updatedTopic.getNumPartitions()));\n+                    assertThat(rereadTopic.getNumReplicas(), is(updatedTopic.getNumReplicas()));\n+                    assertThat(rereadTopic.getConfig(), is(updatedTopic.getConfig()));\n+                })))\n+\n+                // delete it\n+                .compose(v -> store2.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.succeeding())\n+\n+                // assert we can't read it again\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(deletedTopic -> context.verify(() ->\n+                        assertThat(deletedTopic, is(nullValue()))))\n+                )\n+\n+                // delete it again: assert an error\n+                .compose(v -> store1.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.NoSuchEntityExistsException.class));\n+                    async.flag();\n+                })));\n+    }\n+\n+    @AfterEach\n+    public void teardown(VertxTestContext context) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE5MzU3MQ==", "bodyText": "Removed ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r462193571", "createdAt": "2020-07-29T10:18:16Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();\n+                })));\n+\n+        Topic updatedTopic = new Topic.Builder(topic)\n+                .withNumPartitions(3)\n+                .withConfigEntry(\"fruit\", \"apple\")\n+                .build();\n+\n+        failedCreateCompleted.future()\n+                .compose(v -> {\n+                    // update my_topic\n+                    return store2.update(updatedTopic);\n+                })\n+                .onComplete(context.succeeding())\n+\n+                // re-read it and assert equal\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(rereadTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(rereadTopic.getTopicName(), is(updatedTopic.getTopicName()));\n+                    assertThat(rereadTopic.getNumPartitions(), is(updatedTopic.getNumPartitions()));\n+                    assertThat(rereadTopic.getNumReplicas(), is(updatedTopic.getNumReplicas()));\n+                    assertThat(rereadTopic.getConfig(), is(updatedTopic.getConfig()));\n+                })))\n+\n+                // delete it\n+                .compose(v -> store2.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.succeeding())\n+\n+                // assert we can't read it again\n+                .compose(v -> store1.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(deletedTopic -> context.verify(() ->\n+                        assertThat(deletedTopic, is(nullValue()))))\n+                )\n+\n+                // delete it again: assert an error\n+                .compose(v -> store1.delete(updatedTopic.getTopicName()))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.NoSuchEntityExistsException.class));\n+                    async.flag();\n+                })));\n+    }\n+\n+    @AfterEach\n+    public void teardown(VertxTestContext context) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjE2MTkyOA=="}, "originalCommit": {"oid": "0ffc554dffbd0be77cf95ffe2b0004d4cb64980f"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg0MTA5OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDoyMDoxMFrOG60KgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjoyODo1OFrOG63lvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTI0OQ==", "bodyText": "This is the name of the store within Kafka Streams?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464325249", "createdAt": "2020-08-03T10:20:10Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"topic-store\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MTM3Mw==", "bodyText": "Yes, this is there those Topic instances (from TopicStore) are stored.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464381373", "createdAt": "2020-08-03T12:28:58Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"topic-store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTI0OQ=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg0MzEyOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDoyMDo1NVrOG60Lxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjoyOToxMVrOG63mKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTU3NQ==", "bodyText": "Can we include the unit?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final String TC_STALE_RESULT_TIMEOUT = \"STRIMZI_STALE_RESULT_TIMEOUT\";\n          \n          \n            \n                public static final String TC_STALE_RESULT_TIMEOUT_MS = \"STRIMZI_STALE_RESULT_TIMEOUT_MS\";", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464325575", "createdAt": "2020-08-03T10:20:55Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -108,6 +108,12 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     public static final String TC_TLS_KEYSTORE_PASSWORD = \"STRIMZI_KEYSTORE_PASSWORD\";\n     public static final String TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = \"STRIMZI_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM\";\n \n+    public static final String TC_STORE_TOPIC = \"STRIMZI_STORE_TOPIC\";\n+    public static final String TC_STORE_NAME = \"STRIMZI_STORE_NAME\";\n+    public static final String TC_APPLICATION_ID = \"STRIMZI_APPLICATION_ID\";\n+    public static final String TC_APPLICATION_SERVER = \"STRIMZI_APPLICATION_SERVER\";\n+    public static final String TC_STALE_RESULT_TIMEOUT = \"STRIMZI_STALE_RESULT_TIMEOUT\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MTQ4Mw==", "bodyText": "Sure.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464381483", "createdAt": "2020-08-03T12:29:11Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -108,6 +108,12 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     public static final String TC_TLS_KEYSTORE_PASSWORD = \"STRIMZI_KEYSTORE_PASSWORD\";\n     public static final String TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = \"STRIMZI_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM\";\n \n+    public static final String TC_STORE_TOPIC = \"STRIMZI_STORE_TOPIC\";\n+    public static final String TC_STORE_NAME = \"STRIMZI_STORE_NAME\";\n+    public static final String TC_APPLICATION_ID = \"STRIMZI_APPLICATION_ID\";\n+    public static final String TC_APPLICATION_SERVER = \"STRIMZI_APPLICATION_SERVER\";\n+    public static final String TC_STALE_RESULT_TIMEOUT = \"STRIMZI_STALE_RESULT_TIMEOUT\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTU3NQ=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg0NTc1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDoyMTo0OFrOG60NbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOTowNjozNFrOHs8rsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng==", "bodyText": "Something like __strimzi_topic_store would be a better default.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464325996", "createdAt": "2020-08-03T10:21:48Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MTg5Mw==", "bodyText": "Yeah, since those log topics get autogenerated from the store name ... will fix it.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464381893", "createdAt": "2020-08-03T12:29:56Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc0Nzk3Mw==", "bodyText": "I got confused here. This is the topic for storing \"topics\" information so, because it's an internal topic in the infrastructure, @tombentley asked to rename it as __strimzi_topic_store while I see this name used for the store name which is not a topic, right?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516747973", "createdAt": "2020-11-03T15:22:14Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0MzcwNg==", "bodyText": "This is the name of the entry topic in our Streams topology.\nWhere we construct a flow for a Kafka message (that is sent from KafkaStreamsTopicStore), so it ends up in KVStore.\nAh, I see I renamed STORE_NAME (which is the name of our KVStore), but forgot to rename this topic name ... doing it now.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516843706", "createdAt": "2020-11-03T17:38:23Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0NDY0NQ==", "bodyText": "@tombentley but it's gonna be \"__strimzi_store_topic\", since it's a topic not a store.\n(you suggested \"__strimzi_topic_store\", which I used for the STORE_NAME)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516844645", "createdAt": "2020-11-03T17:39:57Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg2NDA0Mw==", "bodyText": "But what it stores is topics. It seems weird to call a topic \"...topic\", because obviously it's a topic. I'd be happy with __strimzi_topic_metadata or something if we want to distinguish from the kv store name.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516864043", "createdAt": "2020-11-03T18:12:45Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5MzQyOQ==", "bodyText": "This topic doesn't store anything.\nThis is just the name of the topic where out Streams topology \"starts\",\nwhere we send Kafka messages from KafkaStreamsTopicStore instance.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516893429", "createdAt": "2020-11-03T19:06:14Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg5MzYxOA==", "bodyText": "Btw, I'm happy with whatever name you decide. :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516893618", "createdAt": "2020-11-03T19:06:34Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +165,19 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"store-topic\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMyNTk5Ng=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg3NDgxOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozMTo0MlrOG60euA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNDozNzozNlrOG68IiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDQyNA==", "bodyText": "Since we're already representing the topic as JSON I think it would be better to represent the command as JSON too, then this would pretty much be a call to Jackson's JsonMapper.\nI also wonder about including an explicit version.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464330424", "createdAt": "2020-08-03T10:31:42Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        try {\n+            ByteArrayOutputStream baos = new ByteArrayOutputStream();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQyMjQ3MA==", "bodyText": "Explicit version?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464422470", "createdAt": "2020-08-03T13:44:45Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        try {\n+            ByteArrayOutputStream baos = new ByteArrayOutputStream();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDQyNA=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1NTgxNw==", "bodyText": "Yeah. I considered it at the time for the TopicSerialization and decided that no version was semantically equivalent to having a version=0. That's still true, of course, so I'm not saying you should do this. IOW, I'm still wondering. Only a need to change the format is really going to force me to decide ;-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464455817", "createdAt": "2020-08-03T14:37:36Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        try {\n+            ByteArrayOutputStream baos = new ByteArrayOutputStream();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDQyNA=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg3NTY2OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozMjowNFrOG60fQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozMjowNFrOG60fQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMDU2MA==", "bodyText": "Javadoc", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464330560", "createdAt": "2020-08-03T10:32:04Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStore.java", "diffHunk": "@@ -20,6 +20,10 @@\n \n     }\n \n+    public static class InvalidStateException extends Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg3ODYzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozMzoxMFrOG60hGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozMzoxMFrOG60hGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMTAzNA==", "bodyText": "Since TO has to deal with topic names being represented as Kube resource names, it's always best to be explicit about which name you mean.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    // Key is topic name -- which is also used for KeyValue store key\n          \n          \n            \n                    // Key is Kafka topic name -- which is also used for KeyValue store key", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464331034", "createdAt": "2020-08-03T10:33:10Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg4MjIwOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozNDoyOVrOG60jXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozNDoyOVrOG60jXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMTYxNA==", "bodyText": "Javadoc", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464331614", "createdAt": "2020-08-03T10:34:29Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key\n+        KStream<String, TopicCommand> topicRequest = builder.stream(\n+                storeTopic,\n+                Consumed.with(Serdes.String(), new TopicCommandSerde())\n+        );\n+\n+        // Data structure holds all topic information\n+        StoreBuilder<KeyValueStore<String /* topic */, Topic>> topicStoreBuilder =\n+                Stores\n+                        .keyValueStoreBuilder(\n+                                Stores.inMemoryKeyValueStore(topicStoreName),\n+                                Serdes.String(), new TopicSerde()\n+                        )\n+                        .withCachingEnabled()\n+                        .withLoggingEnabled(configuration);\n+\n+        builder.addStateStore(topicStoreBuilder);\n+\n+        topicRequest.process(\n+            () -> new TopicCommandTransformer(topicStoreName, dispatcher),\n+            topicStoreName\n+        );\n+\n+        return builder.build(kafkaProperties);\n+    }\n+\n+    private static class TopicCommandTransformer implements Processor<String, TopicCommand> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg4NjIzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozNTo1MFrOG60l0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjo0NjoyN1rOG64HBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjI0MQ==", "bodyText": "When we throw EEE like this should be have updated the store by side-effect? I assume not, since I think ZK would throw without updating the stored state.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464332241", "createdAt": "2020-08-03T10:35:50Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key\n+        KStream<String, TopicCommand> topicRequest = builder.stream(\n+                storeTopic,\n+                Consumed.with(Serdes.String(), new TopicCommandSerde())\n+        );\n+\n+        // Data structure holds all topic information\n+        StoreBuilder<KeyValueStore<String /* topic */, Topic>> topicStoreBuilder =\n+                Stores\n+                        .keyValueStoreBuilder(\n+                                Stores.inMemoryKeyValueStore(topicStoreName),\n+                                Serdes.String(), new TopicSerde()\n+                        )\n+                        .withCachingEnabled()\n+                        .withLoggingEnabled(configuration);\n+\n+        builder.addStateStore(topicStoreBuilder);\n+\n+        topicRequest.process(\n+            () -> new TopicCommandTransformer(topicStoreName, dispatcher),\n+            topicStoreName\n+        );\n+\n+        return builder.build(kafkaProperties);\n+    }\n+\n+    private static class TopicCommandTransformer implements Processor<String, TopicCommand> {\n+        private final String topicStoreName;\n+        private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+        private KeyValueStore<String, Topic> store;\n+\n+        public TopicCommandTransformer(\n+                String topicStoreName,\n+                ForeachAction<? super String, ? super Integer> dispatcher\n+        ) {\n+            this.topicStoreName = topicStoreName;\n+            this.dispatcher = dispatcher;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\"unchecked\")\n+        public void init(ProcessorContext context) {\n+            store = (KeyValueStore<String, Topic>) context.getStateStore(topicStoreName);\n+        }\n+\n+        @Override\n+        public void process(String key, TopicCommand value) {\n+            String uuid = value.getUuid();\n+            TopicCommand.Type type = value.getType();\n+            Integer result = null;\n+            switch (type) {\n+                case CREATE:\n+                    Topic previous = store.putIfAbsent(key, value.getTopic());\n+                    if (previous != null) {\n+                        result = KafkaStreamsTopicStore.toIndex(TopicStore.EntityExistsException.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4MzI4OA==", "bodyText": "That's why I used putIfAbsent", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464383288", "createdAt": "2020-08-03T12:32:52Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key\n+        KStream<String, TopicCommand> topicRequest = builder.stream(\n+                storeTopic,\n+                Consumed.with(Serdes.String(), new TopicCommandSerde())\n+        );\n+\n+        // Data structure holds all topic information\n+        StoreBuilder<KeyValueStore<String /* topic */, Topic>> topicStoreBuilder =\n+                Stores\n+                        .keyValueStoreBuilder(\n+                                Stores.inMemoryKeyValueStore(topicStoreName),\n+                                Serdes.String(), new TopicSerde()\n+                        )\n+                        .withCachingEnabled()\n+                        .withLoggingEnabled(configuration);\n+\n+        builder.addStateStore(topicStoreBuilder);\n+\n+        topicRequest.process(\n+            () -> new TopicCommandTransformer(topicStoreName, dispatcher),\n+            topicStoreName\n+        );\n+\n+        return builder.build(kafkaProperties);\n+    }\n+\n+    private static class TopicCommandTransformer implements Processor<String, TopicCommand> {\n+        private final String topicStoreName;\n+        private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+        private KeyValueStore<String, Topic> store;\n+\n+        public TopicCommandTransformer(\n+                String topicStoreName,\n+                ForeachAction<? super String, ? super Integer> dispatcher\n+        ) {\n+            this.topicStoreName = topicStoreName;\n+            this.dispatcher = dispatcher;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\"unchecked\")\n+        public void init(ProcessorContext context) {\n+            store = (KeyValueStore<String, Topic>) context.getStateStore(topicStoreName);\n+        }\n+\n+        @Override\n+        public void process(String key, TopicCommand value) {\n+            String uuid = value.getUuid();\n+            TopicCommand.Type type = value.getType();\n+            Integer result = null;\n+            switch (type) {\n+                case CREATE:\n+                    Topic previous = store.putIfAbsent(key, value.getTopic());\n+                    if (previous != null) {\n+                        result = KafkaStreamsTopicStore.toIndex(TopicStore.EntityExistsException.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjI0MQ=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4OTg5Mw==", "bodyText": "Ah, Mondays!", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464389893", "createdAt": "2020-08-03T12:46:27Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));\n+\n+        // Input topic command -- store topic\n+        // Key is topic name -- which is also used for KeyValue store key\n+        KStream<String, TopicCommand> topicRequest = builder.stream(\n+                storeTopic,\n+                Consumed.with(Serdes.String(), new TopicCommandSerde())\n+        );\n+\n+        // Data structure holds all topic information\n+        StoreBuilder<KeyValueStore<String /* topic */, Topic>> topicStoreBuilder =\n+                Stores\n+                        .keyValueStoreBuilder(\n+                                Stores.inMemoryKeyValueStore(topicStoreName),\n+                                Serdes.String(), new TopicSerde()\n+                        )\n+                        .withCachingEnabled()\n+                        .withLoggingEnabled(configuration);\n+\n+        builder.addStateStore(topicStoreBuilder);\n+\n+        topicRequest.process(\n+            () -> new TopicCommandTransformer(topicStoreName, dispatcher),\n+            topicStoreName\n+        );\n+\n+        return builder.build(kafkaProperties);\n+    }\n+\n+    private static class TopicCommandTransformer implements Processor<String, TopicCommand> {\n+        private final String topicStoreName;\n+        private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+        private KeyValueStore<String, Topic> store;\n+\n+        public TopicCommandTransformer(\n+                String topicStoreName,\n+                ForeachAction<? super String, ? super Integer> dispatcher\n+        ) {\n+            this.topicStoreName = topicStoreName;\n+            this.dispatcher = dispatcher;\n+        }\n+\n+        @Override\n+        @SuppressWarnings(\"unchecked\")\n+        public void init(ProcessorContext context) {\n+            store = (KeyValueStore<String, Topic>) context.getStateStore(topicStoreName);\n+        }\n+\n+        @Override\n+        public void process(String key, TopicCommand value) {\n+            String uuid = value.getUuid();\n+            TopicCommand.Type type = value.getType();\n+            Integer result = null;\n+            switch (type) {\n+                case CREATE:\n+                    Topic previous = store.putIfAbsent(key, value.getTopic());\n+                    if (previous != null) {\n+                        result = KafkaStreamsTopicStore.toIndex(TopicStore.EntityExistsException.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjI0MQ=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg5MDA0OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozNzoxMlrOG60oDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozNzoxMlrOG60oDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMjgxMw==", "bodyText": "Is this used? Can it not be private?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464332813", "createdAt": "2020-08-03T10:37:12Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * We first register CompletableFuture,\n+ * which will be completed once Streams transformer handles CRUD command.\n+ */\n+public class WaitForResultService implements AsyncBiFunctionService.WithSerdes<String, String, Integer> {\n+    public static final String NAME = \"WaitForResultService\";\n+\n+    private final long timeoutMillis;\n+    private final Map<String, ResultCF> waitingResults = new ConcurrentHashMap<>();\n+    private final ScheduledExecutorService executorService;\n+\n+    public WaitForResultService(long timeoutMillis, ForeachActionDispatcher<String, Integer> dispatcher) {\n+        this.timeoutMillis = timeoutMillis;\n+        dispatcher.register(this::topicUpdated);\n+        executorService = new ScheduledThreadPoolExecutor(1);\n+        executorService.scheduleAtFixedRate(this::checkStaleResults, timeoutMillis / 2, timeoutMillis / 2, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private void checkStaleResults() {\n+        long now = System.currentTimeMillis();\n+        Iterator<Map.Entry<String, ResultCF>> iterator = waitingResults.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            ResultCF rcf = iterator.next().getValue();\n+            if (now - rcf.ts > timeoutMillis) {\n+                rcf.complete(KafkaStreamsTopicStore.toIndex(TopicStore.InvalidStateException.class));\n+                iterator.remove();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Notification (from transformer)\n+     */\n+    private void topicUpdated(String uuid, Integer i) {\n+        CompletableFuture<Integer> cf = waitingResults.remove(uuid);\n+        if (cf != null) {\n+            cf.complete(i);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    public Serde<String> keySerde() {\n+        return Serdes.String();\n+    }\n+\n+    @Override\n+    public Serde<String> reqSerde() {\n+        return Serdes.String();\n+    }\n+\n+    @Override\n+    public Serde<Integer> resSerde() {\n+        return Serdes.Integer();\n+    }\n+\n+    @Override\n+    public CompletionStage<Integer> apply(String name, String uuid) {\n+        ResultCF cf = new ResultCF();\n+        waitingResults.put(uuid, cf);\n+        return cf;\n+    }\n+\n+    private static class ResultCF extends CompletableFuture<Integer> {\n+        final long ts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg5MzI4OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDozODoyNVrOG60p7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjozNjoyMlrOG63zmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMzI5Mw==", "bodyText": "Yeah, here we probably need to assert not only that it threw but that the state was not (or perhaps was) updated.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464333293", "createdAt": "2020-08-03T10:38:25Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4NDkyMw==", "bodyText": "How would this look in this vert.x code?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464384923", "createdAt": "2020-08-03T12:36:22Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreClusterTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.vertx.core.Promise;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.junit.jupiter.api.Assumptions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.Collections;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@ExtendWith(VertxExtension.class)\n+public class KafkaStreamsTopicStoreClusterTest {\n+\n+    @Test\n+    public void testCluster(VertxTestContext context) throws Exception {\n+        Assumptions.assumeTrue(KafkaStreamsTopicStoreTest.isKafkaAvailable());\n+\n+        KafkaStreamsConfiguration ksc1 = KafkaStreamsTopicStoreTest.configuration(Collections.emptyMap());\n+        KafkaStreamsConfiguration ksc2 = KafkaStreamsTopicStoreTest.configuration(Collections.singletonMap(Config.APPLICATION_SERVER.key, \"localhost:9001\"));\n+\n+        TopicStore store1 = ksc1.getTopicStore();\n+        TopicStore store2 = ksc2.getTopicStore();\n+\n+        Checkpoint async = context.checkpoint();\n+\n+        String topicName = \"my_topic_\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic topic = new Topic.Builder(topicName, 2,\n+                (short) 3, Collections.singletonMap(\"foo\", \"bar\")).build();\n+\n+        Promise<Void> failedCreateCompleted = Promise.promise();\n+\n+        // Create the topic\n+        store1.create(topic)\n+                .onComplete(context.succeeding())\n+\n+                // Read the topic\n+                .compose(v -> store2.read(new TopicName(topicName)))\n+                .onComplete(context.succeeding(readTopic -> context.verify(() -> {\n+                    // assert topics equal\n+                    assertThat(readTopic.getTopicName(), is(topic.getTopicName()));\n+                    assertThat(readTopic.getNumPartitions(), is(topic.getNumPartitions()));\n+                    assertThat(readTopic.getNumReplicas(), is(topic.getNumReplicas()));\n+                    assertThat(readTopic.getConfig(), is(topic.getConfig()));\n+                })))\n+\n+                // try to create it again: assert an error\n+                .compose(v -> store2.create(topic))\n+                .onComplete(context.failing(e -> context.verify(() -> {\n+                    assertThat(e, instanceOf(TopicStore.EntityExistsException.class));\n+                    failedCreateCompleted.complete();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzMzI5Mw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTg5OTMzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/ZkTopicStoreTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDo0MDo0MlrOG60tmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjozNToxMFrOG63xYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzNDIzMg==", "bodyText": "Why are we changing this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464334232", "createdAt": "2020-08-03T10:40:42Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/ZkTopicStoreTest.java", "diffHunk": "@@ -6,38 +6,28 @@\n \n import io.strimzi.operator.topic.zk.Zk;\n import io.strimzi.test.EmbeddedZooKeeper;\n-import io.vertx.core.Promise;\n import io.vertx.core.Vertx;\n import io.vertx.junit5.Checkpoint;\n-import io.vertx.junit5.VertxExtension;\n import io.vertx.junit5.VertxTestContext;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Disabled;\n-import org.junit.jupiter.api.Test;\n-import org.junit.jupiter.api.extension.ExtendWith;\n \n import java.io.IOException;\n-import java.util.Collections;\n \n-import static org.hamcrest.CoreMatchers.instanceOf;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.hamcrest.CoreMatchers.nullValue;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-\n-@Disabled\n-@ExtendWith(VertxExtension.class)\n-public class ZkTopicStoreTest {\n-\n-    private EmbeddedZooKeeper zkServer;\n+public class ZkTopicStoreTest extends TopicStoreTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4NDM1Mw==", "bodyText": "To not duplicate the tests -- I extracted them to TopicStoreTestBase class.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464384353", "createdAt": "2020-08-03T12:35:10Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/ZkTopicStoreTest.java", "diffHunk": "@@ -6,38 +6,28 @@\n \n import io.strimzi.operator.topic.zk.Zk;\n import io.strimzi.test.EmbeddedZooKeeper;\n-import io.vertx.core.Promise;\n import io.vertx.core.Vertx;\n import io.vertx.junit5.Checkpoint;\n-import io.vertx.junit5.VertxExtension;\n import io.vertx.junit5.VertxTestContext;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Disabled;\n-import org.junit.jupiter.api.Test;\n-import org.junit.jupiter.api.extension.ExtendWith;\n \n import java.io.IOException;\n-import java.util.Collections;\n \n-import static org.hamcrest.CoreMatchers.instanceOf;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.hamcrest.CoreMatchers.nullValue;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-\n-@Disabled\n-@ExtendWith(VertxExtension.class)\n-public class ZkTopicStoreTest {\n-\n-    private EmbeddedZooKeeper zkServer;\n+public class ZkTopicStoreTest extends TopicStoreTestBase {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzNDIzMg=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTkyODEyOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDo1MDo1NVrOG60-1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNDo1MzowM1rOG68upw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw==", "bodyText": "We should probably validate the topic at this point. We don't really want to rely on autocreation because :\n\nThat will eventually be removed.\nIt's quite easy to end up with a misconfigured topic that way (e.g. if default RF=1).\n\nIt's a little tricky because the TO needs to work in clusters with only 1 broker, but in clusters with 3 or more we'd want RF=3 and min.insync.replicas=2. You should be able to use the Kafka Admin client something like this:\n\nDescribe the cluster.\nCheck whether the topic exists:\nIf the topic does not exist try to create it with RF=min(3, clusterSize) and minISR=RF-1. If created OK we're good. If not (e.g. authorization) then log an error telling the user to create it and exit.\nIf the topic already exists check whether RF=min(3, clusterSize) && minISR=RF-1. If it does we're good. If not log a warning that the durability of the topic is not sufficient for production use and proceed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464338647", "createdAt": "2020-08-03T10:50:55Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4ODg3MA==", "bodyText": "What do you mean under \"describe the cluster\"?\nI guess AdminClient has some API for this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464388870", "createdAt": "2020-08-03T12:44:26Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM5MDg3MQ==", "bodyText": "Yes, describeCluster(), describeTopics() and describeConfigs() are all probably relevant to doing this.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464390871", "createdAt": "2020-08-03T12:48:12Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQwMzE2OA==", "bodyText": "If not (e.g. authorization) then log an error telling the user to create it and exit.\n\n@tombentley out of curiosity, doesn't the TO uses a super-user for doing that anyway?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464403168", "createdAt": "2020-08-03T13:11:28Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ0ODkzOQ==", "bodyText": "@tombentley how many partitions?\nAnd how do I set minISR when creating a new topic?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464448939", "createdAt": "2020-08-03T14:27:19Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MjA1Mw==", "bodyText": "@ppatierno is does when it's deployed by the CO, but there are users who run it stand alone, with Kafka clusters not managed by the CO, so we can't assume we have authz.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464452053", "createdAt": "2020-08-03T14:32:02Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MzkwMQ==", "bodyText": "@alesj min.insync.replicas is a topic config, so NewTopic(...).configs(topicConfigs) will do it (yeah, the fact there there's not a constructor taking the configs makes it a little less discoverable).", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464453901", "createdAt": "2020-08-03T14:34:43Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1NDYwMw==", "bodyText": "tbh I am not sure how many people are using TO in this way but I have no clue otherwise I would remove the support for it.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464454603", "createdAt": "2020-08-03T14:35:43Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1OTI3NA==", "bodyText": "BTH @ppatierno supporting that use case adds very little additional complexity. Removing it would only serve to have fewer users, not simplify anything significantly.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464459274", "createdAt": "2020-08-03T14:42:50Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ2NTU3NQ==", "bodyText": "I know, it's just about a warning :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464465575", "createdAt": "2020-08-03T14:53:03Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODY0Nw=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTkzNzg5OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDo1NDoxMFrOG61Eag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjo0MjowN1rOG63-lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM0MDA3NA==", "bodyText": "As discussed elsewhere, we don't really need the distributed aspect of this. If there some other implementation which would avoid the need for the GRPC layer?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464340074", "createdAt": "2020-08-03T10:54:10Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+\n+            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+            closeables.add(serviceImpl);\n+\n+            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+            streams = new KafkaStreams(topology, kafkaProperties);\n+            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+            closeables.add(streams);\n+            streams.start();\n+\n+            String appServer = config.get(Config.APPLICATION_SERVER);\n+            String[] hostPort = appServer.split(\":\");\n+            log.info(\"Application server gRPC: '{}'\", appServer);\n+            HostInfo hostInfo = new HostInfo(hostPort[0], Integer.parseInt(hostPort[1]));\n+\n+            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n+\n+            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM4NzczMg==", "bodyText": "If it's only ever gonna be single instance, then we're good with Streams already provide -- some sort of in-memory KVStore.\nOtoh, I would somehow leave my current distributed code -- so it doesn't go to waste,\nor if someone needs more then one instance.\nHow do you suggest to do this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464387732", "createdAt": "2020-08-03T12:42:07Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.streams.diservice.LocalService;\n+import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.streams.distore.FilterPredicate;\n+import io.apicurio.registry.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n+import io.apicurio.registry.streams.utils.Lifecycle;\n+import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    private TopicStore topicStore;\n+\n+    public void start(Config config, Properties kafkaProperties) {\n+        try {\n+            String storeTopic = config.get(Config.STORE_TOPIC);\n+            String storeName = config.get(Config.STORE_NAME);\n+\n+            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+            closeables.add(serviceImpl);\n+\n+            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+            streams = new KafkaStreams(topology, kafkaProperties);\n+            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+            closeables.add(streams);\n+            streams.start();\n+\n+            String appServer = config.get(Config.APPLICATION_SERVER);\n+            String[] hostPort = appServer.split(\":\");\n+            log.info(\"Application server gRPC: '{}'\", appServer);\n+            HostInfo hostInfo = new HostInfo(hostPort[0], Integer.parseInt(hostPort[1]));\n+\n+            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n+\n+            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM0MDA3NA=="}, "originalCommit": {"oid": "3f76fa46b74fdd815a7b4c631704e639821047be"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMzMyMjQ2OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNzozODowMlrOG7UuUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwOToxMzo1M1rOG7YD9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODcwNA==", "bodyText": "should we check that it's really just one partition and not more than that?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464858704", "createdAt": "2020-08-04T07:38:02Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwOTQ0NQ==", "bodyText": "If it's more than one partition, which replicas size do we take then? Max, avg, min, ... ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464909445", "createdAt": "2020-08-04T09:06:54Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODcwNA=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMzM5Ng==", "bodyText": "Kafka doesn't really have a first-class concept of RF even though it's routinely talked about. Different partitions of the same topic can be replicated to different numbers of brokers. And indeed, that might happen (transiently) even in a well-managed cluster, e.g. increasing RF, but doing it in a batched way. For our purposes here using the min, or just using partition 0 would make sense, but I guess using the min might be preferable since it detects the case of inhomogeneous topic.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464913396", "createdAt": "2020-08-04T09:13:53Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1ODcwNA=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMzMyODY1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNzozOTo1MlrOG7Ux8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxMjoyNTo0N1rOG7eDeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw==", "bodyText": "the entire logic of this code is kind of blocking because of the usage of the get calls on the different KafkaFuture.\nI was wondering if we need a more async way (so maybe some Vert.x executeBlocking calls) to do that but @tombentley knows better than me if it fits well in the overall TO logic calling this piece of code.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464859633", "createdAt": "2020-08-04T07:39:52Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMDI1NQ==", "bodyText": "This is configuration validation, and the topic needs to exist before Kafka Streams kick-in.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464910255", "createdAt": "2020-08-04T09:08:21Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTU0MQ==", "bodyText": "Yeah, I know but it doesn't mean that the code cannot be async. You can chain async calls in order to do configuration first and then starting Kafka Streams. But as I said I don't know better the calling code and the overall TO architecture, so @tombentley can answer to that. Maybe I am wrong :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464911541", "createdAt": "2020-08-04T09:10:33Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxNzUzMw==", "bodyText": "You should be able to use KafkaFuture.whenComplete() in a similar way to vertx's Future.compose(). This is what KafkaAvailability does IIRC. It's also possible to convert KafkaFuture to vertx Future (see io.strimzi.operator.common.Util#kafkaFutureToVertxFuture, and note that it completes the Future on the vertx event loop thread).", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464917533", "createdAt": "2020-08-04T09:20:38Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkzMTMwMw==", "bodyText": "Do we really want/need to drag Vert.x into this configuration?\nAfais, it's 2-3 get() calls, which shouldn't take long.\nIt would just make this configuration even more complex -- I'm already over the class import limit. :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464931303", "createdAt": "2020-08-04T09:44:36Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkzNjIzMg==", "bodyText": "This is how our operators are developed ... the code is async everywhere using Vert.x. We wrapped the Kubernetes REST API calls in the same way and of course any kind of call to Kafka as Admin client. I don't think you can make any assumption on the time needed for the get() calls.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r464936232", "createdAt": "2020-08-04T09:53:19Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAxMTU3Ng==", "bodyText": "OK, let me try to impl this config::start in an async way then.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465011576", "createdAt": "2020-08-04T12:25:47Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -58,7 +67,34 @@ public void start(Config config, Properties kafkaProperties) {\n             String storeTopic = config.get(Config.STORE_TOPIC);\n             String storeName = config.get(Config.STORE_NAME);\n \n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT);\n+            // check if entry topic has the right configuration\n+            Admin admin = Admin.create(kafkaProperties);\n+            DescribeClusterResult clusterResult = admin.describeCluster();\n+            int clusterSize = clusterResult.nodes().get().size();\n+            Set<String> topics = admin.listTopics().names().get();\n+            if (topics.contains(storeTopic)) {\n+                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n+                int rf = topicDescription.partitions().get(0).replicas().size();\n+                ConfigResource isrCR = new ConfigResource(ConfigResource.Type.TOPIC, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG);\n+                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(isrCR));\n+                int minISR = 0;\n+                try {\n+                    minISR = Integer.parseInt(configsResult.values().get(isrCR).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n+                } catch (Exception ignored) {\n+                }\n+                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n+                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n+                }\n+            } else {\n+                int rf = Math.min(3, clusterSize);\n+                int minISR = rf - 1;\n+                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                admin.createTopics(Collections.singleton(newTopic)).all().get();\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTYzMw=="}, "originalCommit": {"oid": "72bf07efc3d05b13a22a7ce15c7c94126a5fc873"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNTI5OTY0OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNjoxNDozMVrOG7nn0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwNzo0Mzo1OFrOG7-D0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2ODMzNw==", "bodyText": "I think the message should be more actionable: What do they need to do to make this warning do away? And there's no harm is mentioning the topic name explicitly.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465168337", "createdAt": "2020-08-04T16:14:31Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -4,246 +4,163 @@\n  */\n package io.strimzi.operator.topic;\n \n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n-import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n-import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.LocalService;\n-import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n-import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n-import io.apicurio.registry.streams.distore.FilterPredicate;\n-import io.apicurio.registry.streams.distore.KeyValueSerde;\n-import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n-import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n-import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n-import io.apicurio.registry.streams.utils.Lifecycle;\n import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n-import io.apicurio.registry.utils.ConcurrentUtil;\n import io.apicurio.registry.utils.kafka.AsyncProducer;\n import io.apicurio.registry.utils.kafka.ProducerActions;\n-import io.grpc.Server;\n-import io.grpc.ServerBuilder;\n-import io.grpc.ServerInterceptors;\n-import io.grpc.Status;\n import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.admin.DescribeClusterResult;\n-import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.common.KafkaFuture;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.streams.KafkaStreams;\n import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.errors.InvalidStateStoreException;\n-import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.IOException;\n-import java.io.UncheckedIOException;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n import java.util.Properties;\n import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n \n import static java.lang.Integer.parseInt;\n \n /**\n  * Configuration required for KafkaStreamsTopicStore\n  */\n-@SuppressWarnings(\"checkstyle:ClassDataAbstractionCoupling\")\n public class KafkaStreamsConfiguration {\n     private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n \n     private final List<AutoCloseable> closeables = new ArrayList<>();\n \n     /* test */ KafkaStreams streams;\n-    private TopicStore topicStore;\n-\n-    public void start(Config config, Properties kafkaProperties) {\n-        try {\n-            String storeTopic = config.get(Config.STORE_TOPIC);\n-            String storeName = config.get(Config.STORE_NAME);\n-\n-            // check if entry topic has the right configuration\n-            Admin admin = Admin.create(kafkaProperties);\n-            DescribeClusterResult clusterResult = admin.describeCluster();\n-            int clusterSize = clusterResult.nodes().get().size();\n-            Set<String> topics = admin.listTopics().names().get();\n-            if (topics.contains(storeTopic)) {\n-                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n-                int rf = topicDescription.partitions().get(0).replicas().size();\n-                ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n-                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(storeTopicConfigResource));\n-                int minISR = parseInt(configsResult.values().get(storeTopicConfigResource).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n-                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n-                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n-                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n-                }\n-            } else {\n-                int rf = Math.min(3, clusterSize);\n-                int minISR = rf - 1;\n-                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n-                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n-                admin.createTopics(Collections.singleton(newTopic)).all().get();\n-            }\n-\n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n-            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n-            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n-            closeables.add(serviceImpl);\n-\n-            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n-            streams = new KafkaStreams(topology, kafkaProperties);\n-            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n-            closeables.add(streams);\n-            streams.start();\n-\n-            String appServer = config.get(Config.APPLICATION_SERVER);\n-            String[] hostPort = appServer.split(\":\");\n-            log.info(\"Application server gRPC: '{}'\", appServer);\n-            HostInfo hostInfo = new HostInfo(hostPort[0], parseInt(hostPort[1]));\n-\n-            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n-\n-            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n-                    streams,\n-                    hostInfo,\n-                    storeName,\n-                    Serdes.String(),\n-                    new TopicSerde(),\n-                    new DefaultGrpcChannelProvider(),\n-                    true,\n-                    filter\n-            );\n-            closeables.add(store);\n-\n-            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n-                    kafkaProperties,\n-                    Serdes.String().serializer(),\n-                    new TopicCommandSerde()\n-            );\n-            closeables.add(producer);\n-\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n-                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n-            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n-                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n-            );\n-            closeables.add(service);\n-\n-            // gRPC\n-\n-            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n-            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n-            server.start();\n-            AutoCloseable serverCloseable = server::stop;\n-            closeables.add(serverCloseable);\n-\n-            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n-        } catch (Exception e) {\n-            stop(); // stop what we already started for any exception\n-            throw new IllegalStateException(e);\n-        }\n-    }\n-\n-    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n-            KafkaStreams streams,\n-            String storeName,\n-            FilterPredicate<String, Topic> filterPredicate\n-    ) {\n-        return new KeyValueStoreGrpcImplLocalDispatcher(\n-                streams,\n-                KeyValueSerde\n-                        .newRegistry()\n-                        .register(\n-                                storeName,\n-                                Serdes.String(), new TopicSerde()\n-                        ),\n-                filterPredicate\n-        );\n-    }\n-\n-    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n-    ) {\n-        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n-    }\n-\n-    private Lifecycle streamsGrpcServer(\n-            HostInfo localHost,\n-            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n-    ) {\n-        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n-                new UnknownStatusDescriptionInterceptor(\n-                        Map.of(\n-                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n-                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n-                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n-                                Throwable.class, Status.INTERNAL\n-                        )\n-                );\n-\n-        Server server = ServerBuilder\n-                .forPort(localHost.port())\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsStoreGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsAsyncBiFunctionServiceGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .build();\n-\n-        return new Lifecycle() {\n-            @Override\n-            public void start() {\n-                try {\n-                    server.start();\n-                } catch (IOException e) {\n-                    throw new UncheckedIOException(e);\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                                        c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MTc2Ng==", "bodyText": "What would make it go away?\n\"Replication factor should be at least 3 and {} one less.\" ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465191766", "createdAt": "2020-08-04T16:52:19Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -4,246 +4,163 @@\n  */\n package io.strimzi.operator.topic;\n \n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n-import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n-import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.LocalService;\n-import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n-import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n-import io.apicurio.registry.streams.distore.FilterPredicate;\n-import io.apicurio.registry.streams.distore.KeyValueSerde;\n-import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n-import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n-import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n-import io.apicurio.registry.streams.utils.Lifecycle;\n import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n-import io.apicurio.registry.utils.ConcurrentUtil;\n import io.apicurio.registry.utils.kafka.AsyncProducer;\n import io.apicurio.registry.utils.kafka.ProducerActions;\n-import io.grpc.Server;\n-import io.grpc.ServerBuilder;\n-import io.grpc.ServerInterceptors;\n-import io.grpc.Status;\n import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.admin.DescribeClusterResult;\n-import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.common.KafkaFuture;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.streams.KafkaStreams;\n import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.errors.InvalidStateStoreException;\n-import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.IOException;\n-import java.io.UncheckedIOException;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n import java.util.Properties;\n import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n \n import static java.lang.Integer.parseInt;\n \n /**\n  * Configuration required for KafkaStreamsTopicStore\n  */\n-@SuppressWarnings(\"checkstyle:ClassDataAbstractionCoupling\")\n public class KafkaStreamsConfiguration {\n     private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n \n     private final List<AutoCloseable> closeables = new ArrayList<>();\n \n     /* test */ KafkaStreams streams;\n-    private TopicStore topicStore;\n-\n-    public void start(Config config, Properties kafkaProperties) {\n-        try {\n-            String storeTopic = config.get(Config.STORE_TOPIC);\n-            String storeName = config.get(Config.STORE_NAME);\n-\n-            // check if entry topic has the right configuration\n-            Admin admin = Admin.create(kafkaProperties);\n-            DescribeClusterResult clusterResult = admin.describeCluster();\n-            int clusterSize = clusterResult.nodes().get().size();\n-            Set<String> topics = admin.listTopics().names().get();\n-            if (topics.contains(storeTopic)) {\n-                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n-                int rf = topicDescription.partitions().get(0).replicas().size();\n-                ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n-                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(storeTopicConfigResource));\n-                int minISR = parseInt(configsResult.values().get(storeTopicConfigResource).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n-                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n-                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n-                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n-                }\n-            } else {\n-                int rf = Math.min(3, clusterSize);\n-                int minISR = rf - 1;\n-                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n-                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n-                admin.createTopics(Collections.singleton(newTopic)).all().get();\n-            }\n-\n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n-            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n-            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n-            closeables.add(serviceImpl);\n-\n-            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n-            streams = new KafkaStreams(topology, kafkaProperties);\n-            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n-            closeables.add(streams);\n-            streams.start();\n-\n-            String appServer = config.get(Config.APPLICATION_SERVER);\n-            String[] hostPort = appServer.split(\":\");\n-            log.info(\"Application server gRPC: '{}'\", appServer);\n-            HostInfo hostInfo = new HostInfo(hostPort[0], parseInt(hostPort[1]));\n-\n-            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n-\n-            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n-                    streams,\n-                    hostInfo,\n-                    storeName,\n-                    Serdes.String(),\n-                    new TopicSerde(),\n-                    new DefaultGrpcChannelProvider(),\n-                    true,\n-                    filter\n-            );\n-            closeables.add(store);\n-\n-            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n-                    kafkaProperties,\n-                    Serdes.String().serializer(),\n-                    new TopicCommandSerde()\n-            );\n-            closeables.add(producer);\n-\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n-                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n-            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n-                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n-            );\n-            closeables.add(service);\n-\n-            // gRPC\n-\n-            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n-            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n-            server.start();\n-            AutoCloseable serverCloseable = server::stop;\n-            closeables.add(serverCloseable);\n-\n-            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n-        } catch (Exception e) {\n-            stop(); // stop what we already started for any exception\n-            throw new IllegalStateException(e);\n-        }\n-    }\n-\n-    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n-            KafkaStreams streams,\n-            String storeName,\n-            FilterPredicate<String, Topic> filterPredicate\n-    ) {\n-        return new KeyValueStoreGrpcImplLocalDispatcher(\n-                streams,\n-                KeyValueSerde\n-                        .newRegistry()\n-                        .register(\n-                                storeName,\n-                                Serdes.String(), new TopicSerde()\n-                        ),\n-                filterPredicate\n-        );\n-    }\n-\n-    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n-    ) {\n-        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n-    }\n-\n-    private Lifecycle streamsGrpcServer(\n-            HostInfo localHost,\n-            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n-    ) {\n-        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n-                new UnknownStatusDescriptionInterceptor(\n-                        Map.of(\n-                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n-                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n-                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n-                                Throwable.class, Status.INTERNAL\n-                        )\n-                );\n-\n-        Server server = ServerBuilder\n-                .forPort(localHost.port())\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsStoreGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsAsyncBiFunctionServiceGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .build();\n-\n-        return new Lifecycle() {\n-            @Override\n-            public void start() {\n-                try {\n-                    server.start();\n-                } catch (IOException e) {\n-                    throw new UncheckedIOException(e);\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                                        c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2ODMzNw=="}, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTUzNTk1Mg==", "bodyText": "Increase the replication factor of topic {} to at least 3 and configure the min.in.sync.replicas to {}", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465535952", "createdAt": "2020-08-05T07:43:58Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -4,246 +4,163 @@\n  */\n package io.strimzi.operator.topic;\n \n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n-import io.apicurio.registry.streams.diservice.DefaultGrpcChannelProvider;\n-import io.apicurio.registry.streams.diservice.DistributedAsyncBiFunctionService;\n-import io.apicurio.registry.streams.diservice.LocalService;\n-import io.apicurio.registry.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n-import io.apicurio.registry.streams.distore.DistributedReadOnlyKeyValueStore;\n-import io.apicurio.registry.streams.distore.FilterPredicate;\n-import io.apicurio.registry.streams.distore.KeyValueSerde;\n-import io.apicurio.registry.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n-import io.apicurio.registry.streams.distore.UnknownStatusDescriptionInterceptor;\n-import io.apicurio.registry.streams.distore.proto.KeyValueStoreGrpc;\n import io.apicurio.registry.streams.utils.ForeachActionDispatcher;\n-import io.apicurio.registry.streams.utils.Lifecycle;\n import io.apicurio.registry.streams.utils.LoggingStateRestoreListener;\n-import io.apicurio.registry.utils.ConcurrentUtil;\n import io.apicurio.registry.utils.kafka.AsyncProducer;\n import io.apicurio.registry.utils.kafka.ProducerActions;\n-import io.grpc.Server;\n-import io.grpc.ServerBuilder;\n-import io.grpc.ServerInterceptors;\n-import io.grpc.Status;\n import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.admin.DescribeClusterResult;\n-import org.apache.kafka.clients.admin.DescribeConfigsResult;\n import org.apache.kafka.clients.admin.NewTopic;\n-import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.common.KafkaFuture;\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.config.TopicConfig;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.streams.KafkaStreams;\n import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.errors.InvalidStateStoreException;\n-import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.IOException;\n-import java.io.UncheckedIOException;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n import java.util.Properties;\n import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n \n import static java.lang.Integer.parseInt;\n \n /**\n  * Configuration required for KafkaStreamsTopicStore\n  */\n-@SuppressWarnings(\"checkstyle:ClassDataAbstractionCoupling\")\n public class KafkaStreamsConfiguration {\n     private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n \n     private final List<AutoCloseable> closeables = new ArrayList<>();\n \n     /* test */ KafkaStreams streams;\n-    private TopicStore topicStore;\n-\n-    public void start(Config config, Properties kafkaProperties) {\n-        try {\n-            String storeTopic = config.get(Config.STORE_TOPIC);\n-            String storeName = config.get(Config.STORE_NAME);\n-\n-            // check if entry topic has the right configuration\n-            Admin admin = Admin.create(kafkaProperties);\n-            DescribeClusterResult clusterResult = admin.describeCluster();\n-            int clusterSize = clusterResult.nodes().get().size();\n-            Set<String> topics = admin.listTopics().names().get();\n-            if (topics.contains(storeTopic)) {\n-                TopicDescription topicDescription = admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic).get();\n-                int rf = topicDescription.partitions().get(0).replicas().size();\n-                ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n-                DescribeConfigsResult configsResult = admin.describeConfigs(Collections.singleton(storeTopicConfigResource));\n-                int minISR = parseInt(configsResult.values().get(storeTopicConfigResource).get().get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value());\n-                if (rf != Math.min(3, clusterSize) || minISR != rf - 1) {\n-                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n-                            rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, minISR);\n-                }\n-            } else {\n-                int rf = Math.min(3, clusterSize);\n-                int minISR = rf - 1;\n-                NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n-                        .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n-                admin.createTopics(Collections.singleton(newTopic)).all().get();\n-            }\n-\n-            long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n-            ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n-            WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n-            closeables.add(serviceImpl);\n-\n-            Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n-            streams = new KafkaStreams(topology, kafkaProperties);\n-            streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n-            closeables.add(streams);\n-            streams.start();\n-\n-            String appServer = config.get(Config.APPLICATION_SERVER);\n-            String[] hostPort = appServer.split(\":\");\n-            log.info(\"Application server gRPC: '{}'\", appServer);\n-            HostInfo hostInfo = new HostInfo(hostPort[0], parseInt(hostPort[1]));\n-\n-            FilterPredicate<String, Topic> filter = (s, s1, s2, topic) -> true;\n-\n-            DistributedReadOnlyKeyValueStore<String, Topic> store = new DistributedReadOnlyKeyValueStore<>(\n-                    streams,\n-                    hostInfo,\n-                    storeName,\n-                    Serdes.String(),\n-                    new TopicSerde(),\n-                    new DefaultGrpcChannelProvider(),\n-                    true,\n-                    filter\n-            );\n-            closeables.add(store);\n-\n-            ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n-                    kafkaProperties,\n-                    Serdes.String().serializer(),\n-                    new TopicCommandSerde()\n-            );\n-            closeables.add(producer);\n-\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localService =\n-                    new LocalService<>(WaitForResultService.NAME, serviceImpl);\n-            AsyncBiFunctionService<String, String, Integer> service = new DistributedAsyncBiFunctionService<>(\n-                    streams, hostInfo, storeName, localService, new DefaultGrpcChannelProvider()\n-            );\n-            closeables.add(service);\n-\n-            // gRPC\n-\n-            KeyValueStoreGrpc.KeyValueStoreImplBase kvGrpc = streamsKeyValueStoreGrpcImpl(streams, storeName, filter);\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase fnGrpc = streamsAsyncBiFunctionServiceGrpcImpl(localService);\n-            Lifecycle server = streamsGrpcServer(hostInfo, kvGrpc, fnGrpc);\n-            server.start();\n-            AutoCloseable serverCloseable = server::stop;\n-            closeables.add(serverCloseable);\n-\n-            topicStore = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n-        } catch (Exception e) {\n-            stop(); // stop what we already started for any exception\n-            throw new IllegalStateException(e);\n-        }\n-    }\n-\n-    private KeyValueStoreGrpc.KeyValueStoreImplBase streamsKeyValueStoreGrpcImpl(\n-            KafkaStreams streams,\n-            String storeName,\n-            FilterPredicate<String, Topic> filterPredicate\n-    ) {\n-        return new KeyValueStoreGrpcImplLocalDispatcher(\n-                streams,\n-                KeyValueSerde\n-                        .newRegistry()\n-                        .register(\n-                                storeName,\n-                                Serdes.String(), new TopicSerde()\n-                        ),\n-                filterPredicate\n-        );\n-    }\n-\n-    private AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl(\n-            LocalService<AsyncBiFunctionService.WithSerdes<String, String, Integer>> localWaitForResultService\n-    ) {\n-        return new AsyncBiFunctionServiceGrpcLocalDispatcher(Collections.singletonList(localWaitForResultService));\n-    }\n-\n-    private Lifecycle streamsGrpcServer(\n-            HostInfo localHost,\n-            KeyValueStoreGrpc.KeyValueStoreImplBase streamsStoreGrpcImpl,\n-            AsyncBiFunctionServiceGrpc.AsyncBiFunctionServiceImplBase streamsAsyncBiFunctionServiceGrpcImpl\n-    ) {\n-        UnknownStatusDescriptionInterceptor unknownStatusDescriptionInterceptor =\n-                new UnknownStatusDescriptionInterceptor(\n-                        Map.of(\n-                                IllegalArgumentException.class, Status.INVALID_ARGUMENT,\n-                                IllegalStateException.class, Status.FAILED_PRECONDITION,\n-                                InvalidStateStoreException.class, Status.FAILED_PRECONDITION,\n-                                Throwable.class, Status.INTERNAL\n-                        )\n-                );\n-\n-        Server server = ServerBuilder\n-                .forPort(localHost.port())\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsStoreGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .addService(\n-                        ServerInterceptors.intercept(\n-                                streamsAsyncBiFunctionServiceGrpcImpl,\n-                                unknownStatusDescriptionInterceptor\n-                        )\n-                )\n-                .build();\n-\n-        return new Lifecycle() {\n-            @Override\n-            public void start() {\n-                try {\n-                    server.start();\n-                } catch (IOException e) {\n-                    throw new UncheckedIOException(e);\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic is not sufficient for production use - replicationFactor: {}, {}: {}\",\n+                                        c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2ODMzNw=="}, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 239}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNTMwODcxOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNjoxNjozNVrOG7ntUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNjoxNjozNVrOG7ntUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE2OTc0Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Simple local / in-memory store configuration.\n          \n          \n            \n             * Simple local / in-memory store configuration which is sufficient when the TO runs in a single pod.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465169747", "createdAt": "2020-08-04T16:16:35Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * Simple local / in-memory store configuration.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNTM1NjI0OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Zk2KafkaStreams.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNjoyODoyNFrOG7oKcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMToyMzoyOVrOHCO-FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ==", "bodyText": "I think we're going to need something more than just a tool which the user has to run in order to migrate. I think we need the TO to do the migration automatically on start up:\nif !exists(topic) {\n    if exists(`/strimzi` znode) {\n        migrate using the logic you have here. \n        if migrated ok {\n            delete(`/strimzi` znode)\n        } else {\n            delete(topic)\n            exit process\n        }\n    }\n}\n\nWe're also going to need some integration test for this.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465177201", "createdAt": "2020-08-04T16:28:24Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Zk2KafkaStreams.java", "diffHunk": "@@ -18,24 +18,23 @@ public static void main(String[] args) {\n         // TODO\n     }\n \n-    public static void move(Zk zk, Config config, Properties kafkaProperties) {\n+    public static void move(Zk zk, Config config, Properties kafkaProperties) throws Exception {\n         String topicsPath = config.get(Config.TOPICS_PATH);\n         TopicStore zkTopicStore = new ZkTopicStore(zk, topicsPath);\n \n         KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n-        try {\n-            configuration.start(config, kafkaProperties);\n-            TopicStore ksTopicStore = configuration.getTopicStore();\n-\n-            zk.children(topicsPath, result -> result.map(list -> {\n-                list.forEach(topicName -> {\n-                    Future<Topic> ft = zkTopicStore.read(new TopicName(topicName));\n-                    ft.onSuccess(ksTopicStore::create);\n-                });\n-                return null;\n-            }));\n-        } finally {\n-            configuration.stop();\n-        }\n+        configuration.start(config, kafkaProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5MjQ4NQ==", "bodyText": "This is just a PoC, something to discuss, WIP.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r465192485", "createdAt": "2020-08-04T16:53:29Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Zk2KafkaStreams.java", "diffHunk": "@@ -18,24 +18,23 @@ public static void main(String[] args) {\n         // TODO\n     }\n \n-    public static void move(Zk zk, Config config, Properties kafkaProperties) {\n+    public static void move(Zk zk, Config config, Properties kafkaProperties) throws Exception {\n         String topicsPath = config.get(Config.TOPICS_PATH);\n         TopicStore zkTopicStore = new ZkTopicStore(zk, topicsPath);\n \n         KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n-        try {\n-            configuration.start(config, kafkaProperties);\n-            TopicStore ksTopicStore = configuration.getTopicStore();\n-\n-            zk.children(topicsPath, result -> result.map(list -> {\n-                list.forEach(topicName -> {\n-                    Future<Topic> ft = zkTopicStore.read(new TopicName(topicName));\n-                    ft.onSuccess(ksTopicStore::create);\n-                });\n-                return null;\n-            }));\n-        } finally {\n-            configuration.stop();\n-        }\n+        configuration.start(config, kafkaProperties)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ=="}, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM0Mjg4Ng==", "bodyText": "Any progress on this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r471342886", "createdAt": "2020-08-17T09:05:34Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Zk2KafkaStreams.java", "diffHunk": "@@ -18,24 +18,23 @@ public static void main(String[] args) {\n         // TODO\n     }\n \n-    public static void move(Zk zk, Config config, Properties kafkaProperties) {\n+    public static void move(Zk zk, Config config, Properties kafkaProperties) throws Exception {\n         String topicsPath = config.get(Config.TOPICS_PATH);\n         TopicStore zkTopicStore = new ZkTopicStore(zk, topicsPath);\n \n         KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n-        try {\n-            configuration.start(config, kafkaProperties);\n-            TopicStore ksTopicStore = configuration.getTopicStore();\n-\n-            zk.children(topicsPath, result -> result.map(list -> {\n-                list.forEach(topicName -> {\n-                    Future<Topic> ft = zkTopicStore.read(new TopicName(topicName));\n-                    ft.onSuccess(ksTopicStore::create);\n-                });\n-                return null;\n-            }));\n-        } finally {\n-            configuration.stop();\n-        }\n+        configuration.start(config, kafkaProperties)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ=="}, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNDQ2OA==", "bodyText": "Not yet.\nNeed to think of a way to test this ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r472104468", "createdAt": "2020-08-18T11:23:29Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Zk2KafkaStreams.java", "diffHunk": "@@ -18,24 +18,23 @@ public static void main(String[] args) {\n         // TODO\n     }\n \n-    public static void move(Zk zk, Config config, Properties kafkaProperties) {\n+    public static void move(Zk zk, Config config, Properties kafkaProperties) throws Exception {\n         String topicsPath = config.get(Config.TOPICS_PATH);\n         TopicStore zkTopicStore = new ZkTopicStore(zk, topicsPath);\n \n         KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n-        try {\n-            configuration.start(config, kafkaProperties);\n-            TopicStore ksTopicStore = configuration.getTopicStore();\n-\n-            zk.children(topicsPath, result -> result.map(list -> {\n-                list.forEach(topicName -> {\n-                    Future<Topic> ft = zkTopicStore.read(new TopicName(topicName));\n-                    ft.onSuccess(ksTopicStore::create);\n-                });\n-                return null;\n-            }));\n-        } finally {\n-            configuration.stop();\n-        }\n+        configuration.start(config, kafkaProperties)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE3NzIwMQ=="}, "originalCommit": {"oid": "72eba48b82d18923e67ad20d175fd61e42ab87c9"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMjgyNTAxOnYy", "diffSide": "RIGHT", "path": "topic-operator/design/topic-store.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNzo0NzoxOVrOHNDS5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMDoxNjo1NFrOHN5nfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQ0NzUyNg==", "bodyText": "Slightly pedantic point: I don't think the TopicStore interface is used by the watchers, though they do use the underlying ZK client abstraction.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r483447526", "createdAt": "2020-09-04T07:47:19Z", "author": {"login": "tombentley"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,122 @@\n+# Topic store (new Kafka Streams based implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either K8S or Kafka.\n+Currently we have ZooKeeper based working implementation, but with ZooKeeper being removed \n+as part of KIP-500, we decided to implement the store directly in/on Kafka, its Streams\n+extension to be exact.\n+\n+TopicStore interface is a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is used from TopicOperator, which is used from web/Vert.x invocations\n+and ZooKeeper watcher callbacks. The later - ZooKeeper watcher callbacks - also need", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d622b3caecc6295cf632aab08d7c272bad759bd"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDI4Nzk2Nw==", "bodyText": "Re-reading this, I did mean TO being used from Vert.x and ZK watchers.\nBut I guess it should be more clear, so you don't think TS is directly used by the watchers.\nAny suggestion how to reword / rephrase this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r484287967", "createdAt": "2020-09-07T08:52:21Z", "author": {"login": "alesj"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,122 @@\n+# Topic store (new Kafka Streams based implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either K8S or Kafka.\n+Currently we have ZooKeeper based working implementation, but with ZooKeeper being removed \n+as part of KIP-500, we decided to implement the store directly in/on Kafka, its Streams\n+extension to be exact.\n+\n+TopicStore interface is a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is used from TopicOperator, which is used from web/Vert.x invocations\n+and ZooKeeper watcher callbacks. The later - ZooKeeper watcher callbacks - also need", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQ0NzUyNg=="}, "originalCommit": {"oid": "3d622b3caecc6295cf632aab08d7c272bad759bd"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDMzNzUzNQ==", "bodyText": "Something like\nThe TopicStore is only used by the TopicOperator. \nThe ZooKeeper watcher callbacks trigger TopicOperator reconciliations, and this notification mechanism will also need to be replaced before KIP-500 is fully implemented.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r484337535", "createdAt": "2020-09-07T10:16:54Z", "author": {"login": "tombentley"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,122 @@\n+# Topic store (new Kafka Streams based implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either K8S or Kafka.\n+Currently we have ZooKeeper based working implementation, but with ZooKeeper being removed \n+as part of KIP-500, we decided to implement the store directly in/on Kafka, its Streams\n+extension to be exact.\n+\n+TopicStore interface is a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is used from TopicOperator, which is used from web/Vert.x invocations\n+and ZooKeeper watcher callbacks. The later - ZooKeeper watcher callbacks - also need", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzQ0NzUyNg=="}, "originalCommit": {"oid": "3d622b3caecc6295cf632aab08d7c272bad759bd"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDYxNjkxOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjozMDozOFrOHXjH8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToxMDoyN1rOHX7hmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NDc2OA==", "bodyText": "Can we document somewhere, either on the methods or the class that configure() must be called before getStore() and getLookupService() can return non-null?\nAlso I find the name a bit confusing. It's not really the configuration of a store, it's more of a factory for stores and lookups.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494454768", "createdAt": "2020-09-24T16:30:38Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * SPI for the store configuration.\n+ * * in-memory / local\n+ * * distributed\n+ */\n+interface StoreConfiguration {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg1NDU1Mg==", "bodyText": "OK, StoreAndServiceFactory it is.\nAnd it's now just one method, create which returns StoreContext (a tuple with store and service).", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494854552", "createdAt": "2020-09-25T09:10:27Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * SPI for the store configuration.\n+ * * in-memory / local\n+ * * distributed\n+ */\n+interface StoreConfiguration {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NDc2OA=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDYyODg3OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/DistributedStoreConfiguration.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjozMzo1MlrOHXjPZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjozMzo1MlrOHXjPZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NjY3Nw==", "bodyText": "You're assuming the appServer actually contains a (single) :. Best to validate.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494456677", "createdAt": "2020-09-24T16:33:52Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/DistributedStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionServiceGrpcLocalDispatcher;\n+import io.apicurio.registry.utils.streams.diservice.DefaultGrpcChannelProvider;\n+import io.apicurio.registry.utils.streams.diservice.DistributedAsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.diservice.LocalService;\n+import io.apicurio.registry.utils.streams.diservice.proto.AsyncBiFunctionServiceGrpc;\n+import io.apicurio.registry.utils.streams.distore.DistributedReadOnlyKeyValueStore;\n+import io.apicurio.registry.utils.streams.distore.FilterPredicate;\n+import io.apicurio.registry.utils.streams.distore.KeyValueSerde;\n+import io.apicurio.registry.utils.streams.distore.KeyValueStoreGrpcImplLocalDispatcher;\n+import io.apicurio.registry.utils.streams.distore.UnknownStatusDescriptionInterceptor;\n+import io.apicurio.registry.utils.streams.distore.proto.KeyValueStoreGrpc;\n+import io.apicurio.registry.utils.streams.ext.Lifecycle;\n+import io.grpc.Server;\n+import io.grpc.ServerBuilder;\n+import io.grpc.ServerInterceptors;\n+import io.grpc.Status;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n+import org.apache.kafka.streams.state.HostInfo;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Add configuration for distributed store (via gRPC).\n+ * Required when we're running more than one instance/node of topic operator.\n+ */\n+class DistributedStoreConfiguration implements StoreConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(DistributedStoreConfiguration.class);\n+\n+    private ReadOnlyKeyValueStore<String, Topic> store;\n+    private BiFunction<String, String, CompletionStage<Integer>> lookupService;\n+\n+    @Override\n+    public ReadOnlyKeyValueStore<String, Topic> getStore() {\n+        return store;\n+    }\n+\n+    @Override\n+    public BiFunction<String, String, CompletionStage<Integer>> getLookupService() {\n+        return lookupService;\n+    }\n+\n+    public void configure(\n+            Config config,\n+            Properties kafkaProperties,\n+            KafkaStreams streams,\n+            AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl,\n+            List<AutoCloseable> closeables\n+    ) {\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        String appServer = config.get(Config.APPLICATION_SERVER);\n+        String[] hostPort = appServer.split(\":\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDYzNTA1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjozNToyM1rOHXjTMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyMjozN1rOHX78HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NzY1MQ==", "bodyText": "We should document this method. The passing in of closeables which the method adds to it a little non-intuitive and what are the String parameters to the lookupService? From the doc for AsyncBiFunctionService I learn that they're keys and requests, but this doesn't really answer the question \"what are they\"?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494457651", "createdAt": "2020-09-24T16:35:23Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * SPI for the store configuration.\n+ * * in-memory / local\n+ * * distributed\n+ */\n+interface StoreConfiguration {\n+    void configure(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2MTM0MA==", "bodyText": "Docs added.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494861340", "createdAt": "2020-09-25T09:22:37Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/StoreConfiguration.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * SPI for the store configuration.\n+ * * in-memory / local\n+ * * distributed\n+ */\n+interface StoreConfiguration {\n+    void configure(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ1NzY1MQ=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDY2Njk1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo0MzozOVrOHXjm9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyMjoyMFrOHX77fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MjcwOQ==", "bodyText": "This class does not represent configuration (as in, a bunch of values, properties for configuring something). Rather its start() configures Kafka streams and returns a CompletionState of a topic store which happens to use Kafka streams, but it's clearly a component with a lifecycle (it has a stop). So wouldn't something like KafkaStreamTopicStoreService be a better name?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494462709", "createdAt": "2020-09-24T16:43:39Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2MTE4MA==", "bodyText": "OK, renamed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494861180", "createdAt": "2020-09-25T09:22:20Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MjcwOQ=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDY3MTk2OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo0NDo1NFrOHXjqGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyODo0MFrOHX8J1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzUxMw==", "bodyText": "It would help readability if you factored the sequence of actions which you compose into separate methods with sensible names.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494463513", "createdAt": "2020-09-24T16:44:54Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2NDg1Mg==", "bodyText": "Done.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494864852", "createdAt": "2020-09-25T09:28:40Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzUxMw=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDY3MzExOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo0NTowN1rOHXjqvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo0NTowN1rOHXjqvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzY3OA==", "bodyText": "Another method here", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494463678", "createdAt": "2020-09-24T16:45:07Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })\n+                .thenCompose(v -> {\n+                    long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+                    ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+                    WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+                    closeables.add(serviceImpl);\n+\n+                    boolean[] done = new boolean[1];\n+                    CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+                    KafkaStreams.StateListener listener = (newState, oldState) -> {\n+                        if (newState == KafkaStreams.State.RUNNING && !done[0]) {\n+                            done[0] = true; // no need for dup complete\n+                            cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+                        }\n+                        if (newState == KafkaStreams.State.ERROR) {\n+                            cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+                        }\n+                    };\n+\n+                    Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+                    streams = new KafkaStreams(topology, kafkaProperties);\n+                    streams.setStateListener(listener);\n+                    streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+                    closeables.add(streams);\n+                    streams.start();\n+\n+                    return cf;\n+                })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDY3NDExOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo0NToyMVrOHXjrTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo0NToyMVrOHXjrTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2MzgyMA==", "bodyText": "And another method here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494463820", "createdAt": "2020-09-24T16:45:21Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })\n+                .thenCompose(v -> {\n+                    long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+                    ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+                    WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+                    closeables.add(serviceImpl);\n+\n+                    boolean[] done = new boolean[1];\n+                    CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+                    KafkaStreams.StateListener listener = (newState, oldState) -> {\n+                        if (newState == KafkaStreams.State.RUNNING && !done[0]) {\n+                            done[0] = true; // no need for dup complete\n+                            cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+                        }\n+                        if (newState == KafkaStreams.State.ERROR) {\n+                            cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+                        }\n+                    };\n+\n+                    Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+                    streams = new KafkaStreams(topology, kafkaProperties);\n+                    streams.setStateListener(listener);\n+                    streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+                    closeables.add(streams);\n+                    streams.start();\n+\n+                    return cf;\n+                })\n+                .thenApply(serviceImpl -> {\n+                    ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                        kafkaProperties,\n+                        Serdes.String().serializer(),\n+                        new TopicCommandSerde()\n+                    );\n+                    closeables.add(producer);\n+\n+                    StoreConfiguration storeConfiguration;\n+                    if (config.get(Config.DISTRIBUTED_STORE)) {\n+                        storeConfiguration = new DistributedStoreConfiguration();\n+                    } else {\n+                        storeConfiguration = new LocalStoreConfiguration();\n+                    }\n+                    storeConfiguration.configure(config, kafkaProperties, streams, serviceImpl, closeables);\n+                    ReadOnlyKeyValueStore<String, Topic> store = storeConfiguration.getStore();\n+                    BiFunction<String, String, CompletionStage<Integer>> service = storeConfiguration.getLookupService();\n+\n+                    this.store = new KafkaStreamsTopicStore(store, storeTopic, producer, service);\n+                    return this.store;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDY5MTg4OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1MDowMFrOHXj2fA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1MDowMFrOHXj2fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2NjY4NA==", "bodyText": "Does it need to be public? If so you should document the contract with the thing that calls it with integers.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494466684", "createdAt": "2020-09-24T16:50:00Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDcwNjY0OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1Mzo1NlrOHXj__A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjo1OTowMFrOHYCjMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2OTExNg==", "bodyText": "I think a little javadoc explaining what stale results are and why they need to be checked would be valuable.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494469116", "createdAt": "2020-09-24T16:53:56Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * We first register CompletableFuture,\n+ * which will be completed once Streams transformer handles CRUD command.\n+ */\n+public class WaitForResultService implements AsyncBiFunctionService.WithSerdes<String, String, Integer> {\n+    public static final String NAME = \"WaitForResultService\";\n+\n+    private final long timeoutMillis;\n+    private final Map<String, ResultCF> waitingResults = new ConcurrentHashMap<>();\n+    private final ScheduledExecutorService executorService;\n+\n+    public WaitForResultService(long timeoutMillis, ForeachActionDispatcher<String, Integer> dispatcher) {\n+        this.timeoutMillis = timeoutMillis;\n+        dispatcher.register(this::topicUpdated);\n+        executorService = new ScheduledThreadPoolExecutor(1);\n+        executorService.scheduleAtFixedRate(this::checkStaleResults, timeoutMillis / 2, timeoutMillis / 2, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private void checkStaleResults() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2OTY1MA==", "bodyText": "Can you add this doc?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494969650", "createdAt": "2020-09-25T12:59:00Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/WaitForResultService.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serdes;\n+\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * We first register CompletableFuture,\n+ * which will be completed once Streams transformer handles CRUD command.\n+ */\n+public class WaitForResultService implements AsyncBiFunctionService.WithSerdes<String, String, Integer> {\n+    public static final String NAME = \"WaitForResultService\";\n+\n+    private final long timeoutMillis;\n+    private final Map<String, ResultCF> waitingResults = new ConcurrentHashMap<>();\n+    private final ScheduledExecutorService executorService;\n+\n+    public WaitForResultService(long timeoutMillis, ForeachActionDispatcher<String, Integer> dispatcher) {\n+        this.timeoutMillis = timeoutMillis;\n+        dispatcher.register(this::topicUpdated);\n+        executorService = new ScheduledThreadPoolExecutor(1);\n+        executorService.scheduleAtFixedRate(this::checkStaleResults, timeoutMillis / 2, timeoutMillis / 2, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private void checkStaleResults() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ2OTExNg=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDcxMjg5OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1NTo0MFrOHXkEKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1NTo0MFrOHXkEKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDE4NA==", "bodyText": "This should fail the test, right?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470184", "createdAt": "2020-09-24T16:55:40Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDcxMzIyOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1NTo0NVrOHXkEZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyODozNFrOHX8JjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDI0Ng==", "bodyText": "This should fail the test, right?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470246", "createdAt": "2020-09-24T16:55:45Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2NDc4MQ==", "bodyText": "Ah, a debugging leftover ... will fix it ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494864781", "createdAt": "2020-09-25T09:28:34Z", "author": {"login": "alesj"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDI0Ng=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDcxNjY4OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1Njo0MlrOHXkGrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1Njo0MlrOHXkGrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDgyOA==", "bodyText": "You should check that the Future returned by read() succeeded before you go using the result.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470828", "createdAt": "2020-09-24T16:56:42Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+\n+        Properties kafkaProperties = new Properties();\n+        kafkaProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, config.get(Config.KAFKA_BOOTSTRAP_SERVERS));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, config.get(Config.APPLICATION_ID));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, config.get(Config.APPLICATION_SERVER));\n+\n+        ConcurrentUtil.result(Zk2KafkaStreams.upgrade(zk, config, kafkaProperties, true));\n+\n+        KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n+        try {\n+            TopicStore kTS = ConcurrentUtil.result(configuration.start(config, kafkaProperties));\n+            Topic topic1 = kTS.read(new TopicName(tn1)).result();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDcxNzA4OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1Njo0OVrOHXkG6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1Njo0OVrOHXkG6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MDg5MA==", "bodyText": "Same comment.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494470890", "createdAt": "2020-09-24T16:56:49Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/test/java/io/strimzi/operator/topic/TopicStoreUpgradeTest.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.debezium.kafka.KafkaCluster;\n+import io.strimzi.operator.topic.zk.Zk;\n+import io.vertx.core.Vertx;\n+import io.vertx.junit5.Checkpoint;\n+import io.vertx.junit5.VertxExtension;\n+import io.vertx.junit5.VertxTestContext;\n+import org.I0Itec.zkclient.ZkClient;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+@ExtendWith(VertxExtension.class)\n+public class TopicStoreUpgradeTest {\n+    private static final Map<String, String> MANDATORY_CONFIG;\n+\n+    static {\n+        MANDATORY_CONFIG = new HashMap<>();\n+        MANDATORY_CONFIG.put(Config.NAMESPACE.key, \"default\");\n+        MANDATORY_CONFIG.put(Config.TC_TOPICS_PATH, \"/strimzi/topics\");\n+        MANDATORY_CONFIG.put(Config.TC_STALE_RESULT_TIMEOUT_MS, \"5000\"); //5sec\n+    }\n+\n+    private static Vertx vertx;\n+\n+    private Zk zk;\n+\n+    @Test\n+    public void testUpgrade() {\n+        Config config = new Config(MANDATORY_CONFIG);\n+\n+        String topicsPath = config.get(Config.TOPICS_PATH);\n+        TopicStore zkTS = new ZkTopicStore(zk, topicsPath);\n+        String tn1 = \"Topic1\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t1 = new Topic.Builder(tn1, 1).build();\n+        zkTS.create(t1).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+        String tn2 = \"Topic2\" + ThreadLocalRandom.current().nextInt(Integer.MAX_VALUE);\n+        Topic t2 = new Topic.Builder(tn2, 1).build();\n+        zkTS.create(t2).onComplete(ar -> {\n+            if (ar.failed()) {\n+                System.err.println(\"ZkTS create failure: \" + ar.cause());\n+            }\n+        }).result();\n+\n+        Properties kafkaProperties = new Properties();\n+        kafkaProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, config.get(Config.KAFKA_BOOTSTRAP_SERVERS));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_ID_CONFIG, config.get(Config.APPLICATION_ID));\n+        kafkaProperties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, config.get(Config.APPLICATION_SERVER));\n+\n+        ConcurrentUtil.result(Zk2KafkaStreams.upgrade(zk, config, kafkaProperties, true));\n+\n+        KafkaStreamsConfiguration configuration = new KafkaStreamsConfiguration();\n+        try {\n+            TopicStore kTS = ConcurrentUtil.result(configuration.start(config, kafkaProperties));\n+            Topic topic1 = kTS.read(new TopicName(tn1)).result();\n+            Assertions.assertNotNull(topic1);\n+            Topic topic2 = kTS.read(new TopicName(tn2)).result();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDcyNDI2OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1ODo0NVrOHXkLlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNjo1ODo0NVrOHXkLlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3MjA4Ng==", "bodyText": "It wasn't clear to me that this listener is always run on the same thread (and note that it's implicitly initialized to {false} on this thread). It would be safer to just use AtomicBoolean I think.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494472086", "createdAt": "2020-09-24T16:58:45Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsConfiguration.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * Configuration required for KafkaStreamsTopicStore\n+ */\n+public class KafkaStreamsConfiguration {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsConfiguration.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+                        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+                            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+                            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+                            .thenApply(c3 -> {\n+                                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {\n+                                    log.warn(\"Durability of the topic [{}] is not sufficient for production use - replicationFactor: {}, {}: {}. \" +\n+                                            \"Increase the replication factor to at least 3 and configure the {} to {}.\",\n+                                            storeTopic, c3.rf, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR, TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, c3.minISR);\n+                                }\n+                                return null;\n+                            });\n+                    } else {\n+                        int rf = Math.min(3, c.clusterSize);\n+                        int minISR = Math.max(rf - 1, 1);\n+                        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+                            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+                        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+                    }\n+                })\n+                .thenCompose(v -> {\n+                    long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+                    ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+                    WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+                    closeables.add(serviceImpl);\n+\n+                    boolean[] done = new boolean[1];\n+                    CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+                    KafkaStreams.StateListener listener = (newState, oldState) -> {\n+                        if (newState == KafkaStreams.State.RUNNING && !done[0]) {\n+                            done[0] = true; // no need for dup complete", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NDc0ODUwOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxNzowNToyOVrOHXkbXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOTo1MjozNlrOHX9Ahg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjEyNQ==", "bodyText": "AFAICS this is plain old double checked locking. I don't see how it can be thread safe (if I'm wrong can you explain). But I rather suspect that it's not worth trying to optimize here and just making it synchronized invoke() would give adequate performance and be correct from a thread safety pov.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494476125", "createdAt": "2020-09-24T17:05:29Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * Simple local / in-memory store configuration which is sufficient when the TO runs in a single pod\n+ */\n+class LocalStoreConfiguration implements StoreConfiguration {\n+    private ReadOnlyKeyValueStore<String, Topic> store;\n+    private BiFunction<String, String, CompletionStage<Integer>> lookupService;\n+\n+    @Override\n+    public ReadOnlyKeyValueStore<String, Topic> getStore() {\n+        return store;\n+    }\n+\n+    @Override\n+    public BiFunction<String, String, CompletionStage<Integer>> getLookupService() {\n+        return lookupService;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void configure(\n+            Config config,\n+            Properties kafkaProperties,\n+            KafkaStreams streams,\n+            AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl,\n+            List<AutoCloseable> closeables\n+    ) {\n+        String storeName = config.get(Config.STORE_NAME);\n+        // we need to lazily create the store as streams might not be ready yet\n+        store = (ReadOnlyKeyValueStore<String, Topic>) Proxy.newProxyInstance(\n+                getClass().getClassLoader(),\n+                new Class[]{ReadOnlyKeyValueStore.class},\n+                new InvocationHandler() {\n+                    private ReadOnlyKeyValueStore<String, Topic> store;\n+\n+                    @Override\n+                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n+                        if (store == null) {\n+                            synchronized (this) {\n+                                if (store == null) {\n+                                    store = streams.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3MDM2OA==", "bodyText": "This is classic double check to handle concurrency, and you don't need full synchronized on the method.\nBut yeah, no need to optimize this, will just use plain sync on the method.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494870368", "createdAt": "2020-09-25T09:38:23Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * Simple local / in-memory store configuration which is sufficient when the TO runs in a single pod\n+ */\n+class LocalStoreConfiguration implements StoreConfiguration {\n+    private ReadOnlyKeyValueStore<String, Topic> store;\n+    private BiFunction<String, String, CompletionStage<Integer>> lookupService;\n+\n+    @Override\n+    public ReadOnlyKeyValueStore<String, Topic> getStore() {\n+        return store;\n+    }\n+\n+    @Override\n+    public BiFunction<String, String, CompletionStage<Integer>> getLookupService() {\n+        return lookupService;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void configure(\n+            Config config,\n+            Properties kafkaProperties,\n+            KafkaStreams streams,\n+            AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl,\n+            List<AutoCloseable> closeables\n+    ) {\n+        String storeName = config.get(Config.STORE_NAME);\n+        // we need to lazily create the store as streams might not be ready yet\n+        store = (ReadOnlyKeyValueStore<String, Topic>) Proxy.newProxyInstance(\n+                getClass().getClassLoader(),\n+                new Class[]{ReadOnlyKeyValueStore.class},\n+                new InvocationHandler() {\n+                    private ReadOnlyKeyValueStore<String, Topic> store;\n+\n+                    @Override\n+                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n+                        if (store == null) {\n+                            synchronized (this) {\n+                                if (store == null) {\n+                                    store = streams.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjEyNQ=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3ODg1NA==", "bodyText": "AFAIU (and I don't claim to be an expert) double checked locking doesn't work because although the reference to store is handled correctly it's still possible that the state of the allocated object get published unsafely. I don't what streams.store() does, but assuming it's not synchronized I think you have to assume that it returns a new object and so the construction overall is not safe. This is why I think just making the whole thing synchronized makes sense here, because it safes us from having to reason about whether DCL is really safe here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494878854", "createdAt": "2020-09-25T09:52:36Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/LocalStoreConfiguration.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StoreQueryParameters;\n+import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * Simple local / in-memory store configuration which is sufficient when the TO runs in a single pod\n+ */\n+class LocalStoreConfiguration implements StoreConfiguration {\n+    private ReadOnlyKeyValueStore<String, Topic> store;\n+    private BiFunction<String, String, CompletionStage<Integer>> lookupService;\n+\n+    @Override\n+    public ReadOnlyKeyValueStore<String, Topic> getStore() {\n+        return store;\n+    }\n+\n+    @Override\n+    public BiFunction<String, String, CompletionStage<Integer>> getLookupService() {\n+        return lookupService;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void configure(\n+            Config config,\n+            Properties kafkaProperties,\n+            KafkaStreams streams,\n+            AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl,\n+            List<AutoCloseable> closeables\n+    ) {\n+        String storeName = config.get(Config.STORE_NAME);\n+        // we need to lazily create the store as streams might not be ready yet\n+        store = (ReadOnlyKeyValueStore<String, Topic>) Proxy.newProxyInstance(\n+                getClass().getClassLoader(),\n+                new Class[]{ReadOnlyKeyValueStore.class},\n+                new InvocationHandler() {\n+                    private ReadOnlyKeyValueStore<String, Topic> store;\n+\n+                    @Override\n+                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n+                        if (store == null) {\n+                            synchronized (this) {\n+                                if (store == null) {\n+                                    store = streams.store(StoreQueryParameters.fromNameAndType(storeName, QueryableStoreTypes.keyValueStore()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ3NjEyNQ=="}, "originalCommit": {"oid": "e860774cf6c5ce60f8ece0fb7a532dd11bc9531a"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Nzk0NTUyOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjo1Mjo1OFrOHYCWfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjo1Mjo1OFrOHYCWfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2NjM5Nw==", "bodyText": "It's probably a better idea to give the enum members an explict id rather than rely on the declaration order remaining unchanged.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494966397", "createdAt": "2020-09-25T12:52:58Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Nzk0OTIzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjo1NDowN1rOHYCYxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo1NzowMVrOHbw30A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ==", "bodyText": "I can't remember if I asked this before, but maybe it's a good idea to add an explicit version property to the JSON so the schema could be easily evolved?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494966981", "createdAt": "2020-09-25T12:54:07Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMzAxOQ==", "bodyText": "Do we have version elsewhere?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498713019", "createdAt": "2020-10-02T09:25:36Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMzM5NA==", "bodyText": "So you would be able to properly (de)serialize things based on the version?\nIs this really necessary? Or we could just adjust the code to understand any potential changes in the future.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498713394", "createdAt": "2020-10-02T09:26:21Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg3NDMyMA==", "bodyText": "So that deserialization is unambiguous, indeed. Kafka does this religiously everywhere data gets persisted. I know we don't expect this to change and the argument can be made that no version => version 0, but I'd still prefer it.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498874320", "createdAt": "2020-10-02T14:57:01Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2Njk4MQ=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Nzk2MDgwOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjo1NzoyMFrOHYCfwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzoxMzo1MFrOHcCoAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA==", "bodyText": "Does this base64 encode the json bytes? Why not just use the JSON representation?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r494968768", "createdAt": "2020-09-25T12:57:20Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMTM1Mg==", "bodyText": "This is json, as bytes. Using existing/previous to-json code.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498711352", "createdAt": "2020-10-02T09:22:00Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODcxMjI1OA==", "bodyText": "From the jdoc:\n/**\n * Returns the UTF-8 encoded JSON to reflect the given Topic.\n * This is what is stored in the znodes owned by the {@link ZkTopicStore}.\n */\npublic static byte[] toJson(Topic topic) {", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498712258", "createdAt": "2020-10-02T09:23:55Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg3MTQ0NQ==", "bodyText": "So why not just embed the JSON representation, rather than the byte serialization of it?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r498871445", "createdAt": "2020-10-02T14:52:21Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1OTA2MA==", "bodyText": "So to put JsonObject directly ... OK, can do that.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499159060", "createdAt": "2020-10-03T15:52:36Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MDU1NQ==", "bodyText": "Thanks!", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499160555", "createdAt": "2020-10-03T16:12:46Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTE4Nw==", "bodyText": "This is committed now ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499165187", "createdAt": "2020-10-03T17:13:50Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicCommandSerde.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import io.apicurio.registry.utils.kafka.SelfSerde;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+\n+/**\n+ * TopicCommand Kafka Serde\n+ */\n+public class TopicCommandSerde extends SelfSerde<TopicCommand> {\n+\n+    private static final String UUID = \"uuid\";\n+    private static final String TYPE = \"type\";\n+    private static final String TOPIC = \"topic\";\n+    private static final String KEY = \"key\";\n+\n+    @Override\n+    public byte[] serialize(String topic, TopicCommand data) {\n+        return TopicSerialization.toBytes((mapper, root) -> {\n+            root.put(UUID, data.getUuid());\n+            TopicCommand.Type type = data.getType();\n+            root.put(TYPE, type.ordinal());\n+            if (type == TopicCommand.Type.CREATE || type == TopicCommand.Type.UPDATE) {\n+                byte[] json = TopicSerialization.toJson(data.getTopic());\n+                root.put(TOPIC, json);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk2ODc2OA=="}, "originalCommit": {"oid": "2ed45236ececba48537b7cb72c09df6f09cbe0a4"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzA3Njc1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwODo1Mzo0NlrOHcTd4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMDo1MjowOFrOHcXq6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTEyMA==", "bodyText": "shouldn't you move this into the switch as a default case?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499441120", "createdAt": "2020-10-05T08:53:46Z", "author": {"login": "samuel-hawker"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+        }\n+        throw new IllegalStateException(\"Invalid index: \" + index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86d41167a9352c997c6dc7629801994ce6fd4255"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTUwOTk5Mg==", "bodyText": "Moved ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r499509992", "createdAt": "2020-10-05T10:52:08Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+        }\n+        throw new IllegalStateException(\"Invalid index: \" + index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTEyMA=="}, "originalCommit": {"oid": "86d41167a9352c997c6dc7629801994ce6fd4255"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNzY0MzY5OnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTozMDozMFrOHsquQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyNTozMVrOHs5KXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjU5OTM2MA==", "bodyText": "We can move to 2.6.0 here I guess, now that we already support Kafka 2.6.0 in 0.20.0 release and master of course.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516599360", "createdAt": "2020-11-03T11:30:30Z", "author": {"login": "ppatierno"}, "path": "pom.xml", "diffHunk": "@@ -112,6 +112,9 @@\n         <opentracing-kafka.version>0.1.13</opentracing-kafka.version>\n         <strimzi-oauth.version>0.6.1</strimzi-oauth.version>\n         <commons-codec.version>1.13</commons-codec.version>\n+        <registry.version>1.3.0.Final</registry.version>\n+        <kafka.streams.version>2.5.0</kafka.streams.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzNTkzMw==", "bodyText": "This is tied into Registry's distro.\nBut I guess if it doesn't break anything, we're fine yo upgrade.\nI can check with Registry as well, but afair there we rely on Quarkus' version of Streams ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516835933", "createdAt": "2020-11-03T17:25:31Z", "author": {"login": "alesj"}, "path": "pom.xml", "diffHunk": "@@ -112,6 +112,9 @@\n         <opentracing-kafka.version>0.1.13</opentracing-kafka.version>\n         <strimzi-oauth.version>0.6.1</strimzi-oauth.version>\n         <commons-codec.version>1.13</commons-codec.version>\n+        <registry.version>1.3.0.Final</registry.version>\n+        <kafka.streams.version>2.5.0</kafka.streams.version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjU5OTM2MA=="}, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNzY2Mjg4OnYy", "diffSide": "RIGHT", "path": "topic-operator/design/topic-store.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTozNjowOFrOHsq6IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMTozMzozN1rOIOB2lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMjQwMQ==", "bodyText": "referencing two old links let's use 26", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516602401", "createdAt": "2020-11-03T11:36:08Z", "author": {"login": "ppatierno"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,125 @@\n+# Topic store (a new, Kafka Streams based, implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either Kubernetes or Kafka.\n+Currently we have a working ZooKeeper based implementation, but with the ZooKeeper being removed \n+as part of KIP-500, we have decided to implement the store directly in/on Kafka - its Streams\n+extension to be exact.\n+\n+The TopicStore interface represents a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is only used by the TopicOperator. \n+The ZooKeeper watcher callbacks trigger TopicOperator reconciliations, \n+and this notification mechanism will also need to be replaced before KIP-500 is fully implemented.\n+\n+At the moment there seems to be a need for only a single instance of TopicStore, which\n+is an important detail with regard to the new Kafka Streams based implementation.\n+\n+### Kafka Streams based implementation\n+\n+In Kafka Streams you describe and configure your topics, processing, etc using the so called [Topology](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/Topology.html).\n+With it you describe the flow of your messages. As part of this processing you can make use\n+of the [Kafka Streams store](https://kafka.apache.org/20/documentation/streams/developer-guide/interactive-queries.html) ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU4MTMzNA==", "bodyText": "Well, 27 now I guess :-o", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551581334", "createdAt": "2021-01-04T21:33:37Z", "author": {"login": "scholzj"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,125 @@\n+# Topic store (a new, Kafka Streams based, implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either Kubernetes or Kafka.\n+Currently we have a working ZooKeeper based implementation, but with the ZooKeeper being removed \n+as part of KIP-500, we have decided to implement the store directly in/on Kafka - its Streams\n+extension to be exact.\n+\n+The TopicStore interface represents a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is only used by the TopicOperator. \n+The ZooKeeper watcher callbacks trigger TopicOperator reconciliations, \n+and this notification mechanism will also need to be replaced before KIP-500 is fully implemented.\n+\n+At the moment there seems to be a need for only a single instance of TopicStore, which\n+is an important detail with regard to the new Kafka Streams based implementation.\n+\n+### Kafka Streams based implementation\n+\n+In Kafka Streams you describe and configure your topics, processing, etc using the so called [Topology](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/Topology.html).\n+With it you describe the flow of your messages. As part of this processing you can make use\n+of the [Kafka Streams store](https://kafka.apache.org/20/documentation/streams/developer-guide/interactive-queries.html) ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMjQwMQ=="}, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNzY2ODc3OnYy", "diffSide": "RIGHT", "path": "topic-operator/design/topic-store.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTozNzo1NFrOHsq9nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyNjo0MVrOHs5NRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMzI5Mw==", "bodyText": "same here about version", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516603293", "createdAt": "2020-11-03T11:37:54Z", "author": {"login": "ppatierno"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,125 @@\n+# Topic store (a new, Kafka Streams based, implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either Kubernetes or Kafka.\n+Currently we have a working ZooKeeper based implementation, but with the ZooKeeper being removed \n+as part of KIP-500, we have decided to implement the store directly in/on Kafka - its Streams\n+extension to be exact.\n+\n+The TopicStore interface represents a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is only used by the TopicOperator. \n+The ZooKeeper watcher callbacks trigger TopicOperator reconciliations, \n+and this notification mechanism will also need to be replaced before KIP-500 is fully implemented.\n+\n+At the moment there seems to be a need for only a single instance of TopicStore, which\n+is an important detail with regard to the new Kafka Streams based implementation.\n+\n+### Kafka Streams based implementation\n+\n+In Kafka Streams you describe and configure your topics, processing, etc using the so called [Topology](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/Topology.html).\n+With it you describe the flow of your messages. As part of this processing you can make use\n+of the [Kafka Streams store](https://kafka.apache.org/20/documentation/streams/developer-guide/interactive-queries.html) \n+. This is a local in-memory store, which is backed by the Kafka topic (created by the Kafka Streams)\n+so that the data is properly persisted and can be reloaded if there is a failure or a shutdown.\n+\n+The \"problem\" with the store is that out-of-the-box we only get a local in-memory implementation\n+, where the data is distributed using the 'key hashing', which means that it is stored in exactly \n+one of the running instances of this in-memory store - exactly which one it is depends on the Kafka Streams topology consumer instance that's consumimg the message.\n+\n+What if we want an application, that uses the TopicStore, and runs in a clustered setup - distributed?\n+Then we need to provide our own distributed mechanism for the data lookup. Although the distributed implementation \n+is not available, there is a [Kafka Streams API](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/KafkaStreams.html) that provides us with the needed information to (easily) implement\n+such a distributed mechanism - e.g. we can get the key owner's [HostInfo](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/state/HostInfo.html)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzNjY3OQ==", "bodyText": "OK, fixed", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516836679", "createdAt": "2020-11-03T17:26:41Z", "author": {"login": "alesj"}, "path": "topic-operator/design/topic-store.md", "diffHunk": "@@ -0,0 +1,125 @@\n+# Topic store (a new, Kafka Streams based, implementation) - design document\n+\n+Topic store represents a persistent data store where the operator can store its copy of the \n+topic state that won't be modified by either Kubernetes or Kafka.\n+Currently we have a working ZooKeeper based implementation, but with the ZooKeeper being removed \n+as part of KIP-500, we have decided to implement the store directly in/on Kafka - its Streams\n+extension to be exact.\n+\n+The TopicStore interface represents a simple async CRUD API.\n+\n+```\n+interface TopicStore {\n+\n+    /**\n+     * Asynchronously get the topic with the given name\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with\n+     * a null result.\n+     * @param name The name of the topic.\n+     * @return A future which completes with the given topic.\n+     */\n+    Future<Topic> read(TopicName name);\n+\n+    /**\n+     * Asynchronously persist the given topic in the store\n+     * completing the returned future when done.\n+     * If a topic with the given name already exists, the future will complete with an\n+     * {@link EntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been created.\n+     */\n+    Future<Void> create(Topic topic);\n+\n+    /**\n+     * Asynchronously update the given topic in the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been updated.\n+     */\n+    Future<Void> update(Topic topic);\n+\n+    /**\n+     * Asynchronously delete the given topic from the store\n+     * completing the returned future when done.\n+     * If no topic with the given name exists, the future will complete with a\n+     * {@link NoSuchEntityExistsException}.\n+     * @param topic The topic.\n+     * @return A future which completes when the given topic has been deleted.\n+     */\n+    Future<Void> delete(TopicName topic);\n+}\n+```\n+\n+The TopicStore is only used by the TopicOperator. \n+The ZooKeeper watcher callbacks trigger TopicOperator reconciliations, \n+and this notification mechanism will also need to be replaced before KIP-500 is fully implemented.\n+\n+At the moment there seems to be a need for only a single instance of TopicStore, which\n+is an important detail with regard to the new Kafka Streams based implementation.\n+\n+### Kafka Streams based implementation\n+\n+In Kafka Streams you describe and configure your topics, processing, etc using the so called [Topology](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/Topology.html).\n+With it you describe the flow of your messages. As part of this processing you can make use\n+of the [Kafka Streams store](https://kafka.apache.org/20/documentation/streams/developer-guide/interactive-queries.html) \n+. This is a local in-memory store, which is backed by the Kafka topic (created by the Kafka Streams)\n+so that the data is properly persisted and can be reloaded if there is a failure or a shutdown.\n+\n+The \"problem\" with the store is that out-of-the-box we only get a local in-memory implementation\n+, where the data is distributed using the 'key hashing', which means that it is stored in exactly \n+one of the running instances of this in-memory store - exactly which one it is depends on the Kafka Streams topology consumer instance that's consumimg the message.\n+\n+What if we want an application, that uses the TopicStore, and runs in a clustered setup - distributed?\n+Then we need to provide our own distributed mechanism for the data lookup. Although the distributed implementation \n+is not available, there is a [Kafka Streams API](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/KafkaStreams.html) that provides us with the needed information to (easily) implement\n+such a distributed mechanism - e.g. we can get the key owner's [HostInfo](https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/state/HostInfo.html)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMzI5Mw=="}, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNzY4ODMzOnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Session.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTo0Mzo1MlrOHsrJzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyOTowNFrOHs5StA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwNjQxMw==", "bodyText": "what about this comment?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516606413", "createdAt": "2020-11-03T11:43:52Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Session.java", "diffHunk": "@@ -172,7 +186,26 @@ public void start(Promise<Void> start) {\n                 LOGGER.debug(\"Using ZooKeeper {}\", zk);\n \n                 String topicsPath = config.get(Config.TOPICS_PATH);\n-                ZkTopicStore topicStore = new ZkTopicStore(zk, topicsPath);\n+                TopicStore topicStore;\n+                if (config.get(Config.USE_ZOOKEEPER_TOPIC_STORE)) {\n+                    topicStore = new ZkTopicStore(zk, topicsPath);\n+                } else {\n+                    // TODO -- better async handling?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzODA2OA==", "bodyText": "I thought TopicOperator could somehow use async result of TopicStore creation, but I guess that's not the case, hence forcing the result with ConcurrentUtil::result.\nIf anyone has some better async ideas ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516838068", "createdAt": "2020-11-03T17:29:04Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Session.java", "diffHunk": "@@ -172,7 +186,26 @@ public void start(Promise<Void> start) {\n                 LOGGER.debug(\"Using ZooKeeper {}\", zk);\n \n                 String topicsPath = config.get(Config.TOPICS_PATH);\n-                ZkTopicStore topicStore = new ZkTopicStore(zk, topicsPath);\n+                TopicStore topicStore;\n+                if (config.get(Config.USE_ZOOKEEPER_TOPIC_STORE)) {\n+                    topicStore = new ZkTopicStore(zk, topicsPath);\n+                } else {\n+                    // TODO -- better async handling?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwNjQxMw=="}, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNzcwMDQ5OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTo0NzoxOVrOHsrRDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxODowOTo1OVrOHs6ybA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwODI3MA==", "bodyText": "genuine question ... what's the rationale behind this value?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516608270", "createdAt": "2020-11-03T11:47:19Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzOTE2Mg==", "bodyText": "Nothing more then what the javadoc says.\nI saw this being used in most real examples + I remember we used this setting in our previous prod code as well.\nNever really thought too much since the first usage.\n@tombentley any thoughts on this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516839162", "createdAt": "2020-11-03T17:30:46Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwODI3MA=="}, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg2MjU3Mg==", "bodyText": "The default is 1GiB, which would be more than we needed, since it's a compacted topic anyway. Whether 64MiB is the best default, who knows?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r516862572", "createdAt": "2020-11-03T18:09:59Z", "author": {"login": "tombentley"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/TopicStoreTopologyProvider.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.ForeachAction;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.processor.Processor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.StoreBuilder;\n+import org.apache.kafka.streams.state.Stores;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.function.Supplier;\n+\n+/**\n+ * Kafka Streams topology provider for TopicStore.\n+ */\n+public class TopicStoreTopologyProvider implements Supplier<Topology> {\n+    private final String storeTopic;\n+    private final String topicStoreName;\n+    private final Properties kafkaProperties;\n+    private final ForeachAction<? super String, ? super Integer> dispatcher;\n+\n+    public TopicStoreTopologyProvider(\n+            String storeTopic,\n+            String topicStoreName,\n+            Properties kafkaProperties,\n+            ForeachAction<? super String, ? super Integer> dispatcher\n+    ) {\n+        this.storeTopic = storeTopic;\n+        this.topicStoreName = topicStoreName;\n+        this.kafkaProperties = kafkaProperties;\n+        this.dispatcher = dispatcher;\n+    }\n+\n+    @Override\n+    public Topology get() {\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        // Simple defaults\n+        Map<String, String> configuration = new HashMap<>();\n+        configuration.put(TopicConfig.CLEANUP_POLICY_CONFIG, TopicConfig.CLEANUP_POLICY_COMPACT);\n+        configuration.put(TopicConfig.MIN_COMPACTION_LAG_MS_CONFIG, \"0\");\n+        configuration.put(TopicConfig.SEGMENT_BYTES_CONFIG, String.valueOf(64 * 1024 * 1024));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwODI3MA=="}, "originalCommit": {"oid": "3a6636b73e3aea816dcc03fdf83ffa91508c0df3"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTEyODU1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNzoyMTo0MFrOHtLofw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMzo0MTo1OFrOHtYjfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ==", "bodyText": "I am fine with using the __ prefix for an internal topic (see the store topic) but why using this kind of notation for something that isn't a topic, so the store itself? Isn't it enough just \"topics-store\" or something like that? While the topic is something visible to the user (the TO will create a KafkaTopic resource named __strimzi_store_topic), the store is not. @tombentley @alesj any thoughts? My confusion comes from using \"store\" and \"topic\" too many times where the order they are used matters.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517138559", "createdAt": "2020-11-04T07:21:40Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIwNzM3Ng==", "bodyText": "Streams create (log) topics based on the store name.\nThis way you can immediately tell this is an internal topic.\n(although I guess they already have \"log\" in the name, which tells you it's a Streams log topic ...)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517207376", "createdAt": "2020-11-04T09:31:09Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxOTExNg==", "bodyText": "Right, I forgot the log topic ... can you double-check the final name so that we can figure out a better name for the store?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517219116", "createdAt": "2020-11-04T09:49:44Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI2NDU3NQ==", "bodyText": "<application.id>-<store.name>-changelog", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517264575", "createdAt": "2020-11-04T11:04:17Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI2NzUwNA==", "bodyText": "so it's going to be strimzi-topic-store-__strimzi_topic_store-changelog ... I really think it's a crazy one!! :-D\nThe __ doesn't make sense anymore because it's not at the beginning of the topic name so it doesn't highlight it as an internal topic, then again strimzi, topic and store used too much :-) Isn't it possible to override the changelog topic name?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517267504", "createdAt": "2020-11-04T11:09:42Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI3MDE5MA==", "bodyText": "Isn't it possible to override the changelog topic name?\n\nNo idea ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517270190", "createdAt": "2020-11-04T11:14:43Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMzMzEwOA==", "bodyText": "So after some investigation, it turned out that it's not possible to override the name completely. The internal Kafka Streams topics have always -changelog or -repartition suffixes for topics related to store and repartitioning.\nSo we can just \"play\" with <application.id> and <store.name> as for the format you pasted.\nFirst, if we want the final topic starting with __ then we need the application id starting with __.\nMaybe something like:\napplication.id = \"__strimzi-topic-operator-kstreams\"\nstore.name = \"topic-store\"\nso changelog topic will be __strimzi-topic-operator-kstreams-topic-store-changelog\nTbh I run out of ideas :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517333108", "createdAt": "2020-11-04T13:14:04Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzM1MDI3MQ==", "bodyText": "OK, I'll put this values into Config ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517350271", "createdAt": "2020-11-04T13:41:58Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/Config.java", "diffHunk": "@@ -159,6 +176,24 @@ private Value(String key, Type<? extends T> type, boolean required) {\n     /** The endpoint identification algorithm used by clients to validate server host name. The default value is https. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Disable server host name verification by setting to an empty string.**/\n     public static final Value<String> TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM = new Value<>(TC_TLS_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM, STRING, \"HTTPS\");\n \n+    /**\n+     * The store topic for the Kafka Streams based TopicStore\n+     */\n+    public static final Value<String> STORE_TOPIC = new Value<>(TC_STORE_TOPIC, STRING, \"__strimzi_store_topic\");\n+    /** The store name for the Kafka Streams based TopicStore */\n+    public static final Value<String> STORE_NAME = new Value<>(TC_STORE_NAME, STRING, \"__strimzi_topic_store\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzEzODU1OQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTE3MDY1OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreService.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNzozNjo1NlrOHtMAzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMDoxMTozMVrOHtRZZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ==", "bodyText": "So I was thinking about this scenario ...\nInitially, the cluster is made by 3 brokers so we are going to create the store topic with rf = 3 and minISR=2 (as per createNewStoreTopic method).\nThen imagine that we scale up to 9 as far as I understood from this code, the store topic will remain with same configuration. I am not sure that it will be for production use anymore, with a cluster of 9 but a topic with rf = 3 and minISR=2. Shouldn't we increase the replication factor accordingly? @tombentley thoughts?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517144781", "createdAt": "2020-11-04T07:36:56Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreService.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * A service to configure and start/stop KafkaStreamsTopicStore.\n+ */\n+public class KafkaStreamsTopicStoreService {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStoreService.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        return validateExistingStoreTopic(storeTopic, admin, c);\n+                    } else {\n+                        return createNewStoreTopic(storeTopic, admin, c);\n+                    }\n+                })\n+                .thenCompose(v -> createKafkaStreams(config, kafkaProperties, storeTopic, storeName))\n+                .thenApply(serviceImpl -> createKafkaTopicStore(config, kafkaProperties, storeTopic, serviceImpl))\n+                .whenComplete((v, t) -> {\n+                    if (t != null) {\n+                        stop();\n+                    }\n+                });\n+    }\n+\n+    private TopicStore createKafkaTopicStore(Config config, Properties kafkaProperties, String storeTopic, AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl) {\n+        ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                kafkaProperties,\n+            Serdes.String().serializer(),\n+            new TopicCommandSerde()\n+        );\n+        closeables.add(producer);\n+\n+        StoreAndServiceFactory factory;\n+        if (config.get(Config.DISTRIBUTED_STORE)) {\n+            factory = new DistributedStoreAndServiceFactory();\n+        } else {\n+            factory = new LocalStoreAndServiceFactory();\n+        }\n+        StoreAndServiceFactory.StoreContext sc = factory.create(config, kafkaProperties, streams, serviceImpl, closeables);\n+\n+        this.store = new KafkaStreamsTopicStore(sc.getStore(), storeTopic, producer, sc.getService());\n+        return this.store;\n+    }\n+\n+    private CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> createKafkaStreams(Config config, Properties kafkaProperties, String storeTopic, String storeName) {\n+        long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+        ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+        WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+        closeables.add(serviceImpl);\n+\n+        AtomicBoolean done = new AtomicBoolean(false); // no need for dup complete\n+        CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+        KafkaStreams.StateListener listener = (newState, oldState) -> {\n+            if (newState == KafkaStreams.State.RUNNING && !done.getAndSet(true)) {\n+                cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+            }\n+            if (newState == KafkaStreams.State.ERROR) {\n+                cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+            }\n+        };\n+\n+        Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+        streams = new KafkaStreams(topology, kafkaProperties);\n+        streams.setStateListener(listener);\n+        streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+        closeables.add(streams);\n+        streams.start();\n+\n+        return cf;\n+    }\n+\n+    private CompletionStage<Void> createNewStoreTopic(String storeTopic, Admin admin, Context c) {\n+        int rf = Math.min(3, c.clusterSize);\n+        int minISR = Math.max(rf - 1, 1);\n+        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+    }\n+\n+    private CompletionStage<Void> validateExistingStoreTopic(String storeTopic, Admin admin, Context c) {\n+        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+            .thenApply(c3 -> {\n+                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxMTc1Mg==", "bodyText": "This is what @tombentley suggested ... dunno if you really need (rf == cluster size) with big clusters ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517211752", "createdAt": "2020-11-04T09:38:02Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreService.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * A service to configure and start/stop KafkaStreamsTopicStore.\n+ */\n+public class KafkaStreamsTopicStoreService {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStoreService.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        return validateExistingStoreTopic(storeTopic, admin, c);\n+                    } else {\n+                        return createNewStoreTopic(storeTopic, admin, c);\n+                    }\n+                })\n+                .thenCompose(v -> createKafkaStreams(config, kafkaProperties, storeTopic, storeName))\n+                .thenApply(serviceImpl -> createKafkaTopicStore(config, kafkaProperties, storeTopic, serviceImpl))\n+                .whenComplete((v, t) -> {\n+                    if (t != null) {\n+                        stop();\n+                    }\n+                });\n+    }\n+\n+    private TopicStore createKafkaTopicStore(Config config, Properties kafkaProperties, String storeTopic, AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl) {\n+        ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                kafkaProperties,\n+            Serdes.String().serializer(),\n+            new TopicCommandSerde()\n+        );\n+        closeables.add(producer);\n+\n+        StoreAndServiceFactory factory;\n+        if (config.get(Config.DISTRIBUTED_STORE)) {\n+            factory = new DistributedStoreAndServiceFactory();\n+        } else {\n+            factory = new LocalStoreAndServiceFactory();\n+        }\n+        StoreAndServiceFactory.StoreContext sc = factory.create(config, kafkaProperties, streams, serviceImpl, closeables);\n+\n+        this.store = new KafkaStreamsTopicStore(sc.getStore(), storeTopic, producer, sc.getService());\n+        return this.store;\n+    }\n+\n+    private CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> createKafkaStreams(Config config, Properties kafkaProperties, String storeTopic, String storeName) {\n+        long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+        ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+        WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+        closeables.add(serviceImpl);\n+\n+        AtomicBoolean done = new AtomicBoolean(false); // no need for dup complete\n+        CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+        KafkaStreams.StateListener listener = (newState, oldState) -> {\n+            if (newState == KafkaStreams.State.RUNNING && !done.getAndSet(true)) {\n+                cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+            }\n+            if (newState == KafkaStreams.State.ERROR) {\n+                cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+            }\n+        };\n+\n+        Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+        streams = new KafkaStreams(topology, kafkaProperties);\n+        streams.setStateListener(listener);\n+        streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+        closeables.add(streams);\n+        streams.start();\n+\n+        return cf;\n+    }\n+\n+    private CompletionStage<Void> createNewStoreTopic(String storeTopic, Admin admin, Context c) {\n+        int rf = Math.min(3, c.clusterSize);\n+        int minISR = Math.max(rf - 1, 1);\n+        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+    }\n+\n+    private CompletionStage<Void> validateExistingStoreTopic(String storeTopic, Admin admin, Context c) {\n+        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+            .thenApply(c3 -> {\n+                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxNTg1NQ==", "bodyText": "Maybe not exactly cluster size but something more close to it", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517215855", "createdAt": "2020-11-04T09:44:39Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreService.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * A service to configure and start/stop KafkaStreamsTopicStore.\n+ */\n+public class KafkaStreamsTopicStoreService {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStoreService.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        return validateExistingStoreTopic(storeTopic, admin, c);\n+                    } else {\n+                        return createNewStoreTopic(storeTopic, admin, c);\n+                    }\n+                })\n+                .thenCompose(v -> createKafkaStreams(config, kafkaProperties, storeTopic, storeName))\n+                .thenApply(serviceImpl -> createKafkaTopicStore(config, kafkaProperties, storeTopic, serviceImpl))\n+                .whenComplete((v, t) -> {\n+                    if (t != null) {\n+                        stop();\n+                    }\n+                });\n+    }\n+\n+    private TopicStore createKafkaTopicStore(Config config, Properties kafkaProperties, String storeTopic, AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl) {\n+        ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                kafkaProperties,\n+            Serdes.String().serializer(),\n+            new TopicCommandSerde()\n+        );\n+        closeables.add(producer);\n+\n+        StoreAndServiceFactory factory;\n+        if (config.get(Config.DISTRIBUTED_STORE)) {\n+            factory = new DistributedStoreAndServiceFactory();\n+        } else {\n+            factory = new LocalStoreAndServiceFactory();\n+        }\n+        StoreAndServiceFactory.StoreContext sc = factory.create(config, kafkaProperties, streams, serviceImpl, closeables);\n+\n+        this.store = new KafkaStreamsTopicStore(sc.getStore(), storeTopic, producer, sc.getService());\n+        return this.store;\n+    }\n+\n+    private CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> createKafkaStreams(Config config, Properties kafkaProperties, String storeTopic, String storeName) {\n+        long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+        ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+        WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+        closeables.add(serviceImpl);\n+\n+        AtomicBoolean done = new AtomicBoolean(false); // no need for dup complete\n+        CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+        KafkaStreams.StateListener listener = (newState, oldState) -> {\n+            if (newState == KafkaStreams.State.RUNNING && !done.getAndSet(true)) {\n+                cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+            }\n+            if (newState == KafkaStreams.State.ERROR) {\n+                cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+            }\n+        };\n+\n+        Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+        streams = new KafkaStreams(topology, kafkaProperties);\n+        streams.setStateListener(listener);\n+        streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+        closeables.add(streams);\n+        streams.start();\n+\n+        return cf;\n+    }\n+\n+    private CompletionStage<Void> createNewStoreTopic(String storeTopic, Admin admin, Context c) {\n+        int rf = Math.min(3, c.clusterSize);\n+        int minISR = Math.max(rf - 1, 1);\n+        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+    }\n+\n+    private CompletionStage<Void> validateExistingStoreTopic(String storeTopic, Admin admin, Context c) {\n+        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+            .thenApply(c3 -> {\n+                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzMjk5OQ==", "bodyText": "I'll let @tombentley work out the rf value algorithm", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517232999", "createdAt": "2020-11-04T10:11:31Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStoreService.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.streams.diservice.AsyncBiFunctionService;\n+import io.apicurio.registry.utils.streams.ext.ForeachActionDispatcher;\n+import io.apicurio.registry.utils.streams.ext.LoggingStateRestoreListener;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.common.KafkaFuture;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.config.TopicConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.Topology;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static java.lang.Integer.parseInt;\n+\n+/**\n+ * A service to configure and start/stop KafkaStreamsTopicStore.\n+ */\n+public class KafkaStreamsTopicStoreService {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStoreService.class);\n+\n+    private final List<AutoCloseable> closeables = new ArrayList<>();\n+\n+    /* test */ KafkaStreams streams;\n+    /* test */ TopicStore store;\n+\n+    public CompletionStage<TopicStore> start(Config config, Properties kafkaProperties) {\n+        String storeTopic = config.get(Config.STORE_TOPIC);\n+        String storeName = config.get(Config.STORE_NAME);\n+\n+        // check if entry topic has the right configuration\n+        Admin admin = Admin.create(kafkaProperties);\n+        return toCS(admin.describeCluster().nodes())\n+                .thenApply(nodes -> new Context(nodes.size()))\n+                .thenCompose(c -> toCS(admin.listTopics().names()).thenApply(c::setTopics))\n+                .thenCompose(c -> {\n+                    if (c.topics.contains(storeTopic)) {\n+                        return validateExistingStoreTopic(storeTopic, admin, c);\n+                    } else {\n+                        return createNewStoreTopic(storeTopic, admin, c);\n+                    }\n+                })\n+                .thenCompose(v -> createKafkaStreams(config, kafkaProperties, storeTopic, storeName))\n+                .thenApply(serviceImpl -> createKafkaTopicStore(config, kafkaProperties, storeTopic, serviceImpl))\n+                .whenComplete((v, t) -> {\n+                    if (t != null) {\n+                        stop();\n+                    }\n+                });\n+    }\n+\n+    private TopicStore createKafkaTopicStore(Config config, Properties kafkaProperties, String storeTopic, AsyncBiFunctionService.WithSerdes<String, String, Integer> serviceImpl) {\n+        ProducerActions<String, TopicCommand> producer = new AsyncProducer<>(\n+                kafkaProperties,\n+            Serdes.String().serializer(),\n+            new TopicCommandSerde()\n+        );\n+        closeables.add(producer);\n+\n+        StoreAndServiceFactory factory;\n+        if (config.get(Config.DISTRIBUTED_STORE)) {\n+            factory = new DistributedStoreAndServiceFactory();\n+        } else {\n+            factory = new LocalStoreAndServiceFactory();\n+        }\n+        StoreAndServiceFactory.StoreContext sc = factory.create(config, kafkaProperties, streams, serviceImpl, closeables);\n+\n+        this.store = new KafkaStreamsTopicStore(sc.getStore(), storeTopic, producer, sc.getService());\n+        return this.store;\n+    }\n+\n+    private CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> createKafkaStreams(Config config, Properties kafkaProperties, String storeTopic, String storeName) {\n+        long timeoutMillis = config.get(Config.STALE_RESULT_TIMEOUT_MS);\n+        ForeachActionDispatcher<String, Integer> dispatcher = new ForeachActionDispatcher<>();\n+        WaitForResultService serviceImpl = new WaitForResultService(timeoutMillis, dispatcher);\n+        closeables.add(serviceImpl);\n+\n+        AtomicBoolean done = new AtomicBoolean(false); // no need for dup complete\n+        CompletableFuture<AsyncBiFunctionService.WithSerdes<String, String, Integer>> cf = new CompletableFuture<>();\n+        KafkaStreams.StateListener listener = (newState, oldState) -> {\n+            if (newState == KafkaStreams.State.RUNNING && !done.getAndSet(true)) {\n+                cf.completeAsync(() -> serviceImpl); // complete in a different thread\n+            }\n+            if (newState == KafkaStreams.State.ERROR) {\n+                cf.completeExceptionally(new IllegalStateException(\"KafkaStreams error\"));\n+            }\n+        };\n+\n+        Topology topology = new TopicStoreTopologyProvider(storeTopic, storeName, kafkaProperties, dispatcher).get();\n+        streams = new KafkaStreams(topology, kafkaProperties);\n+        streams.setStateListener(listener);\n+        streams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\n+        closeables.add(streams);\n+        streams.start();\n+\n+        return cf;\n+    }\n+\n+    private CompletionStage<Void> createNewStoreTopic(String storeTopic, Admin admin, Context c) {\n+        int rf = Math.min(3, c.clusterSize);\n+        int minISR = Math.max(rf - 1, 1);\n+        NewTopic newTopic = new NewTopic(storeTopic, 1, (short) rf)\n+            .configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, String.valueOf(minISR)));\n+        return toCS(admin.createTopics(Collections.singleton(newTopic)).all());\n+    }\n+\n+    private CompletionStage<Void> validateExistingStoreTopic(String storeTopic, Admin admin, Context c) {\n+        ConfigResource storeTopicConfigResource = new ConfigResource(ConfigResource.Type.TOPIC, storeTopic);\n+        return toCS(admin.describeTopics(Collections.singleton(storeTopic)).values().get(storeTopic))\n+            .thenApply(td -> c.setRf(td.partitions().stream().map(tp -> tp.replicas().size()).min(Integer::compare).orElseThrow()))\n+            .thenCompose(c2 -> toCS(admin.describeConfigs(Collections.singleton(storeTopicConfigResource)).values().get(storeTopicConfigResource))\n+                    .thenApply(cr -> c2.setMinISR(parseInt(cr.get(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG).value()))))\n+            .thenApply(c3 -> {\n+                if (c3.rf != Math.min(3, c3.clusterSize) || c3.minISR != c3.rf - 1) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0NDc4MQ=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTIwNjI4OnYy", "diffSide": "RIGHT", "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwNzo0ODo0MFrOHtMVHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxMDoxMToyMlrOHtRZFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg==", "bodyText": "genuine question because I don't know Apicurio ... what's the value of using this \"apicurio\" producer instead of a plain Kafka producer? Aren't we going just to send a message in the store topic and then the Kafka Streams application takes care of it storing information in the topic store?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517149982", "createdAt": "2020-11-04T07:48:40Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+            default:\n+                throw new IllegalStateException(\"Invalid index: \" + index);\n+        }\n+    }\n+\n+    public static Integer toIndex(Class<? extends Throwable> ct) {\n+        if (ct.equals(TopicStore.EntityExistsException.class)) {\n+            return 1;\n+        }\n+        if (ct.equals(TopicStore.NoSuchEntityExistsException.class)) {\n+            return 2;\n+        }\n+        if (ct.equals(TopicStore.InvalidStateException.class)) {\n+            return 3;\n+        }\n+        throw new IllegalStateException(\"Unexpected value: \" + ct);\n+    }\n+\n+    @Override\n+    public Future<Topic> read(TopicName name) {\n+        try {\n+            Topic topic = topicStore.get(name.toString());\n+            return Future.succeededFuture(topic);\n+        } catch (Throwable t) {\n+            return Future.failedFuture(t);\n+        }\n+    }\n+\n+    private Future<Void> handleTopicCommand(TopicCommand cmd) {\n+        String key = cmd.getKey();\n+        CompletionStage<Throwable> result = resultService.apply(key, cmd.getUuid())\n+                .thenApply(KafkaStreamsTopicStore::toThrowable);\n+        // Kafka Streams can re-balance in-between these two calls ...\n+        producer.apply(new ProducerRecord<>(storeTopic, key, cmd))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxNTQ1MA==", "bodyText": "It's just a bit more resilient then plain Kafka producer -- see the code.\nIt also transparently handles fatal errors in callback as well.\nAnd it uses CS as a callback API, which makes it easier to use -- when you have a CS chain.\n(here we actually don't take full advantage of this)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517215450", "createdAt": "2020-11-04T09:44:04Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+            default:\n+                throw new IllegalStateException(\"Invalid index: \" + index);\n+        }\n+    }\n+\n+    public static Integer toIndex(Class<? extends Throwable> ct) {\n+        if (ct.equals(TopicStore.EntityExistsException.class)) {\n+            return 1;\n+        }\n+        if (ct.equals(TopicStore.NoSuchEntityExistsException.class)) {\n+            return 2;\n+        }\n+        if (ct.equals(TopicStore.InvalidStateException.class)) {\n+            return 3;\n+        }\n+        throw new IllegalStateException(\"Unexpected value: \" + ct);\n+    }\n+\n+    @Override\n+    public Future<Topic> read(TopicName name) {\n+        try {\n+            Topic topic = topicStore.get(name.toString());\n+            return Future.succeededFuture(topic);\n+        } catch (Throwable t) {\n+            return Future.failedFuture(t);\n+        }\n+    }\n+\n+    private Future<Void> handleTopicCommand(TopicCommand cmd) {\n+        String key = cmd.getKey();\n+        CompletionStage<Throwable> result = resultService.apply(key, cmd.getUuid())\n+                .thenApply(KafkaStreamsTopicStore::toThrowable);\n+        // Kafka Streams can re-balance in-between these two calls ...\n+        producer.apply(new ProducerRecord<>(storeTopic, key, cmd))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIyMDM2MQ==", "bodyText": "My concern is that we are adding an apicurio dependency in the topic operator just for this. Or apicurio stuff is used even somewhere else?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517220361", "createdAt": "2020-11-04T09:51:39Z", "author": {"login": "ppatierno"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+            default:\n+                throw new IllegalStateException(\"Invalid index: \" + index);\n+        }\n+    }\n+\n+    public static Integer toIndex(Class<? extends Throwable> ct) {\n+        if (ct.equals(TopicStore.EntityExistsException.class)) {\n+            return 1;\n+        }\n+        if (ct.equals(TopicStore.NoSuchEntityExistsException.class)) {\n+            return 2;\n+        }\n+        if (ct.equals(TopicStore.InvalidStateException.class)) {\n+            return 3;\n+        }\n+        throw new IllegalStateException(\"Unexpected value: \" + ct);\n+    }\n+\n+    @Override\n+    public Future<Topic> read(TopicName name) {\n+        try {\n+            Topic topic = topicStore.get(name.toString());\n+            return Future.succeededFuture(topic);\n+        } catch (Throwable t) {\n+            return Future.failedFuture(t);\n+        }\n+    }\n+\n+    private Future<Void> handleTopicCommand(TopicCommand cmd) {\n+        String key = cmd.getKey();\n+        CompletionStage<Throwable> result = resultService.apply(key, cmd.getUuid())\n+                .thenApply(KafkaStreamsTopicStore::toThrowable);\n+        // Kafka Streams can re-balance in-between these two calls ...\n+        producer.apply(new ProducerRecord<>(storeTopic, key, cmd))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIzMjkxOQ==", "bodyText": "Yes, in a few classes.\nMostly in order to easily abstract away local vs distributed storage + async function.\nAnd some utils - concurrency and serde.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r517232919", "createdAt": "2020-11-04T10:11:22Z", "author": {"login": "alesj"}, "path": "topic-operator/src/main/java/io/strimzi/operator/topic/KafkaStreamsTopicStore.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.operator.topic;\n+\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.vertx.core.Future;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.BiFunction;\n+\n+/**\n+ * TopicStore based on Kafka Streams and\n+ * Apicurio Registry's gRPC based Kafka Streams ReadOnlyKeyValueStore\n+ */\n+public class KafkaStreamsTopicStore implements TopicStore {\n+    private static final Logger log = LoggerFactory.getLogger(KafkaStreamsTopicStore.class);\n+\n+    private final ReadOnlyKeyValueStore<String, Topic> topicStore;\n+\n+    private final String storeTopic;\n+    private final ProducerActions<String, TopicCommand> producer;\n+\n+    private final BiFunction<String, String, CompletionStage<Integer>> resultService;\n+\n+    public KafkaStreamsTopicStore(\n+            ReadOnlyKeyValueStore<String, Topic> topicStore,\n+            String storeTopic,\n+            ProducerActions<String, TopicCommand> producer,\n+            BiFunction<String, String, CompletionStage<Integer>> resultService) {\n+        this.topicStore = topicStore;\n+        this.storeTopic = storeTopic;\n+        this.producer = producer;\n+        this.resultService = resultService;\n+    }\n+\n+    public static Throwable toThrowable(Integer index) {\n+        if (index == null) {\n+            return null;\n+        }\n+        switch (index) {\n+            case 1:\n+                return new TopicStore.EntityExistsException();\n+            case 2:\n+                return new TopicStore.NoSuchEntityExistsException();\n+            case 3:\n+                return new TopicStore.InvalidStateException();\n+            default:\n+                throw new IllegalStateException(\"Invalid index: \" + index);\n+        }\n+    }\n+\n+    public static Integer toIndex(Class<? extends Throwable> ct) {\n+        if (ct.equals(TopicStore.EntityExistsException.class)) {\n+            return 1;\n+        }\n+        if (ct.equals(TopicStore.NoSuchEntityExistsException.class)) {\n+            return 2;\n+        }\n+        if (ct.equals(TopicStore.InvalidStateException.class)) {\n+            return 3;\n+        }\n+        throw new IllegalStateException(\"Unexpected value: \" + ct);\n+    }\n+\n+    @Override\n+    public Future<Topic> read(TopicName name) {\n+        try {\n+            Topic topic = topicStore.get(name.toString());\n+            return Future.succeededFuture(topic);\n+        } catch (Throwable t) {\n+            return Future.failedFuture(t);\n+        }\n+    }\n+\n+    private Future<Void> handleTopicCommand(TopicCommand cmd) {\n+        String key = cmd.getKey();\n+        CompletionStage<Throwable> result = resultService.apply(key, cmd.getUuid())\n+                .thenApply(KafkaStreamsTopicStore::toThrowable);\n+        // Kafka Streams can re-balance in-between these two calls ...\n+        producer.apply(new ProducerRecord<>(storeTopic, key, cmd))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE0OTk4Mg=="}, "originalCommit": {"oid": "f4fcd5861a9ea12a286f414ee231f0b2e9f6aced"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzNzEwMjkwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOFQxMDoxMjozNVrOH7X6iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOVQxNzoxMzo0NFrOH7lQzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ==", "bodyText": "You did IMHO not changed anything in User Operator. So this should not be here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532019849", "createdAt": "2020-11-28T10:12:35Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNjEzOA==", "bodyText": "Yeah ... but if I didn't prolong the delay here as well, it didn't go through ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532236138", "createdAt": "2020-11-29T16:54:39Z", "author": {"login": "alesj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzE5NA==", "bodyText": "The tests are passing completely fine outside of your PR. So there is no need to change this unless you changed something in the User Operator.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237194", "createdAt": "2020-11-29T17:02:28Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzc0OA==", "bodyText": "This will definitely go out, just wanted to get a clean run of ST locally and CI ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237748", "createdAt": "2020-11-29T17:06:49Z", "author": {"login": "alesj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzODU0MQ==", "bodyText": "Two of the testsets timed out - which could be also very well related to these changes since you probably increased the time needed to run the tests by more then one hour. So keep in mind that this might not always just help.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532238541", "createdAt": "2020-11-29T17:13:44Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAxOTg0OQ=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzNzExNDg0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOFQxMDoxNToxM1rOH7YDLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOVQxNzowNTo1NlrOH7lNRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA==", "bodyText": "Same as above ... should this really be here? Does the TO now with your changes really need 2 minutes to start? I do not see to be changed for the regular just for the STs which suggests it should not be here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532022060", "createdAt": "2020-11-28T10:15:13Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n                     .endUserOperator()\n                     .editTopicOperator()\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNjQwMA==", "bodyText": "No idea what's going on, but I saw in the log that KafkaStreamsTopicStoreService took ~1min to start (from \"Starting\" log to \"Started\").\nIt does a simple Kafka AdminClient check + starts Kafka Streams (topology),\nwhich I really don't see why it should take so long ...", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532236400", "createdAt": "2020-11-29T16:56:35Z", "author": {"login": "alesj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n                     .endUserOperator()\n                     .editTopicOperator()\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzI2Nw==", "bodyText": "If this is expected from the new Topic Operator, then it certainly needs to be increased for all other places as well. Not just for running the system tests.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237267", "createdAt": "2020-11-29T17:03:17Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n                     .endUserOperator()\n                     .editTopicOperator()\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjIzNzYzNw==", "bodyText": "It's not expected, it should be fast -- not as fast as ZKTopicStore, which didn't have anything to pre-start,\nbut few AdminClient calls + starting up Kafka Streams shouldn't take 1min.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r532237637", "createdAt": "2020-11-29T17:05:56Z", "author": {"login": "alesj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java", "diffHunk": "@@ -185,11 +186,23 @@ private static KafkaBuilder defaultKafka(Kafka kafka, String name, int kafkaRepl\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n                     .endUserOperator()\n                     .editTopicOperator()\n                         .withNewInlineLogging()\n                             .addToLoggers(\"rootLogger.level\", \"DEBUG\")\n                         .endInlineLogging()\n+                        .withReadinessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )\n+                        .withLivenessProbe(\n+                                new ProbeBuilder().withFailureThreshold(5).withInitialDelaySeconds(120).build()\n+                        )", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjAyMjA2MA=="}, "originalCommit": {"oid": "38be9898fef957959a1060e93ea024e932459c98"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2OTg5OTQ5OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNDozNDoyMFrOINz2fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxMDoyMTo1N1rOIORuGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTM1MTkzNA==", "bodyText": "what's this? :-)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551351934", "createdAt": "2021-01-04T14:34:20Z", "author": {"login": "ppatierno"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1382,7 +1382,10 @@ void testConsumerOffsetFiles() {\n         String result = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n                 \"/bin/bash\", \"-c\", commandToGetFiles).out();\n \n-        assertThat(\"Folder kafka-log0 has data in files\", result.equals(\"\"));\n+        // TODO / FIXME\n+        //assertThat(\"Folder kafka-log0 has data in files:\\n\" + result, result.equals(\"\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MTMwNg==", "bodyText": "This was empty before, as ZKTS didn't use Kafka.\nWith KSTS, this is already populated -- I guess it can be removed, unless someone has a better idea?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551841306", "createdAt": "2021-01-05T10:21:57Z", "author": {"login": "alesj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1382,7 +1382,10 @@ void testConsumerOffsetFiles() {\n         String result = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n                 \"/bin/bash\", \"-c\", commandToGetFiles).out();\n \n-        assertThat(\"Folder kafka-log0 has data in files\", result.equals(\"\"));\n+        // TODO / FIXME\n+        //assertThat(\"Folder kafka-log0 has data in files:\\n\" + result, result.equals(\"\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTM1MTkzNA=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MTMyODE1OnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMToxNDoyOFrOIOBTjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxMDoyNTowNFrOIOR0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MjM2NQ==", "bodyText": "Does this have to be 2.6.0? Or can this be 2.7.0 now? It seems a bit weird that we use 2.7.0 for client but 2.6.0 for Streams. I'm not sure how Streams 2.6 deals with newer KAfka clients.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551572365", "createdAt": "2021-01-04T21:14:28Z", "author": {"login": "scholzj"}, "path": "pom.xml", "diffHunk": "@@ -112,6 +112,9 @@\n         <opentracing-kafka.version>0.1.13</opentracing-kafka.version>\n         <strimzi-oauth.version>0.6.1</strimzi-oauth.version>\n         <commons-codec.version>1.13</commons-codec.version>\n+        <registry.version>1.3.0.Final</registry.version>\n+        <kafka.streams.version>2.6.0</kafka.streams.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0Mjg2OA==", "bodyText": "This is what's used in Registry, via that utils-streams dependency ....\nWe can try to upgrade and see if it all works out.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551842868", "createdAt": "2021-01-05T10:25:04Z", "author": {"login": "alesj"}, "path": "pom.xml", "diffHunk": "@@ -112,6 +112,9 @@\n         <opentracing-kafka.version>0.1.13</opentracing-kafka.version>\n         <strimzi-oauth.version>0.6.1</strimzi-oauth.version>\n         <commons-codec.version>1.13</commons-codec.version>\n+        <registry.version>1.3.0.Final</registry.version>\n+        <kafka.streams.version>2.6.0</kafka.streams.version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MjM2NQ=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MTMzNjIyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMToxNjo1NVrOIOBYcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMTo0NTo1MFrOIO7liw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg==", "bodyText": "What are the plans for fixing this? Also, any GitHub issues to link to here for this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551573616", "createdAt": "2021-01-04T21:16:55Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MzIxNw==", "bodyText": "@tombentley any idea what can be done?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551843217", "createdAt": "2021-01-05T10:25:44Z", "author": {"login": "alesj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0NzcyMA==", "bodyText": "Did I disable it? Anyway, I've no recollection of doing this.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551847720", "createdAt": "2021-01-05T10:34:15Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1MjY0NA==", "bodyText": "@tombentley no, I disabled it ... but this is what you had to say about it :-)\n\n    @Disabled // fix-it ...\n    /*\n     * I'm guessing that this test passed before because we never had a consumer in this test,\n     * so there was no __consumer_offsets for it to fail on.\n     * The test topic which is used has min.isr == replicas,\n     * so KafkaAvailability would always have ignored that because it's unrollable otherwise.\n     * Now there's a partition with a minisr set to 1 but with >1 replicas,\n     * so KafkaAvailability cannot ignore it.\n     * And because the key was changed it means that the other brokers don't trust the new broker 0 cert\n     * (because it's signed with a key they don't yet trust). So they can't connect to do follower fetches.\n     * So we've kind of deadlocked.\n     */", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551852644", "createdAt": "2021-01-05T10:43:40Z", "author": {"login": "alesj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2Mjk1NA==", "bodyText": "Thanks, I remember now. The test isn't so realistic, having a topic with minisr== replicas. Having 3 replicas and min.isr=2 would be more realistic. So I think we should change the test. WDYT @Frawless @scholzj ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551862954", "createdAt": "2021-01-05T11:03:45Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2MzE3Mg==", "bodyText": "@alesj I think the explanation makes sense. But the // fix-it ... comment alone is not the right solution. We should either fix it in this PR (whatever we decide is the right fix - maybe we just remove it, maybe we want to add some additional logging to make the situation more readable for users etc.). Or we should have a Github issue for fixing this later and have it linked here so that we get back to it.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551863172", "createdAt": "2021-01-05T11:04:15Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2NzE2Mg==", "bodyText": "@tombentley I think the problem is that having the test more realistic also means it does not pass. When the CA secret is deleted, the operator just generates a new one and rolls the pods. But the cluster has to get partitioned during that. The old (not-yet-rolled) pods and the new (already-rolled) pods will each use distinct set of SSL keys and will form two separate partitions which will not link. So with some traffic going on, this always has to fail because you cannot keep the replicas in-sync. So I think this test will be deleted sooner or later. But we should probably think about if we need to somehow improve the logging in these situations etc. Because it could be confusing for the user.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551867162", "createdAt": "2021-01-05T11:12:38Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjUyNzI0Mw==", "bodyText": "#4187", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r552527243", "createdAt": "2021-01-06T11:45:50Z", "author": {"login": "alesj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/AlternativeReconcileTriggersST.java", "diffHunk": "@@ -208,6 +209,18 @@ void testManualTriggeringRollingUpdate() {\n      *  7. Try consumer, producer and consume rmeessages again with new certificates\n      */\n     @Test\n+    @Disabled // fix-it ...", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3MzYxNg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MTMzODY0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMToxNzo1MlrOIOBaDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxMDoyNjozM1rOIOR3Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3NDAyOA==", "bodyText": "Can you keep the formatting we had before here? It was intentional.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551574028", "createdAt": "2021-01-04T21:17:52Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java", "diffHunk": "@@ -72,19 +72,27 @@ void testKafkaRollsWhenTopicIsUnderReplicated() {\n         String topicName = KafkaTopicUtils.generateRandomNameOfTopic();\n         timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n \n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 4)\n-            .editSpec()\n+        // We need to start with 3 replicas / brokers,\n+        // so that KafkaStreamsTopicStore topic gets set/distributed on this first 3 [0, 1, 2],\n+        // since this topic has replication-factor 3 and minISR 2.\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+                .editSpec()\n                 .editKafka()\n-                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .addToConfig(\"auto.create.topics.enable\", \"false\")\n                 .endKafka()\n-            .endSpec()\n-            .done();\n+                .endSpec()\n+                .done();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MzY3MQ==", "bodyText": "Ah, OK.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551843671", "createdAt": "2021-01-05T10:26:33Z", "author": {"login": "alesj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/rollingupdate/KafkaRollerST.java", "diffHunk": "@@ -72,19 +72,27 @@ void testKafkaRollsWhenTopicIsUnderReplicated() {\n         String topicName = KafkaTopicUtils.generateRandomNameOfTopic();\n         timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n \n-        KafkaResource.kafkaPersistent(CLUSTER_NAME, 4)\n-            .editSpec()\n+        // We need to start with 3 replicas / brokers,\n+        // so that KafkaStreamsTopicStore topic gets set/distributed on this first 3 [0, 1, 2],\n+        // since this topic has replication-factor 3 and minISR 2.\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+                .editSpec()\n                 .editKafka()\n-                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .addToConfig(\"auto.create.topics.enable\", \"false\")\n                 .endKafka()\n-            .endSpec()\n-            .done();\n+                .endSpec()\n+                .done();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU3NDAyOA=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ3MTM5MTQyOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMTozNToyOFrOIOB6Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxMDozNjowOFrOIOSK8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU4MjI0Mg==", "bodyText": "Are these the names of the topics? Shouldn't they have the __ prefix? Also, would it make sense to start with the same name? E.g. strimzi-topic-operator-store instead of strimzi-store-topic when the other topic is named strimzi-topic-operator...?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551582242", "createdAt": "2021-01-04T21:35:28Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java", "diffHunk": "@@ -671,6 +671,8 @@ void setupEnvironment() throws Exception {\n         list.add(CruiseControlUtils.CRUISE_CONTROL_METRICS_TOPIC);\n         list.add(CruiseControlUtils.CRUISE_CONTROL_MODEL_TRAINING_SAMPLES_TOPIC);\n         list.add(CruiseControlUtils.CRUISE_CONTROL_PARTITION_METRICS_SAMPLES_TOPIC);\n+        list.add(\"strimzi-store-topic\");\n+        list.add(\"strimzi-topic-operator-kstreams-topic-store-changelog\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0ODY5MA==", "bodyText": "This was already a long discussion ... but sure, we can always change it?\n@ppatierno @tombentley wdyt? ^", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3405#discussion_r551848690", "createdAt": "2021-01-05T10:36:08Z", "author": {"login": "alesj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/metrics/MetricsST.java", "diffHunk": "@@ -671,6 +671,8 @@ void setupEnvironment() throws Exception {\n         list.add(CruiseControlUtils.CRUISE_CONTROL_METRICS_TOPIC);\n         list.add(CruiseControlUtils.CRUISE_CONTROL_MODEL_TRAINING_SAMPLES_TOPIC);\n         list.add(CruiseControlUtils.CRUISE_CONTROL_PARTITION_METRICS_SAMPLES_TOPIC);\n+        list.add(\"strimzi-store-topic\");\n+        list.add(\"strimzi-topic-operator-kstreams-topic-store-changelog\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU4MjI0Mg=="}, "originalCommit": {"oid": "6f586b15027e8873b6f8c56d01c6e6e1f4699b2d"}, "originalPosition": 47}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1557, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}