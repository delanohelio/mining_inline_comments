{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MDUzMjc1", "number": 3469, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDowNjo0MlrOEV-Z6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo1MTo1M1rOEZ7Efg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDc3OTkzOnYy", "diffSide": "RIGHT", "path": "CHANGELOG.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDowNjo0MlrOG9Ce1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDowNjo0MlrOG9Ce1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1Njk4MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Support dynamically changeable logging in the Kafka brokers\n          \n          \n            \n            * Support dynamically changeable logging configuration in the Kafka brokers", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r466656981", "createdAt": "2020-08-06T20:06:42Z", "author": {"login": "scholzj"}, "path": "CHANGELOG.md", "diffHunk": "@@ -7,6 +7,7 @@\n * Cruise Control metrics integration:\n   * enable metrics JMX exporter configuration in the `cruiseControl` property of the Kafka custom resource\n   * new Grafana dashboard for the Cruise Control metrics\n+* Support dynamically changeable logging in the Kafka brokers", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88e9fdb021e0332369ca3b84a2b8a87329b90792"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDc5MjA5OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxMDo1M1rOG9Cmeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxMDo1M1rOG9Cmeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1ODkzOA==", "bodyText": "I'm not sure I understand the naming and the Javadoc here. If you add something to the map, it is not updated yet so the parameter name is wrong? I'm also not sure why a separate method is needed here. It looks like any call for this method can be done in single line.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r466658938", "createdAt": "2020-08-06T20:10:53Z", "author": {"login": "scholzj"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -111,10 +112,11 @@ private boolean isEntryReadOnly(ConfigEntry entry) {\n     }\n \n     /**\n-     * @return A map which can be used for dynamic configuration of kafka broker\n+     * Adds an entry to a map which can be used for dynamic configuration of kafka broker\n+     * @param updatedConfig map to add an entry\n      */\n-    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n-        return Collections.singletonMap(Util.getBrokersConfig(brokerId), diff);\n+    public void addConfigDiff(Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88e9fdb021e0332369ca3b84a2b8a87329b90792"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNDgwNjgwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/con-kafka-logging.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxNTo1MVrOG9Cv2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoxNTo1MVrOG9Cv2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2MTMzOA==", "bodyText": "It looks like this covers also Zookeeper which is not dynamically configurable?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r466661338", "createdAt": "2020-08-06T20:15:51Z", "author": {"login": "scholzj"}, "path": "documentation/modules/con-kafka-logging.adoc", "diffHunk": "@@ -86,6 +85,8 @@ spec:\n   # ...\n ----\n \n+Both external and inline logging changes are configured dynamically.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88e9fdb021e0332369ca3b84a2b8a87329b90792"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMzQ1NjQ2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQxMzo1MzoxM1rOG-PhOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwOTo0NzoyN1rOG-wewA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw==", "bodyText": "Can you please move these tests before the @BeforeAll (after the bridge logging test)?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r467919163", "createdAt": "2020-08-10T13:53:13Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -472,4 +472,212 @@ protected void tearDownEnvironmentAfterAll() {\n     protected void assertNoCoErrorsLogged(long sinceSeconds) {\n         LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n     }\n+\n+    @Test\n+    void testDynamicallySetKafkaLoggingLevels() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging ilOff = new InlineLogging();\n+        ilOff.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"INFO\"));\n+\n+        LOGGER.info(\"Changing rootLogger level to DEBUG with inline logging\");\n+        InlineLogging ilDebug = new InlineLogging();\n+        ilDebug.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"DEBUG\"));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(ilDebug);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                .contains(\"root=DEBUG\"));\n+\n+        LOGGER.info(\"Setting external logging INFO\");\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-configmap\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+\n+        ExternalLogging elKafka = new ExternalLoggingBuilder()\n+                .withName(\"external-configmap\")\n+                .build();\n+\n+        LOGGER.info(\"Setting log level of kafka INFO\");\n+        // change to external logging\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(elKafka);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                    \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                    .contains(\"root=INFO\"));\n+\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLogger() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.logger.paprika\", \"INFO\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"paprika=INFO\"));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLoggerValue() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"PAPRIKA\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetKafkaExternalLogging() {\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        ExternalLogging el = new ExternalLogging();\n+        el.setName(\"external-cm\");\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1)\n+                .editOrNewSpec()\n+                    .editKafka()\n+                        .withExternalLogging(el)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=ERROR\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=ERROR\"));\n+\n+        // log4j.appender.CONSOLE.layout.ConversionPattern is changed and thus we need RU\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=DEBUG\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=DEBUG\"));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dde48c57ebd145860e333bcd33b20c40869e1d8"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3OTA3OA==", "bodyText": "Do the STs have BeforeAll methods at the end of the class?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468179078", "createdAt": "2020-08-10T20:52:33Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -472,4 +472,212 @@ protected void tearDownEnvironmentAfterAll() {\n     protected void assertNoCoErrorsLogged(long sinceSeconds) {\n         LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n     }\n+\n+    @Test\n+    void testDynamicallySetKafkaLoggingLevels() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging ilOff = new InlineLogging();\n+        ilOff.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"INFO\"));\n+\n+        LOGGER.info(\"Changing rootLogger level to DEBUG with inline logging\");\n+        InlineLogging ilDebug = new InlineLogging();\n+        ilDebug.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"DEBUG\"));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(ilDebug);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                .contains(\"root=DEBUG\"));\n+\n+        LOGGER.info(\"Setting external logging INFO\");\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-configmap\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+\n+        ExternalLogging elKafka = new ExternalLoggingBuilder()\n+                .withName(\"external-configmap\")\n+                .build();\n+\n+        LOGGER.info(\"Setting log level of kafka INFO\");\n+        // change to external logging\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(elKafka);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                    \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                    .contains(\"root=INFO\"));\n+\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLogger() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.logger.paprika\", \"INFO\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"paprika=INFO\"));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLoggerValue() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"PAPRIKA\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetKafkaExternalLogging() {\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        ExternalLogging el = new ExternalLogging();\n+        el.setName(\"external-cm\");\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1)\n+                .editOrNewSpec()\n+                    .editKafka()\n+                        .withExternalLogging(el)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=ERROR\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=ERROR\"));\n+\n+        // log4j.appender.CONSOLE.layout.ConversionPattern is changed and thus we need RU\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=DEBUG\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=DEBUG\"));\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, "originalCommit": {"oid": "5dde48c57ebd145860e333bcd33b20c40869e1d8"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODQ1ODg2Mw==", "bodyText": "They should have \ud83d\ude04 at least for me it's more readable. I think that some STs are little bit mixed, but I'm gonna fix this in some future PRs :)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468458863", "createdAt": "2020-08-11T09:46:50Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -472,4 +472,212 @@ protected void tearDownEnvironmentAfterAll() {\n     protected void assertNoCoErrorsLogged(long sinceSeconds) {\n         LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n     }\n+\n+    @Test\n+    void testDynamicallySetKafkaLoggingLevels() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging ilOff = new InlineLogging();\n+        ilOff.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"INFO\"));\n+\n+        LOGGER.info(\"Changing rootLogger level to DEBUG with inline logging\");\n+        InlineLogging ilDebug = new InlineLogging();\n+        ilDebug.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"DEBUG\"));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(ilDebug);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                .contains(\"root=DEBUG\"));\n+\n+        LOGGER.info(\"Setting external logging INFO\");\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-configmap\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+\n+        ExternalLogging elKafka = new ExternalLoggingBuilder()\n+                .withName(\"external-configmap\")\n+                .build();\n+\n+        LOGGER.info(\"Setting log level of kafka INFO\");\n+        // change to external logging\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(elKafka);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                    \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                    .contains(\"root=INFO\"));\n+\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLogger() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.logger.paprika\", \"INFO\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"paprika=INFO\"));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLoggerValue() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"PAPRIKA\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetKafkaExternalLogging() {\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        ExternalLogging el = new ExternalLogging();\n+        el.setName(\"external-cm\");\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1)\n+                .editOrNewSpec()\n+                    .editKafka()\n+                        .withExternalLogging(el)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=ERROR\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=ERROR\"));\n+\n+        // log4j.appender.CONSOLE.layout.ConversionPattern is changed and thus we need RU\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=DEBUG\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=DEBUG\"));\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, "originalCommit": {"oid": "5dde48c57ebd145860e333bcd33b20c40869e1d8"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODQ1OTIwMA==", "bodyText": "Or do you think that BeforeAll should be on the beginning of the class?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468459200", "createdAt": "2020-08-11T09:47:27Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -472,4 +472,212 @@ protected void tearDownEnvironmentAfterAll() {\n     protected void assertNoCoErrorsLogged(long sinceSeconds) {\n         LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n     }\n+\n+    @Test\n+    void testDynamicallySetKafkaLoggingLevels() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging ilOff = new InlineLogging();\n+        ilOff.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"INFO\"));\n+\n+        LOGGER.info(\"Changing rootLogger level to DEBUG with inline logging\");\n+        InlineLogging ilDebug = new InlineLogging();\n+        ilDebug.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"DEBUG\"));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(ilDebug);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                .contains(\"root=DEBUG\"));\n+\n+        LOGGER.info(\"Setting external logging INFO\");\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-configmap\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+\n+        ExternalLogging elKafka = new ExternalLoggingBuilder()\n+                .withName(\"external-configmap\")\n+                .build();\n+\n+        LOGGER.info(\"Setting log level of kafka INFO\");\n+        // change to external logging\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(elKafka);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                    \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                    .contains(\"root=INFO\"));\n+\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLogger() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.logger.paprika\", \"INFO\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"paprika=INFO\"));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLoggerValue() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"PAPRIKA\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetKafkaExternalLogging() {\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        ExternalLogging el = new ExternalLogging();\n+        el.setName(\"external-cm\");\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1)\n+                .editOrNewSpec()\n+                    .editKafka()\n+                        .withExternalLogging(el)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=ERROR\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=ERROR\"));\n+\n+        // log4j.appender.CONSOLE.layout.ConversionPattern is changed and thus we need RU\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=DEBUG\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=DEBUG\"));\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, "originalCommit": {"oid": "5dde48c57ebd145860e333bcd33b20c40869e1d8"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg1Mzg5OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowMDoyMlrOHBeUpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowMDoyMlrOHBeUpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNzQzMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (entry.getKey().startsWith(\"log4j.appender\")) {\n          \n          \n            \n                        if (entry.getKey().startsWith(\"log4j.appender.\")) {", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471307430", "createdAt": "2020-08-17T08:00:22Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {\n+        OrderedProperties ops = new OrderedProperties();\n+        ops.addStringPairs(loggingConfiguration);\n+        StringBuilder result = new StringBuilder();\n+        for (Map.Entry<String, String> entry: ops.asMap().entrySet()) {\n+            if (entry.getKey().startsWith(\"log4j.appender\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg1OTM2OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowMjowMlrOHBeYCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowMjowMlrOHBeYCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwODI5Nw==", "bodyText": "Factor out patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true) into a constant.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471308297", "createdAt": "2020-08-17T08:02:02Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -159,8 +159,8 @@ private static boolean isIgnorableProperty(String key) {\n \n         fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n \n-        JsonNode source = patchMapper().valueToTree(currentMap);\n-        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg2MjU5OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowMzowNFrOHBeZ6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowMzowNFrOHBeZ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwODc3Ng==", "bodyText": "And use the constant here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471308776", "createdAt": "2020-08-17T08:03:04Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg4MDYxOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowODo0N1rOHBekmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODowODo0N1rOHBekmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMTUxNQ==", "bodyText": "Any reason why you can't change this in the desired map like you do for the ones starting log4j.logger?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471311515", "createdAt": "2020-08-17T08:08:47Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg4OTQzOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxMToxN1rOHBeptw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxMToxN1rOHBeptw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMjgyMw==", "bodyText": "More efficient to use indexOf(\",\") + trim(). Both split and replaceAll will have to use Pattern under the hood.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471312823", "createdAt": "2020-08-17T08:11:17Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                        String level = parseLogLevelFromAppenderCouple(desiredMap.get(pathValueWithoutSlash));\n+                        if (!level.equals(currentMap.get(\"root\"))) {\n+                            updateOrAddRoot(level, updatedCE);\n+                        }\n+                    } else {\n+                        // entry is not in the current, it is added\n+                        updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String lev) {\n+        String[] arr = lev.split(\",\");\n+        String level = arr[0].replaceAll(\"\\\\s\", \"\");\n+        return level;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg5NDc0OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxMjo0MlrOHBes1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxMjo0MlrOHBes1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMzYyMw==", "bodyText": "What do you think I'm going to tell you about these two methods?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471313623", "createdAt": "2020-08-17T08:12:42Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                        String level = parseLogLevelFromAppenderCouple(desiredMap.get(pathValueWithoutSlash));\n+                        if (!level.equals(currentMap.get(\"root\"))) {\n+                            updateOrAddRoot(level, updatedCE);\n+                        }\n+                    } else {\n+                        // entry is not in the current, it is added\n+                        updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String lev) {\n+        String[] arr = lev.split(\",\");\n+        String level = arr[0].replaceAll(\"\\\\s\", \"\");\n+        return level;\n+    }\n+\n+    private static void updateOrAddRoot(String level, Collection<AlterConfigOp> updatedCE) {\n+        level = parseLogLevelFromAppenderCouple(level);\n+        if (isValidLoggerLevel(level)) {\n+            updatedCE.add(new AlterConfigOp(new ConfigEntry(\"root\", level), AlterConfigOp.OpType.SET));\n+            log.trace(\"{} not set in current or has deprecated value. Setting to {}\", \"root\", level);\n+        } else {\n+            log.warn(\"Level {} is not valid logging level\", level);\n+        }\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n+            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n+            if (isValidLoggerLevel(level)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n+            } else {\n+                log.warn(\"Level {} is not valid logging level\", level);\n+            }\n+        }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTg5Njk2OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxMzoxOFrOHBeuJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxMzoxOFrOHBeuJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMzk1OQ==", "bodyText": "It doesn't return a Future.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471313959", "createdAt": "2020-08-17T08:13:18Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -433,22 +444,44 @@ private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, Int\n      * @return a Future which completes with the config of the given broker.\n      */\n     protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n-        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        ConfigResource resource = Util.getBrokersConfig(brokerId);\n         return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n             30, TimeUnit.SECONDS,\n             error -> new ForceableProblem(\"Error getting broker config\", error)\n         );\n     }\n \n-    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n+    /**\n+     * Returns a Future which completes with the logging of the given broker.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTkxODkwOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxODo0N1rOHBe6bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoxODo0N1rOHBe6bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzEwMw==", "bodyText": "TBH, I'm not completely convinced it's worth adding the dynamic logging changes to the KafkaRoller. KafkaRoller's job is to safely orchestrate the restart or reconfiguration of some or all brokers in the cluster. Changing a broker's logging doesn't seem to me to be a very risky thing to do. It's not going to need any special pre- or post-conditions before/after happening, for example. I can see that from a certain PoV it makes sense to do it here, but the question is whether adding extra complexity to KafkaRoller to do it here really makes sense, compared with some other phase of the reconciliation which updates the logging after brokers have been restarted/reconfigured.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471317103", "createdAt": "2020-08-17T08:18:47Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -271,13 +274,15 @@ public String toString() {\n         private final boolean needsRestart;\n         private final boolean needsReconfig;\n         private final KafkaBrokerConfigurationDiff diff;\n+        private final KafkaBrokerLoggingConfigurationDiff logDiff;\n         private final Admin adminClient;\n \n-        public RestartPlan(Admin adminClient, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff) {\n+        public RestartPlan(Admin adminClient, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff, KafkaBrokerLoggingConfigurationDiff logDiff) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0NTkyMzg5OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwODoyMDoxMFrOHBe9Vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QwOTo1NDo1NVrOHBiHOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw==", "bodyText": "Do we need this? It seems the annotation is added, but never used? If we do need it then I think a little Javadoc about how it's used wouldn't go amiss.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471317847", "createdAt": "2020-08-17T08:20:10Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM1NjYxNA==", "bodyText": "It is used in KAO", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471356614", "createdAt": "2020-08-17T09:31:01Z", "author": {"login": "sknot-rh"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2Mzg5NA==", "bodyText": "Yeah, it's added there, (https://github.com/strimzi/strimzi-kafka-operator/pull/3469/files#diff-f19c7c3bdf294affc86939228666be84R2458), but never read. Why is it needed if it's never read? Is it simply so that we can tell that the logging was changed? If so then add a comment explaining how that works.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471363894", "createdAt": "2020-08-17T09:44:29Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2Njk5NA==", "bodyText": "It changes the annotation an thus kafka pods are rolled (appenders are not changeable dynamically). I will add a doc.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471366994", "createdAt": "2020-08-17T09:50:30Z", "author": {"login": "sknot-rh"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2OTUyOA==", "bodyText": "Thanks, I know it makes sense to us, here and now, but I for one will forget this within about 5 minutes and stuff like this makes it harder for people who are not familiar with it to understand what the purpose of this is. Can you also comment the other hash which is used in the same way?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471369528", "createdAt": "2020-08-17T09:54:55Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, "originalCommit": {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzM2OTIzOnYy", "diffSide": "RIGHT", "path": "CHANGELOG.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDo1MTo1NFrOHCmPpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDo1MTo1NFrOHCmPpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4NTc5OA==", "bodyText": "Maybe check with someone from docs ... should it be of Kafka brokers?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472485798", "createdAt": "2020-08-18T20:51:54Z", "author": {"login": "scholzj"}, "path": "CHANGELOG.md", "diffHunk": "@@ -11,6 +11,7 @@\n   * new Grafana dashboard for the Cruise Control metrics\n * Configure Cluster Operator logging using ConfigMap instead of environment variable and support dynamic changes  \n * Switch to use the `AclAuthorizer` class for the `simple` Kafka authorization type. `AclAuthorizer` contains new features such as the ability to control the amount of authorization logs in the broker logs.\n+* Support dynamically changeable logging configuration in the Kafka brokers  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef83c883c47bf680286850675c6bca21245ecdc1"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzM3NjY4OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDo1Mzo1MVrOHCmT6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwNjoyMDozN1rOHC2XJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ==", "bodyText": "What is the ANNO_STRIMZI_LOGGING_HASH used for now? Is it still used somewhere? I saw it is not used anymore on line 2458 in Kafka assembly operator.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472486889", "createdAt": "2020-08-18T20:53:51Z", "author": {"login": "scholzj"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef83c883c47bf680286850675c6bca21245ecdc1"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc0NTA4OA==", "bodyText": "It is still used for rolling updates of ZK pods when ZK logging is changed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472745088", "createdAt": "2020-08-19T06:13:58Z", "author": {"login": "sknot-rh"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ=="}, "originalCommit": {"oid": "ef83c883c47bf680286850675c6bca21245ecdc1"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc0OTg2Mw==", "bodyText": "Ok, makes sense. Thanks.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472749863", "createdAt": "2020-08-19T06:20:37Z", "author": {"login": "scholzj"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ=="}, "originalCommit": {"oid": "ef83c883c47bf680286850675c6bca21245ecdc1"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzU0NjgxOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMTo0OTo1N1rOHCn66g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMTo0OTo1N1rOHCn66g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjUxMzI1OA==", "bodyText": "Should these be in Annotations.java?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472513258", "createdAt": "2020-08-18T21:49:57Z", "author": {"login": "scholzj"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef83c883c47bf680286850675c6bca21245ecdc1"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA5OTk4OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozMDozMVrOHDBPAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwNzoxMDoyMFrOHDvAWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyODAwMA==", "bodyText": "Can be static?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472928000", "createdAt": "2020-08-19T10:30:31Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzY3NzkxNA==", "bodyText": "In the other PR it is under Utils class so it will be refactored later.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r473677914", "createdAt": "2020-08-20T07:10:20Z", "author": {"login": "sknot-rh"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyODAwMA=="}, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjEwODk0OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozMzowNFrOHDBUXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozMzowNFrOHDBUXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTM3NA==", "bodyText": "I think we probably don't want to force the rest of Fabric 8's patch code to use ordered maps when computing diffs. So we should probably instantiate our own ObjectMapper here since it's the only place where we do case about order (right?)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472929374", "createdAt": "2020-08-19T10:33:04Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private static final ObjectMapper MAPPER = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjExNjY5OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozNDo1OFrOHDBYng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozNDo1OFrOHDBYng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMDQ2Mg==", "bodyText": "Why did use use a List for VALID_LOGGER_LEVELS if you're only needing contains()? A HashSet will be more efficient since it likely needs only 1 hash() and 1 or maybe 2 equals().", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472930462", "createdAt": "2020-08-19T10:34:58Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private static final ObjectMapper MAPPER = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = MAPPER.valueToTree(currentMap);\n+        JsonNode target = MAPPER.valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    // entry is not in the current, it is added\n+                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String level) {\n+        int index = level.indexOf(\",\");\n+        if (index > 0) {\n+            return level.substring(0, index).trim();\n+        } else {\n+            return level.trim();\n+        }\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n+            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n+            if (isValidLoggerLevel(level)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n+            } else {\n+                log.warn(\"Level {} is not valid logging level\", level);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * All loggers can be set dynamically. If the logger is not set in desire, set it to ERROR. Loggers with already set to ERROR should be skipped.\n+     * ERROR is set as inactive because log4j does not support OFF logger value.\n+     * We want to skip \"root\" logger as well to avoid duplicated key in alterConfigOps collection.\n+     * @param alterConfigOps collection of AlterConfigOp\n+     * @param pathValueWithoutSlash name of \"removed\" logger\n+     * @param entry entry to be removed (set to ERROR)\n+     */\n+    private static void removeProperty(Collection<AlterConfigOp> alterConfigOps, String pathValueWithoutSlash, ConfigEntry entry) {\n+        if (!pathValueWithoutSlash.contains(\"log4j.appender\") && !pathValueWithoutSlash.equals(\"root\") && !\"ERROR\".equals(entry.value())) {\n+            alterConfigOps.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, \"ERROR\"), AlterConfigOp.OpType.SET));\n+            log.trace(\"{} not set in desired, setting to ERROR\", entry.name());\n+        }\n+    }\n+\n+    /**\n+     * @return whether the current config and the desired config are identical (thus, no update is necessary).\n+     */\n+    @Override\n+    public boolean isEmpty() {\n+        return  diff.size() == 0;\n+    }\n+\n+    private static boolean isValidLoggerLevel(String level) {\n+        return VALID_LOGGER_LEVELS.contains(level);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjEyODY3OnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozODowM1rOHDBfTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozODowM1rOHDBfTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMjE3NA==", "bodyText": "You just wrote a Util method for new ConfigResource(ConfigResource.Type.BROKER_LOGGER, so use it!", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472932174", "createdAt": "2020-08-19T10:38:03Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -420,35 +427,61 @@ private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, Int\n                     needsRestart = true;\n                 }\n             }\n+            if (loggingDiff.getDiffSize() > 0) {\n+                log.info(\"{}: Pod {} logging needs to be reconfigured.\", reconciliation, podId);\n+                needsReconfig = true;\n+            }\n         } else {\n             log.info(\"{}: Pod {} needs to be restarted. Reason: {}\", reconciliation, podId, reasonToRestartPod);\n         }\n-        return new RestartPlan(adminClient, needsRestart, needsReconfig, diff);\n+        return new RestartPlan(adminClient, needsRestart, needsReconfig, diff, loggingDiff);\n     }\n \n     /**\n-     * Returns a Future which completes with the config of the given broker.\n+     * Returns a config of the given broker.\n      * @param ac The admin client\n      * @param brokerId The id of the broker.\n      * @return a Future which completes with the config of the given broker.\n      */\n     protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n-        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        ConfigResource resource = Util.getBrokersConfig(brokerId);\n         return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n             30, TimeUnit.SECONDS,\n             error -> new ForceableProblem(\"Error getting broker config\", error)\n         );\n     }\n \n-    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n+    /**\n+     * Returns logging of the given broker.\n+     * @param ac The admin client\n+     * @param brokerId The id of the broker.\n+     * @return a Future which completes with the logging of the given broker.\n+     */\n+    protected Config brokerLogging(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n+        ConfigResource resource = Util.getBrokersLogging(brokerId);\n+        return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n+                30, TimeUnit.SECONDS,\n+            error -> new ForceableProblem(\"Error getting broker logging\", error)\n+        );\n+    }\n+\n+    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff, KafkaBrokerLoggingConfigurationDiff logDiff)\n             throws ForceableProblem, InterruptedException {\n-        Map<ConfigResource, Collection<AlterConfigOp>> configDiff = configurationDiff.getConfigDiff();\n-        log.debug(\"{}: Altering broker {} with {}\", reconciliation, podId, configDiff);\n-        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configDiff);\n+        Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig = new HashMap<>(2);\n+        updatedConfig.put(Util.getBrokersConfig(podId), configurationDiff.getConfigDiff());\n+        updatedConfig.put(Util.getBrokersLogging(podId), logDiff.getLoggingDiff());\n+\n+        log.info(\"{}: Altering broker {} with {}\", reconciliation, podId, updatedConfig);\n+\n+        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(updatedConfig);\n         KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(podId)));\n+        KafkaFuture<Void> brokerLoggingConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER_LOGGER, Integer.toString(podId)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjEzNjYwOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0MDoyMFrOHDBj2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0MDoyMFrOHDBj2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMzMzNw==", "bodyText": "Is it even worth putting a two line file in a resource like that? Just use a String literal.\nIf you must use a file, the name desired-kafka-broker-logging.conf is a bit weird. It's a properties file, so call it with .properties suffix at least.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472933337", "createdAt": "2020-08-19T10:40:20Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaBrokerLoggingConfigurationDiffTest {\n+\n+    private final int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker-logging.conf\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE0MjEyOnYy", "diffSide": "RIGHT", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0MTo0OVrOHDBnFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0MTo0OVrOHDBnFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDE2NA==", "bodyText": "Same comment.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472934164", "createdAt": "2020-08-19T10:41:49Z", "author": {"login": "tombentley"}, "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaBrokerLoggingConfigurationDiffTest {\n+\n+    private final int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker-logging.conf\")) {\n+            StringBuilder desiredConfigString = new StringBuilder(TestUtils.readResource(is));\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString.append(\"\\n\").append(ce.name()).append(\"=\").append(ce.value());\n+            }\n+\n+            return desiredConfigString.toString();\n+        } catch (IOException e) {\n+            fail(e);\n+        }\n+        return \"\";\n+    }\n+\n+    private Config getCurrentConfiguration(List<ConfigEntry> additional) {\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker-logging.conf\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE0OTA5OnYy", "diffSide": "RIGHT", "path": "documentation/assemblies/assembly-external-logging.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0Mzo1N1rOHDBrUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0Mzo1N1rOHDBrUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNTI1MA==", "bodyText": "Looking at https://strimzi.io/docs/operators/master/using.html#external-logging_str\nit seems that people might try to configure the TO or UO like this, which would be wrong since they use log4j2. I think we need to move the explanation about the different logging up before we start giving examples on internal vs external logging configuration.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472935250", "createdAt": "2020-08-19T10:43:57Z", "author": {"login": "tombentley"}, "path": "documentation/assemblies/assembly-external-logging.adoc", "diffHunk": "@@ -14,7 +14,7 @@ spec:\n   logging:\n     type: inline\n     loggers:\n-      kafka.root.logger.level: \"INFO\"\n+      log4j.logger.kafka: \"INFO\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE3Mjk0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/con-kafka-logging.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo1MDo0OFrOHDB5ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo1MDo0OFrOHDB5ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzODg1OA==", "bodyText": "Lets use language that's more self explanatory:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Both external and inline logging changes in Kafka brokers are configured dynamically.\n          \n          \n            \n            Changes to both external and inline logging levels will be applied to Kafka brokers without a restart.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472938858", "createdAt": "2020-08-19T10:50:48Z", "author": {"login": "tombentley"}, "path": "documentation/modules/con-kafka-logging.adoc", "diffHunk": "@@ -86,6 +85,8 @@ spec:\n   # ...\n ----\n \n+Both external and inline logging changes in Kafka brokers are configured dynamically.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE3NjYyOnYy", "diffSide": "RIGHT", "path": "operator-common/src/main/java/io/strimzi/operator/common/Annotations.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo1MTo1M1rOHDB7lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo1MTo1M1rOHDB7lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzOTQxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n          \n          \n            \n                 * Annotations for rolling a cluster whenever the logging (or it's part) has changed.\n          \n          \n            \n                   By changing the annotation we force a restart since the pod will be out of date compared to the statefulset.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472939414", "createdAt": "2020-08-19T10:51:53Z", "author": {"login": "tombentley"}, "path": "operator-common/src/main/java/io/strimzi/operator/common/Annotations.java", "diffHunk": "@@ -18,6 +18,12 @@\n \n     public static final String STRIMZI_DOMAIN = \"strimzi.io/\";\n     public static final String STRIMZI_LOGGING_ANNOTATION = STRIMZI_DOMAIN + \"logging\";\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1276, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}