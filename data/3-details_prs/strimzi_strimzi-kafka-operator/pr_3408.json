{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4Mzc0Mjk3", "number": 3408, "title": "[MO] - [dyn.conf] -> system tests", "bodyText": "Signed-off-by: morsak xorsak02@stud.fit.vutbr.cz\nType of change\n\nEnhancement / new feature\n\nDescription\nThis PR adding tests for dynamic configuration.\nHow do these tests work?\nBasically each property is verified by method verifyDynamicConfiguration from KafkaUtils. Inside verifyDynamicConfiguration we have two major steps. The first one is to update the selected property and then we call the method verifyThatRunningPodsAreStable from PodUtils to make sure that pods did not roll (dynamic conf). The second one is verified phase when I just compare 2 values.\nHow long?\nfor suite DynamicConfigurationSharedST :\n\nChecklist\n\n Write tests\n Make sure all tests pass", "createdAt": "2020-07-29T11:23:16Z", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408", "merged": true, "mergeCommit": {"oid": "4d4a467a0a7baf763708d8169d878ad098a2e410"}, "closed": true, "closedAt": "2020-09-09T18:35:08Z", "author": {"login": "see-quick"}, "timelineItems": {"totalCount": 73, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5rKTMAFqTQ1NzUyNzk1Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHHh5gAFqTQ4NDc1NTE2NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3NTI3OTUy", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-457527952", "createdAt": "2020-07-29T13:36:56Z", "commit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3ODc0MTY2", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-457874166", "createdAt": "2020-07-29T20:31:37Z", "commit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozMTozN1rOG5JDCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQyMDozODoxM1rOG5JQsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDI1MA==", "bodyText": "Wouldn't it be better to make some variable for this? Because you use it in AssertionError too.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570250", "createdAt": "2020-07-29T20:31:37Z", "author": {"login": "im-konge"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3MDU4Mg==", "bodyText": "To the variable I mentioned above (just as note)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462570582", "createdAt": "2020-07-29T20:32:13Z", "author": {"login": "im-konge"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);\n+\n+        boolean result = KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()) == value;\n+\n+        if (!result) {\n+            throw new AssertionError(KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString() + \" value doesn't match to expected value \" + value));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU3Mzc0Ng==", "bodyText": "Just thinking -> is this encapsulation needed? I know the code is cleaner, but I don't know if this is worth for these two lines.\nAnyway the KafkaUtils is not needed.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r462573746", "createdAt": "2020-07-29T20:38:13Z", "author": {"login": "im-konge"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +153,47 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, which encapsulates the update phase of dyn. configuration + verifying that updating configuration were successfully done\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void verifyDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaUtils.updateConfigurationWithStabilityWait(clusterName, kafkaDynamicConfiguration, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95e0f621c7e649132ed6849083368832adb6970a"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4NjQ4Mjg0", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-458648284", "createdAt": "2020-07-30T18:21:14Z", "commit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODoyMToxNFrOG5uqOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxODozNjoyNlrOG5vKXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NjQ4OQ==", "bodyText": "KafkaDynamicConfiguration? typo?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463186489", "createdAt": "2020-07-30T18:21:14Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4NzcxOA==", "bodyText": "What is the source of these? I'm a bit in doubt about this. If it is so important that we have to test every single of these values, then this should be probably autogenerated from the Kafka sources and needs to also distinguish and test it for all Kafka versions.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463187718", "createdAt": "2020-07-30T18:23:29Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/enums/KafkaDynamicConfiguration.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.enums;\n+\n+/**\n+ * KafkaConfiguration enum class, which provides all supported configuration, which does not need to trigger rolling-update (dynamic configuration)\n+ */\n+public enum KafkaDynamicConfiguration {\n+\n+    background_threads,\n+    compression_type,\n+    min_insync_replicas,\n+    unclean_leader_election_enable,\n+    message_max_bytes,\n+    metric_reporters,\n+\n+    log_flush_interval_messages,\n+    log_flush_interval_ms,\n+    log_retention_bytes,\n+    log_retention_ms,\n+    log_roll_jitter_ms,\n+    log_roll_ms,\n+    log_segment_bytes,\n+    log_segment_delete_delay_ms,\n+    log_cleaner_backoff_ms,\n+    log_cleaner_dedupe_buffer_size,\n+    log_cleaner_delete_retention_ms,\n+    log_cleaner_io_buffer_load_factor,\n+    log_cleaner_io_buffer_size,\n+    log_cleaner_io_max_bytes_per_second,\n+    log_cleaner_max_compaction_lag_ms,\n+    log_cleaner_min_cleanable_ratio,\n+    log_cleaner_min_compaction_lag_ms,\n+    log_cleaner_threads,\n+    log_cleanup_policy,\n+    log_index_interval_bytes,\n+    log_index_size_max_bytes,\n+    log_message_timestamp_difference_max_ms,\n+    log_message_timestamp_type,\n+    log_message_downconversion_enable,\n+    log_preallocate,\n+\n+    num_io_threads,\n+    num_network_threads,\n+    num_recovery_threads_per_data_dir,\n+    num_replica_fetchers,\n+\n+    max_connections,\n+    max_connections_per_ip,\n+    max_connections_per_ip_overrides;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTU5OQ==", "bodyText": "The unclean.leader.election.enable should show up here as well. So maybe you can assert it too?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189599", "createdAt": "2020-07-30T18:26:46Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE4OTk4Nw==", "bodyText": "Is this some copy paster left-over? Or what is the value of this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463189987", "createdAt": "2020-07-30T18:27:29Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MTQxOA==", "bodyText": "Using both node port and loadbalancer in the same test will mean that it works only in environment which supports both. Can't we find some listener change which is less restrictive to the environment where you run this? For example use only node ports and change advertised hostnames?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463191418", "createdAt": "2020-07-30T18:30:07Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjMyNA==", "bodyText": "A very long test which seems a bit complicated. Having a comment properly explaining what it does would be helpful.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192324", "createdAt": "2020-07-30T18:31:44Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MjY0NA==", "bodyText": "You seem to be doing rolling updates. Please use persistent cluster.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192644", "createdAt": "2020-07-30T18:32:27Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5Mjk3MQ==", "bodyText": "Again ... can we find something less restrictive than both load balancers and node ports? Also, should these tags be on the text above as well?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463192971", "createdAt": "2020-07-30T18:33:11Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5MzY0MA==", "bodyText": "Are you saying that the change here doesn't trigger RU?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463193640", "createdAt": "2020-07-30T18:34:26Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = \"john\";\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 327}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw==", "bodyText": "Hmm, I thought that this would be a bit more dynamically generated. This will be very expensive to maintain.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463194717", "createdAt": "2020-07-30T18:36:26Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4OTYyMjkx", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-458962291", "createdAt": "2020-07-31T07:13:34Z", "commit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzoxMzozNVrOG5-XuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNzozNjozMlrOG5-7Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0Mzg5Ng==", "bodyText": "How the output of this log looks? It's formated to be easily readable ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463443896", "createdAt": "2020-07-31T07:13:35Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDQwMA==", "bodyText": "Maybe we should show dyn.configuration in case of error as well?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444400", "createdAt": "2020-07-31T07:15:01Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,75 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration.toString(), value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateConfigurationWithStabilityWait(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration.toString(),\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()),\n+            kafkaDynamicConfiguration.toString(),\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration.toString()).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, KafkaDynamicConfiguration kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+            LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+            if (!result.contains(kafkaDynamicConfiguration + \"=\" + value)) {\n+                LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), kafkaDynamicConfiguration.toString(), value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDY1NA==", "bodyText": "Why 2 brokers instead of 3?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463444654", "createdAt": "2020-07-31T07:15:34Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MTQ1MQ==", "bodyText": "Are you sure we don't have similar test like this? Maybe in ListenersST or in rollingupdate ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463451451", "createdAt": "2020-07-31T07:32:33Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,374 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        int kafkaReplicas = 2;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, is(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"default.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.4\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPodsSnapshot);\n+    }\n+\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        int kafkaReplicas = 2;\n+        int zkReplicas = 1;\n+        Map<String, Object> kafkaConfig = new HashMap<>();\n+        kafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        kafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        kafkaConfig.put(\"default.replication.factor\", \"1\");\n+        kafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+\n+        Map<String, Object> updatedKafkaConfig = new HashMap<>();\n+        updatedKafkaConfig.put(\"offsets.topic.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"transaction.state.log.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"default.replication.factor\", \"1\");\n+        updatedKafkaConfig.put(\"log.message.format.version\", \"2.4\");\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+\n+        KafkaResource.kafkaEphemeral(CLUSTER_NAME, kafkaReplicas, zkReplicas)\n+                .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalLoadBalancer()\n+                        .endKafkaListenerExternalLoadBalancer()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+                .endSpec()\n+                .done();\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"false\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=false\"));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaReplicas, kafkaPods);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+\n+        // change dynamically changeable option\n+        updatedKafkaConfig.put(\"unclean.leader.election.enable\", \"true\");\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating configuration of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setConfig(updatedKafkaConfig);\n+        });\n+\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(false));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MjgzNg==", "bodyText": "Ye, I suggest to use https://junit.org/junit5/docs/current/user-guide/#writing-tests-parameterized-tests or https://junit.org/junit5/docs/current/user-guide/#writing-tests-dynamic-tests", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463452836", "createdAt": "2020-07-31T07:36:07Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.enums.KafkaDynamicConfiguration;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+\n+import static io.strimzi.systemtest.Constants.ACCEPTANCE;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+public class DynamicConfigurationSharedST extends AbstractST {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE5NDcxNw=="}, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzAxMQ==", "bodyText": "Indent", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r463453011", "createdAt": "2020-07-31T07:36:32Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/KafkaST.java", "diffHunk": "@@ -1534,332 +1524,18 @@ void testKafkaOffsetsReplicationFactorHigherThanReplicas() {\n         int replicas = 3;\n         Kafka kafka = KafkaResource.kafkaWithoutWait(KafkaResource.defaultKafka(CLUSTER_NAME, replicas, 1)\n             .editSpec()\n-                .editKafka()\n-                    .addToConfig(\"offsets.topic.replication.factor\", 4)\n-                    .addToConfig(\"transaction.state.log.min.isr\", 4)\n-                    .addToConfig(\"transaction.state.log.replication.factor\", 4)\n-                .endKafka()\n+            .editKafka()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2b6a98e958649fdc656a2032c6c8a42f3923eca"}, "originalPosition": 70}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "da9dc68b1431bd09c0456db4111300effa494dec", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/da9dc68b1431bd09c0456db4111300effa494dec", "committedDate": "2020-08-04T11:46:27Z", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/277b305b0db5eb6b9d0d93d0840e91a974b15d3f", "committedDate": "2020-08-04T11:47:58Z", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMDk0NzMx", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-461094731", "createdAt": "2020-08-04T19:14:07Z", "commit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxNDowN1rOG7t5Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxOToxOTowN1rOG7uDWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MTEzNQ==", "bodyText": "Indent?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465271135", "createdAt": "2020-08-04T19:14:07Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MjY0NQ==", "bodyText": "I think this is generated if you don't specify it. But maybe I'm wrong. (This is same for above code)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465272645", "createdAt": "2020-08-04T19:17:01Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.LOADBALANCER_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(LOADBALANCER_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+                .editSpec()\n+                    .editKafka()\n+                        .withNewListeners()\n+                            .withNewKafkaListenerExternalLoadBalancer()\n+                                .withTls(false)\n+                            .endKafkaListenerExternalLoadBalancer()\n+                        .endListeners()\n+                        .withConfig(kafkaConfig)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI3MzY4OQ==", "bodyText": "This is little bit weird for me, I know what you want to say by that, but wouldn't it be better just like parametrizedTest? Maybe @samuel-hawker will be better for this \ud83d\ude04", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r465273689", "createdAt": "2020-08-04T19:19:07Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+\n+        \"log.flush.interval.ms, \" + 20,\n+\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.cleaner.threads, \" + 1,\n+\n+        \"num.network.threads, \" + 2,\n+        \"testLogIndexLogMessageLogMessage, \" + 5,\n+        \"log.message.timestamp.difference.max.ms, \" + 12_000,\n+        \"log.preallocate, \" + true,\n+\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048\n+    })\n+    void testParametrizedTest(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47c73e4bf8bad1b92721fd4b3cd76cac0013e601"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyNDIxODc5", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-462421879", "createdAt": "2020-08-06T11:10:19Z", "commit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToxMDoxOVrOG8vA5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToyMjozOFrOG8vW6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODAyMw==", "bodyText": "What enum?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338023", "createdAt": "2020-08-06T11:10:19Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzODkzMQ==", "bodyText": "For all these I would call it brokerConfigName or something. You don't care, in these methods whether the particular config supports dynamic update or not.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466338931", "createdAt": "2020-08-06T11:12:22Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTI3OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n          \n          \n            \n                 * Verifies that updated configuration was successfully changed inside Kafka pods", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339278", "createdAt": "2020-08-06T11:13:07Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMzOTcyNw==", "bodyText": "Is there any reason for doing this via kafka-configs.sh and not just using an Admin client instance?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466339727", "createdAt": "2020-08-06T11:14:12Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -151,4 +154,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration enum instance, which defines all supported configurations\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(kafkaDynamicConfiguration, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        updateSpecificConfiguration(clusterName, kafkaDynamicConfiguration, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Method, verifying that updating configuration were successfully changed inside Kafka CR\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String kafkaDynamicConfiguration, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            kafkaDynamicConfiguration,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration),\n+            kafkaDynamicConfiguration,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(kafkaDynamicConfiguration).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param kafkaDynamicConfiguration key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String kafkaDynamicConfiguration, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MDg0NA==", "bodyText": "Why are you calling both like this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466340844", "createdAt": "2020-08-06T11:16:32Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA==", "bodyText": "Is this a complete list? If not, how did you decide which to tests? How will we keep it up to date as more configs are added in new versions of Kafka?\nIt's not a problem for this PR, but I do think we really need, written down, a list of which configs can be changed dynamically and which not. It needs to be part of the docs, so users can have certainty that changing some particular configs will or won't result in a rolling restart. Because new configs get regularly added it needs to be generated from the code, rather than something which we maintain by hand. @stanlyDoge would you be able to do this?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466343658", "createdAt": "2020-08-06T11:22:38Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyNDMyMDg2", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-462432086", "createdAt": "2020-08-06T11:28:05Z", "commit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMToyODowNVrOG8vgsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMTozNjo1OVrOG8vxQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0NjE2Mg==", "bodyText": "Is this meant to be only for kafka config or also for the various (kafka, CO, ...) dynamic logging configurations?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466346162", "createdAt": "2020-08-06T11:28:05Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0ODQ5OA==", "bodyText": "Should we add a parameter String dynConfProperty to make this more general?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466348498", "createdAt": "2020-08-06T11:32:53Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM1MDQwMw==", "bodyText": "Just a nit. Should this be execute phase?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466350403", "createdAt": "2020-08-06T11:36:59Z", "author": {"login": "sknot-rh"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,\n+    })\n+    void testLogDynamicKafkaConfigurationProperties(String kafkaDynamicConfigurationKey, Object kafkaDynamicConfigurationValue) {\n+        // exercise phase", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyNTQzMjgz", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-462543283", "createdAt": "2020-08-06T13:56:14Z", "commit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMzo1NjoxNFrOG80sDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMzo1NjoxNFrOG80sDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQzMDk4OA==", "bodyText": "I think the indent is still same here.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466430988", "createdAt": "2020-08-06T13:56:14Z", "author": {"login": "im-konge"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyODE3MTQx", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-462817141", "createdAt": "2020-08-06T19:32:56Z", "commit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTozMjo1N1rOG9BbhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxOTo0MzozMlrOG9BxUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTc0OA==", "bodyText": "I assume this will be changing with every Kafka release. So you should make this somehow dynamic maybe?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639748", "createdAt": "2020-08-06T19:32:57Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzOTkyNw==", "bodyText": "I'm not sure I understand why are we asserting these when we don't configure them anywhere.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466639927", "createdAt": "2020-08-06T19:33:19Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDEwNA==", "bodyText": "Why should this be \"true\"?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640104", "createdAt": "2020-08-06T19:33:41Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MDIwOA==", "bodyText": "Why should this be \"true\"?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466640208", "createdAt": "2020-08-06T19:33:52Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTE1OQ==", "bodyText": "This has generic name, but it is not generic method. It should ba then maybe named updateAndVerifyDynUncleanLeaderElectionConf", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641159", "createdAt": "2020-08-06T19:35:30Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0MTY3NQ==", "bodyText": "I think the use of a class field with unclear value is a bit dubious. Why are we not just setting the single options you are setting? Or why don't you pass kafkaConfig as parameter?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466641675", "createdAt": "2020-08-06T19:36:24Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        // change dynamically changeable option\n+        kafkaConfig.put(\"unclean.leader.election.enable\", dynConfValue);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NDUxNw==", "bodyText": "Can we add some Javadoc explaining what this does?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466644517", "createdAt": "2020-08-06T19:42:06Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();\n+\n+        Map<String, String> kafkaPodsSnapshot = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        LOGGER.info(\"Updating logging of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPodsSnapshot);\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+            .editKafka()\n+                .withNewListeners()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                .endListeners()\n+                .withConfig(kafkaConfig)\n+            .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+        updateAndVerifyDynConf(\"true\");\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        updateAndVerifyDynConf(\"false\");\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+\n+        KafkaTopicResource.topic(CLUSTER_NAME, TOPIC_NAME).done();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, USER_NAME).done();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientTls = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withKafkaUsername(USER_NAME)\n+            .withSecurityProtocol(SecurityProtocol.SSL)\n+            .build();\n+\n+        BasicExternalKafkaClient basicExternalKafkaClientPlain = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(SecurityProtocol.PLAINTEXT)\n+            .build();\n+\n+        String userName = KafkaUserUtils.generateRandomNameOfKafkaUser();\n+        KafkaUserResource.tlsUser(CLUSTER_NAME, userName).done();\n+\n+        basicExternalKafkaClientTls.setKafkaUsername(userName);\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withNewKafkaListenerAuthenticationTlsAuth()\n+                        .endKafkaListenerAuthenticationTlsAuth()\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        kafkaPods = StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        basicExternalKafkaClientTls.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientTls.sendMessagesTls(),\n+                basicExternalKafkaClientTls.sendMessagesTls()\n+        );\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientPlain.sendMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientPlain.receiveMessagesPlain(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to tls communication\");\n+        });\n+\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaListeners updatedKl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                        .withTls(false)\n+                    .endKafkaListenerExternalNodePort()\n+                    .build();\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            kafkaClusterSpec.setListeners(updatedKl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+\n+        assertThrows(Exception.class, () -> {\n+            basicExternalKafkaClientTls.sendMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            basicExternalKafkaClientTls.receiveMessagesTls(Constants.GLOBAL_CLIENTS_EXCEPT_ERROR_TIMEOUT);\n+            LOGGER.error(\"Producer & Consumer did not send and receive messages because external listener is set to plain communication\");\n+        });\n+\n+        basicExternalKafkaClientPlain.verifyProducedAndConsumedMessages(\n+                basicExternalKafkaClientPlain.sendMessagesPlain(),\n+                basicExternalKafkaClientPlain.receiveMessagesPlain()\n+        );\n+    }\n+\n+    private void updateAndVerifyDynConf(String dynConfValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY0NTMyOQ==", "bodyText": "Why are we suddenly changing logging? Standa is working on a PR to make logging changes not roll the pods. So this will stop working soon.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r466645329", "createdAt": "2020-08-06T19:43:32Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.InlineLogging;\n+import io.strimzi.api.kafka.model.InlineLoggingBuilder;\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+\n+        updateAndVerifyDynConf(\"true\");\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=2.5\"));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+\n+        InlineLogging il = new InlineLoggingBuilder().withLoggers(Collections.singletonMap(\"kafka.logger.level\", \"INFO\")).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3e061ab713da9d51258d389a8ed6b1822ce9e0e"}, "originalPosition": 81}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fc1e537e5d5732d6711f0eec3c3a4a67f1627c2c", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fc1e537e5d5732d6711f0eec3c3a4a67f1627c2c", "committedDate": "2020-08-10T08:05:07Z", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "7517de0b3496641bd930171d41daeccd54ff86ce", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7517de0b3496641bd930171d41daeccd54ff86ce", "committedDate": "2020-08-10T08:06:07Z", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7517de0b3496641bd930171d41daeccd54ff86ce", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7517de0b3496641bd930171d41daeccd54ff86ce", "committedDate": "2020-08-10T08:06:07Z", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "22a5375f16130e257b4e13364bfd459227de4a7e", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/22a5375f16130e257b4e13364bfd459227de4a7e", "committedDate": "2020-08-10T14:15:12Z", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1OTgzMDUw", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-465983050", "createdAt": "2020-08-12T14:39:26Z", "commit": {"oid": "defb26bb06660cf9d8eb8f07c9b899cb920b4796"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MDUyMzEz", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-466052313", "createdAt": "2020-08-12T15:52:49Z", "commit": {"oid": "defb26bb06660cf9d8eb8f07c9b899cb920b4796"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTo1Mjo0OVrOG_nwaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNTo1NjozMVrOG_n61g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NDg0MA==", "bodyText": "What does this tell you that the result of the update to the Kafka CR does not?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469364840", "createdAt": "2020-08-12T15:52:49Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +156,79 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "defb26bb06660cf9d8eb8f07c9b899cb920b4796"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2NzUxMA==", "bodyText": "No. You could include columns in the CSV defining the version range between which to test with that config, skipping versions outside that range. But the key point here is that to get complete coverage we need to automate this, which means not relying on an annotation to drive the values, but that generated file I described.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r469367510", "createdAt": "2020-08-12T15:56:31Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationSharedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationSharedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-shared-cluster-test\";\n+\n+    @ParameterizedTest\n+    @CsvSource({\n+        \"background.threads, \" + 12,\n+        \"compression.type,  snappy\",\n+        \"compression.type,  gzip\",\n+        \"compression.type,  lz4\",\n+        \"compression.type,  zstd\",\n+        \"log.flush.interval.ms, \" + 20,\n+        \"log.retention.ms,  \" + 20,\n+        \"log.retention.bytes, \" + 250,\n+        \"log.segment.bytes,   \" + 1_100,\n+        \"log.segment.delete.delay.ms,  \" + 400,\n+        \"log.roll.jitter.ms, \" + 500,\n+        \"log.roll.ms, \" + 300,\n+        \"log.cleaner.dedupe.buffer.size, \" + 4_000_000,\n+        \"log.cleaner.delete.retention.ms, \" + 1_000,\n+        \"log.cleaner.io.buffer.load.factor, \" + 12,\n+        \"log.cleaner.io.buffer.size, \" + 10_000,\n+        \"log.cleaner.io.max.bytes.per.second, \" + 1.523,\n+        \"log.cleaner.max.compaction.lag.ms, \" + 32_000,\n+        \"log.cleaner.min.compaction.lag.ms, \" + 1_000,\n+        \"log.preallocate, \" + true,\n+        \"max.connections, \" + 10,\n+        \"max.connections.per.ip, \" + 20,\n+        \"unclean.leader.election.enable, \" + true,\n+        \"message.max.bytes, \" + 2048,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM0MzY1OA=="}, "originalCommit": {"oid": "64d1ac624b2f41b7401f52d4bd2054a8dc893294"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4NTk1NDUz", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-468595453", "createdAt": "2020-08-17T16:01:52Z", "commit": {"oid": "347dd238537142b016a9fa33e8fe7c077da7583c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowMTo1MlrOHBu_ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxNjowMzoyM1rOHBvDYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MDU3OQ==", "bodyText": "I think it should be in kafka directory in systemtest package", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471580579", "createdAt": "2020-08-17T16:01:52Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationSharedST.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "347dd238537142b016a9fa33e8fe7c077da7583c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU4MTUzOA==", "bodyText": "It should be in kafka directory of systemtest I think.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r471581538", "createdAt": "2020-08-17T16:03:23Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.dynamicconfiguration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "347dd238537142b016a9fa33e8fe7c077da7583c"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a0aaedf66d5c1dce1ad33639f699da1790d00eeb", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/a0aaedf66d5c1dce1ad33639f699da1790d00eeb", "committedDate": "2020-08-17T17:30:59Z", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/58b10ba7d48706f744cd81e4924a02eea22d660b", "committedDate": "2020-08-17T18:38:29Z", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5MjA1MDA4", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-469205008", "createdAt": "2020-08-18T10:44:28Z", "commit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMDo0NDoyOFrOHCN0Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMToyOToxMFrOHCPJmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA4NTUxMQ==", "bodyText": "Shouldn't be this annotation from spotbugs?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472085511", "createdAt": "2020-08-18T10:44:28Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -4,6 +4,8 @@\n  */\n package io.strimzi.systemtest.utils.kafkaUtils;\n \n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEwNzQxNg==", "bodyText": "Maybe we should use prefixes from FORBIDDEN_PREFIXES ?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472107416", "createdAt": "2020-08-18T11:29:10Z", "author": {"login": "Frawless"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return Map<String, Object> all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b"}, "originalPosition": 162}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "58b10ba7d48706f744cd81e4924a02eea22d660b", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/58b10ba7d48706f744cd81e4924a02eea22d660b", "committedDate": "2020-08-17T18:38:29Z", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/581a847e561524a3b7c849c4a53f2fc5ce2dbb33", "committedDate": "2020-08-18T15:03:25Z", "message": "doc\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5Nzk2Mjc0", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-469796274", "createdAt": "2020-08-18T20:11:50Z", "commit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDoxMTo1MFrOHCkaLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMDo0Mjo0NFrOHCl9Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ1NTcyNA==", "bodyText": "I think that @im-konge is in other PR adding the tag ROLLING_UPDATE. I think we should use it here as well because these are closely related.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472455724", "createdAt": "2020-08-18T20:11:50Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/Constants.java", "diffHunk": "@@ -240,4 +240,9 @@\n      * Tag for tests where cruise control used\n      */\n     String CRUISE_CONTROL = \"cruisecontrol\";\n+\n+    /**\n+     * Tag for tests where mainly dynamic configuration is used\n+     */\n+    String DYNAMIC_CONFIGURATION = \"dynamicconfiguration\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4MTA0Mg==", "bodyText": "How does this deal with the exceptions to the forbidden prefixes?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r472481042", "createdAt": "2020-08-18T20:42:44Z", "author": {"login": "scholzj"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic\n+     * @param kafkaVersion specific kafka version\n+     * @return all dynamic properties for specific kafka version\n+     */\n+    @SuppressWarnings({\"checkstyle:CyclomaticComplexity\", \"checkstyle:BooleanExpressionComplexity\", \"unchecked\"})\n+    public static Map<String, Object> getDynamicConfigurationProperties(String kafkaVersion)  {\n+\n+        JsonObject kafkaConfig = KafkaUtils.loadSupportedKafkaConfigs(kafkaVersion);\n+\n+        Map<String, Object> dynamicConfigs = kafkaConfig.getJsonObject(\"configs\")\n+            .getMap()\n+            .entrySet()\n+            .stream()\n+            .filter(a ->\n+                // ignoring everything which is READ_ONLY\n+                !((LinkedHashMap<String, String>) a.getValue()).get(\"scope\").equals(\"READ_ONLY\") &&\n+                    // filtering configs with following prefixes\n+                    // \"listeners, advertised., broker., listener., host.name, port, inter.broker.listener.name, sasl., ssl.,\n+                    // security., password., principal.builder.class, log.dir, zookeeper.connect, zookeeper.set.acl, authorizer.,\n+                    // super.user, cruise.control.metrics.topic, cruise.control.metrics.reporter.bootstrap.servers\n+                    !(\n+                        a.getKey().startsWith(\"listeners\") ||\n+                            a.getKey().startsWith(\"advertised\") ||\n+                            a.getKey().startsWith(\"broker\") ||\n+                            a.getKey().startsWith(\"listener\") ||\n+                            a.getKey().startsWith(\"host.name\") ||\n+                            a.getKey().startsWith(\"port\") ||\n+                            a.getKey().startsWith(\"inter.broker.listener.name\") ||\n+                            a.getKey().startsWith(\"sasl\") ||\n+                            a.getKey().startsWith(\"ssl\") ||\n+                            a.getKey().startsWith(\"security\") ||\n+                            a.getKey().startsWith(\"password\") ||\n+                            a.getKey().startsWith(\"principal.builder.class\") ||\n+                            a.getKey().startsWith(\"log.dir\") ||\n+                            a.getKey().startsWith(\"zookeeper.connect\") ||\n+                            a.getKey().startsWith(\"zookeeper.set.acl\") ||\n+                            a.getKey().startsWith(\"authorizer\") ||\n+                            a.getKey().startsWith(\"super.user\") ||\n+                            a.getKey().startsWith(\"cruise.control.metrics.topic\") ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 183}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyNTM0NDA3", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-472534407", "createdAt": "2020-08-21T14:23:51Z", "commit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDoyMzo1MVrOHEvTGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNDo0NDo1N1rOHEwE9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMTI5MA==", "bodyText": "I think you mean @link.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474731290", "createdAt": "2020-08-21T14:23:51Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczMzczOQ==", "bodyText": "You don't have to say it's a method (we already know that!). Javadocs are best when they're to the point:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n          \n          \n            \n                 * Loads all kafka config parameters supported by the given {@code kafkaVersion}, as generated by #KafkaConfigModelGenerator in config-model-generator.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474733739", "createdAt": "2020-08-21T14:27:58Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczNDA3NQ==", "bodyText": "rethrow a RuntimeException", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474734075", "createdAt": "2020-08-21T14:28:33Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDczODI1Ng==", "bodyText": "You should be able to use the config-model module. This is what KafkaConfiguration does:\nprivate Map<String, ConfigModel> readConfigModel(KafkaVersion kafkaVersion) {\n        String name = \"/kafka-\" + kafkaVersion.version() + \"-config-model.json\";\n        try {\n            try (InputStream in = KafkaConfiguration.class.getResourceAsStream(name)) {\n                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n                if (!kafkaVersion.version().equals(configModels.getVersion())) {\n                    throw new RuntimeException(\"Incorrect version\");\n                }\n                return configModels.getConfigs();\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n        }\n    }\nThat will give you nicely typed ConfigModel to deal with, rather than JsonObject. Please try to refactor KafkaConfiguration to use a common implementation rather than copying the above code.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474738256", "createdAt": "2020-08-21T14:35:04Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -153,4 +163,151 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @see updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+        PodUtils.verifyThatRunningPodsAreStable(KafkaResources.kafkaStatefulSetName(clusterName));\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, CR_CREATION_TIMEOUT,\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return JsonObject all supported kafka properties\n+     */\n+    @SuppressFBWarnings(\"RR_NOT_CHECKED\")\n+    public static JsonObject loadSupportedKafkaConfigs(String kafkaVersion) {\n+\n+        File file = new File(\"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\");\n+        byte[] data = new byte[0];\n+\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+\n+            data = new byte[(int) file.length()];\n+            fis.read(data);\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+\n+        String kafkaConfigs = new String(data, Charset.defaultCharset());\n+\n+        return new JsonObject(kafkaConfigs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MDg3NA==", "bodyText": "Not for this PR, but I'm assuming we run the kafka tools in the pod quite often in the tests. So we could consider refactoring so we can say something like:\ncmdKubeClient().execKafkaConfigsInPod(podName, \"--entity-type brokers --entity-name 0 --describe\")", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474740874", "createdAt": "2020-08-21T14:39:34Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0MzQ5Nw==", "bodyText": "I think we increasingly need javadocs explaining what the test is trying to test. Or at least ore descriptive method names. This one is not really about withExternalListeners, it's that changing an external listeners config causes a RU. So the name would be better as testUpdateToExternalListenerCausesRollingRestart", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474743497", "createdAt": "2020-08-21T14:44:04Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDc0NDA1NA==", "bodyText": "Same comment about javadoc or method name.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r474744054", "createdAt": "2020-08-21T14:44:57Z", "author": {"login": "tombentley"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/dynamicconfiguration/DynamicConfigurationIsolatedST.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafka.dynamicconfiguration;\n+\n+import io.strimzi.api.kafka.model.KafkaClusterSpec;\n+import io.strimzi.api.kafka.model.KafkaResources;\n+import io.strimzi.api.kafka.model.listener.KafkaListeners;\n+import io.strimzi.api.kafka.model.listener.KafkaListenersBuilder;\n+import io.strimzi.systemtest.AbstractST;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.Environment;\n+import io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.strimzi.systemtest.resources.crd.KafkaTopicResource;\n+import io.strimzi.systemtest.resources.crd.KafkaUserResource;\n+import io.strimzi.systemtest.utils.TestKafkaVersion;\n+import io.strimzi.systemtest.utils.kafkaUtils.KafkaUserUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.controllers.StatefulSetUtils;\n+import io.strimzi.systemtest.utils.kubeUtils.objects.PodUtils;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Tag;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.kafkaStatefulSetName;\n+import static io.strimzi.systemtest.Constants.DYNAMIC_CONFIGURATION;\n+import static io.strimzi.systemtest.Constants.EXTERNAL_CLIENTS_USED;\n+import static io.strimzi.systemtest.Constants.NODEPORT_SUPPORTED;\n+import static io.strimzi.systemtest.Constants.REGRESSION;\n+import static io.strimzi.systemtest.resources.ResourceManager.cmdKubeClient;\n+import static io.strimzi.systemtest.resources.ResourceManager.kubeClient;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+\n+/**\n+ * DynamicConfigurationIsolatedST is responsible for verify that if we change dynamic Kafka configuration it will not\n+ * trigger rolling update.\n+ * Isolated -> for each test case we have different configuration of Kafka resource\n+ */\n+@Tag(REGRESSION)\n+@Tag(DYNAMIC_CONFIGURATION)\n+public class DynamicConfigurationIsolatedST extends AbstractST {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(DynamicConfigurationIsolatedST.class);\n+    private static final String NAMESPACE = \"kafka-configuration-isolated-cluster-test\";\n+    private static final int KAFKA_REPLICAS = 1;\n+\n+    private Map<String, Object> kafkaConfig;\n+\n+    @Test\n+    void testSimpleDynamicConfiguration() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        LOGGER.info(\"Verify values after update\");\n+        kafkaConfiguration = kubeClient().getConfigMap(KafkaResources.kafkaMetricsAndLogConfigMapName(CLUSTER_NAME)).getData().get(\"server.config\");\n+        assertThat(kafkaConfiguration, containsString(\"offsets.topic.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"transaction.state.log.replication.factor=1\"));\n+        assertThat(kafkaConfiguration, containsString(\"log.message.format.version=\" + TestKafkaVersion.getKafkaVersionsInMap().get(Environment.ST_KAFKA_VERSION).messageVersion()));\n+        assertThat(kafkaConfiguration, containsString(\"unclean.leader.election.enable=true\"));\n+    }\n+\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Test\n+    void testDynamicConfigurationWithExternalListeners() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, KAFKA_REPLICAS, 1)\n+            .editSpec()\n+                .editKafka()\n+                    .withNewListeners()\n+                        .withNewKafkaListenerExternalNodePort()\n+                            .withTls(false)\n+                        .endKafkaListenerExternalNodePort()\n+                        .withNewPlain()\n+                        .endPlain()\n+                    .endListeners()\n+                    .withConfig(kafkaConfig)\n+                .endKafka()\n+            .endSpec()\n+            .done();\n+\n+        String kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Edit listeners - this should cause RU (because of new crts)\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewKafkaListenerExternalNodePort()\n+                    .endKafkaListenerExternalNodePort()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"compression.type\", \"snappy\");\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"compression.type=snappy\"));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", true);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + true));\n+\n+        // Remove external listeners (node port) - this should cause RU (we need to update advertised.listeners)\n+        // Other external listeners cases are rolling because of crts\n+        kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStatefulSetName(CLUSTER_NAME));\n+        LOGGER.info(\"Updating listeners of Kafka cluster\");\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            KafkaClusterSpec kafkaClusterSpec = k.getSpec().getKafka();\n+            KafkaListeners kl = new KafkaListenersBuilder()\n+                    .withNewPlain()\n+                    .endPlain()\n+                    .build();\n+            kafkaClusterSpec.setListeners(kl);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaStatefulSetName(CLUSTER_NAME), KAFKA_REPLICAS, kafkaPods);\n+        assertThat(StatefulSetUtils.ssHasRolled(kafkaStatefulSetName(CLUSTER_NAME), kafkaPods), is(true));\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"Dynamic configs for broker 0 are:\\n\"));\n+\n+        kafkaConfig.put(\"unclean.leader.election.enable\", false);\n+\n+        updateAndVerifyDynConf(kafkaConfig);\n+\n+        kafkaConfigurationFromPod = cmdKubeClient().execInPod(KafkaResources.kafkaPodName(CLUSTER_NAME, 0), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+        assertThat(kafkaConfigurationFromPod, containsString(\"unclean.leader.election.enable=\" + false));\n+    }\n+\n+    @Test\n+    @Tag(NODEPORT_SUPPORTED)\n+    @Tag(EXTERNAL_CLIENTS_USED)\n+    void testDynamicConfigurationExternalTls() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "581a847e561524a3b7c849c4a53f2fc5ce2dbb33"}, "originalPosition": 189}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b3b4e28c9d69520bd6d7ccc4f83bf85feb0e6fb1", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b3b4e28c9d69520bd6d7ccc4f83bf85feb0e6fb1", "committedDate": "2020-08-21T18:14:47Z", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0213a6ace36a75f02d4c9cb58134774bcf0e0ce1", "committedDate": "2020-08-21T18:19:14Z", "message": "tags\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e6c0279e6451c2b045e1b54db88d4927c74f1274", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e6c0279e6451c2b045e1b54db88d4927c74f1274", "committedDate": "2020-08-26T12:41:15Z", "message": "sds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9bc6b07c0fc7a7a17ebaf447d03b48931ffdb63d", "committedDate": "2020-08-26T13:48:07Z", "message": "exceptions\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1ODE2NzMw", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-475816730", "createdAt": "2020-08-26T20:01:23Z", "commit": {"oid": "5f2d8699f501acd5dc51cbb7215684acb3f48e46"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2NjY0Njk1", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-476664695", "createdAt": "2020-08-27T12:21:58Z", "commit": {"oid": "308c3ecdb4ea1d3e0389c44e4da596f89bf08acd"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoyMTo1OFrOHINzmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoyMTo1OFrOHINzmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3Njg1Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Method, which process all supported configs by Kafka and filter all which are not dynamic\n          \n          \n            \n                 * Return dynamic Kafka configs supported by the the given version of Kafka.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#discussion_r478376856", "createdAt": "2020-08-27T12:21:58Z", "author": {"login": "tombentley"}, "path": "systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java", "diffHunk": "@@ -157,4 +170,152 @@ public static void waitForClusterStability(String clusterName) {\n             return false;\n         });\n     }\n+\n+    /**\n+     * Method which, update/replace Kafka configuration\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void updateSpecificConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        KafkaResource.replaceKafkaResource(clusterName, kafka -> {\n+            LOGGER.info(\"Kafka config before updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+            Map<String, Object> config = kafka.getSpec().getKafka().getConfig();\n+            config.put(brokerConfigName, value);\n+            kafka.getSpec().getKafka().setConfig(config);\n+            LOGGER.info(\"Kafka config after updating '{}'\", kafka.getSpec().getKafka().getConfig().toString());\n+        });\n+    }\n+\n+    /**\n+     * Method which, extends the @link updateConfiguration(String clusterName, KafkaConfiguration kafkaConfiguration, Object value) method\n+     * with stability and ensures after update of Kafka resource there will be not rolling update\n+     * @param clusterName name of the cluster where Kafka resource can be found\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static void  updateConfigurationWithStabilityWait(String clusterName, String brokerConfigName, Object value) {\n+        updateSpecificConfiguration(clusterName, brokerConfigName, value);\n+    }\n+\n+    /**\n+     * Verifies that updated configuration was successfully changed inside Kafka CR\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     */\n+    public static boolean verifyCrDynamicConfiguration(String clusterName, String brokerConfigName, Object value) {\n+        LOGGER.info(\"Dynamic Configuration in Kafka CR is {}={} and excepted is {}={}\",\n+            brokerConfigName,\n+            KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName),\n+            brokerConfigName,\n+            value);\n+\n+        return KafkaResource.kafkaClient().inNamespace(kubeClient().getNamespace()).withName(clusterName).get().getSpec().getKafka().getConfig().get(brokerConfigName).equals(value);\n+    }\n+\n+    /**\n+     * Method, which, verifying that updating configuration were successfully changed inside Kafka pods\n+     * @param kafkaPodNamePrefix prefix of Kafka pods\n+     * @param brokerConfigName key of specific property\n+     * @param value value of specific property\n+     * @return\n+     * true = if specific property match the excepted property\n+     * false = if specific property doesn't match the excepted property\n+     */\n+    public static boolean verifyPodDynamicConfiguration(String kafkaPodNamePrefix, String brokerConfigName, Object value) {\n+\n+        List<Pod> kafkaPods = kubeClient().listPodsByPrefixInName(kafkaPodNamePrefix);\n+\n+        for (Pod pod : kafkaPods) {\n+\n+            TestUtils.waitFor(\"Wait until dyn.configuration is changed\", Constants.GLOBAL_POLL_INTERVAL, Constants.RECONCILIATION_INTERVAL + Duration.ofSeconds(10).toMillis(),\n+                () -> {\n+                    String result = cmdKubeClient().execInPod(pod.getMetadata().getName(), \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe\").out();\n+\n+                    LOGGER.debug(\"This dyn.configuration {} inside the Kafka pod {}\", result, pod.getMetadata().getName());\n+\n+                    if (!result.contains(brokerConfigName + \"=\" + value)) {\n+                        LOGGER.error(\"Kafka Pod {} doesn't contain {} with value {}\", pod.getMetadata().getName(), brokerConfigName, value);\n+                        LOGGER.error(\"Kafka configuration {}\", result);\n+                        return false;\n+                    }\n+                    return true;\n+                });\n+        }\n+        return true;\n+    }\n+\n+    /**\n+     * Method, which load all supported kafka configuration generated by #KafkaConfigModelGenerator in config-model-generator\n+     * @param kafkaVersion specific kafka version\n+     * @return all supported kafka properties\n+     */\n+    public static Map<String, ConfigModel> readConfigModel(String kafkaVersion) {\n+        String name = \"../cluster-operator/src/main/resources/kafka-\" + kafkaVersion + \"-config-model.json\";\n+        try {\n+            try (InputStream in = new FileInputStream(name)) {\n+                ConfigModels configModels = new ObjectMapper().readValue(in, ConfigModels.class);\n+                if (!kafkaVersion.equals(configModels.getVersion())) {\n+                    throw new RuntimeException(\"Incorrect version\");\n+                }\n+                return configModels.getConfigs();\n+            }\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Error reading from classpath resource \" + name, e);\n+        }\n+    }\n+\n+    /**\n+     * Method, which process all supported configs by Kafka and filter all which are not dynamic", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "308c3ecdb4ea1d3e0389c44e4da596f89bf08acd"}, "originalPosition": 137}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9687e5149966ebe7a88afcd3f46f92176ad18aa4", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9687e5149966ebe7a88afcd3f46f92176ad18aa4", "committedDate": "2020-09-07T09:02:20Z", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "76de14021f24172b40ce8bc26d3bceb3babb323d", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76de14021f24172b40ce8bc26d3bceb3babb323d", "committedDate": "2020-09-07T09:03:22Z", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5a1f8b89006bbbb2d706a3a779e7fc9c0877cdc2", "committedDate": "2020-09-08T10:55:59Z", "message": "[MO] - [dyn.conf] -> tests draft\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "959776c5b0016187d4f31d166bdb1aaa6b973c50", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/959776c5b0016187d4f31d166bdb1aaa6b973c50", "committedDate": "2020-09-08T10:55:59Z", "message": "[MO] - [tests] -> additional tests\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ec6c5aa6228e72783b9cfdfa3bbbc2cf6c2ee14b", "committedDate": "2020-09-08T10:55:59Z", "message": "adding pod verification + changing the way how to verify\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7183c843117f568922ac13319fb0281e40d1aabd", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7183c843117f568922ac13319fb0281e40d1aabd", "committedDate": "2020-09-08T10:56:39Z", "message": "create shared, iso + separate phases\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "faa36204a6d0a281eb1f3e232040f6675fd4d853", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/faa36204a6d0a281eb1f3e232040f6675fd4d853", "committedDate": "2020-09-08T10:56:39Z", "message": "some nits\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e095f29aaafd8abfd9b8a1975033b711292393a3", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e095f29aaafd8abfd9b8a1975033b711292393a3", "committedDate": "2020-09-08T10:56:39Z", "message": "[MO] -> changes\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fac2acd69f7c72748c8086553260001d86926804", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fac2acd69f7c72748c8086553260001d86926804", "committedDate": "2020-09-08T10:56:39Z", "message": "parametrized tst\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffc6a3ff77fd82360658159a45c05ad8a1f7c874", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ffc6a3ff77fd82360658159a45c05ad8a1f7c874", "committedDate": "2020-09-08T10:56:39Z", "message": "updatee\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76541b66628223a9dea92fb49d2a35b1b87f1906", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76541b66628223a9dea92fb49d2a35b1b87f1906", "committedDate": "2020-09-08T10:56:39Z", "message": "[MO] - adding new profile + tests separation\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f47afcab2df41630f38fa641f4567d8bda5cb3bc", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f47afcab2df41630f38fa641f4567d8bda5cb3bc", "committedDate": "2020-09-08T10:56:39Z", "message": "indent/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7aceb5d6871ac5eb185c28ff5a0b638d9f682e40", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7aceb5d6871ac5eb185c28ff5a0b638d9f682e40", "committedDate": "2020-09-08T10:56:39Z", "message": "RU fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "699cd9422f1387d2cfde7a2b2fd0da48951c6bb8", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/699cd9422f1387d2cfde7a2b2fd0da48951c6bb8", "committedDate": "2020-09-08T10:56:39Z", "message": "removing un-used toString()\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c8ccb0a22dde9b835f5631e29881736159fc2ba", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9c8ccb0a22dde9b835f5631e29881736159fc2ba", "committedDate": "2020-09-08T10:56:39Z", "message": "fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "241f3f4b8fc82b9e955f604cf3dca7b4202cd606", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/241f3f4b8fc82b9e955f604cf3dca7b4202cd606", "committedDate": "2020-09-08T10:56:39Z", "message": "[MO] - commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb6c7f8804e8dd7c04c5446337f474a67da0fe66", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/eb6c7f8804e8dd7c04c5446337f474a67da0fe66", "committedDate": "2020-09-08T10:56:39Z", "message": "[MO] dyn\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "280900459f501a8cc4e97a9d5a489d268c5ccb0f", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/280900459f501a8cc4e97a9d5a489d268c5ccb0f", "committedDate": "2020-09-08T10:56:39Z", "message": "[MO] - fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f331eb558309ec2340070057f9ab4fd3f8daf250", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f331eb558309ec2340070057f9ab4fd3f8daf250", "committedDate": "2020-09-08T10:56:39Z", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac9787b55a1c2b97ceae9da41a42c4ee1da7eff6", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ac9787b55a1c2b97ceae9da41a42c4ee1da7eff6", "committedDate": "2020-09-08T10:56:39Z", "message": "resolve commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e02bbaf61dfc1fcaa6fe804123f0fbf22cd15cba", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e02bbaf61dfc1fcaa6fe804123f0fbf22cd15cba", "committedDate": "2020-09-08T10:56:39Z", "message": "adding diff property\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60da26d546e118073bf7edf2d19f6a5ab3761ef8", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/60da26d546e118073bf7edf2d19f6a5ab3761ef8", "committedDate": "2020-09-08T10:56:39Z", "message": "description\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6e3d352cc392b605d51c6a5c9a38489dfa7b8c7", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b6e3d352cc392b605d51c6a5c9a38489dfa7b8c7", "committedDate": "2020-09-08T10:56:39Z", "message": "space\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b4f05888d312f2167e5ac74927e73d78665eb1a", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7b4f05888d312f2167e5ac74927e73d78665eb1a", "committedDate": "2020-09-08T10:56:39Z", "message": "automatical generation of test cases\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "964470971f3e228be1dcb17efd8156cc1ef95007", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/964470971f3e228be1dcb17efd8156cc1ef95007", "committedDate": "2020-09-08T10:56:39Z", "message": "update fix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/10e4cbdc8ec0e8e860223fd3dcbbd40ed174d595", "committedDate": "2020-09-08T10:56:40Z", "message": "test factory removing csv file\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f787c7fa0c0b534b9022a0fa2ec772ec2b514500", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f787c7fa0c0b534b9022a0fa2ec772ec2b514500", "committedDate": "2020-09-08T10:56:40Z", "message": "supress unchecked\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d151f9c44ef7b1c761b44ff4be9cd6ededbdbbc3", "committedDate": "2020-09-08T10:56:40Z", "message": "move\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "222629b3400b246e42bf0627e1091fbea41850fa", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/222629b3400b246e42bf0627e1091fbea41850fa", "committedDate": "2020-09-08T10:56:40Z", "message": "spotbugs\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0234793179c6624771a00322a0150d7eedb7ac05", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0234793179c6624771a00322a0150d7eedb7ac05", "committedDate": "2020-09-08T10:56:40Z", "message": "doc\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7decfc02e8ba8e7917ba90a42841dcda405a7508", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/7decfc02e8ba8e7917ba90a42841dcda405a7508", "committedDate": "2020-09-08T10:56:40Z", "message": "commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "854f623fa48d6f9cd3c8104748cbfeeba8e89d7b", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/854f623fa48d6f9cd3c8104748cbfeeba8e89d7b", "committedDate": "2020-09-08T10:56:40Z", "message": "tags\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64b5044b6cfa6ebf95e6e2f60418ad312f552b00", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/64b5044b6cfa6ebf95e6e2f60418ad312f552b00", "committedDate": "2020-09-08T10:56:40Z", "message": "dependecies\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c47407b41c4d5ff203dba0f935177e44f7d7bf9", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3c47407b41c4d5ff203dba0f935177e44f7d7bf9", "committedDate": "2020-09-08T10:56:40Z", "message": "sd\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff69976bca9ce196e746465f8f444bbb5d584eeb", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ff69976bca9ce196e746465f8f444bbb5d584eeb", "committedDate": "2020-09-08T10:56:40Z", "message": "fix all\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0423f843d88ec5cf1a8f9da3a76eda2fec322aa5", "committedDate": "2020-09-08T10:56:40Z", "message": "dsds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fe509f09a63587f1103f9d178e25094c00fb47d6", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fe509f09a63587f1103f9d178e25094c00fb47d6", "committedDate": "2020-09-08T10:56:40Z", "message": "sds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "683cf641b3b84c75858d28950653dcb5c526adc3", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/683cf641b3b84c75858d28950653dcb5c526adc3", "committedDate": "2020-09-08T10:56:40Z", "message": "exceptions\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61c88a6c58a3b515eccca7d620aa128f192727d4", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/61c88a6c58a3b515eccca7d620aa128f192727d4", "committedDate": "2020-09-08T10:56:40Z", "message": "done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ff1560d339af68e38b497e676ed7f847bb349b7", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/1ff1560d339af68e38b497e676ed7f847bb349b7", "committedDate": "2020-09-08T10:56:40Z", "message": "ds\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9e4a0e926c0102ca69eb3cafadda43a7e9bff75", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b9e4a0e926c0102ca69eb3cafadda43a7e9bff75", "committedDate": "2020-09-08T10:56:40Z", "message": "this\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab78bbec5d646b8a9672d92f6cb4a1a4ccf18f3b", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ab78bbec5d646b8a9672d92f6cb4a1a4ccf18f3b", "committedDate": "2020-09-08T10:56:40Z", "message": "dpj\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "163425f0d185336b2edefd0b9f3c1e96d613bce8", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/163425f0d185336b2edefd0b9f3c1e96d613bce8", "committedDate": "2020-09-08T10:56:40Z", "message": "last change\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "237b7528382790286d1251af98c3069a6043a497", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/237b7528382790286d1251af98c3069a6043a497", "committedDate": "2020-09-08T10:56:40Z", "message": "Tom commends\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f674e4ccfd53635b09216a64c39c276b73d5f714", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f674e4ccfd53635b09216a64c39c276b73d5f714", "committedDate": "2020-09-08T10:56:40Z", "message": "check\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ab3dceb6eaf4cfdbb324bccef2375b20ece2a95", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/2ab3dceb6eaf4cfdbb324bccef2375b20ece2a95", "committedDate": "2020-09-08T10:56:40Z", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b9106c92559dee1288b5f9176a211ba4c47948f", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b9106c92559dee1288b5f9176a211ba4c47948f", "committedDate": "2020-09-08T14:34:28Z", "message": "rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "76de14021f24172b40ce8bc26d3bceb3babb323d", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76de14021f24172b40ce8bc26d3bceb3babb323d", "committedDate": "2020-09-07T09:03:22Z", "message": "user prefix\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}, "afterCommit": {"oid": "9b9106c92559dee1288b5f9176a211ba4c47948f", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/9b9106c92559dee1288b5f9176a211ba4c47948f", "committedDate": "2020-09-08T14:34:28Z", "message": "rebase\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2ce488bde5d2749f61f27611840297fef8c6542", "author": {"user": {"login": "see-quick", "name": "Ors\u00e1k Maro\u0161"}}, "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f2ce488bde5d2749f61f27611840297fef8c6542", "committedDate": "2020-09-08T17:41:28Z", "message": "last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg0NzU1MTY1", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3408#pullrequestreview-484755165", "createdAt": "2020-09-09T08:01:04Z", "commit": {"oid": "f2ce488bde5d2749f61f27611840297fef8c6542"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1412, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}