{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4NjkzNDky", "number": 2353, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNVQxOTo0MDo1MFrODV21kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNVQxOTo1NTozOFrODV235g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjQ1MTM2OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNVQxOTo0MDo1MFrOFaSpZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxMDoxMDo1M1rOFaZ3Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMjgwNw==", "bodyText": "Is this comment still valid? You seem to have a cluster with 3 topics and topic with 3 replicas?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363112807", "createdAt": "2020-01-05T19:40:50Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -218,11 +219,7 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         assertEquals(3, initialReplicas);\n \n         // Create topic before scale up to ensure no partitions created on last broker (which will mess up scale down)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIzMTAxMQ==", "bodyText": "Removed", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363231011", "createdAt": "2020-01-06T10:10:53Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -218,11 +219,7 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         assertEquals(3, initialReplicas);\n \n         // Create topic before scale up to ensure no partitions created on last broker (which will mess up scale down)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMjgwNw=="}, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjQ1MjY1OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNVQxOTo0NDoxN1rOFaSqDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxMDoxMToxMlrOFaZ3hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMjk3NA==", "bodyText": "You don't seem to be doing anything between scale-up and the initial deployment. So why not just deploy the cluster with 4 nodes?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363112974", "createdAt": "2020-01-05T19:44:17Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -269,6 +266,73 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         receiveMessagesExternal(NAMESPACE, topicName, messageCount);\n     }\n \n+    /**\n+     * This test cover case, when KafkaRoller will not roll Kafka pods, because created topic doesn't meet requirements for roll remaining pods\n+     * 1. Deploy kafka cluster with 3 pods\n+     * 2. Create topic with 4 replicas\n+     * 3. Scale kafka cluster to 7 replicas and wait, until all pods are ready\n+     * 4. Scale down kafka cluster to 3 replicas\n+     * 5. Trigger rolling update for Kafka cluster\n+     * 6. Rolling update will not be performed, because topic which we created had some replicas on deleted pods - manual fix is needed in that case\n+     */\n+    @Test\n+    void testKafkaWontRollUp() throws Exception {\n+        String topicName = \"test-topic-\" + new Random().nextInt(Integer.MAX_VALUE);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .endKafka()\n+            .endSpec().done();\n+\n+        LOGGER.info(\"Running kafkaScaleUpScaleDown {}\", CLUSTER_NAME);\n+        final int initialReplicas = kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)).getStatus().getReplicas();\n+        assertEquals(3, initialReplicas);\n+\n+        // scale up", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIzMTExMA==", "bodyText": "Removed scaleUp phase", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363231110", "createdAt": "2020-01-06T10:11:12Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -269,6 +266,73 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         receiveMessagesExternal(NAMESPACE, topicName, messageCount);\n     }\n \n+    /**\n+     * This test cover case, when KafkaRoller will not roll Kafka pods, because created topic doesn't meet requirements for roll remaining pods\n+     * 1. Deploy kafka cluster with 3 pods\n+     * 2. Create topic with 4 replicas\n+     * 3. Scale kafka cluster to 7 replicas and wait, until all pods are ready\n+     * 4. Scale down kafka cluster to 3 replicas\n+     * 5. Trigger rolling update for Kafka cluster\n+     * 6. Rolling update will not be performed, because topic which we created had some replicas on deleted pods - manual fix is needed in that case\n+     */\n+    @Test\n+    void testKafkaWontRollUp() throws Exception {\n+        String topicName = \"test-topic-\" + new Random().nextInt(Integer.MAX_VALUE);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .endKafka()\n+            .endSpec().done();\n+\n+        LOGGER.info(\"Running kafkaScaleUpScaleDown {}\", CLUSTER_NAME);\n+        final int initialReplicas = kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)).getStatus().getReplicas();\n+        assertEquals(3, initialReplicas);\n+\n+        // scale up", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMjk3NA=="}, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjQ1MjgwOnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNVQxOTo0NDozNlrOFaSqIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxMDoxMToxN1rOFaZ3oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMjk5NA==", "bodyText": "The comment does not seem to match with what is going on?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363112994", "createdAt": "2020-01-05T19:44:36Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -269,6 +266,73 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         receiveMessagesExternal(NAMESPACE, topicName, messageCount);\n     }\n \n+    /**\n+     * This test cover case, when KafkaRoller will not roll Kafka pods, because created topic doesn't meet requirements for roll remaining pods\n+     * 1. Deploy kafka cluster with 3 pods\n+     * 2. Create topic with 4 replicas\n+     * 3. Scale kafka cluster to 7 replicas and wait, until all pods are ready\n+     * 4. Scale down kafka cluster to 3 replicas\n+     * 5. Trigger rolling update for Kafka cluster\n+     * 6. Rolling update will not be performed, because topic which we created had some replicas on deleted pods - manual fix is needed in that case\n+     */\n+    @Test\n+    void testKafkaWontRollUp() throws Exception {\n+        String topicName = \"test-topic-\" + new Random().nextInt(Integer.MAX_VALUE);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .endKafka()\n+            .endSpec().done();\n+\n+        LOGGER.info(\"Running kafkaScaleUpScaleDown {}\", CLUSTER_NAME);\n+        final int initialReplicas = kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)).getStatus().getReplicas();\n+        assertEquals(3, initialReplicas);\n+\n+        // scale up\n+        final int scaleTo = initialReplicas + 4;\n+        final int newPodId = initialReplicas;\n+        final String newPodName = KafkaResources.kafkaPodName(CLUSTER_NAME,  newPodId);\n+        LOGGER.info(\"Scaling up to {}\", scaleTo);\n+        // Create snapshot of current cluster\n+        String kafkaStsName = kafkaStatefulSetName(CLUSTER_NAME);\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> k.getSpec().getKafka().setReplicas(scaleTo));\n+        // No need to roll kafka cluster during scale up (no external listeners in Kafka)\n+        StatefulSetUtils.waitForAllStatefulSetPodsReady(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), scaleTo);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStsName);\n+        LOGGER.info(\"Scaling to {} finished\", scaleTo);\n+\n+        // Create topic before scale up to ensure no partitions created on last broker (which will mess up scale down)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIzMTEzNg==", "bodyText": "Removed", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363231136", "createdAt": "2020-01-06T10:11:17Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -269,6 +266,73 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         receiveMessagesExternal(NAMESPACE, topicName, messageCount);\n     }\n \n+    /**\n+     * This test cover case, when KafkaRoller will not roll Kafka pods, because created topic doesn't meet requirements for roll remaining pods\n+     * 1. Deploy kafka cluster with 3 pods\n+     * 2. Create topic with 4 replicas\n+     * 3. Scale kafka cluster to 7 replicas and wait, until all pods are ready\n+     * 4. Scale down kafka cluster to 3 replicas\n+     * 5. Trigger rolling update for Kafka cluster\n+     * 6. Rolling update will not be performed, because topic which we created had some replicas on deleted pods - manual fix is needed in that case\n+     */\n+    @Test\n+    void testKafkaWontRollUp() throws Exception {\n+        String topicName = \"test-topic-\" + new Random().nextInt(Integer.MAX_VALUE);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .endKafka()\n+            .endSpec().done();\n+\n+        LOGGER.info(\"Running kafkaScaleUpScaleDown {}\", CLUSTER_NAME);\n+        final int initialReplicas = kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)).getStatus().getReplicas();\n+        assertEquals(3, initialReplicas);\n+\n+        // scale up\n+        final int scaleTo = initialReplicas + 4;\n+        final int newPodId = initialReplicas;\n+        final String newPodName = KafkaResources.kafkaPodName(CLUSTER_NAME,  newPodId);\n+        LOGGER.info(\"Scaling up to {}\", scaleTo);\n+        // Create snapshot of current cluster\n+        String kafkaStsName = kafkaStatefulSetName(CLUSTER_NAME);\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> k.getSpec().getKafka().setReplicas(scaleTo));\n+        // No need to roll kafka cluster during scale up (no external listeners in Kafka)\n+        StatefulSetUtils.waitForAllStatefulSetPodsReady(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), scaleTo);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStsName);\n+        LOGGER.info(\"Scaling to {} finished\", scaleTo);\n+\n+        // Create topic before scale up to ensure no partitions created on last broker (which will mess up scale down)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMjk5NA=="}, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0MjQ1NzM0OnYy", "diffSide": "RIGHT", "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNVQxOTo1NTozOFrOFaSsZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wNlQxMDoxMjo1OVrOFaZ5_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMzU3NQ==", "bodyText": "What is expected to triger the rolling update here? And how do you check the 6.?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363113575", "createdAt": "2020-01-05T19:55:38Z", "author": {"login": "scholzj"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -269,6 +266,73 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         receiveMessagesExternal(NAMESPACE, topicName, messageCount);\n     }\n \n+    /**\n+     * This test cover case, when KafkaRoller will not roll Kafka pods, because created topic doesn't meet requirements for roll remaining pods\n+     * 1. Deploy kafka cluster with 3 pods\n+     * 2. Create topic with 4 replicas\n+     * 3. Scale kafka cluster to 7 replicas and wait, until all pods are ready\n+     * 4. Scale down kafka cluster to 3 replicas\n+     * 5. Trigger rolling update for Kafka cluster\n+     * 6. Rolling update will not be performed, because topic which we created had some replicas on deleted pods - manual fix is needed in that case\n+     */\n+    @Test\n+    void testKafkaWontRollUp() throws Exception {\n+        String topicName = \"test-topic-\" + new Random().nextInt(Integer.MAX_VALUE);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .endKafka()\n+            .endSpec().done();\n+\n+        LOGGER.info(\"Running kafkaScaleUpScaleDown {}\", CLUSTER_NAME);\n+        final int initialReplicas = kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)).getStatus().getReplicas();\n+        assertEquals(3, initialReplicas);\n+\n+        // scale up\n+        final int scaleTo = initialReplicas + 4;\n+        final int newPodId = initialReplicas;\n+        final String newPodName = KafkaResources.kafkaPodName(CLUSTER_NAME,  newPodId);\n+        LOGGER.info(\"Scaling up to {}\", scaleTo);\n+        // Create snapshot of current cluster\n+        String kafkaStsName = kafkaStatefulSetName(CLUSTER_NAME);\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> k.getSpec().getKafka().setReplicas(scaleTo));\n+        // No need to roll kafka cluster during scale up (no external listeners in Kafka)\n+        StatefulSetUtils.waitForAllStatefulSetPodsReady(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), scaleTo);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStsName);\n+        LOGGER.info(\"Scaling to {} finished\", scaleTo);\n+\n+        // Create topic before scale up to ensure no partitions created on last broker (which will mess up scale down)\n+        KafkaTopicResource.topic(CLUSTER_NAME, topicName, 4, 4, 4).done();\n+\n+        //Test that the new pod does not have errors or failures in events\n+        String uid = kubeClient().getPodUid(newPodName);\n+        List<Event> events = kubeClient().listEvents(uid);\n+        assertThat(events, hasAllOfReasons(Scheduled, Pulled, Created, Started));\n+\n+        //Test that CO doesn't have any exceptions in log\n+        timeMeasuringSystem.stopOperation(timeMeasuringSystem.getOperationID());\n+        assertNoCoErrorsLogged(timeMeasuringSystem.getDurationInSecconds(testClass, testName, timeMeasuringSystem.getOperationID()));\n+\n+        // scale down\n+        LOGGER.info(\"Scaling down to {}\", initialReplicas);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.SCALE_DOWN));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> k.getSpec().getKafka().setReplicas(initialReplicas));\n+\n+        PodUtils.waitUntilPodsCountIsPresent(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), 3);\n+        // Wait for first reconciliation\n+        StUtils.waitForReconciliation(testClass, testName, NAMESPACE);\n+        // Wait for second reconciliation and check that pods are not rollable\n+        StUtils.waitForReconciliation(testClass, testName, NAMESPACE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIzMTc0MA==", "bodyText": "After offline discussion added code for trigger rolling update manually and created issue for unexpected rolling update in that case.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2353#discussion_r363231740", "createdAt": "2020-01-06T10:12:59Z", "author": {"login": "Frawless"}, "path": "systemtest/src/test/java/io/strimzi/systemtest/RollingUpdateST.java", "diffHunk": "@@ -269,6 +266,73 @@ void testKafkaAndZookeeperScaleUpScaleDown() throws Exception {\n         receiveMessagesExternal(NAMESPACE, topicName, messageCount);\n     }\n \n+    /**\n+     * This test cover case, when KafkaRoller will not roll Kafka pods, because created topic doesn't meet requirements for roll remaining pods\n+     * 1. Deploy kafka cluster with 3 pods\n+     * 2. Create topic with 4 replicas\n+     * 3. Scale kafka cluster to 7 replicas and wait, until all pods are ready\n+     * 4. Scale down kafka cluster to 3 replicas\n+     * 5. Trigger rolling update for Kafka cluster\n+     * 6. Rolling update will not be performed, because topic which we created had some replicas on deleted pods - manual fix is needed in that case\n+     */\n+    @Test\n+    void testKafkaWontRollUp() throws Exception {\n+        String topicName = \"test-topic-\" + new Random().nextInt(Integer.MAX_VALUE);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.CLUSTER_RECOVERY));\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3)\n+            .editSpec()\n+                .editKafka()\n+                    .addToConfig(\"auto.create.topics.enable\", \"false\")\n+                .endKafka()\n+            .endSpec().done();\n+\n+        LOGGER.info(\"Running kafkaScaleUpScaleDown {}\", CLUSTER_NAME);\n+        final int initialReplicas = kubeClient().getStatefulSet(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME)).getStatus().getReplicas();\n+        assertEquals(3, initialReplicas);\n+\n+        // scale up\n+        final int scaleTo = initialReplicas + 4;\n+        final int newPodId = initialReplicas;\n+        final String newPodName = KafkaResources.kafkaPodName(CLUSTER_NAME,  newPodId);\n+        LOGGER.info(\"Scaling up to {}\", scaleTo);\n+        // Create snapshot of current cluster\n+        String kafkaStsName = kafkaStatefulSetName(CLUSTER_NAME);\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> k.getSpec().getKafka().setReplicas(scaleTo));\n+        // No need to roll kafka cluster during scale up (no external listeners in Kafka)\n+        StatefulSetUtils.waitForAllStatefulSetPodsReady(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), scaleTo);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaStsName);\n+        LOGGER.info(\"Scaling to {} finished\", scaleTo);\n+\n+        // Create topic before scale up to ensure no partitions created on last broker (which will mess up scale down)\n+        KafkaTopicResource.topic(CLUSTER_NAME, topicName, 4, 4, 4).done();\n+\n+        //Test that the new pod does not have errors or failures in events\n+        String uid = kubeClient().getPodUid(newPodName);\n+        List<Event> events = kubeClient().listEvents(uid);\n+        assertThat(events, hasAllOfReasons(Scheduled, Pulled, Created, Started));\n+\n+        //Test that CO doesn't have any exceptions in log\n+        timeMeasuringSystem.stopOperation(timeMeasuringSystem.getOperationID());\n+        assertNoCoErrorsLogged(timeMeasuringSystem.getDurationInSecconds(testClass, testName, timeMeasuringSystem.getOperationID()));\n+\n+        // scale down\n+        LOGGER.info(\"Scaling down to {}\", initialReplicas);\n+        timeMeasuringSystem.setOperationID(timeMeasuringSystem.startTimeMeasuring(Operation.SCALE_DOWN));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> k.getSpec().getKafka().setReplicas(initialReplicas));\n+\n+        PodUtils.waitUntilPodsCountIsPresent(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME), 3);\n+        // Wait for first reconciliation\n+        StUtils.waitForReconciliation(testClass, testName, NAMESPACE);\n+        // Wait for second reconciliation and check that pods are not rollable\n+        StUtils.waitForReconciliation(testClass, testName, NAMESPACE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzExMzU3NQ=="}, "originalCommit": {"oid": "38724841a2300de21dbdcee6186baa68e0935bfb"}, "originalPosition": 104}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 540, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}