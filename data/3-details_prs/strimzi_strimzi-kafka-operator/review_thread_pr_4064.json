{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM0Mzg3ODcy", "number": 4064, "reviewThreads": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTozODo0N1rOFGqiZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNTowMTo1OFrOFGwEEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTMyNzExOnYy", "diffSide": "RIGHT", "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTozODo0N1rOIHxl7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNTo1NjoxNVrOIIotMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyMzQ3MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Configuration options are also available for ZooKeeper and the Entity Operator.\n          \n          \n            \n            Configuration options are also available for ZooKeeper and the Entity Operator within the `Kafka` resource.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545023470", "createdAt": "2020-12-17T11:38:47Z", "author": {"login": "laidan6000"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.\n+\n+You configure Kafka using the `Kafka` resource.\n+Configuration options are also available for ZooKeeper and the Entity Operator.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyNjA1NA==", "bodyText": "Perhaps add a brief explanation of the Entity Operator to reduce confusion.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545026054", "createdAt": "2020-12-17T11:43:12Z", "author": {"login": "laidan6000"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.\n+\n+You configure Kafka using the `Kafka` resource.\n+Configuration options are also available for ZooKeeper and the Entity Operator.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyMzQ3MA=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkyNjQ1MQ==", "bodyText": "I added The Entity Operator comprises the Topic Operator and User Operator, which manage Kafka topics and users.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545926451", "createdAt": "2020-12-18T15:56:15Z", "author": {"login": "PaulRMellor"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.\n+\n+You configure Kafka using the `Kafka` resource.\n+Configuration options are also available for ZooKeeper and the Entity Operator.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyMzQ3MA=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTMyOTMxOnYy", "diffSide": "RIGHT", "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTozOToyMVrOIHxnKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTozOToyMVrOIHxnKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyMzc4NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The full schema of the `Kafka` resource is described in xref:type-Kafka-reference[].\n          \n          \n            \n            The full schema of the `Kafka` resource is described in the xref:type-Kafka-reference[].", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545023784", "createdAt": "2020-12-17T11:39:21Z", "author": {"login": "laidan6000"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.\n+\n+You configure Kafka using the `Kafka` resource.\n+Configuration options are also available for ZooKeeper and the Entity Operator.\n+\n+The full schema of the `Kafka` resource is described in xref:type-Kafka-reference[].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM0MDI1OnYy", "diffSide": "RIGHT", "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0MjowN1rOIHxtXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNjowNTo1OVrOIIpC_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyNTM3NQ==", "bodyText": "I suggest adding a sentence that defines a Kafka deployment. A Kafka cluster, a ZooKeeper cluster, anything else?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545025375", "createdAt": "2020-12-17T11:42:07Z", "author": {"login": "laidan6000"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkzMjAyOA==", "bodyText": "Switched around a little, so we now have:\n\nThis section describes how to configure a Kafka deployment in your Strimzi cluster.\nA Kafka cluster is deployed with a ZooKeeper cluster. The deployment can also include the Topic Operator and User Operator, which are used to manage Kafka topics and users.\nYou configure Kafka using the Kafka resource.\nConfiguration options are also available for ZooKeeper and the Entity Operator within the Kafka resource.\nThe Entity Operator comprises the Topic Operator and User Operator.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545932028", "createdAt": "2020-12-18T16:05:59Z", "author": {"login": "PaulRMellor"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyNTM3NQ=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM1MDY1OnYy", "diffSide": "RIGHT", "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0NDozMlrOIHxzKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0NDozMlrOIHxzKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyNjg1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For more information on configuring listeners for connecting brokers, see xref:configuration-points-listeners-{context}[Listener configuration]\n          \n          \n            \n            For more information on configuring listeners for connecting brokers, see xref:configuration-points-listeners-{context}[Listener configuration].", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545026858", "createdAt": "2020-12-17T11:44:32Z", "author": {"login": "laidan6000"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.\n+\n+You configure Kafka using the `Kafka` resource.\n+Configuration options are also available for ZooKeeper and the Entity Operator.\n+\n+The full schema of the `Kafka` resource is described in xref:type-Kafka-reference[].\n+\n+.Listener configuration\n+You configure listeners for connecting to Kafka brokers.\n+For more information on configuring listeners for connecting brokers, see xref:configuration-points-listeners-{context}[Listener configuration]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM2NDE1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0Nzo1M1rOIHx65g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0Nzo1M1rOIHx65g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyODgzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Cruise Control\n          \n          \n            \n            * Cruise Control for cluster rebalancing", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545028838", "createdAt": "2020-12-17T11:47:53Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM2NzEyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0ODo0NVrOIHx8qQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo0ODo0NVrOIHx8qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAyOTI4OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n          \n          \n            \n            See the _Deploying and Upgrading Strimzi_ guide for instructions on deploying a:", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545029289", "createdAt": "2020-12-17T11:48:45Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM3NTY0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1MTowMVrOIHyBhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNjoxOTo0OVrOIIpjew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMDUzMg==", "bodyText": "I suggest using the term replicas here, especially because the cross-ref goes to the replicas reference. \"Broker nodes\" is not standard Kafka terminology, but it should probably remain rather than expanding the scope of this PR.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545030532", "createdAt": "2020-12-17T11:51:01Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk0MDM0Nw==", "bodyText": "Changed to The number of replica nodes", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545940347", "createdAt": "2020-12-18T16:19:49Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMDUzMg=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM5MDI4OnYy", "diffSide": "RIGHT", "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1NDo0MVrOIHyKCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1NDo0MVrOIHyKCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMjcxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            You configure listeners for connecting to Kafka brokers.\n          \n          \n            \n            You configure listeners for connecting clients to Kafka brokers.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545032713", "createdAt": "2020-12-17T11:54:41Z", "author": {"login": "laidan6000"}, "path": "documentation/assemblies/configuring/assembly-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,56 @@\n+// This assembly is included in the following assemblies:\n+//\n+// assembly-deployment-configuration.adoc\n+\n+[id='assembly-config-kafka-{context}']\n+= Kafka cluster configuration\n+\n+This section describes how to configure a Kafka deployment in your Strimzi cluster.\n+\n+You configure Kafka using the `Kafka` resource.\n+Configuration options are also available for ZooKeeper and the Entity Operator.\n+\n+The full schema of the `Kafka` resource is described in xref:type-Kafka-reference[].\n+\n+.Listener configuration\n+You configure listeners for connecting to Kafka brokers.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM5MTc1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1NTowOFrOIHyK7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1NTowOFrOIHyK7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMjk0Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * Listeners (and authentication)\n          \n          \n            \n            * Listeners (and authentication of clients)", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545032942", "createdAt": "2020-12-17T11:55:08Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM5NTkyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1NjowOFrOIHyNQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNjoyMzowMlrOIIpqyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzUzNw==", "bodyText": "The Kafka version you want to change to must be supported by a version of AMQ Streams. It might be simpler to add a general statement about upgrades.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545033537", "createdAt": "2020-12-17T11:56:08Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk0MjIxNw==", "bodyText": "Hard to keep this succinct, so I changed to Kafka version, which can be changed to a supported version by following the upgrade procedure", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545942217", "createdAt": "2020-12-18T16:23:02Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzUzNw=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTM5ODUzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1Njo0OFrOIHyOyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1Njo0OFrOIHyOyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzkzMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n          \n          \n            \n            <8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection from inside or outside the Kubernetes cluster].", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545033930", "createdAt": "2020-12-17T11:56:48Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQxMDM3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1OTo0OFrOIHyVog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMTo1OTo0OFrOIHyVog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNTY4Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n          \n          \n            \n            <16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` specifies a `Secret` that contains a server certificate and a private key. You can configure Kafka listener certificates for both `tls` and `external` listeners.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545035682", "createdAt": "2020-12-17T11:59:48Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQxNzAzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMTozNFrOIHyZnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMTozNFrOIHyZnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNjcwMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n          \n          \n            \n            <17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0, or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545036700", "createdAt": "2020-12-17T12:01:34Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQyMDQ0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMjoxOFrOIHybiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMjoxOFrOIHybiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzE5Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n          \n          \n            \n            <18> The `config` specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545037192", "createdAt": "2020-12-17T12:02:18Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQyNTA4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMzozNFrOIHyeUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMzozNFrOIHyeUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzkwNg==", "bodyText": "I suggest putting the cross-ref on \"Storage\".", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545037906", "createdAt": "2020-12-17T12:03:34Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ0NTQxOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowODoyN1rOIHypcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNjozMjozNFrOIIqBKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDc1NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n          \n          \n            \n            <24> xref:con-common-configuration-prometheus-reference[Prometheus metrics] enabled. In this example, metrics are configured for the Prometheus JMX Exporter (the default metrics exporter). You can enable metrics without further configuration using `metrics: {}`.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545040755", "createdAt": "2020-12-17T12:08:27Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MTY0OA==", "bodyText": "If the metrics property is empty, are metrics still collected using the Prometheus JMX Exporter?\nPerhaps add a link to the Metrics section of the Deploying guide.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545041648", "createdAt": "2020-12-17T12:09:55Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDc1NQ=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk0Nzk0Nw==", "bodyText": "Empty object just means the default config is used.\nThe section the callout links to has a link to the deploying guide. Metrics is changing to metricsConfig (#3948), so this will callout change.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545947947", "createdAt": "2020-12-18T16:32:34Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDc1NQ=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ1ODM2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxMTozOFrOIHywmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxMTozOFrOIHywmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MjU4Ng==", "bodyText": "Through the Prometheus JMX Exporter?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545042586", "createdAt": "2020-12-17T12:11:38Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ2MjY4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxMjo0M1rOIHyy_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxMjo0M1rOIHyy_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MzE5Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n          \n          \n            \n            <25> Prometheus rules for exporting metrics to a Grafana dashboard through the Prometheus JMX Exporter. A set of Prometheus rules provided with Strimzi may be copied to your Kafka resource configuration.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545043196", "createdAt": "2020-12-17T12:12:43Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ3MjMyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNTowOVrOIHy4bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNTowOVrOIHy4bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDU5MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n          \n          \n            \n            <32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels]. This example uses `inline` logging.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545044590", "createdAt": "2020-12-17T12:15:09Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 222}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ3MzU4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNToyN1rOIHy5Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNToyN1rOIHy5Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDc2Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n          \n          \n            \n            <33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels]. This example uses `inline` logging.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545044767", "createdAt": "2020-12-17T12:15:27Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 223}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ3ODA4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNjozM1rOIHy7sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzowMDoxMlrOIIrBXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTQyNg==", "bodyText": "I think the long description of 34 muddies the waters.\nPerhaps use a much shorter description stating that Kafka Exporter is an optional component that monitors consumer lag.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545045426", "createdAt": "2020-12-17T12:16:33Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk2NDM4Mw==", "bodyText": "changed to: Kafka Exporter is an optional component for extracting metrics data from Kafka brokers, in particular consumer lag data.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545964383", "createdAt": "2020-12-18T17:00:12Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTQyNg=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ4MjI1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNzozNVrOIHy98A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNzozNVrOIHy98A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NjAwMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            <35> Cruise Control configuration, which is used xref:cruise-control-concepts-str[to rebalance the Kafka cluster].\n          \n          \n            \n            <35> Optional configuration for Cruise Control, which is used to xref:cruise-control-concepts-str[rebalance the Kafka cluster].", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545046000", "createdAt": "2020-12-17T12:17:35Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.\n+For information on setting up Kafka Exporter and why it is important to monitor consumer lag for performance, see link:{BookURLDeploying}#assembly-metrics-kafka-exporter-str[Kafka Exporter] in the _Deploying and Upgrading Strimzi_ guide.\n+<35> Cruise Control configuration, which is used xref:cruise-control-concepts-str[to rebalance the Kafka cluster].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 226}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ4ODAyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxOTowNFrOIHzBLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzowMzo0MlrOIIrI8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NjgyOA==", "bodyText": "For what it's worth, I don't think we should divide this into separate sections. It serves as a really useful reference to the Kafka resource and users might view it side-by-side with the YAML example resource.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545046828", "createdAt": "2020-12-17T12:19:04Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk2NjMyMQ==", "bodyText": "Yes. It's probably better to leave it together. I was just wondering about the scrolling up and down between example and callout", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545966321", "createdAt": "2020-12-18T17:03:42Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NjgyOA=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ5Njc0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoyMToxMFrOIHzGIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzoxMTo1N1rOIIraJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0ODA5Ng==", "bodyText": "Does this refer to a Kafka resource in the example files? Remember that we provide a Kafka resource with metrics configuration.\nIn other words, does the YAML below refer to a specific file in the AMQ Streams download?", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545048096", "createdAt": "2020-12-17T12:21:10Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk3MDcyNQ==", "bodyText": "I'm not sure we need to distinguish. Just that these are the config options for your custom resource irrespective. You can also add the metrics config without using the specific kafka resource with metrics", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545970725", "createdAt": "2020-12-18T17:11:57Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0ODA5Ng=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTUwMDgyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoyMjowOVrOIHzIUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzoxMjo1OVrOIIrcYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0ODY1Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            . Create or update the resource:\n          \n          \n            \n            . After you have edited the configuration, create or update the resource:", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545048657", "createdAt": "2020-12-17T12:22:09Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.\n+For information on setting up Kafka Exporter and why it is important to monitor consumer lag for performance, see link:{BookURLDeploying}#assembly-metrics-kafka-exporter-str[Kafka Exporter] in the _Deploying and Upgrading Strimzi_ guide.\n+<35> Cruise Control configuration, which is used xref:cruise-control-concepts-str[to rebalance the Kafka cluster].\n+<36> Cruise Control xref:type-TlsSidecar-reference[TLS sidecar configuration]. Cruise Control uses the TLS sidecar for secure communication with ZooKeeper.\n+\n+. Create or update the resource:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk3MTI5Ng==", "bodyText": "Not sure we need this. We've moved on to a new step.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545971296", "createdAt": "2020-12-18T17:12:59Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.\n+For information on setting up Kafka Exporter and why it is important to monitor consumer lag for performance, see link:{BookURLDeploying}#assembly-metrics-kafka-exporter-str[Kafka Exporter] in the _Deploying and Upgrading Strimzi_ guide.\n+<35> Cruise Control configuration, which is used xref:cruise-control-concepts-str[to rebalance the Kafka cluster].\n+<36> Cruise Control xref:type-TlsSidecar-reference[TLS sidecar configuration]. Cruise Control uses the TLS sidecar for secure communication with ZooKeeper.\n+\n+. Create or update the resource:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0ODY1Nw=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTUwMjkwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-config-kafka.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoyMjozOFrOIHzJfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxNzoxNToyNVrOIIrhiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0ODk1OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            kubectl apply -f _KAFKA-CONFIG-FILE_\n          \n          \n            \n            kubectl apply -f _KAFKA-CONFIG-FILE_.yaml", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545048958", "createdAt": "2020-12-17T12:22:38Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.\n+For information on setting up Kafka Exporter and why it is important to monitor consumer lag for performance, see link:{BookURLDeploying}#assembly-metrics-kafka-exporter-str[Kafka Exporter] in the _Deploying and Upgrading Strimzi_ guide.\n+<35> Cruise Control configuration, which is used xref:cruise-control-concepts-str[to rebalance the Kafka cluster].\n+<36> Cruise Control xref:type-TlsSidecar-reference[TLS sidecar configuration]. Cruise Control uses the TLS sidecar for secure communication with ZooKeeper.\n+\n+. Create or update the resource:\n++\n+[source,shell,subs=+quotes]\n+kubectl apply -f _KAFKA-CONFIG-FILE_", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk3MjYxNg==", "bodyText": "If we're taking this approach, and maybe we should, there are quite a few places where we don't. So maybe this deserves a separate PR/update of its own.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545972616", "createdAt": "2020-12-18T17:15:25Z", "author": {"login": "PaulRMellor"}, "path": "documentation/modules/configuring/proc-config-kafka.adoc", "diffHunk": "@@ -0,0 +1,232 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-config-kafka-{context}']\n+= Configuring Kafka\n+\n+Use the properties of the `Kafka` resource to configure your Kafka deployment.\n+\n+As well as configuring Kafka, you can add configuration for ZooKeeper and the Strimzi Operators.\n+Common configuration properties, such as logging and healthchecks, are configured independently for each component.\n+\n+This procedure shows only some of the possible configuration options, but those that are particularly important include:\n+\n+* Resource requests (CPU / Memory)\n+* JVM options for maximum and minimum memory allocation\n+* Listeners (and authentication)\n+* Authentication\n+* Storage\n+* Rack awareness\n+* Metrics\n+* Cruise Control\n+\n+.Prerequisites\n+\n+* An OpenShift cluster\n+* A running Cluster Operator\n+\n+See the _Deploying and Upgrading Strimzi_ guide for instructions on running a:\n+\n+* link:{BookURLDeploying}#cluster-operator-str[Cluster Operator^]\n+* link:{BookURLDeploying}#deploying-kafka-cluster-str[Kafka cluster^]\n+\n+.Procedure\n+\n+. Edit the `spec` properties for the `Kafka` resource.\n++\n+The properties you can configure are shown in this example configuration:\n++\n+[source,shell,subs=\"+attributes\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    replicas: 3 <1>\n+    version: {ProductVersion} <2>\n+    logging: <3>\n+      type: inline\n+      loggers:\n+        kafka.root.logger.level: \"INFO\"\n+    resources: <4>\n+      requests:\n+        memory: 64Gi\n+        cpu: \"8\"\n+      limits:\n+        memory: 64Gi\n+        cpu: \"12\"\n+    readinessProbe: <5>\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    livenessProbe:\n+      initialDelaySeconds: 15\n+      timeoutSeconds: 5\n+    jvmOptions: <6>\n+      -Xms: 8192m\n+      -Xmx: 8192m\n+    image: my-org/my-image:latest <7>\n+    listeners: <8>\n+      - name: plain <9>\n+        port: 9092 <10>\n+        type: internal <11>\n+        tls: false <12>\n+        configuration:\n+          useServiceDnsDomain: true <13>\n+      - name: tls\n+        port: 9093\n+        type: internal\n+        tls: true\n+        authentication: <14>\n+          type: tls\n+      - name: external <15>\n+        port: 9094\n+        type: route\n+        tls: true\n+        configuration:\n+          brokerCertChainAndKey: <16>\n+            secretName: my-secret\n+            certificate: my-certificate.crt\n+            key: my-key.key\n+    authorization: <17>\n+      type: simple\n+    config: <18>\n+      auto.create.topics.enable: \"false\"\n+      offsets.topic.replication.factor: 3\n+      transaction.state.log.replication.factor: 3\n+      transaction.state.log.min.isr: 2\n+      ssl.cipher.suites: \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" <19>\n+      ssl.enabled.protocols: \"TLSv1.2\"\n+      ssl.protocol: \"TLSv1.2\"\n+    storage: <20>\n+      type: persistent-claim <21>\n+      size: 10000Gi <22>\n+    rack: <23>\n+      topologyKey: topology.kubernetes.io/zone\n+    metrics: <24>\n+      lowercaseOutputName: true\n+      rules: <25>\n+      # Special cases and very specific rules\n+      - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value\n+        name: kafka_server_$1_$2\n+        type: GAUGE\n+        labels:\n+          clientId: \"$3\"\n+          topic: \"$4\"\n+          partition: \"$5\"\n+    jmxOptions: <26>\n+      authentication:\n+        type: \"password\"\n+    # ...\n+  zookeeper: <27>\n+    replicas: 3 <28>\n+    logging: <29>\n+      type: inline\n+      loggers:\n+        zookeeper.root.logger: \"INFO\"\n+    resources:\n+      requests:\n+        memory: 8Gi\n+        cpu: \"2\"\n+      limits:\n+        memory: 8Gi\n+        cpu: \"2\"\n+    jvmOptions:\n+      -Xms: 4096m\n+      -Xmx: 4096m\n+    storage:\n+      type: persistent-claim\n+      size: 1000Gi\n+    metrics:\n+      # ...\n+  entityOperator: <30>\n+    tlsSidecar: <31>\n+      resources:\n+        requests:\n+          cpu: 200m\n+          memory: 64Mi\n+        limits:\n+          cpu: 500m\n+          memory: 128Mi\n+    topicOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <32>\n+        type: inline\n+        loggers:\n+          rootLogger.level: \"INFO\"\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+    userOperator:\n+      watchedNamespace: my-topic-namespace\n+      reconciliationIntervalSeconds: 60\n+      logging: <33>\n+        type: inline\n+        loggers:\n+          rootLogger.level: INFO\n+      resources:\n+        requests:\n+          memory: 512Mi\n+          cpu: \"1\"\n+        limits:\n+          memory: 512Mi\n+          cpu: \"1\"\n+  kafkaExporter: <34>\n+    # ...\n+  cruiseControl: <35>\n+    # ...\n+    tlsSidecar: <36>\n+    # ...\n+----\n+<1> xref:con-common-configuration-replicas-reference[The number of broker nodes]. If your cluster already has topics defined, you can\n+xref:scaling-clusters-{context}[scale clusters].\n+<2> Kafka version, which can be changed by following link:{BookURLDeploying}#assembly-upgrade-str[the upgrade procedure].\n+<3> Specified xref:property-kafka-logging-reference[Kafka loggers and log levels] added directly (`inline`) or indirectly (`external`) through a ConfigMap. A custom ConfigMap must be placed under the `log4j.properties` key. For the Kafka `kafka.root.logger.level` logger, you can set the log level to INFO, ERROR, WARN, TRACE, DEBUG, FATAL or OFF.\n+<4> Requests for reservation of xref:con-common-configuration-resources-reference[supported resources], currently `cpu` and `memory`, and limits to specify the maximum resources that can be consumed.\n+<5> xref:con-common-configuration-healthchecks-reference[Healthchecks] to know when to restart a container (liveness) and when a container can accept traffic (readiness).\n+<6> xref:con-common-configuration-jvm-reference[JVM configuration options] to optimize performance for the Virtual Machine (VM) running Kafka.\n+<7> ADVANCED OPTION: xref:con-common-configuration-images-reference[Container image configuration], which is recommended only in special situations.\n+<8> Listeners configure how clients connect to the Kafka cluster via bootstrap addresses. Listeners are xref:assembly-securing-kafka-brokers-str[configured as _internal_ or _external_ listeners for connection inside or outside the Kubernetes cluster].\n+<9> Name to identify the listener. Must be unique within the Kafka cluster.\n+<10> Port number used by the listener inside Kafka. The port number has to be unique within a given Kafka cluster. Allowed port numbers are 9092 and higher with the exception of ports 9404 and 9999, which are already used for Prometheus and JMX. Depending on the listener type, the port number might not be the same as the port number that connects Kafka clients.\n+<11> Listener type specified as `internal`, or for external listeners, as `route`, `loadbalancer`, `nodeport` or `ingress`.\n+<12> Enables TLS encryption for each listener. Default is `false`. TLS encryption is not required for `route` listeners.\n+<13> Defines whether the fully-qualified DNS names including the cluster service suffix (usually `.cluster.local`) are assigned.\n+<14> Listener authentication mechanism xref:assembly-securing-kafka-brokers-str[specified as mutual TLS, SCRAM-SHA-512 or token-based OAuth 2.0].\n+<15> External listener configuration specifies xref:assembly-configuring-external-listeners-str[how the Kafka cluster is exposed outside Kubernetes, such as through a `route`, `loadbalancer` or `nodeport`].\n+<16> Optional configuration for a xref:kafka-listener-certificates-str[Kafka listener certificate] managed by an external Certificate Authority. The `brokerCertChainAndKey` property specifies a `Secret` that holds a server certificate and a private key. Kafka listener certificates can also be configured for TLS listeners.\n+<17> Authorization xref:con-securing-kafka-authorization-str[enables simple, OAUTH 2.0 or OPA authorization on the Kafka broker.] Simple authorization uses the `AclAuthorizer` Kafka plugin.\n+<18> Config specifies the broker configuration. xref:property-kafka-config-reference[Standard Apache Kafka configuration may be provided, restricted to those properties not managed directly by Strimzi].\n+<19> xref:con-common-configuration-ssl-reference[SSL properties for external listeners to run with a specific _cipher suite_ for a TLS version].\n+<20> Storage is xref:assembly-storage-{context}[configured as `ephemeral`, `persistent-claim` or `jbod`].\n+<21> Storage size for xref:proc-resizing-persistent-volumes-{context}[persistent volumes may be increased] and additional xref:proc-adding-volumes-to-jbod-storage-{context}[volumes may be added to JBOD storage].\n+<22> Persistent storage has xref:ref-persistent-storage-{context}[additional configuration options], such as a storage `id` and `class` for dynamic volume provisioning.\n+<23> xref:type-Rack-reference[Rack awareness] is configured to spread replicas across different racks. A `topologykey` must match the label of a cluster node.\n+<24> xref:con-common-configuration-prometheus-reference[Prometheus metrics], which are enabled with configuration for the Prometheus JMX exporter in this example. You can enable metrics without further configuration using `metrics: {}`.\n+<25> Kafka rules for exporting metrics to a Grafana dashboard through the JMX Exporter. A set of rules provided with Strimzi may be copied to your Kafka resource configuration.\n+<26> xref:property-kafka-jmx-reference[JMX options] to open JMX port 9999 to obtain JMX metrics, which is configured for password protection in this example. You can open the port without protection using `jmxOptions: {}`.\n+<27> ZooKeeper-specific configuration, which contains properties similar to the Kafka configuration.\n+<28> xref:con-common-configuration-replicas-reference[The number of ZooKeeper nodes]. ZooKeeper clusters or ensembles usually run with an odd number of nodes, typically three, five, or seven. The majority of nodes must be available in order to maintain an effective quorum.\n+If the ZooKeeper cluster loses its quorum, it will stop responding to clients and the Kafka brokers will stop working.\n+Having a stable and highly available ZooKeeper cluster is crucial for Strimzi.\n+<29> Specified xref:property-zookeeper-logging-reference[ZooKeeper loggers and log levels].\n+<30> Entity Operator configuration, which xref:assembly-kafka-entity-operator-{context}[specifies the configuration for the Topic Operator and User Operator].\n+<31> Entity Operator xref:type-TlsSidecar-reference[TLS sidecar configuration]. Entity Operator uses the TLS sidecar for secure communication with ZooKeeper.\n+<32> Specified xref:property-topic-operator-logging-reference[Topic Operator loggers and log levels].\n+<33> Specified xref:property-user-operator-logging-reference[User Operator loggers and log levels].\n+<34> Kafka Exporter configuration, to deploy Kafka Exporter in your cluster and expose data as Prometheus metrics. Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics.\n+For information on setting up Kafka Exporter and why it is important to monitor consumer lag for performance, see link:{BookURLDeploying}#assembly-metrics-kafka-exporter-str[Kafka Exporter] in the _Deploying and Upgrading Strimzi_ guide.\n+<35> Cruise Control configuration, which is used xref:cruise-control-concepts-str[to rebalance the Kafka cluster].\n+<36> Cruise Control xref:type-TlsSidecar-reference[TLS sidecar configuration]. Cruise Control uses the TLS sidecar for secure communication with ZooKeeper.\n+\n+. Create or update the resource:\n++\n+[source,shell,subs=+quotes]\n+kubectl apply -f _KAFKA-CONFIG-FILE_", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0ODk1OA=="}, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 232}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTk1MTg2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-kafka-entity-operator.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDowNjowNlrOIH3LOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDowNjowNlrOIH3LOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTExNDkzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For more information on the properties to configure the Entity Operator, see the xref:type-EntityUserOperatorSpec-reference[`EntityUserOperatorSpec` schema reference].\n          \n          \n            \n            For more information on the properties used to configure the Entity Operator, see the xref:type-EntityUserOperatorSpec-reference[`EntityUserOperatorSpec` schema reference].", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545114939", "createdAt": "2020-12-17T14:06:06Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-kafka-entity-operator.adoc", "diffHunk": "@@ -0,0 +1,49 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-kafka-entity-operator.adoc\n+\n+[id='ref-kafka-entity-operator-{context}']\n+= Entity Operator configuration properties\n+\n+Use the `entityOperator` property in `Kafka.spec` to configure the Entity Operator.\n+\n+The `entityOperator` property supports several sub-properties:\n+\n+* `tlsSidecar`\n+* `topicOperator`\n+* `userOperator`\n+* `template`\n+\n+The `tlsSidecar` property contains the configuration of the TLS sidecar container, which is used to communicate with ZooKeeper.\n+\n+The `template` property contains the configuration of the Entity Operator pod, such as labels, annotations, affinity, and tolerations.\n+For more information on configuring templates, see xref:assembly-customizing-kubernetes-resources-str[].\n+\n+The `topicOperator` property contains the configuration of the Topic Operator.\n+When this option is missing, the Entity Operator is deployed without the Topic Operator.\n+\n+The `userOperator` property contains the configuration of the User Operator.\n+When this option is missing, the Entity Operator is deployed without the User Operator.\n+\n+For more information on the properties to configure the Entity Operator, see the xref:type-EntityUserOperatorSpec-reference[`EntityUserOperatorSpec` schema reference].", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTk2NDEwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/con-configuring-topic-operator.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDowODozOFrOIH3SQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDowODozOFrOIH3SQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTExNjczOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Consider increasing this value when topic creation could take more time due to the number of partitions or replicas.\n          \n          \n            \n            Consider increasing this value when topic creation might take more time due to the number of partitions or replicas.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545116738", "createdAt": "2020-12-17T14:08:38Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/con-configuring-topic-operator.adoc", "diffHunk": "@@ -0,0 +1,59 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-kafka-entity-operator.adoc\n+\n+[id='topic-operator-{context}']\n+= Topic Operator configuration properties\n+\n+Topic Operator deployment can be configured using additional options inside the `topicOperator` object.\n+The following properties are supported:\n+\n+`watchedNamespace`::\n+The Kubernetes namespace in which the topic operator watches for `KafkaTopics`.\n+Default is the namespace where the Kafka cluster is deployed.\n+\n+`reconciliationIntervalSeconds`::\n+The interval between periodic reconciliations in seconds.\n+Default `90`.\n+\n+`zookeeperSessionTimeoutSeconds`::\n+The ZooKeeper session timeout in seconds.\n+Default `20`.\n+\n+`topicMetadataMaxAttempts`::\n+The number of attempts at getting topic metadata from Kafka.\n+The time between each attempt is defined as an exponential back-off.\n+Consider increasing this value when topic creation could take more time due to the number of partitions or replicas.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAwMjA3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNjoyM1rOIH3ozg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNjoyM1rOIH3ozg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyMjUxMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.\n          \n          \n            \n            Ephemeral storage uses `{K8sEmptyDir}` volumes to store data.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545122510", "createdAt": "2020-12-17T14:16:23Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "diffHunk": "@@ -0,0 +1,40 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-ephemeral-storage-{context}']\n+= Ephemeral storage\n+\n+Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAwMzI3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNjozNlrOIH3pew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNjozNlrOIH3pew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyMjY4Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To use ephemeral storage, the `type` field should be set to `ephemeral`.\n          \n          \n            \n            To use ephemeral storage, set the `storage.type` field to `ephemeral`.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545122683", "createdAt": "2020-12-17T14:16:36Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "diffHunk": "@@ -0,0 +1,40 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-ephemeral-storage-{context}']\n+= Ephemeral storage\n+\n+Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.\n+To use ephemeral storage, the `type` field should be set to `ephemeral`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAwNTczOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNzowNVrOIH3q6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNzowNVrOIH3q6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyMzA0OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            IMPORTANT: `emptyDir` volumes are not persistent and the data stored in them will be lost when the Pod is restarted.\n          \n          \n            \n            IMPORTANT: `emptyDir` volumes are not persistent and the data stored in them is lost when the pod is restarted.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545123049", "createdAt": "2020-12-17T14:17:05Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "diffHunk": "@@ -0,0 +1,40 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-ephemeral-storage-{context}']\n+= Ephemeral storage\n+\n+Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.\n+To use ephemeral storage, the `type` field should be set to `ephemeral`.\n+\n+IMPORTANT: `emptyDir` volumes are not persistent and the data stored in them will be lost when the Pod is restarted.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAwNzYwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNzoyOVrOIH3sEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxNzoyOVrOIH3sEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyMzM0Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            After the new pod is started, it has to recover all data from other nodes of the cluster.\n          \n          \n            \n            After the new pod is started, it must recover all data from the other nodes of the cluster.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545123346", "createdAt": "2020-12-17T14:17:29Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "diffHunk": "@@ -0,0 +1,40 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-ephemeral-storage-{context}']\n+= Ephemeral storage\n+\n+Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.\n+To use ephemeral storage, the `type` field should be set to `ephemeral`.\n+\n+IMPORTANT: `emptyDir` volumes are not persistent and the data stored in them will be lost when the Pod is restarted.\n+After the new pod is started, it has to recover all data from other nodes of the cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAxMTc4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxODoyNVrOIH3umQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxODoyNVrOIH3umQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyMzk5Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Ephemeral storage is not suitable for use with single node ZooKeeper clusters and for Kafka topics with replication factor 1, because it will lead to data loss.\n          \n          \n            \n            Ephemeral storage is not suitable for use with single-node ZooKeeper clusters or for Kafka topics with a replication factor of 1. This configuration will cause data loss.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545123993", "createdAt": "2020-12-17T14:18:25Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "diffHunk": "@@ -0,0 +1,40 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-ephemeral-storage-{context}']\n+= Ephemeral storage\n+\n+Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.\n+To use ephemeral storage, the `type` field should be set to `ephemeral`.\n+\n+IMPORTANT: `emptyDir` volumes are not persistent and the data stored in them will be lost when the Pod is restarted.\n+After the new pod is started, it has to recover all data from other nodes of the cluster.\n+Ephemeral storage is not suitable for use with single node ZooKeeper clusters and for Kafka topics with replication factor 1, because it will lead to data loss.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAyMDA0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxOTo1OVrOIH3zRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoxOTo1OVrOIH3zRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyNTE5MQ==", "bodyText": "The italics are not being applied; need to add the Asciidoctor substitution block.\nUse upper-case and italics for replaceables.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545125191", "createdAt": "2020-12-17T14:19:59Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-ephemeral.adoc", "diffHunk": "@@ -0,0 +1,40 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-ephemeral-storage-{context}']\n+= Ephemeral storage\n+\n+Ephemeral storage uses the `{K8sEmptyDir}` volumes to store data.\n+To use ephemeral storage, the `type` field should be set to `ephemeral`.\n+\n+IMPORTANT: `emptyDir` volumes are not persistent and the data stored in them will be lost when the Pod is restarted.\n+After the new pod is started, it has to recover all data from other nodes of the cluster.\n+Ephemeral storage is not suitable for use with single node ZooKeeper clusters and for Kafka topics with replication factor 1, because it will lead to data loss.\n+\n+.An example of Ephemeral storage\n+[source,yaml,subs=\"attributes+\"]\n+----\n+apiVersion: {KafkaApiVersion}\n+kind: Kafka\n+metadata:\n+  name: my-cluster\n+spec:\n+  kafka:\n+    # ...\n+    storage:\n+      type: ephemeral\n+    # ...\n+  zookeeper:\n+    # ...\n+    storage:\n+      type: ephemeral\n+    # ...\n+----\n+\n+== Log directories\n+\n+The ephemeral volume will be used by the Kafka brokers as log directories mounted into the following path:\n+\n+`/var/lib/kafka/data/kafka-log_idx_`::", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjAzNzg1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/ref-storage-persistent.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoyMzozM1rOIH39vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoyMzozM1rOIH39vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEyNzg2OA==", "bodyText": "Same issue with the markup.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545127868", "createdAt": "2020-12-17T14:23:33Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/ref-storage-persistent.adoc", "diffHunk": "@@ -0,0 +1,155 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-storage.adoc\n+\n+[id='ref-persistent-storage-{context}']\n+= Persistent storage\n+\n+Persistent storage uses {K8sPersistentVolumeClaims} to provision persistent volumes for storing data.\n+Persistent Volume Claims can be used to provision volumes of many different types, depending on the {K8SStorageClass} which will provision the volume.\n+The data types which can be used with persistent volume claims include many types of SAN storage as well as {K8sLocalPersistentVolumes}.\n+\n+To use persistent storage, the `type` has to be set to `persistent-claim`.\n+Persistent storage supports additional configuration options:\n+\n+`id` (optional)::\n+Storage identification number. This option is mandatory for storage volumes defined in a JBOD storage declaration.\n+Default is `0`.\n+\n+`size` (required)::\n+Defines the size of the persistent volume claim, for example, \"1000Gi\".\n+\n+`class` (optional)::\n+The Kubernetes {K8SStorageClass} to use for dynamic volume provisioning.\n+\n+`selector` (optional)::\n+Allows selecting a specific persistent volume to use.\n+It contains key:value pairs representing labels for selecting such a volume.\n+\n+`deleteClaim` (optional)::\n+Boolean value which specifies if the Persistent Volume Claim has to be deleted when the cluster is undeployed.\n+Default is `false`.\n+\n+WARNING: Increasing the size of persistent volumes in an existing Strimzi cluster is only supported in Kubernetes versions that support persistent volume resizing. The persistent volume to be resized must use a storage class that supports volume expansion.\n+For other versions of Kubernetes and storage classes which do not support volume expansion, you must decide the necessary storage size before deploying the cluster.\n+Decreasing the size of existing persistent volumes is not possible.\n+\n+.Example fragment of persistent storage configuration with 1000Gi `size`\n+[source,yaml]\n+----\n+# ...\n+storage:\n+  type: persistent-claim\n+  size: 1000Gi\n+# ...\n+----\n+\n+The following example demonstrates the use of a storage class.\n+\n+.Example fragment of persistent storage configuration with specific Storage Class\n+[source,yaml,subs=\"attributes+\"]\n+----\n+# ...\n+storage:\n+  type: persistent-claim\n+  size: 1Gi\n+  class: my-storage-class\n+# ...\n+----\n+\n+Finally, a `selector` can be used to select a specific labeled persistent volume to provide needed features such as an SSD.\n+\n+.Example fragment of persistent storage configuration with selector\n+[source,yaml,subs=\"attributes+\"]\n+----\n+# ...\n+storage:\n+  type: persistent-claim\n+  size: 1Gi\n+  selector:\n+    hdd-type: ssd\n+  deleteClaim: true\n+# ...\n+----\n+\n+== Storage class overrides\n+\n+You can specify a different storage class for one or more Kafka brokers or ZooKeeper nodes, instead of using the default storage class.\n+This is useful if, for example, storage classes are restricted to different availability zones or data centers.\n+You can use the `overrides` field for this purpose.\n+\n+In this example, the default storage class is named `my-storage-class`:\n+\n+.Example Strimzi cluster using storage class overrides\n+[source,yaml,subs=\"attributes+\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: Kafka\n+metadata:\n+  labels:\n+    app: my-cluster\n+  name: my-cluster\n+  namespace: myproject\n+spec:\n+  # ...\n+  kafka:\n+    replicas: 3\n+    storage:\n+      deleteClaim: true\n+      size: 100Gi\n+      type: persistent-claim\n+      class: my-storage-class\n+      overrides:\n+        - broker: 0\n+          class: my-storage-class-zone-1a\n+        - broker: 1\n+          class: my-storage-class-zone-1b\n+        - broker: 2\n+          class: my-storage-class-zone-1c\n+  # ...\n+  zookeeper:\n+    replicas: 3\n+    storage:\n+      deleteClaim: true\n+      size: 100Gi\n+      type: persistent-claim\n+      class: my-storage-class\n+      overrides:\n+        - broker: 0\n+          class: my-storage-class-zone-1a\n+        - broker: 1\n+          class: my-storage-class-zone-1b\n+        - broker: 2\n+          class: my-storage-class-zone-1c\n+  # ...\n+----\n+\n+As a result of the configured `overrides` property, the volumes use the following storage classes:\n+\n+* The persistent volumes of ZooKeeper node 0 will use `my-storage-class-zone-1a`.\n+* The persistent volumes of ZooKeeper node 1 will use `my-storage-class-zone-1b`.\n+* The persistent volumes of ZooKeeepr node 2 will use `my-storage-class-zone-1c`.\n+* The persistent volumes of Kafka broker 0 will use `my-storage-class-zone-1a`.\n+* The persistent volumes of Kafka broker 1 will use `my-storage-class-zone-1b`.\n+* The persistent volumes of Kafka broker 2 will use `my-storage-class-zone-1c`.\n+\n+The `overrides` property is currently used only to override storage class configurations. Overriding other storage configuration fields is not currently supported.\n+Other fields from the storage configuration are currently not supported.\n+\n+[[pvc-naming]]\n+== Persistent Volume Claim naming\n+\n+When persistent storage is used, it creates Persistent Volume Claims with the following names:\n+\n+`data-_cluster-name_-kafka-_idx_`::\n+Persistent Volume Claim for the volume used for storing data for the Kafka broker pod `_idx_`.\n+\n+`data-_cluster-name_-zookeeper-_idx_`::\n+Persistent Volume Claim for the volume used for storing data for the ZooKeeper node pod `_idx_`.\n+\n+== Log directories\n+\n+The persistent volume will be used by the Kafka brokers as log directories mounted into the following path:\n+\n+`/var/lib/kafka/data/kafka-log_idx_`::", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjIzMjQ4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/configuring/proc-connecting-to-zookeeper.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNTowMTo1OFrOIH5xiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNTowMTo1OFrOIH5xiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTE1NzUxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            However, if you want to use Kafka CLI tools that require a connection to ZooKeeper, such as the `kafka-topics` tool, you can open a terminal inside a Kafka container and connect to the local end of the TLS tunnel to ZooKeeper by using `localhost:2181` as the ZooKeeper address.\n          \n          \n            \n            However, if you want to use Kafka CLI tools that require a connection to ZooKeeper, such as the `kafka-topics.sh` tool, you can open a terminal inside a Kafka container and connect to the local end of the TLS tunnel to ZooKeeper by using `localhost:2181` as the ZooKeeper address.", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/4064#discussion_r545157513", "createdAt": "2020-12-17T15:01:58Z", "author": {"login": "laidan6000"}, "path": "documentation/modules/configuring/proc-connecting-to-zookeeper.adoc", "diffHunk": "@@ -0,0 +1,32 @@\n+// Module included in the following assemblies:\n+//\n+// assembly-config-kafka.adoc\n+\n+[id='proc-connnecting-to-zookeeper-{context}']\n+= Connecting to ZooKeeper from a terminal\n+\n+Most Kafka CLI tools can connect directly to Kafka, so under normal circumstances you should not need to connect to ZooKeeper.\n+ZooKeeper services are secured with encryption and authentication and are not intended to be used by external applications that are not part of Strimzi.\n+\n+However, if you want to use Kafka CLI tools that require a connection to ZooKeeper, such as the `kafka-topics` tool, you can open a terminal inside a Kafka container and connect to the local end of the TLS tunnel to ZooKeeper by using `localhost:2181` as the ZooKeeper address.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d6a38b3455d0d732d737bf236305a7c94a49ad3"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 823, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}