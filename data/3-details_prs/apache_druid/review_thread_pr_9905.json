{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxMTEzMzky", "number": 9905, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMToyNjozNlrOECybKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNzo1ODo1N1rOEDC_2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzU4NzYzOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMToyNjozNlrOGfdc8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNlQwODowMzoxMFrOGgCiVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU4Ng==", "bodyText": "Would you explain why you changed this? The intention here was to make some unit tests failed when the timeline somehow returns unsorted intervals which is supposed to be a bug.", "url": "https://github.com/apache/druid/pull/9905#discussion_r435641586", "createdAt": "2020-06-05T01:26:36Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -710,20 +728,8 @@ private static DimensionsSpec createDimensionsSpec(List<Pair<QueryableIndex, Dat\n     // Dimensions are extracted from the recent segments to olders because recent segments are likely to be queried more\n     // frequently, and thus the performance should be optimized for recent ones rather than old ones.\n \n-    // timelineSegments are sorted in order of interval, but we do a sanity check here.\n-    final Comparator<Interval> intervalComparator = Comparators.intervalsByStartThenEnd();\n-    for (int i = 0; i < queryableIndices.size() - 1; i++) {\n-      final Interval shouldBeSmaller = queryableIndices.get(i).lhs.getDataInterval();\n-      final Interval shouldBeLarger = queryableIndices.get(i + 1).lhs.getDataInterval();\n-      Preconditions.checkState(\n-          intervalComparator.compare(shouldBeSmaller, shouldBeLarger) <= 0,\n-          \"QueryableIndexes are not sorted! Interval[%s] of segment[%s] is laster than interval[%s] of segment[%s]\",\n-          shouldBeSmaller,\n-          queryableIndices.get(i).rhs.getId(),\n-          shouldBeLarger,\n-          queryableIndices.get(i + 1).rhs.getId()\n-      );\n-    }\n+    // sort timelineSegments in order of interval.\n+    queryableIndices.sort((o1, o2) -> Comparators.intervalsByStartThenEnd().compare(o1.rhs.getInterval(), o2.rhs.getInterval()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "052e579e75df78a65c5eb39412f65f18102895a9"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTczNTY1MQ==", "bodyText": "Hi @jihoonson\nHere I intended to fix the error in step 5 as described here.\nWhen compacting the two segments\n\ninline_data_2020-05-18T00:00:00.000Z_2020-05-19T00:00:00.000Z_version_1\ninline_data_2020-05-18T20:00:00.000Z_2020-05-18T21:00:00.000Z_version_2\n\nthere will be three timelineObjectHolders\n\n2020-05-18T00:00:00.000Z/2020-05-18T20:00:00.000Z backed by segment inline_data_2020-05-18T00:00:00.000Z_2020-05-19T00:00:00.000Z_version_1\n2020-05-18T20:00:00.000Z/2020-05-18T21:00:00.000Z backed by segment inline_data_2020-05-18T20:00:00.000Z_2020-05-18T21:00:00.000Z_version_2\n2020-05-18T21:00:00.000Z/2020-05-19T00:00:00.000Z backed by segment inline_data_2020-05-18T00:00:00.000Z_2020-05-19T00:00:00.000Z_version_1\n\nAnd the last two pairs of queryableIndices will fail to pass the previous sanity check. Also I found that now timeline already has unit tests about sorting timelineObjectHolders and the sanity check will pass in most case. So I decided to fix the error by re-sorting them anyway. And I added a link in the comment. Do it make sense?", "url": "https://github.com/apache/druid/pull/9905#discussion_r435735651", "createdAt": "2020-06-05T07:23:15Z", "author": {"login": "yuanlihan"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -710,20 +728,8 @@ private static DimensionsSpec createDimensionsSpec(List<Pair<QueryableIndex, Dat\n     // Dimensions are extracted from the recent segments to olders because recent segments are likely to be queried more\n     // frequently, and thus the performance should be optimized for recent ones rather than old ones.\n \n-    // timelineSegments are sorted in order of interval, but we do a sanity check here.\n-    final Comparator<Interval> intervalComparator = Comparators.intervalsByStartThenEnd();\n-    for (int i = 0; i < queryableIndices.size() - 1; i++) {\n-      final Interval shouldBeSmaller = queryableIndices.get(i).lhs.getDataInterval();\n-      final Interval shouldBeLarger = queryableIndices.get(i + 1).lhs.getDataInterval();\n-      Preconditions.checkState(\n-          intervalComparator.compare(shouldBeSmaller, shouldBeLarger) <= 0,\n-          \"QueryableIndexes are not sorted! Interval[%s] of segment[%s] is laster than interval[%s] of segment[%s]\",\n-          shouldBeSmaller,\n-          queryableIndices.get(i).rhs.getId(),\n-          shouldBeLarger,\n-          queryableIndices.get(i + 1).rhs.getId()\n-      );\n-    }\n+    // sort timelineSegments in order of interval.\n+    queryableIndices.sort((o1, o2) -> Comparators.intervalsByStartThenEnd().compare(o1.rhs.getInterval(), o2.rhs.getInterval()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU4Ng=="}, "originalCommit": {"oid": "052e579e75df78a65c5eb39412f65f18102895a9"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA3NTc5Nw==", "bodyText": "@yuanlihan thanks for the explanation! What this method does is finding the most reasonable order of dimensions when input segments have mixed orders. The current algorithm is assuming that more recent segments will likely have the order what you want more (there would be some reason if you have changed the dimension order at some point). Maybe a potential improvement can be considering the segment version as well if some segment intervals are overlapped, but I think this could be done in a separate PR.\n\nAlso I found that now timeline already has unit tests about sorting timelineObjectHolders and the sanity check will pass in most case.\n\nBTW, out of curiosity, can you point me out where this test is?", "url": "https://github.com/apache/druid/pull/9905#discussion_r436075797", "createdAt": "2020-06-05T17:55:57Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -710,20 +728,8 @@ private static DimensionsSpec createDimensionsSpec(List<Pair<QueryableIndex, Dat\n     // Dimensions are extracted from the recent segments to olders because recent segments are likely to be queried more\n     // frequently, and thus the performance should be optimized for recent ones rather than old ones.\n \n-    // timelineSegments are sorted in order of interval, but we do a sanity check here.\n-    final Comparator<Interval> intervalComparator = Comparators.intervalsByStartThenEnd();\n-    for (int i = 0; i < queryableIndices.size() - 1; i++) {\n-      final Interval shouldBeSmaller = queryableIndices.get(i).lhs.getDataInterval();\n-      final Interval shouldBeLarger = queryableIndices.get(i + 1).lhs.getDataInterval();\n-      Preconditions.checkState(\n-          intervalComparator.compare(shouldBeSmaller, shouldBeLarger) <= 0,\n-          \"QueryableIndexes are not sorted! Interval[%s] of segment[%s] is laster than interval[%s] of segment[%s]\",\n-          shouldBeSmaller,\n-          queryableIndices.get(i).rhs.getId(),\n-          shouldBeLarger,\n-          queryableIndices.get(i + 1).rhs.getId()\n-      );\n-    }\n+    // sort timelineSegments in order of interval.\n+    queryableIndices.sort((o1, o2) -> Comparators.intervalsByStartThenEnd().compare(o1.rhs.getInterval(), o2.rhs.getInterval()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU4Ng=="}, "originalCommit": {"oid": "052e579e75df78a65c5eb39412f65f18102895a9"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI0OTE3NQ==", "bodyText": "Hi @jihoonson\n\nBTW, out of curiosity, can you point me out where this test is?\n\nI found that the expected timelines in test cases of VersionedIntervalTimelineTest are all intentionally provided in well-ordered. Maybe I should said that there are implicit tests that cover sorting timelineObjectHolders.", "url": "https://github.com/apache/druid/pull/9905#discussion_r436249175", "createdAt": "2020-06-06T08:03:10Z", "author": {"login": "yuanlihan"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -710,20 +728,8 @@ private static DimensionsSpec createDimensionsSpec(List<Pair<QueryableIndex, Dat\n     // Dimensions are extracted from the recent segments to olders because recent segments are likely to be queried more\n     // frequently, and thus the performance should be optimized for recent ones rather than old ones.\n \n-    // timelineSegments are sorted in order of interval, but we do a sanity check here.\n-    final Comparator<Interval> intervalComparator = Comparators.intervalsByStartThenEnd();\n-    for (int i = 0; i < queryableIndices.size() - 1; i++) {\n-      final Interval shouldBeSmaller = queryableIndices.get(i).lhs.getDataInterval();\n-      final Interval shouldBeLarger = queryableIndices.get(i + 1).lhs.getDataInterval();\n-      Preconditions.checkState(\n-          intervalComparator.compare(shouldBeSmaller, shouldBeLarger) <= 0,\n-          \"QueryableIndexes are not sorted! Interval[%s] of segment[%s] is laster than interval[%s] of segment[%s]\",\n-          shouldBeSmaller,\n-          queryableIndices.get(i).rhs.getId(),\n-          shouldBeLarger,\n-          queryableIndices.get(i + 1).rhs.getId()\n-      );\n-    }\n+    // sort timelineSegments in order of interval.\n+    queryableIndices.sort((o1, o2) -> Comparators.intervalsByStartThenEnd().compare(o1.rhs.getInterval(), o2.rhs.getInterval()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU4Ng=="}, "originalCommit": {"oid": "052e579e75df78a65c5eb39412f65f18102895a9"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzU5Mzc1OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwMTozMDozNFrOGfdgyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwNzoyMDowMFrOGfjGsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MjU3MQ==", "bodyText": "Could you use a more intuitive name such as numUmbrellaIntervals?", "url": "https://github.com/apache/druid/pull/9905#discussion_r435642571", "createdAt": "2020-06-05T01:30:34Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java", "diffHunk": "@@ -191,28 +194,33 @@ public static void setupClass()\n     MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-05-01/2017-06-01\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n     MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-01/2017-07-01\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n \n+    MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-01/2017-06-02\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n+    MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-15/2017-06-16\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n+    MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-30/2017-07-01\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n+\n     DIMENSIONS = new HashMap<>();\n     AGGREGATORS = new ArrayList<>();\n \n     DIMENSIONS.put(ColumnHolder.TIME_COLUMN_NAME, new LongDimensionSchema(ColumnHolder.TIME_COLUMN_NAME));\n     DIMENSIONS.put(TIMESTAMP_COLUMN, new LongDimensionSchema(TIMESTAMP_COLUMN));\n-    for (int i = 0; i < SEGMENT_INTERVALS.size(); i++) {\n+    int num = 6;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "052e579e75df78a65c5eb39412f65f18102895a9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTczNDE5Mg==", "bodyText": "Naming updated", "url": "https://github.com/apache/druid/pull/9905#discussion_r435734192", "createdAt": "2020-06-05T07:20:00Z", "author": {"login": "yuanlihan"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java", "diffHunk": "@@ -191,28 +194,33 @@ public static void setupClass()\n     MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-05-01/2017-06-01\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n     MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-01/2017-07-01\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n \n+    MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-01/2017-06-02\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n+    MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-15/2017-06-16\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n+    MIXED_TYPE_COLUMN_MAP.put(Intervals.of(\"2017-06-30/2017-07-01\"), new DoubleDimensionSchema(MIXED_TYPE_COLUMN));\n+\n     DIMENSIONS = new HashMap<>();\n     AGGREGATORS = new ArrayList<>();\n \n     DIMENSIONS.put(ColumnHolder.TIME_COLUMN_NAME, new LongDimensionSchema(ColumnHolder.TIME_COLUMN_NAME));\n     DIMENSIONS.put(TIMESTAMP_COLUMN, new LongDimensionSchema(TIMESTAMP_COLUMN));\n-    for (int i = 0; i < SEGMENT_INTERVALS.size(); i++) {\n+    int num = 6;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MjU3MQ=="}, "originalCommit": {"oid": "052e579e75df78a65c5eb39412f65f18102895a9"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNjMwMjk2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNzo1ODo1N1rOGf4Dug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNlQwODowNzozOVrOGgCjaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA3NzQ5OA==", "bodyText": "Oops, sorry I missed the CI failure.\n[ERROR] org.apache.druid.indexing.common.task.CompactionTask.createIngestionSchema(TaskToolbox, CompactionTask$SegmentProvider, CompactionTask$PartitionConfigurationManager, DimensionsSpec, AggregatorFactory[], Granularity, ObjectMapper, CoordinatorClient, SegmentLoaderFactory, RetryPolicyFactory) makes inefficient use of keySet iterator instead of entrySet iterator [org.apache.druid.indexing.common.task.CompactionTask] At CompactionTask.java:[line 531] WMI_WRONG_MAP_ITERATOR\n\nThis means you are calling HashMap.get() even though you can iterate the Entry set. Would you fix this please?", "url": "https://github.com/apache/druid/pull/9905#discussion_r436077498", "createdAt": "2020-06-05T17:58:57Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -524,10 +524,28 @@ private String createIndexTaskSpecId(int i)\n                                  .add(p)\n       );\n \n-      final List<ParallelIndexIngestionSpec> specs = new ArrayList<>(intervalToSegments.size());\n-      for (Entry<Interval, List<Pair<QueryableIndex, DataSegment>>> entry : intervalToSegments.entrySet()) {\n-        final Interval interval = entry.getKey();\n-        final List<Pair<QueryableIndex, DataSegment>> segmentsToCompact = entry.getValue();\n+      // unify overlapping intervals to ensure overlapping segments compacting in the same indexSpec\n+      List<Pair<Interval, List<Pair<QueryableIndex, DataSegment>>>> intervalToSegmentsUnified = new ArrayList<>();\n+      Iterator<Interval> iterator = intervalToSegments.keySet().iterator();\n+      Interval union = iterator.next();\n+      List<Pair<QueryableIndex, DataSegment>> segments = new ArrayList<>(intervalToSegments.get(union));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce51e7d25119f2f8fcbc1ea5936ff410c4e5e8d0"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI0OTQ1MQ==", "bodyText": "fixed CI failure.", "url": "https://github.com/apache/druid/pull/9905#discussion_r436249451", "createdAt": "2020-06-06T08:07:39Z", "author": {"login": "yuanlihan"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/CompactionTask.java", "diffHunk": "@@ -524,10 +524,28 @@ private String createIndexTaskSpecId(int i)\n                                  .add(p)\n       );\n \n-      final List<ParallelIndexIngestionSpec> specs = new ArrayList<>(intervalToSegments.size());\n-      for (Entry<Interval, List<Pair<QueryableIndex, DataSegment>>> entry : intervalToSegments.entrySet()) {\n-        final Interval interval = entry.getKey();\n-        final List<Pair<QueryableIndex, DataSegment>> segmentsToCompact = entry.getValue();\n+      // unify overlapping intervals to ensure overlapping segments compacting in the same indexSpec\n+      List<Pair<Interval, List<Pair<QueryableIndex, DataSegment>>>> intervalToSegmentsUnified = new ArrayList<>();\n+      Iterator<Interval> iterator = intervalToSegments.keySet().iterator();\n+      Interval union = iterator.next();\n+      List<Pair<QueryableIndex, DataSegment>> segments = new ArrayList<>(intervalToSegments.get(union));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA3NzQ5OA=="}, "originalCommit": {"oid": "ce51e7d25119f2f8fcbc1ea5936ff410c4e5e8d0"}, "originalPosition": 33}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2460, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}