{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1MDg2MzM5", "number": 9360, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTozNjo1MVrODhMfWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzo0ODowNlrODiHNlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MTM0MjM0OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTozNjo1MVrOFr05EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTozNjo1MVrOFr05EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTQ5OTY2NA==", "bodyText": "Extra \"and\"\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the whole indexing process. It splits the input data and and issues worker tasks\n          \n          \n            \n            the whole indexing process. It splits the input data and issues worker tasks", "url": "https://github.com/apache/druid/pull/9360#discussion_r381499664", "createdAt": "2020-02-19T19:36:51Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MTM4MDgyOnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTo0ODo0M1rOFr1R1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDoxMzo1NVrOFshqHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUwNjAwNg==", "bodyText": "It\u2019s a little unclear to me who's doing what in this. Is the following accurate/clearer?\n\u201cThe index_parallel task is a supervisor task that orchestrates the indexing process. The task splits input data for processing by Overlord worker tasks, which process the input splits assigned to them and create segments from the input. Once a worker task successfully processes all assigned input splits, it reports the generated segment list to the supervisor task.\u201d\nIf not, for a lighter edit, maybe just clarify that it's the worker tasks more specifically, rather than the overlord, that is processing input splits (if that's the case).", "url": "https://github.com/apache/druid/pull/9360#discussion_r381506006", "createdAt": "2020-02-19T19:48:43Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks\n+to the Overlord which actually process the assigned input split and create segments.\n+Once a worker task successfully processes all assigned input split, it reports the generated segment list to the supervisor task. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIzMzExOQ==", "bodyText": "Thanks for taking a look!\n\nIf not, for a lighter edit, maybe just clarify that it's the worker tasks more specifically, rather than the overlord, that is processing input splits (if that's the case).\n\nThis is correct. I tried to make it more clear.\nThe Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid\u2019s resource and\ndoesn\u2019t depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task that orchestrates\nthe whole indexing process. The supervisor task splits the input data and creates worker tasks to process those splits.\nThe created worker tasks are issued to the Overlord so that they can be scheduled and run on MiddleManagers or Indexers.\nOnce a worker task successfully processes the assigned input split, it reports the generated segment list to the supervisor task.\nThe supervisor task periodically checks the status of worker tasks. If one of them fails, it retries the failed task\nuntil the number of retries reaches the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalizes ingestion.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382233119", "createdAt": "2020-02-20T20:13:55Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks\n+to the Overlord which actually process the assigned input split and create segments.\n+Once a worker task successfully processes all assigned input split, it reports the generated segment list to the supervisor task. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUwNjAwNg=="}, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MTM5NDY0OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTo1MjozOVrOFr1aLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTo1MjozOVrOFr1aLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUwODE0MA==", "bodyText": "Light line edit..\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            until the number of retries reaches to the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalize the ingestion.\n          \n          \n            \n            until the number of retries reaches the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalizes ingestion.", "url": "https://github.com/apache/druid/pull/9360#discussion_r381508140", "createdAt": "2020-02-19T19:52:39Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -42,11 +42,12 @@ demonstrates the \"simple\" (single-task) mode.\n ## Parallel task\n \n The Parallel task (type `index_parallel`) is a task for parallel batch indexing. This task only uses Druid's resource and\n-doesn't depend on other external systems like Hadoop. `index_parallel` task is a supervisor task which basically creates\n-multiple worker tasks and submits them to the Overlord. Each worker task reads input data and creates segments. Once they\n-successfully generate segments for all input data, they report the generated segment list to the supervisor task. \n+doesn't depend on other external systems like Hadoop. The `index_parallel` task is a supervisor task which orchestrates\n+the whole indexing process. It splits the input data and and issues worker tasks\n+to the Overlord which actually process the assigned input split and create segments.\n+Once a worker task successfully processes all assigned input split, it reports the generated segment list to the supervisor task. \n The supervisor task periodically checks the status of worker tasks. If one of them fails, it retries the failed task\n-until the number of retries reaches to the configured limit. If all worker tasks succeed, then it publishes the reported segments at once and finalize the ingestion.\n+until the number of retries reaches to the configured limit. If all worker tasks succeed, it publishes the reported segments at once and finalize the ingestion.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MTQwOTAwOnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxOTo1NzoxNFrOFr1jWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMDoxMzo1OFrOFshqQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUxMDQ4OA==", "bodyText": "Could this match the wording used below, so:\n\"....in a single task. (Files are never split across tasks.)", "url": "https://github.com/apache/druid/pull/9360#discussion_r381510488", "createdAt": "2020-02-19T19:57:14Z", "author": {"login": "sthetland"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -226,7 +224,14 @@ The tuningConfig is optional and default parameters will be used if no tuningCon\n `SplitHintSpec` is used to give a hint when the supervisor task creates input splits.\n Note that each worker task processes a single input split. You can control the amount of data each worker task will read during the first phase.\n \n-Currently only one splitHintSpec, i.e., `segments`, is available.\n+#### `MaxSizeSplitHintSpec`\n+\n+`MaxSizeSplitHintSpec` is respected by all splittable input sources except for the HTTP input source.\n+\n+|property|description|default|required?|\n+|--------|-----------|-------|---------|\n+|type|This should always be `maxSize`.|none|yes|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (splitting a large file is not supported yet).|500MB|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIzMzE1Mg==", "bodyText": "\ud83d\udc4d I added \"yet\" at the end of the sentence since we may want to split files across tasks in the future.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382233152", "createdAt": "2020-02-20T20:13:58Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -226,7 +224,14 @@ The tuningConfig is optional and default parameters will be used if no tuningCon\n `SplitHintSpec` is used to give a hint when the supervisor task creates input splits.\n Note that each worker task processes a single input split. You can control the amount of data each worker task will read during the first phase.\n \n-Currently only one splitHintSpec, i.e., `segments`, is available.\n+#### `MaxSizeSplitHintSpec`\n+\n+`MaxSizeSplitHintSpec` is respected by all splittable input sources except for the HTTP input source.\n+\n+|property|description|default|required?|\n+|--------|-----------|-------|---------|\n+|type|This should always be `maxSize`.|none|yes|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (splitting a large file is not supported yet).|500MB|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTUxMDQ4OA=="}, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MTkzNDgxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/SpecificFilesLocalInputSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMjo1MjoyOVrOFr6pLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQyMjo1MjoyOVrOFr6pLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU5MzkwMA==", "bodyText": "If it isn't too much trouble, it seems like this would be better to just be a part of LocalInputSource to be more consistent with the cloud file input sources, rather than introducing a new type. Though if it is needlessly complicated then is probably fine as is.", "url": "https://github.com/apache/druid/pull/9360#discussion_r381593900", "createdAt": "2020-02-19T22:52:29Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/SpecificFilesLocalInputSource.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input.impl;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SpecificFilesLocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0210ba15127a05a68e51d2e2c87691f8c7627133"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2MzcwNTIyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQwNzo0NTowNFrOFsI_sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQyMTowODo0OFrOFsjP-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgyOTA0MQ==", "bodyText": "Is this better to accept both baseDir + filter and explicit files list, or should you specify one or the other exclusively?\nIf you think accepting both is better then this exception message should probably say 'At least one of ...' instead of 'Either one of'.", "url": "https://github.com/apache/druid/pull/9360#discussion_r381829041", "createdAt": "2020-02-20T07:45:04Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -34,28 +39,46 @@\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files\n   )\n   {\n-    this.baseDir = Preconditions.checkNotNull(baseDir, \"baseDir\");\n-    this.filter = Preconditions.checkNotNull(filter, \"filter\");\n+    this.baseDir = baseDir;\n+    this.filter = baseDir != null ? Preconditions.checkNotNull(filter, \"filter\") : filter;\n+    this.files = files;\n+\n+    if (baseDir == null && CollectionUtils.isNullOrEmpty(files)) {\n+      throw new IAE(\"Either one of baseDir or files should be specified\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "76fb01c8c51efbd16562e79ff083ead309db0718"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIzMzAxNw==", "bodyText": "Oops, thanks. I'm not sure why we cannot have both at the same time as long as we don't process the same file more than once. It can be more aligned with the cloud input sources though.. (Also, why do we do this?)", "url": "https://github.com/apache/druid/pull/9360#discussion_r382233017", "createdAt": "2020-02-20T20:13:42Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -34,28 +39,46 @@\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files\n   )\n   {\n-    this.baseDir = Preconditions.checkNotNull(baseDir, \"baseDir\");\n-    this.filter = Preconditions.checkNotNull(filter, \"filter\");\n+    this.baseDir = baseDir;\n+    this.filter = baseDir != null ? Preconditions.checkNotNull(filter, \"filter\") : filter;\n+    this.files = files;\n+\n+    if (baseDir == null && CollectionUtils.isNullOrEmpty(files)) {\n+      throw new IAE(\"Either one of baseDir or files should be specified\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgyOTA0MQ=="}, "originalCommit": {"oid": "76fb01c8c51efbd16562e79ff083ead309db0718"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjI1OTE5NA==", "bodyText": "Yeah, I think actually it probably would be better to allow both uris and prefixes in the cloud file input sources and any others that match this pattern, not sure why we do only one or the other currently..", "url": "https://github.com/apache/druid/pull/9360#discussion_r382259194", "createdAt": "2020-02-20T21:08:48Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -34,28 +39,46 @@\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files\n   )\n   {\n-    this.baseDir = Preconditions.checkNotNull(baseDir, \"baseDir\");\n-    this.filter = Preconditions.checkNotNull(filter, \"filter\");\n+    this.baseDir = baseDir;\n+    this.filter = baseDir != null ? Preconditions.checkNotNull(filter, \"filter\") : filter;\n+    this.files = files;\n+\n+    if (baseDir == null && CollectionUtils.isNullOrEmpty(files)) {\n+      throw new IAE(\"Either one of baseDir or files should be specified\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgyOTA0MQ=="}, "originalCommit": {"oid": "76fb01c8c51efbd16562e79ff083ead309db0718"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDI2ODQxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMDowNzo0MlrOFtH-ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MjozOVrOFttvDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg2MDkyMw==", "bodyText": "Looks like the splitSize + size < maxSplitSize and current.isEmpty() block can be combined", "url": "https://github.com/apache/druid/pull/9360#discussion_r382860923", "createdAt": "2020-02-22T00:07:42Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;\n+\n+      @Override\n+      public boolean hasNext()\n+      {\n+        return peeking != null || inputIterator.hasNext();\n+      }\n+\n+      @Override\n+      public List<T> next()\n+      {\n+        final List<T> current = new ArrayList<>();\n+        long splitSize = 0;\n+        while (splitSize < maxSplitSize && (peeking != null || inputIterator.hasNext())) {\n+          if (peeking == null) {\n+            peeking = inputIterator.next();\n+          }\n+          final long size = inputAttributeExtractor.apply(peeking).getSize();\n+          if (current.isEmpty()) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else if (splitSize + size < maxSplitSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcbb345b0cc6fc65b205b8912c675c3467338f51"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTU2Nw==", "bodyText": "Ah good catch. Fixed.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479567", "createdAt": "2020-02-24T19:52:39Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;\n+\n+      @Override\n+      public boolean hasNext()\n+      {\n+        return peeking != null || inputIterator.hasNext();\n+      }\n+\n+      @Override\n+      public List<T> next()\n+      {\n+        final List<T> current = new ArrayList<>();\n+        long splitSize = 0;\n+        while (splitSize < maxSplitSize && (peeking != null || inputIterator.hasNext())) {\n+          if (peeking == null) {\n+            peeking = inputIterator.next();\n+          }\n+          final long size = inputAttributeExtractor.apply(peeking).getSize();\n+          if (current.isEmpty()) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else if (splitSize + size < maxSplitSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg2MDkyMw=="}, "originalCommit": {"oid": "bcbb345b0cc6fc65b205b8912c675c3467338f51"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDMyODI4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMTowNDo0OVrOFtIh0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1Mjo0MlrOFttvJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg2OTk2OA==", "bodyText": "Can you add this new property to the LocalInputSource property docs?", "url": "https://github.com/apache/druid/pull/9360#discussion_r382869968", "createdAt": "2020-02-22T01:04:49Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -21,40 +21,64 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Iterators;\n import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOCase;\n+import org.apache.commons.io.filefilter.AndFileFilter;\n+import org.apache.commons.io.filefilter.IOFileFilter;\n+import org.apache.commons.io.filefilter.NameFileFilter;\n+import org.apache.commons.io.filefilter.NotFileFilter;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.commons.io.filefilter.WildcardFileFilter;\n import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n import org.apache.druid.data.input.InputFormat;\n import org.apache.druid.data.input.InputRowSchema;\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Objects;\n-import java.util.Spliterator;\n-import java.util.Spliterators;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n-import java.util.stream.StreamSupport;\n \n-public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<File>\n+public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTU5MA==", "bodyText": "Oops, added.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479590", "createdAt": "2020-02-24T19:52:42Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -21,40 +21,64 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Iterators;\n import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOCase;\n+import org.apache.commons.io.filefilter.AndFileFilter;\n+import org.apache.commons.io.filefilter.IOFileFilter;\n+import org.apache.commons.io.filefilter.NameFileFilter;\n+import org.apache.commons.io.filefilter.NotFileFilter;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.commons.io.filefilter.WildcardFileFilter;\n import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n import org.apache.druid.data.input.InputFormat;\n import org.apache.druid.data.input.InputRowSchema;\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Objects;\n-import java.util.Spliterator;\n-import java.util.Spliterators;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n-import java.util.stream.StreamSupport;\n \n-public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<File>\n+public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg2OTk2OA=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDMyOTQ5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMTowNjowMVrOFtIicA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1Mjo0NVrOFttvOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDEyOA==", "bodyText": "nit: could call this sourceIterator", "url": "https://github.com/apache/druid/pull/9360#discussion_r382870128", "createdAt": "2020-02-22T01:06:01Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "diffHunk": "@@ -48,23 +48,23 @@\n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      Stream<InputEntity> sourceStream,\n+      Iterator<? extends InputEntity> sourceStream,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTYxMQ==", "bodyText": "Fixed.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479611", "createdAt": "2020-02-24T19:52:45Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "diffHunk": "@@ -48,23 +48,23 @@\n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      Stream<InputEntity> sourceStream,\n+      Iterator<? extends InputEntity> sourceStream,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDEyOA=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDMyOTY1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMTowNjoxN1rOFtIijQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1Mjo0OFrOFttvUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDE1Nw==", "bodyText": "nit: could call this sourceCloseableIterator", "url": "https://github.com/apache/druid/pull/9360#discussion_r382870157", "createdAt": "2020-02-22T01:06:17Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "diffHunk": "@@ -48,23 +48,23 @@\n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      Stream<InputEntity> sourceStream,\n+      Iterator<? extends InputEntity> sourceStream,\n       File temporaryDirectory\n   )\n   {\n-    this(inputRowSchema, inputFormat, CloseableIterators.withEmptyBaggage(sourceStream.iterator()), temporaryDirectory);\n+    this(inputRowSchema, inputFormat, CloseableIterators.withEmptyBaggage(sourceStream), temporaryDirectory);\n   }\n \n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      CloseableIterator<InputEntity> sourceIterator,\n+      CloseableIterator<? extends InputEntity> sourceIterator,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTYzMg==", "bodyText": "Fixed.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479632", "createdAt": "2020-02-24T19:52:48Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/InputEntityIteratingReader.java", "diffHunk": "@@ -48,23 +48,23 @@\n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      Stream<InputEntity> sourceStream,\n+      Iterator<? extends InputEntity> sourceStream,\n       File temporaryDirectory\n   )\n   {\n-    this(inputRowSchema, inputFormat, CloseableIterators.withEmptyBaggage(sourceStream.iterator()), temporaryDirectory);\n+    this(inputRowSchema, inputFormat, CloseableIterators.withEmptyBaggage(sourceStream), temporaryDirectory);\n   }\n \n   public InputEntityIteratingReader(\n       InputRowSchema inputRowSchema,\n       InputFormat inputFormat,\n-      CloseableIterator<InputEntity> sourceIterator,\n+      CloseableIterator<? extends InputEntity> sourceIterator,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDE1Nw=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDMzMzQwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMToxMDoyNVrOFtIkpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1Mjo1MVrOFttvag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDY5NQ==", "bodyText": "Can add a @Nullable here", "url": "https://github.com/apache/druid/pull/9360#discussion_r382870695", "createdAt": "2020-02-22T01:10:25Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -21,40 +21,64 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Iterators;\n import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOCase;\n+import org.apache.commons.io.filefilter.AndFileFilter;\n+import org.apache.commons.io.filefilter.IOFileFilter;\n+import org.apache.commons.io.filefilter.NameFileFilter;\n+import org.apache.commons.io.filefilter.NotFileFilter;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.commons.io.filefilter.WildcardFileFilter;\n import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n import org.apache.druid.data.input.InputFormat;\n import org.apache.druid.data.input.InputRowSchema;\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Objects;\n-import java.util.Spliterator;\n-import java.util.Spliterators;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n-import java.util.stream.StreamSupport;\n \n-public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<File>\n+public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTY1OA==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479658", "createdAt": "2020-02-24T19:52:51Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -21,40 +21,64 @@\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Iterators;\n import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOCase;\n+import org.apache.commons.io.filefilter.AndFileFilter;\n+import org.apache.commons.io.filefilter.IOFileFilter;\n+import org.apache.commons.io.filefilter.NameFileFilter;\n+import org.apache.commons.io.filefilter.NotFileFilter;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.commons.io.filefilter.WildcardFileFilter;\n import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFileAttribute;\n import org.apache.druid.data.input.InputFormat;\n import org.apache.druid.data.input.InputRowSchema;\n import org.apache.druid.data.input.InputSourceReader;\n import org.apache.druid.data.input.InputSplit;\n import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.java.util.common.IAE;\n+import org.apache.druid.utils.CollectionUtils;\n+import org.apache.druid.utils.Streams;\n \n import javax.annotation.Nullable;\n import java.io.File;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Objects;\n-import java.util.Spliterator;\n-import java.util.Spliterators;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n-import java.util.stream.StreamSupport;\n \n-public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<File>\n+public class LocalInputSource extends AbstractInputSource implements SplittableInputSource<List<File>>\n {\n   private final File baseDir;\n   private final String filter;\n+  private final Set<File> files;\n \n   @JsonCreator\n   public LocalInputSource(\n       @JsonProperty(\"baseDir\") File baseDir,\n-      @JsonProperty(\"filter\") String filter\n+      @JsonProperty(\"filter\") String filter,\n+      @JsonProperty(\"files\") Set<File> files", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MDY5NQ=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDM0MDYxOnYy", "diffSide": "RIGHT", "path": "extensions-core/google-extensions/src/main/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMToxODo1N1rOFtIoqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1Mjo1OVrOFttvpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MTcyMg==", "bodyText": "Should this propagate the exception instead? If we get an object with a byte size that can't be stored in a long, something seems very wrong", "url": "https://github.com/apache/druid/pull/9360#discussion_r382871722", "createdAt": "2020-02-22T01:18:57Z", "author": {"login": "jon-wei"}, "path": "extensions-core/google-extensions/src/main/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSource.java", "diffHunk": "@@ -59,23 +65,42 @@ public GoogleCloudStorageInputSource(\n   }\n \n   @Override\n-  protected GoogleCloudStorageEntity createEntity(InputSplit<CloudObjectLocation> split)\n+  protected InputEntity createEntity(CloudObjectLocation location)\n   {\n-    return new GoogleCloudStorageEntity(storage, split.get());\n+    return new GoogleCloudStorageEntity(storage, location);\n   }\n \n   @Override\n-  protected Stream<InputSplit<CloudObjectLocation>> getPrefixesSplitStream()\n+  protected Stream<InputSplit<List<CloudObjectLocation>>> getPrefixesSplitStream(@Nonnull SplitHintSpec splitHintSpec)\n   {\n-    return StreamSupport.stream(storageObjectIterable().spliterator(), false)\n-                        .map(this::byteSourceFromStorageObject)\n-                        .map(InputSplit::new);\n+    final Iterator<List<StorageObject>> splitIterator = splitHintSpec.split(\n+        storageObjectIterable().iterator(),\n+        storageObject -> {\n+          final BigInteger sizeInBigInteger = storageObject.getSize();\n+          long sizeInLong;\n+          if (sizeInBigInteger == null) {\n+            sizeInLong = Long.MAX_VALUE;\n+          } else {\n+            try {\n+              sizeInLong = sizeInBigInteger.longValueExact();\n+            }\n+            catch (ArithmeticException e) {\n+              sizeInLong = Long.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTcxOQ==", "bodyText": "The length of a google storage object is the unsigned long type (https://cloud.google.com/storage/docs/json_api/v1/objects#resource-representations). I think it's better to work instead of failing. Added a warning log about the exception.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479719", "createdAt": "2020-02-24T19:52:59Z", "author": {"login": "jihoonson"}, "path": "extensions-core/google-extensions/src/main/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSource.java", "diffHunk": "@@ -59,23 +65,42 @@ public GoogleCloudStorageInputSource(\n   }\n \n   @Override\n-  protected GoogleCloudStorageEntity createEntity(InputSplit<CloudObjectLocation> split)\n+  protected InputEntity createEntity(CloudObjectLocation location)\n   {\n-    return new GoogleCloudStorageEntity(storage, split.get());\n+    return new GoogleCloudStorageEntity(storage, location);\n   }\n \n   @Override\n-  protected Stream<InputSplit<CloudObjectLocation>> getPrefixesSplitStream()\n+  protected Stream<InputSplit<List<CloudObjectLocation>>> getPrefixesSplitStream(@Nonnull SplitHintSpec splitHintSpec)\n   {\n-    return StreamSupport.stream(storageObjectIterable().spliterator(), false)\n-                        .map(this::byteSourceFromStorageObject)\n-                        .map(InputSplit::new);\n+    final Iterator<List<StorageObject>> splitIterator = splitHintSpec.split(\n+        storageObjectIterable().iterator(),\n+        storageObject -> {\n+          final BigInteger sizeInBigInteger = storageObject.getSize();\n+          long sizeInLong;\n+          if (sizeInBigInteger == null) {\n+            sizeInLong = Long.MAX_VALUE;\n+          } else {\n+            try {\n+              sizeInLong = sizeInBigInteger.longValueExact();\n+            }\n+            catch (ArithmeticException e) {\n+              sizeInLong = Long.MAX_VALUE;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MTcyMg=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDM0NTEwOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQwMToyNTozMFrOFtIrUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzowNVrOFttv4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MjQwMA==", "bodyText": "Since it would get converted into a MaxSizeSplitHintSpec in createSplit, could this create a MaxSizeSplitHintSpec directly? (Does this also mean SegmentsSplitHintSpec is deprecated?)", "url": "https://github.com/apache/druid/pull/9360#discussion_r382872400", "createdAt": "2020-02-22T01:25:30Z", "author": {"login": "jon-wei"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -228,13 +232,15 @@ protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nu\n     // segmentIds is supposed to be specified by the supervisor task during the parallel indexing.\n     // If it's not null, segments are already split by the supervisor task and further split won't happen.\n     if (segmentIds == null) {\n-      return createSplits(\n-          coordinatorClient,\n-          retryPolicyFactory,\n-          dataSource,\n-          interval,\n-          splitHintSpec == null ? new SegmentsSplitHintSpec(null) : splitHintSpec\n-      ).stream();\n+      return Streams.sequentialStreamFrom(\n+          createSplits(\n+              coordinatorClient,\n+              retryPolicyFactory,\n+              dataSource,\n+              interval,\n+              splitHintSpec == null ? new SegmentsSplitHintSpec(null) : splitHintSpec", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTc3Ng==", "bodyText": "Changed to create MaxSizeSplitHintSpec directly.\n\nDoes this also mean SegmentsSplitHintSpec is deprecated?\n\nGood question. MaxSizeSplitHintSpec and SegmentsSplitHintSpec work exactly same for now, but I think SegmentsSplitHintSpec can be further optimized in the future. Added some comment about the future improvement.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479776", "createdAt": "2020-02-24T19:53:05Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/input/DruidInputSource.java", "diffHunk": "@@ -228,13 +232,15 @@ protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nu\n     // segmentIds is supposed to be specified by the supervisor task during the parallel indexing.\n     // If it's not null, segments are already split by the supervisor task and further split won't happen.\n     if (segmentIds == null) {\n-      return createSplits(\n-          coordinatorClient,\n-          retryPolicyFactory,\n-          dataSource,\n-          interval,\n-          splitHintSpec == null ? new SegmentsSplitHintSpec(null) : splitHintSpec\n-      ).stream();\n+      return Streams.sequentialStreamFrom(\n+          createSplits(\n+              coordinatorClient,\n+              retryPolicyFactory,\n+              dataSource,\n+              interval,\n+              splitHintSpec == null ? new SegmentsSplitHintSpec(null) : splitHintSpec", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg3MjQwMA=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDk1MzgxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzoyMzoxOVrOFtNXpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzoxMlrOFttwIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTI4NQ==", "bodyText": "I think you should make spec classes be pure data objects (or beans). Adding methods like split to them makes them complicated and adds logic that makes it hard to version them in the future. We should think of data objects as literals, not as objects with business logic.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949285", "createdAt": "2020-02-22T23:23:19Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTg0Mg==", "bodyText": "Good point. I agree it is a better structure, but the problem is there are too many classes doing this kind of things especially on the ingestion side. I don't think it's possible to apply the suggested design to all classes anytime soon. Also, I think it's better to promote SQL for ingestion as well so that Druid users don't have to worry about the API changes.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479842", "createdAt": "2020-02-24T19:53:12Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTI4NQ=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDk1ODE1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzozMzo1N1rOFtNZvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzoxN1rOFttwTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTgyMQ==", "bodyText": "I think you can simplify the logic of the next method below if you initialize peeking to inputIterator.next(), and only set peeking to null when inputIterator.hasNext() is false. In your next() below, you would just keeping shifting values from inputIterator into current after each iteration as long as there are more inputs.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949821", "createdAt": "2020-02-22T23:33:57Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTg4Nw==", "bodyText": "I don't understand how it works. peeking is to keep the last fetched input from the underlying iterator because it can be returned or not based on the total size of inputs in the current list. If the last fetched input was not added, it should be returned in the following next() call.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479887", "createdAt": "2020-02-24T19:53:17Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTgyMQ=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDk1ODI3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzozNDozNFrOFtNZ0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzoyMVrOFttwbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTg0Mg==", "bodyText": "equals and hashCode need unit tests", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949842", "createdAt": "2020-02-22T23:34:34Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;\n+\n+      @Override\n+      public boolean hasNext()\n+      {\n+        return peeking != null || inputIterator.hasNext();\n+      }\n+\n+      @Override\n+      public List<T> next()\n+      {\n+        final List<T> current = new ArrayList<>();\n+        long splitSize = 0;\n+        while (splitSize < maxSplitSize && (peeking != null || inputIterator.hasNext())) {\n+          if (peeking == null) {\n+            peeking = inputIterator.next();\n+          }\n+          final long size = inputAttributeExtractor.apply(peeking).getSize();\n+          if (current.isEmpty()) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else if (splitSize + size < maxSplitSize) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else {\n+            break;\n+          }\n+        }\n+        assert !current.isEmpty();\n+        return current;\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    MaxSizeSplitHintSpec that = (MaxSizeSplitHintSpec) o;\n+    return maxSplitSize == that.maxSplitSize;\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(maxSplitSize);\n+  }\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTkxNw==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479917", "createdAt": "2020-02-24T19:53:21Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.annotations.VisibleForTesting;\n+\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+/**\n+ * A SplitHintSpec that can create splits of multiple files.\n+ * A split created by this class can have one or more input files.\n+ * If there is only one file in the split, its size can be larger than {@link #maxSplitSize}.\n+ * If there are two or more files in the split, their total size cannot be larger than {@link #maxSplitSize}.\n+ */\n+public class MaxSizeSplitHintSpec implements SplitHintSpec\n+{\n+  public static final String TYPE = \"maxSize\";\n+\n+  @VisibleForTesting\n+  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+\n+  private final long maxSplitSize;\n+\n+  @JsonCreator\n+  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  {\n+    this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+  }\n+\n+  @JsonProperty\n+  public long getMaxSplitSize()\n+  {\n+    return maxSplitSize;\n+  }\n+\n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)\n+  {\n+    return new Iterator<List<T>>()\n+    {\n+      private T peeking;\n+\n+      @Override\n+      public boolean hasNext()\n+      {\n+        return peeking != null || inputIterator.hasNext();\n+      }\n+\n+      @Override\n+      public List<T> next()\n+      {\n+        final List<T> current = new ArrayList<>();\n+        long splitSize = 0;\n+        while (splitSize < maxSplitSize && (peeking != null || inputIterator.hasNext())) {\n+          if (peeking == null) {\n+            peeking = inputIterator.next();\n+          }\n+          final long size = inputAttributeExtractor.apply(peeking).getSize();\n+          if (current.isEmpty()) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else if (splitSize + size < maxSplitSize) {\n+            current.add(peeking);\n+            splitSize += size;\n+            peeking = null;\n+          } else {\n+            break;\n+          }\n+        }\n+        assert !current.isEmpty();\n+        return current;\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    MaxSizeSplitHintSpec that = (MaxSizeSplitHintSpec) o;\n+    return maxSplitSize == that.maxSplitSize;\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(maxSplitSize);\n+  }\n+}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTg0Mg=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDk1OTQzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/SegmentsSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzozNzo0MlrOFtNaXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzoyNFrOFttwlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTk4Mw==", "bodyText": "Seems like this method really doesn't belong here if not all subclasses or implementation need it? Or should this class be abstract instead?", "url": "https://github.com/apache/druid/pull/9360#discussion_r382949983", "createdAt": "2020-02-22T23:37:42Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/SegmentsSplitHintSpec.java", "diffHunk": "@@ -56,6 +57,12 @@ public long getMaxInputSegmentBytesPerTask()\n     return maxInputSegmentBytesPerTask;\n   }\n \n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTk1Ng==", "bodyText": "Added comment about it.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479956", "createdAt": "2020-02-24T19:53:24Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/SegmentsSplitHintSpec.java", "diffHunk": "@@ -56,6 +57,12 @@ public long getMaxInputSegmentBytesPerTask()\n     return maxInputSegmentBytesPerTask;\n   }\n \n+  @Override\n+  public <T> Iterator<List<T>> split(Iterator<T> inputIterator, Function<T, InputFileAttribute> inputAttributeExtractor)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk0OTk4Mw=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDk2MTc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzo0MzozN1rOFtNbfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzoyNlrOFttwpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk1MDI3MA==", "bodyText": "equals and hashCode need unit tests for maintainability.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382950270", "createdAt": "2020-02-22T23:43:37Z", "author": {"login": "jnaous"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -131,14 +197,15 @@ public boolean equals(Object o)\n     if (o == null || getClass() != o.getClass()) {\n       return false;\n     }\n-    LocalInputSource source = (LocalInputSource) o;\n-    return Objects.equals(baseDir, source.baseDir) &&\n-           Objects.equals(filter, source.filter);\n+    LocalInputSource that = (LocalInputSource) o;\n+    return Objects.equals(baseDir, that.baseDir) &&\n+           Objects.equals(filter, that.filter) &&\n+           Objects.equals(files, that.files);\n   }\n \n   @Override\n   public int hashCode()\n   {\n-    return Objects.hash(baseDir, filter);\n+    return Objects.hash(baseDir, filter, files);\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTk3Mg==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479972", "createdAt": "2020-02-24T19:53:26Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/LocalInputSource.java", "diffHunk": "@@ -131,14 +197,15 @@ public boolean equals(Object o)\n     if (o == null || getClass() != o.getClass()) {\n       return false;\n     }\n-    LocalInputSource source = (LocalInputSource) o;\n-    return Objects.equals(baseDir, source.baseDir) &&\n-           Objects.equals(filter, source.filter);\n+    LocalInputSource that = (LocalInputSource) o;\n+    return Objects.equals(baseDir, that.baseDir) &&\n+           Objects.equals(filter, that.filter) &&\n+           Objects.equals(files, that.files);\n   }\n \n   @Override\n   public int hashCode()\n   {\n-    return Objects.hash(baseDir, filter);\n+    return Objects.hash(baseDir, filter, files);\n   }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk1MDI3MA=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 195}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3MDk2MzQxOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/firehose/WindowedSegmentId.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMlQyMzo0ODowNlrOFtNcUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQxOTo1MzoyOVrOFttwtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk1MDQ4MA==", "bodyText": "tests for equals and hashcode please.", "url": "https://github.com/apache/druid/pull/9360#discussion_r382950480", "createdAt": "2020-02-22T23:48:06Z", "author": {"login": "jnaous"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/firehose/WindowedSegmentId.java", "diffHunk": "@@ -56,6 +63,35 @@ public String getSegmentId()\n   @JsonProperty\n   public List<Interval> getIntervals()\n   {\n-    return intervals;\n+    return Collections.unmodifiableList(intervals);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    WindowedSegmentId segmentId1 = (WindowedSegmentId) o;\n+    return Objects.equals(segmentId, segmentId1.segmentId) &&\n+           Objects.equals(intervals, segmentId1.intervals);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(segmentId, intervals);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ3OTk4OA==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/9360#discussion_r383479988", "createdAt": "2020-02-24T19:53:29Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/firehose/WindowedSegmentId.java", "diffHunk": "@@ -56,6 +63,35 @@ public String getSegmentId()\n   @JsonProperty\n   public List<Interval> getIntervals()\n   {\n-    return intervals;\n+    return Collections.unmodifiableList(intervals);\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    WindowedSegmentId segmentId1 = (WindowedSegmentId) o;\n+    return Objects.equals(segmentId, segmentId1.segmentId) &&\n+           Objects.equals(intervals, segmentId1.intervals);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(segmentId, intervals);\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk1MDQ4MA=="}, "originalCommit": {"oid": "689d4671588b8e069cb17e99e1be195ccbb0ef11"}, "originalPosition": 48}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2762, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}