{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQwMzkwODIx", "number": 10082, "title": "Fix RetryQueryRunner to actually do the job", "bodyText": "Description\nRetryQueryRunner is responsible for retrying the query when some segments are missing during the query (it's possible since the coordinator can move segments anytime). However, it currently doesn't work as expected since it checks the missing segments in the response context before issuing the query to the query nodes. This PR fixes this bug and adds a sanity check that makes checking missing segments failed if the broker hasn't gotten all responses from the query nodes yet.\nSome integration tests will be added as a follow-up.\n\nThis PR has:\n\n been self-reviewed.\n\n using the concurrency checklist (Remove this item if the PR doesn't have any relation to concurrency.)\n\n\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths, ensuring the threshold for code coverage is met.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-06-26T05:35:36Z", "url": "https://github.com/apache/druid/pull/10082", "merged": true, "mergeCommit": {"oid": "657f8ee80fa86779cf26a01072814b1530277aa7"}, "closed": true, "closedAt": "2020-07-01T21:02:22Z", "author": {"login": "jihoonson"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcu8PKkAH2gAyNDQwMzkwODIxOjkzMTY0NjY1ZjEzMWY2Y2EzMTUxYjdiMjVlNDllYmNkMmY5MzFlMzc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcwwugKgFqTQ0MTE5MzcwNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "93164665f131f6ca3151b7b25e49ebcd2f931e37", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/93164665f131f6ca3151b7b25e49ebcd2f931e37", "committedDate": "2020-06-26T05:17:28Z", "message": "Fix RetryQueryRunner to actually do the job"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "324ec1c6404f2a5407ab1ba611f086b711936401", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/324ec1c6404f2a5407ab1ba611f086b711936401", "committedDate": "2020-06-26T05:31:01Z", "message": "more javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7c3fb65092a225409809014caca0beda25c3787", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/c7c3fb65092a225409809014caca0beda25c3787", "committedDate": "2020-06-26T06:33:44Z", "message": "fix test and checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/a4326807fecbf4e64ed28318a7c4e26a1a152c71", "committedDate": "2020-06-26T16:48:48Z", "message": "don't combine for testing"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NDcwMTU2", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-438470156", "createdAt": "2020-06-26T17:25:09Z", "commit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNzoyNToxMFrOGpo49Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODoyMTozN1rOGpqgiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMxNDc0MQ==", "bodyText": "This should be added to DirectDruidClient.removeMagicResponseContextFields or something similar, so it doesn't end up in the response to the user.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446314741", "createdAt": "2020-06-26T17:25:10Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/query/context/ResponseContext.java", "diffHunk": "@@ -112,6 +112,16 @@\n         \"uncoveredIntervalsOverflowed\",\n             (oldValue, newValue) -> (boolean) oldValue || (boolean) newValue\n     ),\n+    /**\n+     * Expected remaining number of responses from query nodes.\n+     * The value is initialized in {@code CachingClusteredClient} when it initializes the connection to the query nodes,\n+     * and is updated whenever they respond (@code DirectDruidClient). {@code RetryQueryRunner} uses this value to\n+     * check if the {@link #MISSING_SEGMENTS} is valid.\n+     */\n+    REMAINING_RESPONSES_FROM_QUERY_NODES(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyMzI2Ng==", "bodyText": "I'm not sure this will work properly if there are multiple CCC queries associated with a given Druid query. This can happen with union queries and with subqueries. I believe they share response contexts, and this put would potentially cause things to get clobbered. Could you please look into this?\nBtw, the reason I believe they share contexts is that I only currently see concurrent contexts getting created in QueryLifecycle (which can wrap multiple CCC queries).\nIf it is a real issue, we might be able to address it by splitting out the number of servers remaining by subquery ID. However, in that case, we need to make sure subquery ID is set for union datasources. I don't think it is currently.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446323266", "createdAt": "2020-06-26T17:43:05Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -618,6 +619,7 @@ private void addSequencesFromServer(\n         final SortedMap<DruidServer, List<SegmentDescriptor>> segmentsByServer\n     )\n     {\n+      responseContext.put(Key.REMAINING_RESPONSES_FROM_QUERY_NODES, segmentsByServer.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMyNTAxOA==", "bodyText": "Is this change related?", "url": "https://github.com/apache/druid/pull/10082#discussion_r446325018", "createdAt": "2020-06-26T17:46:42Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -766,7 +766,6 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n                    .withDruidCluster(cluster)\n                    .withLoadManagementPeons(loadManagementPeons)\n                    .withSegmentReplicantLookup(segmentReplicantLookup)\n-                   .withBalancerReferenceTimestamp(DateTimes.nowUtc())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNDM4NQ==", "bodyText": "Is there a guarantee that there will be 1 retry? Why won't the the correct, new server for the segment be selected the first time?", "url": "https://github.com/apache/druid/pull/10082#discussion_r446334385", "createdAt": "2020-06-26T18:06:43Z", "author": {"login": "gianm"}, "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNTQ2Mg==", "bodyText": "Should be ++retryCount, right? (You would want the first retry to say \"Retry attempt [1]\".)\nHowever, prefix and postfix incrementing in log messages can make things harder to read, so IMO it'd be better to split this up into two separate lines.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446335462", "createdAt": "2020-06-26T18:09:08Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  private int numTotalRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @VisibleForTesting\n+  int getNumTotalRetries()\n+  {\n+    return numTotalRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    final Sequence<T> baseSequence = baseRunner.run(queryPlus, context);\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseSequence);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                numTotalRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        (Integer) context.get(Key.REMAINING_RESPONSES_FROM_QUERY_NODES),\n+        \"%s in responseContext\",\n+        Key.REMAINING_RESPONSES_FROM_QUERY_NODES.getName()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);\n+    }\n+\n+    final Object maybeMissingSegments = context.get(ResponseContext.Key.MISSING_SEGMENTS);\n+    if (maybeMissingSegments == null) {\n+      return Collections.emptyList();\n+    }\n+\n+    return jsonMapper.convertValue(\n+        maybeMissingSegments,\n+        new TypeReference<List<SegmentDescriptor>>()\n+        {\n+        }\n+    );\n+  }\n+\n+  /**\n+   * A lazy iterator populating {@link Sequence} by retrying the query. The first returned sequence is always the base\n+   * sequence given in the constructor. Subsequent sequences are created dynamically whenever it retries the query. All\n+   * the sequences populated by this iterator will be merged (not combined) with the base sequence.\n+   *\n+   * The design of this iterator depends on how {@link MergeSequence} works; the MergeSequence pops an item from\n+   * each underlying sequence and pushes them to a {@link java.util.PriorityQueue}. Whenever it pops from the queue,\n+   * it pushes a new item from the sequence where the returned item was originally from. Since the first returned\n+   * sequence from this iterator is always the base sequence, the MergeSequence will call {@link Sequence#toYielder}\n+   * on the base sequence first which in turn initializing query distribution tree. Once this tree is built, the query\n+   * nodes (historicals and realtime tasks) will lock all segments to read and report missing segments to the broker.\n+   * If there are missing segments reported, this iterator will rewrite the query with those reported segments and\n+   * reissue the rewritten query.\n+   *\n+   * @see org.apache.druid.client.CachingClusteredClient\n+   * @see org.apache.druid.client.DirectDruidClient\n+   */\n+  private class RetryingSequenceIterator implements Iterator<Sequence<T>>\n+  {\n+    private final QueryPlus<T> queryPlus;\n+    private final ResponseContext context;\n+    private Sequence<T> sequence;\n+    private int retryCount = 0;\n+\n+    private RetryingSequenceIterator(QueryPlus<T> queryPlus, ResponseContext context, Sequence<T> baseSequence)\n+    {\n+      this.queryPlus = queryPlus;\n+      this.context = context;\n+      this.sequence = baseSequence;\n+    }\n+\n+    @Override\n+    public boolean hasNext()\n+    {\n+      if (sequence != null) {\n+        return true;\n+      } else {\n+        final List<SegmentDescriptor> missingSegments = getMissingSegments(context);\n+        if (missingSegments.isEmpty()) {\n+          return false;\n+        } else if (retryCount >= config.getNumTries()) {\n+          if (!config.isReturnPartialResults()) {\n+            throw new SegmentMissingException(\"No results found for segments[%s]\", missingSegments);\n+          } else {\n+            return false;\n+          }\n+        } else {\n+          LOG.info(\"[%,d] missing segments found. Retry attempt [%,d]\", missingSegments.size(), retryCount++);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNTgwMg==", "bodyText": "Should be getTotalNumRetries(). \"Num total retries\" makes it sound like the number of times we did a total retry.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446335802", "createdAt": "2020-06-26T18:09:56Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  private int numTotalRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @VisibleForTesting\n+  int getNumTotalRetries()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzNzI1Ng==", "bodyText": "Could you add some javadocs to these methods explaining what they do?\nIt's non-obvious that dropSegmentFromServer doesn't modify the server view, and that dropSegmentFromServerAndAddNewServerForSegment does modify the server view, but only for the new server, not the old one.\nAnywhere there is a lack of symmetry and obviousness like this, doc comments are especially important.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446337256", "createdAt": "2020-06-26T18:12:46Z", "author": {"login": "gianm"}, "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    // Note that we dropped a segment from a server, but it's still announced in the server view.\n+    // As a result, we may get the full result or not depending on what server will get the retry query.\n+    // If we hit the same server, the query will return incomplete result.\n+    Assert.assertTrue(queryResult.size() > 8);\n+    Assert.assertEquals(expectedTimeseriesResult(queryResult.size()), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryUntilWeGetFullResult()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(100, false), // retry up to 100\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertTrue(0 < queryRunner.getNumTotalRetries());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testFailWithPartialResultsAfterRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    dropSegmentFromServer(servers.get(0));\n+\n+    expectedException.expect(SegmentMissingException.class);\n+    expectedException.expectMessage(\"No results found for segments\");\n+    try {\n+      sequence.toList();\n+    }\n+    finally {\n+      Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    }\n+  }\n+\n+  private void prepareCluster(int numServers)\n+  {\n+    for (int i = 0; i < numServers; i++) {\n+      final DataSegment segment = newSegment(SCHEMA_INFO.getDataInterval(), i);\n+      addServer(\n+          SimpleServerView.createServer(i + 1),\n+          segment,\n+          segmentGenerator.generate(segment, SCHEMA_INFO, Granularities.NONE, 10)\n+      );\n+    }\n+  }\n+\n+  private Pair<SegmentId, QueryableIndex> dropSegmentFromServer(DruidServer fromServer)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDY1NA==", "bodyText": "Is there a guarantee that there will be more than zero retries? Why won't the the correct, new server for the segment be selected the first time?", "url": "https://github.com/apache/druid/pull/10082#discussion_r446340654", "createdAt": "2020-06-26T18:20:07Z", "author": {"login": "gianm"}, "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    // Note that we dropped a segment from a server, but it's still announced in the server view.\n+    // As a result, we may get the full result or not depending on what server will get the retry query.\n+    // If we hit the same server, the query will return incomplete result.\n+    Assert.assertTrue(queryResult.size() > 8);\n+    Assert.assertEquals(expectedTimeseriesResult(queryResult.size()), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryUntilWeGetFullResult()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(100, false), // retry up to 100\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertTrue(0 < queryRunner.getNumTotalRetries());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MTAzNg==", "bodyText": "Could we use exact sizes here? (If we can't be sure what we'll get, maybe check that there's one of two sizes, and run the test repeatedly for a minimum number of times and verify that we actually get both.)", "url": "https://github.com/apache/druid/pull/10082#discussion_r446341036", "createdAt": "2020-06-26T18:21:04Z", "author": {"login": "gianm"}, "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest\n+{\n+  private static final Closer CLOSER = Closer.create();\n+  private static final String DATASOURCE = \"datasource\";\n+  private static final GeneratorSchemaInfo SCHEMA_INFO = GeneratorBasicSchemas.SCHEMA_MAP.get(\"basic\");\n+  private static final boolean USE_PARALLEL_MERGE_POOL_CONFIGURED = false;\n+\n+  @Rule\n+  public ExpectedException expectedException = ExpectedException.none();\n+\n+  private final ObjectMapper objectMapper = new DefaultObjectMapper();\n+  private final QueryToolChestWarehouse toolChestWarehouse;\n+  private final QueryRunnerFactoryConglomerate conglomerate;\n+\n+  private SegmentGenerator segmentGenerator;\n+  private TestHttpClient httpClient;\n+  private SimpleServerView simpleServerView;\n+  private CachingClusteredClient cachingClusteredClient;\n+  private List<DruidServer> servers;\n+\n+  public RetryQueryRunnerTest()\n+  {\n+    conglomerate = QueryStackTests.createQueryRunnerFactoryConglomerate(CLOSER, USE_PARALLEL_MERGE_POOL_CONFIGURED);\n+\n+    toolChestWarehouse = new QueryToolChestWarehouse()\n+    {\n+      @Override\n+      public <T, QueryType extends Query<T>> QueryToolChest<T, QueryType> getToolChest(final QueryType query)\n+      {\n+        return conglomerate.findFactory(query).getToolchest();\n+      }\n+    };\n+  }\n+\n+  @AfterClass\n+  public static void tearDownClass() throws IOException\n+  {\n+    CLOSER.close();\n+  }\n+\n+  @Before\n+  public void setup()\n+  {\n+    segmentGenerator = new SegmentGenerator();\n+    httpClient = new TestHttpClient(objectMapper);\n+    simpleServerView = new SimpleServerView(toolChestWarehouse, objectMapper, httpClient);\n+    cachingClusteredClient = new CachingClusteredClient(\n+        toolChestWarehouse,\n+        simpleServerView,\n+        MapCache.create(0),\n+        objectMapper,\n+        new ForegroundCachePopulator(objectMapper, new CachePopulatorStats(), 0),\n+        new CacheConfig(),\n+        new DruidHttpClientConfig(),\n+        QueryStackTests.getProcessingConfig(USE_PARALLEL_MERGE_POOL_CONFIGURED),\n+        ForkJoinPool.commonPool(),\n+        QueryStackTests.DEFAULT_NOOP_SCHEDULER\n+    );\n+    servers = new ArrayList<>();\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException\n+  {\n+    segmentGenerator.close();\n+  }\n+\n+  private void addServer(DruidServer server, DataSegment dataSegment, QueryableIndex queryableIndex)\n+  {\n+    servers.add(server);\n+    simpleServerView.addServer(server, dataSegment);\n+    httpClient.addServerAndRunner(server, new SimpleServerManager(conglomerate, dataSegment.getId(), queryableIndex));\n+  }\n+\n+  @Test\n+  public void testNoRetry()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, false),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(0, queryRunner.getNumTotalRetries());\n+    Assert.assertFalse(queryResult.isEmpty());\n+    Assert.assertEquals(expectedTimeseriesResult(10), queryResult);\n+  }\n+\n+  @Test\n+  public void testRetryForMovedSegment()\n+  {\n+    prepareCluster(10);\n+    final TimeseriesQuery query = timeseriesQuery(SCHEMA_INFO.getDataInterval());\n+    final RetryQueryRunner<Result<TimeseriesResultValue>> queryRunner = createQueryRunner(\n+        newRetryQueryRunnerConfig(1, true),\n+        query\n+    );\n+    final Sequence<Result<TimeseriesResultValue>> sequence = queryRunner.run(QueryPlus.wrap(query));\n+\n+    // Let's move a segment\n+    dropSegmentFromServerAndAddNewServerForSegment(servers.get(0));\n+\n+    final List<Result<TimeseriesResultValue>> queryResult = sequence.toList();\n+    Assert.assertEquals(1, queryRunner.getNumTotalRetries());\n+    // Note that we dropped a segment from a server, but it's still announced in the server view.\n+    // As a result, we may get the full result or not depending on what server will get the retry query.\n+    // If we hit the same server, the query will return incomplete result.\n+    Assert.assertTrue(queryResult.size() > 8);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MTI1OQ==", "bodyText": "A general comment on testing: we should have a real integration test for this. It is the kind of thing that is tough to get right in unit tests, as evidence by the fact that we already had an existing RetryQueryRunnerTest, but it isn't testing the right thing.\nI reviewed the unit test and it seems reasonable enough (with some comments), but for this particular functionality we need an integration test that uses the real servers and a real network.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446341259", "createdAt": "2020-06-26T18:21:37Z", "author": {"login": "gianm"}, "path": "server/src/test/java/org/apache/druid/query/RetryQueryRunnerTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.druid.client.CachingClusteredClient;\n+import org.apache.druid.client.DirectDruidClient;\n+import org.apache.druid.client.DruidServer;\n+import org.apache.druid.client.SimpleServerView;\n+import org.apache.druid.client.TestHttpClient;\n+import org.apache.druid.client.TestHttpClient.SimpleServerManager;\n+import org.apache.druid.client.cache.CacheConfig;\n+import org.apache.druid.client.cache.CachePopulatorStats;\n+import org.apache.druid.client.cache.ForegroundCachePopulator;\n+import org.apache.druid.client.cache.MapCache;\n+import org.apache.druid.guice.http.DruidHttpClientConfig;\n+import org.apache.druid.jackson.DefaultObjectMapper;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.Pair;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.query.aggregation.CountAggregatorFactory;\n+import org.apache.druid.query.timeseries.TimeseriesQuery;\n+import org.apache.druid.query.timeseries.TimeseriesResultValue;\n+import org.apache.druid.segment.QueryableIndex;\n+import org.apache.druid.segment.SegmentMissingException;\n+import org.apache.druid.segment.generator.GeneratorBasicSchemas;\n+import org.apache.druid.segment.generator.GeneratorSchemaInfo;\n+import org.apache.druid.segment.generator.SegmentGenerator;\n+import org.apache.druid.server.QueryStackTests;\n+import org.apache.druid.timeline.DataSegment;\n+import org.apache.druid.timeline.SegmentId;\n+import org.apache.druid.timeline.partition.NumberedShardSpec;\n+import org.joda.time.Interval;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public class RetryQueryRunnerTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4326807fecbf4e64ed28318a7c4e26a1a152c71"}, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7b6e8e3da104e493e693c4f8e7f5cda0d015c9d", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/b7b6e8e3da104e493e693c4f8e7f5cda0d015c9d", "committedDate": "2020-06-26T21:39:02Z", "message": "Merge branch 'master' of github.com:apache/druid into fix-retry-query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1f3db30dcac12d035b41d3e3ad89af30cdd413d", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/f1f3db30dcac12d035b41d3e3ad89af30cdd413d", "committedDate": "2020-06-26T23:28:57Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d403b6b9ce0af1087db2c7f6e50fa42ed526b6a1", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/d403b6b9ce0af1087db2c7f6e50fa42ed526b6a1", "committedDate": "2020-06-27T03:35:48Z", "message": "fix unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5182e4744200184a715732843220fa4b04e84dbb", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/5182e4744200184a715732843220fa4b04e84dbb", "committedDate": "2020-06-28T01:44:03Z", "message": "always initialize response context in cachingClusteredClient"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/5a6cf8d198bb91b92d8610514eb606feb5e890c5", "committedDate": "2020-06-28T03:05:46Z", "message": "fix subquery"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NzYyNDIw", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-438762420", "createdAt": "2020-06-28T09:29:20Z", "commit": {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOToyOToyMFrOGp75zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQxMTo1ODoxMFrOGp81Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg==", "bodyText": "Hmm, I feel like this could be done without polluting the production code with test stuff.\nHow about:\n\nmaking RetryingSequenceIterator visible for testing so you can extend and override hasNext to do this runnable thing\nmove building the retry sequence in the run method into a new method that you can override, so to have it make the test iterator with the runnable instead", "url": "https://github.com/apache/druid/pull/10082#discussion_r446626252", "createdAt": "2020-06-28T09:29:20Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final ConcurrentHashMap<String, Integer> idToRemainingResponses =\n+        (ConcurrentHashMap<String, Integer>) Preconditions.checkNotNull(\n+            context.get(Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS),\n+            \"%s in responseContext\",\n+            Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS.getName()\n+        );\n+\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        idToRemainingResponses.get(queryPlus.getQuery().getMostRelevantId()),\n+        \"Number of remaining responses for query[%s]\",\n+        queryPlus.getQuery().getMostRelevantId()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);\n+    }\n+\n+    final Object maybeMissingSegments = context.get(ResponseContext.Key.MISSING_SEGMENTS);\n+    if (maybeMissingSegments == null) {\n+      return Collections.emptyList();\n+    }\n+\n+    return jsonMapper.convertValue(\n+        maybeMissingSegments,\n+        new TypeReference<List<SegmentDescriptor>>()\n+        {\n+        }\n+    );\n+  }\n+\n+  /**\n+   * A lazy iterator populating {@link Sequence} by retrying the query. The first returned sequence is always the base\n+   * sequence from the baseQueryRunner. Subsequent sequences are created dynamically whenever it retries the query. All\n+   * the sequences populated by this iterator will be merged (not combined) with the base sequence.\n+   *\n+   * The design of this iterator depends on how {@link MergeSequence} works; the MergeSequence pops an item from\n+   * each underlying sequence and pushes them to a {@link java.util.PriorityQueue}. Whenever it pops from the queue,\n+   * it pushes a new item from the sequence where the returned item was originally from. Since the first returned\n+   * sequence from this iterator is always the base sequence, the MergeSequence will call {@link Sequence#toYielder}\n+   * on the base sequence first which in turn initializing query distribution tree. Once this tree is built, the query\n+   * servers (historicals and realtime tasks) will lock all segments to read and report missing segments to the broker.\n+   * If there are missing segments reported, this iterator will rewrite the query with those reported segments and\n+   * reissue the rewritten query.\n+   *\n+   * @see org.apache.druid.client.CachingClusteredClient\n+   * @see org.apache.druid.client.DirectDruidClient\n+   */\n+  private class RetryingSequenceIterator implements Iterator<Sequence<T>>\n+  {\n+    private final QueryPlus<T> queryPlus;\n+    private final ResponseContext context;\n+    private final QueryRunner<T> baseQueryRunner;\n+    private final Runnable runnableAfterFirstAttempt;\n+\n+    private boolean first = true;\n+    private Sequence<T> sequence = null;\n+    private int retryCount = 0;\n+\n+    private RetryingSequenceIterator(\n+        QueryPlus<T> queryPlus,\n+        ResponseContext context,\n+        QueryRunner<T> baseQueryRunner,\n+        Runnable runnableAfterFirstAttempt\n+    )\n+    {\n+      this.queryPlus = queryPlus;\n+      this.context = context;\n+      this.baseQueryRunner = baseQueryRunner;\n+      this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+    }\n+\n+    @Override\n+    public boolean hasNext()\n+    {\n+      if (first) {\n+        sequence = baseQueryRunner.run(queryPlus, context);\n+        // runnableAfterFirstAttempt is only for testing, it must be no-op for production code.\n+        runnableAfterFirstAttempt.run();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjk4MQ==", "bodyText": "nit: I think getMostSpecificId() might be a better name for this method\ntangent: it might be nice if the debug logs in DirectDruidClient logged both queryId and subQueryId if set", "url": "https://github.com/apache/druid/pull/10082#discussion_r446626981", "createdAt": "2020-06-28T09:36:02Z", "author": {"login": "clintropolis"}, "path": "processing/src/main/java/org/apache/druid/query/Query.java", "diffHunk": "@@ -154,6 +161,17 @@ default String getSqlQueryId()\n     return null;\n   }\n \n+  /**\n+   * Returns a most relevant ID of this query; if it is a subquery, this will return its subquery ID.\n+   * If it is a regular query without subqueries, this will return its query ID.\n+   * This method should be called after the relevant ID is assigned using {@link #withId} or {@link #withSubQueryId}.\n+   */\n+  default String getMostRelevantId()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY0MTUwNg==", "bodyText": "I'm surprised that the builder doesn't have an option to set queryId.", "url": "https://github.com/apache/druid/pull/10082#discussion_r446641506", "createdAt": "2020-06-28T11:58:10Z", "author": {"login": "clintropolis"}, "path": "server/src/test/java/org/apache/druid/client/CachingClusteredClientTest.java", "diffHunk": "@@ -497,10 +499,11 @@ public void testTimeseriesCaching()\n     );\n \n \n-    TimeseriesQuery query = builder.intervals(\"2011-01-01/2011-01-10\")\n-                                   .aggregators(RENAMED_AGGS)\n-                                   .postAggregators(RENAMED_POST_AGGS)\n-                                   .build();\n+    TimeseriesQuery query = (TimeseriesQuery) builder.intervals(\"2011-01-01/2011-01-10\")\n+                                                     .aggregators(RENAMED_AGGS)\n+                                                     .postAggregators(RENAMED_POST_AGGS)\n+                                                     .build()\n+                                                     .withId(\"queryId\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5"}, "originalPosition": 28}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bcf4a71141184dc7c2333d4e64a44ca2c8f18026", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/bcf4a71141184dc7c2333d4e64a44ca2c8f18026", "committedDate": "2020-06-30T00:11:28Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a96ea8a4f0ec5cacee884e04d236977fc76c52d", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6a96ea8a4f0ec5cacee884e04d236977fc76c52d", "committedDate": "2020-06-30T02:26:31Z", "message": "fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e600e5b81dbb1e4d853d39bf8738363421aa5b99", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/e600e5b81dbb1e4d853d39bf8738363421aa5b99", "committedDate": "2020-06-30T06:38:15Z", "message": "query id for builders"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5ODA1MzIx", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-439805321", "createdAt": "2020-06-30T08:50:26Z", "commit": {"oid": "e600e5b81dbb1e4d853d39bf8738363421aa5b99"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwODo1MDoyN1rOGqyfXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwODo1ODoxNVrOGqyzUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUyMDYwNw==", "bodyText": "I suppose we can agree to disagree; I don't feel super strongly about this particular case since it's not a hot iterator, but I still think the test code in here complicates things overall and makes it harder to casually know what's going on, and wouldn't have done this in this way.", "url": "https://github.com/apache/druid/pull/10082#discussion_r447520607", "createdAt": "2020-06-30T08:50:27Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final ConcurrentHashMap<String, Integer> idToRemainingResponses =\n+        (ConcurrentHashMap<String, Integer>) Preconditions.checkNotNull(\n+            context.get(Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS),\n+            \"%s in responseContext\",\n+            Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS.getName()\n+        );\n+\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        idToRemainingResponses.get(queryPlus.getQuery().getMostRelevantId()),\n+        \"Number of remaining responses for query[%s]\",\n+        queryPlus.getQuery().getMostRelevantId()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);\n+    }\n+\n+    final Object maybeMissingSegments = context.get(ResponseContext.Key.MISSING_SEGMENTS);\n+    if (maybeMissingSegments == null) {\n+      return Collections.emptyList();\n+    }\n+\n+    return jsonMapper.convertValue(\n+        maybeMissingSegments,\n+        new TypeReference<List<SegmentDescriptor>>()\n+        {\n+        }\n+    );\n+  }\n+\n+  /**\n+   * A lazy iterator populating {@link Sequence} by retrying the query. The first returned sequence is always the base\n+   * sequence from the baseQueryRunner. Subsequent sequences are created dynamically whenever it retries the query. All\n+   * the sequences populated by this iterator will be merged (not combined) with the base sequence.\n+   *\n+   * The design of this iterator depends on how {@link MergeSequence} works; the MergeSequence pops an item from\n+   * each underlying sequence and pushes them to a {@link java.util.PriorityQueue}. Whenever it pops from the queue,\n+   * it pushes a new item from the sequence where the returned item was originally from. Since the first returned\n+   * sequence from this iterator is always the base sequence, the MergeSequence will call {@link Sequence#toYielder}\n+   * on the base sequence first which in turn initializing query distribution tree. Once this tree is built, the query\n+   * servers (historicals and realtime tasks) will lock all segments to read and report missing segments to the broker.\n+   * If there are missing segments reported, this iterator will rewrite the query with those reported segments and\n+   * reissue the rewritten query.\n+   *\n+   * @see org.apache.druid.client.CachingClusteredClient\n+   * @see org.apache.druid.client.DirectDruidClient\n+   */\n+  private class RetryingSequenceIterator implements Iterator<Sequence<T>>\n+  {\n+    private final QueryPlus<T> queryPlus;\n+    private final ResponseContext context;\n+    private final QueryRunner<T> baseQueryRunner;\n+    private final Runnable runnableAfterFirstAttempt;\n+\n+    private boolean first = true;\n+    private Sequence<T> sequence = null;\n+    private int retryCount = 0;\n+\n+    private RetryingSequenceIterator(\n+        QueryPlus<T> queryPlus,\n+        ResponseContext context,\n+        QueryRunner<T> baseQueryRunner,\n+        Runnable runnableAfterFirstAttempt\n+    )\n+    {\n+      this.queryPlus = queryPlus;\n+      this.context = context;\n+      this.baseQueryRunner = baseQueryRunner;\n+      this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+    }\n+\n+    @Override\n+    public boolean hasNext()\n+    {\n+      if (first) {\n+        sequence = baseQueryRunner.run(queryPlus, context);\n+        // runnableAfterFirstAttempt is only for testing, it must be no-op for production code.\n+        runnableAfterFirstAttempt.run();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI1Mg=="}, "originalCommit": {"oid": "5a6cf8d198bb91b92d8610514eb606feb5e890c5"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzUyNTcxNA==", "bodyText": "nit: I think a dedicated type would be more appropriate than using a generic Pair, since it makes it a lot easier to follow what is going on than having to keep a mental model of what lhs and rhs are.\nSomething like:\nclass ClusterQueryResults<T>\n{\n    Sequence<T> results;\n    int numServers;\n}\n\nThis would be much easier to understand, and has the added benefit of when we realize we need a 3rd thing we can just add it.", "url": "https://github.com/apache/druid/pull/10082#discussion_r447525714", "createdAt": "2020-06-30T08:58:15Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -283,13 +301,23 @@ public CachingClusteredClient(\n       return contextBuilder.build();\n     }\n \n-    Sequence<T> run(final UnaryOperator<TimelineLookup<String, ServerSelector>> timelineConverter)\n+    /**\n+     * Builds a query distribution and merge plan.\n+     *\n+     * This method returns an empty sequence if the query datasource is unknown or there is matching result-level cache.\n+     * Otherwise, it creates a sequence merging sequences from the regular broker cache and remote servers. If parallel\n+     * merge is enabled, it can merge and *combine* the underlying sequences in parallel.\n+     *\n+     * @return a pair of a sequence merging results from remote query servers and the number of remote servers\n+     *         participating in query processing.\n+     */\n+    NonnullPair<Sequence<T>, Integer> run(final UnaryOperator<TimelineLookup<String, ServerSelector>> timelineConverter)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e600e5b81dbb1e4d853d39bf8738363421aa5b99"}, "originalPosition": 56}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22edff0be8bced4b6baffc152eaa4c82a5719fe7", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/22edff0be8bced4b6baffc152eaa4c82a5719fe7", "committedDate": "2020-06-30T17:30:09Z", "message": "make queryId optional in the builders and ClusterQueryResult"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91b5748a1cfd659d4a08e8ed72cb6b070c134947", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/91b5748a1cfd659d4a08e8ed72cb6b070c134947", "committedDate": "2020-06-30T20:00:15Z", "message": "fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "45db79ff2944d34b13de9eb4ae52f1a16c5efe05", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/45db79ff2944d34b13de9eb4ae52f1a16c5efe05", "committedDate": "2020-06-30T22:34:57Z", "message": "suppress tests and unused methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d27ec05790b47dfde2a05e7adbffe9561007823", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/4d27ec05790b47dfde2a05e7adbffe9561007823", "committedDate": "2020-06-30T22:51:08Z", "message": "exclude groupBy builder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/f1af75fd72a34b509ec8c695a116d5d1cff34c47", "committedDate": "2020-06-30T23:51:49Z", "message": "fix jacoco exclusion"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNDk2NzY1", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-440496765", "createdAt": "2020-07-01T01:55:16Z", "commit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMjowOTozMlrOGrUZ6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNDowMTo1NVrOGrV-6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3NjI2NA==", "bodyText": "we are planning to parallelize running subqueries\n\nI'm not sure if we are, but, we also haven't decided definitely not to.\nI think it would also be okay to not handle parallel queries here, but instead build in a sanity check that verifies the subqueries are issued in series. Maybe by verifying that when the id changes, the previous number of responses remaining must be down to zero. It's up to you. I think your current code is okay too.", "url": "https://github.com/apache/druid/pull/10082#discussion_r448076264", "createdAt": "2020-07-01T02:09:32Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/query/context/ResponseContext.java", "diffHunk": "@@ -112,6 +114,30 @@\n         \"uncoveredIntervalsOverflowed\",\n             (oldValue, newValue) -> (boolean) oldValue || (boolean) newValue\n     ),\n+    /**\n+     * Map of most relevant query ID to remaining number of responses from query nodes.\n+     * The value is initialized in {@code CachingClusteredClient} when it initializes the connection to the query nodes,\n+     * and is updated whenever they respond (@code DirectDruidClient). {@code RetryQueryRunner} uses this value to\n+     * check if the {@link #MISSING_SEGMENTS} is valid.\n+     *\n+     * Currently, the broker doesn't run subqueries in parallel, the remaining number of responses will be updated\n+     * one by one per subquery. However, since we are planning to parallelize running subqueries, we store them", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3ODYxOA==", "bodyText": "Why only log this if the id and subquery id are both nonnull?\nMeaning: why not log it in all cases?\nBtw, since this is going to happen for every server + query pair, it will be very chatty, so I'd suggest trace-level instead of debug.", "url": "https://github.com/apache/druid/pull/10082#discussion_r448078618", "createdAt": "2020-07-01T02:19:07Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/client/DirectDruidClient.java", "diffHunk": "@@ -231,7 +237,19 @@ private InputStream dequeue() throws InterruptedException\n \n           final boolean continueReading;\n           try {\n+            if (query.getId() != null && query.getSubQueryId() != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4MjY5Nw==", "bodyText": "If this happens, is it a bug? Or might this happen for some legitimate reason?\nIf it is a bug: please include a comment that this message means there was a bug. (So people that get the message and search for it in the code will see that it is a sign of a bug.)\nIf there could be a legitimate reason: in this case we should improve the error message to help the user understand what the legitimate reason might be.\n(a nit: spelling: should be \"missing responses\" rather than \"missing responds\".)", "url": "https://github.com/apache/druid/pull/10082#discussion_r448082697", "createdAt": "2020-07-01T02:35:38Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    // Sanity check before retrieving missingSegments from responseContext.\n+    // The missingSegments in the responseContext is only valid when all servers have responded to the broker.\n+    // The remainingResponses must be not null but 0 in the responseContext at this point.\n+    final ConcurrentHashMap<String, Integer> idToRemainingResponses =\n+        (ConcurrentHashMap<String, Integer>) Preconditions.checkNotNull(\n+            context.get(Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS),\n+            \"%s in responseContext\",\n+            Key.REMAINING_RESPONSES_FROM_QUERY_SERVERS.getName()\n+        );\n+\n+    final int remainingResponses = Preconditions.checkNotNull(\n+        idToRemainingResponses.get(queryPlus.getQuery().getMostSpecificId()),\n+        \"Number of remaining responses for query[%s]\",\n+        queryPlus.getQuery().getMostSpecificId()\n+    );\n+    if (remainingResponses > 0) {\n+      throw new ISE(\"Failed to check missing segments due to missing responds from [%d] servers\", remainingResponses);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA4NzA1MQ==", "bodyText": "I'm not sure where to put this comment, but I noticed that the MISSING_SEGMENTS array could get truncated by the historical that generated it; see ResponseContext.serializeWith. It looks like this was discussed in #2331. We can't allow this if we are going to rely on the missing segments list for query correctness. I think that means we need to introduce an option that tells the QueryResource that it should throw an error rather than truncate, and we should always set that option when communicating from the Broker to data servers.", "url": "https://github.com/apache/druid/pull/10082#discussion_r448087051", "createdAt": "2020-07-01T02:54:02Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/query/RetryQueryRunner.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.ISE;\n+import org.apache.druid.java.util.common.guava.BaseSequence;\n+import org.apache.druid.java.util.common.guava.BaseSequence.IteratorMaker;\n+import org.apache.druid.java.util.common.guava.MergeSequence;\n+import org.apache.druid.java.util.common.guava.Sequence;\n+import org.apache.druid.java.util.common.guava.Yielder;\n+import org.apache.druid.java.util.common.guava.YieldingAccumulator;\n+import org.apache.druid.java.util.common.guava.YieldingSequenceBase;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.query.context.ResponseContext;\n+import org.apache.druid.query.context.ResponseContext.Key;\n+import org.apache.druid.segment.SegmentMissingException;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.BiFunction;\n+\n+public class RetryQueryRunner<T> implements QueryRunner<T>\n+{\n+  private static final Logger LOG = new Logger(RetryQueryRunner.class);\n+\n+  private final QueryRunner<T> baseRunner;\n+  private final BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn;\n+  private final RetryQueryRunnerConfig config;\n+  private final ObjectMapper jsonMapper;\n+\n+  /**\n+   * Runnable executed after the broker creates query distribution tree for the first attempt. This is only\n+   * for testing and must not be used in production code.\n+   */\n+  private final Runnable runnableAfterFirstAttempt;\n+\n+  private int totalNumRetries;\n+\n+  public RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper\n+  )\n+  {\n+    this(baseRunner, retryRunnerCreateFn, config, jsonMapper, () -> {});\n+  }\n+\n+  /**\n+   * Constructor only for testing.\n+   */\n+  @VisibleForTesting\n+  RetryQueryRunner(\n+      QueryRunner<T> baseRunner,\n+      BiFunction<Query<T>, List<SegmentDescriptor>, QueryRunner<T>> retryRunnerCreateFn,\n+      RetryQueryRunnerConfig config,\n+      ObjectMapper jsonMapper,\n+      Runnable runnableAfterFirstAttempt\n+  )\n+  {\n+    this.baseRunner = baseRunner;\n+    this.retryRunnerCreateFn = retryRunnerCreateFn;\n+    this.config = config;\n+    this.jsonMapper = jsonMapper;\n+    this.runnableAfterFirstAttempt = runnableAfterFirstAttempt;\n+  }\n+\n+  @VisibleForTesting\n+  int getTotalNumRetries()\n+  {\n+    return totalNumRetries;\n+  }\n+\n+  @Override\n+  public Sequence<T> run(final QueryPlus<T> queryPlus, final ResponseContext context)\n+  {\n+    return new YieldingSequenceBase<T>()\n+    {\n+      @Override\n+      public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n+      {\n+        final Sequence<Sequence<T>> retryingSequence = new BaseSequence<>(\n+            new IteratorMaker<Sequence<T>, RetryingSequenceIterator>()\n+            {\n+              @Override\n+              public RetryingSequenceIterator make()\n+              {\n+                return new RetryingSequenceIterator(queryPlus, context, baseRunner, runnableAfterFirstAttempt);\n+              }\n+\n+              @Override\n+              public void cleanup(RetryingSequenceIterator iterFromMake)\n+              {\n+                totalNumRetries = iterFromMake.retryCount;\n+              }\n+            }\n+        );\n+        return new MergeSequence<>(queryPlus.getQuery().getResultOrdering(), retryingSequence)\n+            .toYielder(initValue, accumulator);\n+      }\n+    };\n+  }\n+\n+  private List<SegmentDescriptor> getMissingSegments(QueryPlus<T> queryPlus, final ResponseContext context)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODEwMjEyMQ==", "bodyText": "Oops, not including a context here was a mistake, thanks for fixing it.\nNow that I think about it, though, making a new context here will mean we aren't going to properly return context from subqueries up to the original caller. This includes not reporting missing segments in the case where RetryQueryRunnerConfig.isReturnPartialResults = true.\nIt would be better to share the context that was created in QueryLifecycle. Is it feasible to do this? Maybe by moving some of this logic to be lazy and happen inside the returned QueryRunner? (It will get a copy of the context.)\nBtw, this sounds like it might be tough to do, so we could also address it with documentation about known limitations. But I think we either need to fix it, or document it.", "url": "https://github.com/apache/druid/pull/10082#discussion_r448102121", "createdAt": "2020-07-01T04:01:55Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/server/ClientQuerySegmentWalker.java", "diffHunk": "@@ -329,15 +329,18 @@ private DataSource inlineIfNecessary(\n         }\n       } else if (canRunQueryUsingLocalWalker(subQuery) || canRunQueryUsingClusterWalker(subQuery)) {\n         // Subquery needs to be inlined. Assign it a subquery id and run it.\n-        final Query subQueryWithId = subQuery.withSubQueryId(UUID.randomUUID().toString());\n+        final Query subQueryWithId = subQuery.withDefaultSubQueryId();\n \n         final Sequence<?> queryResults;\n \n         if (dryRun) {\n           queryResults = Sequences.empty();\n         } else {\n           final QueryRunner subqueryRunner = subQueryWithId.getRunner(this);\n-          queryResults = subqueryRunner.run(QueryPlus.wrap(subQueryWithId));\n+          queryResults = subqueryRunner.run(\n+              QueryPlus.wrap(subQueryWithId),\n+              DirectDruidClient.makeResponseContextForQuery()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1af75fd72a34b509ec8c695a116d5d1cff34c47"}, "originalPosition": 32}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7de425745b16c3d3aa3bfe4a38a46f0697c34f6", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/d7de425745b16c3d3aa3bfe4a38a46f0697c34f6", "committedDate": "2020-07-01T04:17:55Z", "message": "add tests for builders"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9ce3d5045af48deb7c4f46ea2b4c8355ef944eb", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/a9ce3d5045af48deb7c4f46ea2b4c8355ef944eb", "committedDate": "2020-07-01T05:59:47Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/d960cf8e1b7621962b69c907e3436ef61c5d3c94", "committedDate": "2020-07-01T09:14:22Z", "message": "don't truncate"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMDgyMTE1", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-441082115", "createdAt": "2020-07-01T17:53:41Z", "commit": {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo1Mzo0MVrOGrv2dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo1NToxMlrOGrv5vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyNTk0MA==", "bodyText": "This should be added to the documentation (all of these error codes are spelled out in a table in querying/querying.md).", "url": "https://github.com/apache/druid/pull/10082#discussion_r448525940", "createdAt": "2020-07-01T17:53:41Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/query/QueryInterruptedException.java", "diffHunk": "@@ -105,6 +106,8 @@ private static String getErrorCodeFromThrowable(Throwable e)\n       return RESOURCE_LIMIT_EXCEEDED;\n     } else if (e instanceof UnsupportedOperationException) {\n       return UNSUPPORTED_OPERATION;\n+    } else if (e instanceof TruncatedResponseContextException) {\n+      return TRUNCATED_RESPONSE_CONTEXT;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyNjc4Mw==", "bodyText": "Should be withFailOnTruncatedResponseContext, not setFailOnTruncatedResponseContext, because nothing's being set (a modified copy is being returned).", "url": "https://github.com/apache/druid/pull/10082#discussion_r448526783", "createdAt": "2020-07-01T17:55:12Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/query/QueryContexts.java", "diffHunk": "@@ -344,6 +346,19 @@ public String toString()\n     return defaultTimeout;\n   }\n \n+  public static <T> Query<T> setFailOnTruncatedResponseContext(Query<T> query)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMTY2NzE2", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-441166716", "createdAt": "2020-07-01T20:13:38Z", "commit": {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMTkzNzA0", "url": "https://github.com/apache/druid/pull/10082#pullrequestreview-441193704", "createdAt": "2020-07-01T21:00:41Z", "commit": {"oid": "d960cf8e1b7621962b69c907e3436ef61c5d3c94"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2162, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}