{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4MjU2Nzc5", "number": 10288, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQwMDowODowOFrOEleMGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzoyNlrOEm9hWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NzI3Mzg1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQwMDowODowOFrOHU-Jew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwMjowMjowMVrOHVoEzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MTgwMw==", "bodyText": "it might be worth mentioning why this class should only be used for ingestion so it isn't left as an exercise for the reader", "url": "https://github.com/apache/druid/pull/10288#discussion_r491751803", "createdAt": "2020-09-21T00:08:08Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.Rows;\n+\n+import javax.annotation.Nullable;\n+import java.util.List;\n+\n+/**\n+ * This class is used for hash partitioning during ingestion. The {@link ShardSpecLookup} returned from\n+ * {@link #createHashLookup} is used to determine what hash bucket the given input row will belong to.\n+ *\n+ * Note: this class must be used only for ingestion. For segment pruning at query time,\n+ * {@link HashBasedNumberedShardSpec#partitionFunction} should be used instead.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODczMg==", "bodyText": "Hmm, the reason I added this note was that the partition function could be null before and HashPartitioner uses a default partition function when it's null unlike in query. So, if it was used in a query, then segment pruning could be done based on a wrong default. However, now, the partition function can never be null in ingestion tasks, HashPartitioner doesn't have to be used only in ingestion even though it is for now. I just removed this note.", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438732", "createdAt": "2020-09-22T02:02:01Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.Rows;\n+\n+import javax.annotation.Nullable;\n+import java.util.List;\n+\n+/**\n+ * This class is used for hash partitioning during ingestion. The {@link ShardSpecLookup} returned from\n+ * {@link #createHashLookup} is used to determine what hash bucket the given input row will belong to.\n+ *\n+ * Note: this class must be used only for ingestion. For segment pruning at query time,\n+ * {@link HashBasedNumberedShardSpec#partitionFunction} should be used instead.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MTgwMw=="}, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NzI3NjM3OnYy", "diffSide": "RIGHT", "path": "docs/querying/query-context.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQwMDoxMDo1MFrOHU-Kqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwMjowMjowN1rOHVoE3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjEwNg==", "bodyText": "Since the broker is always pruning on interval, should the docs on this clarify a bit more that this pruning happens within a time chunk, or should this be rebranded as something like secondarySegmentPruning since it maps to the secondary partitioning, or something?\nBasically, I'm wondering if it sort of unclear what happens if this isn't set, if I didn't already know how druid was functioning internally.", "url": "https://github.com/apache/druid/pull/10288#discussion_r491752106", "createdAt": "2020-09-21T00:10:50Z", "author": {"login": "clintropolis"}, "path": "docs/querying/query-context.md", "diffHunk": "@@ -58,6 +58,7 @@ These parameters apply to all query types.\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |useFilterCNF|`false`| If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects.|\n+|segmentPruning|`true`|Enable segment pruning on the Broker. Segment pruning can be applied to only the segments partitioned by hash or range.|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODc0OA==", "bodyText": "Good point. I renamed it as secondaryPartitionPruning since we might want to add tertiary partitioning in the future. Also updated the description.", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438748", "createdAt": "2020-09-22T02:02:07Z", "author": {"login": "jihoonson"}, "path": "docs/querying/query-context.md", "diffHunk": "@@ -58,6 +58,7 @@ These parameters apply to all query types.\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |useFilterCNF|`false`| If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects.|\n+|segmentPruning|`true`|Enable segment pruning on the Broker. Segment pruning can be applied to only the segments partitioned by hash or range.|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjEwNg=="}, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NzI3ODI3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQwMDoxMzowMFrOHU-LlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwMjowMjoxMFrOHVoE7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjM0MQ==", "bodyText": "should we also stress here that anything added here must be backwards compatible, forever?", "url": "https://github.com/apache/druid/pull/10288#discussion_r491752341", "createdAt": "2020-09-21T00:13:00Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonValue;\n+import com.google.common.hash.Hashing;\n+import org.apache.druid.java.util.common.StringUtils;\n+\n+/**\n+ * An enum of supported hash partition functions. This enum should be updated when we want to use a new function\n+ * for hash partitioning. This function is a part of {@link HashBasedNumberedShardSpec} which is stored\n+ * in the metadata store.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODc2NA==", "bodyText": "Sounds good. Added.", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438764", "createdAt": "2020-09-22T02:02:10Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonValue;\n+import com.google.common.hash.Hashing;\n+import org.apache.druid.java.util.common.StringUtils;\n+\n+/**\n+ * An enum of supported hash partition functions. This enum should be updated when we want to use a new function\n+ * for hash partitioning. This function is a part of {@link HashBasedNumberedShardSpec} which is stored\n+ * in the metadata store.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjM0MQ=="}, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3ODg0NTA1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxMTo1Mzo0NlrOHVMVVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwMjowMjoxOFrOHVoE-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4NDIxMg==", "bodyText": "is there any reason not to set the default in the shard specs used for making new segments?", "url": "https://github.com/apache/druid/pull/10288#discussion_r491984212", "createdAt": "2020-09-21T11:53:46Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java", "diffHunk": "@@ -58,6 +61,7 @@ public BuildingHashBasedNumberedShardSpec(\n     this.partitionDimensions = partitionDimensions == null\n                                ? HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS\n                                : partitionDimensions;\n+    this.partitionFunction = partitionFunction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODc3OQ==", "bodyText": "Good point. I thought it will break some things at first.. But, thinking about it more, I think having a default in partitionsSpec will be fine even though there are at least 2 issues.\n\nIn auto compaction, the last compaction state of each segment includes the partitionsSpec of the compaction task which created the segment. The auto compaction searches for all segments which don't have the last compaction state matched to the current configuration. So, for the segments compacted before, the HashedPartitionsSpec can have an empty partition function. This can be translated to different default hash functions if we ever change the default hash function. Then the auto compaction can silently ignore the changed hash function. However, this doesn't seem that bad.\nIn rolling upgrades which replace machines of an old version with ones of a new version, there can be a mixed version of middleManagers or indexers. In this case, all parallel tasks will be broken as each subtask can use different partition functions if we have changed the default partition function between those two versions. I think this will be acceptable as long as we announce it as a known issue in the release notes. (And we are not probably going to change the default partition function in the near future.)", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438779", "createdAt": "2020-09-22T02:02:18Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java", "diffHunk": "@@ -58,6 +61,7 @@ public BuildingHashBasedNumberedShardSpec(\n     this.partitionDimensions = partitionDimensions == null\n                                ? HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS\n                                : partitionDimensions;\n+    this.partitionFunction = partitionFunction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4NDIxMg=="}, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTU0MjAyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo1MTozMVrOHWL6yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo1MTozMVrOHWL6yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyNTk5NQ==", "bodyText": "suggest rewording the second line to something like \"The current version of Druid will always specify a partitionFunction on newly created segments\"", "url": "https://github.com/apache/druid/pull/10288#discussion_r493025995", "createdAt": "2020-09-22T20:51:31Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -98,141 +107,44 @@ public int getNumBuckets()\n     return partitionDimensions;\n   }\n \n-  @Override\n-  public List<String> getDomainDimensions()\n+  @JsonProperty\n+  public @Nullable HashPartitionFunction getPartitionFunction()\n   {\n-    return partitionDimensions;\n+    return partitionFunction;\n   }\n \n   @Override\n-  public boolean isInChunk(long timestamp, InputRow inputRow)\n-  {\n-    return getBucketIndex(hash(timestamp, inputRow), numBuckets) == bucketId % numBuckets;\n-  }\n-\n-  /**\n-   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n-   * are of {@code partitionDimensionsValues}\n-   *\n-   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n-   *\n-   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n-   */\n-  private boolean isInChunk(Map<String, String> partitionDimensionsValues)\n-  {\n-    assert !partitionDimensions.isEmpty();\n-    List<Object> groupKey = Lists.transform(\n-        partitionDimensions,\n-        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n-    );\n-    try {\n-      return getBucketIndex(hash(jsonMapper, groupKey), numBuckets) == bucketId % numBuckets;\n-    }\n-    catch (JsonProcessingException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  /**\n-   * This method calculates the hash based on whether {@param partitionDimensions} is null or not.\n-   * If yes, then both {@param timestamp} and dimension columns in {@param inputRow} are used {@link Rows#toGroupKey}\n-   * Or else, columns in {@param partitionDimensions} are used\n-   *\n-   * @param timestamp should be bucketed with query granularity\n-   * @param inputRow row from input data\n-   *\n-   * @return hash value\n-   */\n-  protected int hash(long timestamp, InputRow inputRow)\n-  {\n-    return hash(jsonMapper, partitionDimensions, timestamp, inputRow);\n-  }\n-\n-  public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions, long timestamp, InputRow inputRow)\n-  {\n-    final List<Object> groupKey = getGroupKey(partitionDimensions, timestamp, inputRow);\n-    try {\n-      return hash(jsonMapper, groupKey);\n-    }\n-    catch (JsonProcessingException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n-  {\n-    if (partitionDimensions.isEmpty()) {\n-      return Rows.toGroupKey(timestamp, inputRow);\n-    } else {\n-      return Lists.transform(partitionDimensions, inputRow::getDimension);\n-    }\n-  }\n-\n-  @VisibleForTesting\n-  public static int hash(ObjectMapper jsonMapper, List<Object> objects) throws JsonProcessingException\n+  public List<String> getDomainDimensions()\n   {\n-    return HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(objects)).asInt();\n+    return partitionDimensions;\n   }\n \n   @Override\n   public ShardSpecLookup getLookup(final List<? extends ShardSpec> shardSpecs)\n   {\n-    return createHashLookup(jsonMapper, partitionDimensions, shardSpecs, numBuckets);\n-  }\n-\n-  static ShardSpecLookup createHashLookup(\n-      ObjectMapper jsonMapper,\n-      List<String> partitionDimensions,\n-      List<? extends ShardSpec> shardSpecs,\n-      int numBuckets\n-  )\n-  {\n-    return (long timestamp, InputRow row) -> {\n-      int index = getBucketIndex(hash(jsonMapper, partitionDimensions, timestamp, row), numBuckets);\n-      return shardSpecs.get(index);\n-    };\n+    // partitionFunction can be null when you read a shardSpec of a segment created in an old version of Druid.\n+    // It can never be null for segments to create during ingestion.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTU2MDMwOnYy", "diffSide": "RIGHT", "path": "docs/ingestion/index.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo1NzowOVrOHWMGjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzowMjo0NVrOHWPQRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyOTAwNg==", "bodyText": "is the -1 a typo here?", "url": "https://github.com/apache/druid/pull/10288#discussion_r493029006", "createdAt": "2020-09-22T20:57:09Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/index.md", "diffHunk": "@@ -90,7 +90,7 @@ This table compares the three available options:\n | **Input locations** | Any [`inputSource`](./native-batch.md#input-sources). | Any Hadoop FileSystem or Druid datasource. | Any [`inputSource`](./native-batch.md#input-sources). |\n | **File formats** | Any [`inputFormat`](./data-formats.md#input-format). | Any Hadoop InputFormat. | Any [`inputFormat`](./data-formats.md#input-format). |\n | **[Rollup modes](#rollup)** | Perfect if `forceGuaranteedRollup` = true in the [`tuningConfig`](native-batch.md#tuningconfig).  | Always perfect. | Perfect if `forceGuaranteedRollup` = true in the [`tuningConfig`](native-batch.md#tuningconfig). |\n-| **Partitioning options** | Dynamic, hash-based, and range-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. | Hash-based or range-based partitioning via [`partitionsSpec`](hadoop.md#partitionsspec). | Dynamic and hash-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. |\n+| **Partitioning options** | Dynamic, hash-based, and range-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. | Hash-based or range-based partitioning via [`partitionsSpec`](hadoop.md#partitionsspec). | Dynamic and hash-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec-1) for details. |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDY0Nw==", "bodyText": "No, the anchor here was wrong before since native-batch.md#partitionsspec is the partitionsSpec for the parallel task. It should be https://druid.apache.org/docs/latest/ingestion/native-batch.html#partitionsspec-1 which is for the simple task.", "url": "https://github.com/apache/druid/pull/10288#discussion_r493080647", "createdAt": "2020-09-22T23:02:45Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/index.md", "diffHunk": "@@ -90,7 +90,7 @@ This table compares the three available options:\n | **Input locations** | Any [`inputSource`](./native-batch.md#input-sources). | Any Hadoop FileSystem or Druid datasource. | Any [`inputSource`](./native-batch.md#input-sources). |\n | **File formats** | Any [`inputFormat`](./data-formats.md#input-format). | Any Hadoop InputFormat. | Any [`inputFormat`](./data-formats.md#input-format). |\n | **[Rollup modes](#rollup)** | Perfect if `forceGuaranteedRollup` = true in the [`tuningConfig`](native-batch.md#tuningconfig).  | Always perfect. | Perfect if `forceGuaranteedRollup` = true in the [`tuningConfig`](native-batch.md#tuningconfig). |\n-| **Partitioning options** | Dynamic, hash-based, and range-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. | Hash-based or range-based partitioning via [`partitionsSpec`](hadoop.md#partitionsspec). | Dynamic and hash-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. |\n+| **Partitioning options** | Dynamic, hash-based, and range-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. | Hash-based or range-based partitioning via [`partitionsSpec`](hadoop.md#partitionsspec). | Dynamic and hash-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec-1) for details. |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyOTAwNg=="}, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTU2NzYwOnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo1OToyMVrOHWMLFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzowMjo1MVrOHWPQjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMDE2Ng==", "bodyText": "partitionFunction must be set to enable segment pruning.\n\nIs it necessary to set partitionFunction? It's shown as optional in the tables below", "url": "https://github.com/apache/druid/pull/10288#discussion_r493030166", "createdAt": "2020-09-22T20:59:21Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -260,7 +260,7 @@ The three `partitionsSpec` types have different characteristics.\n | PartitionsSpec | Ingestion speed | Partitioning method | Supported rollup mode | Segment pruning at query time |\n |----------------|-----------------|---------------------|-----------------------|-------------------------------|\n | `dynamic` | Fastest  | Partitioning based on number of rows in segment. | Best-effort rollup | N/A |\n-| `hashed`  | Moderate | Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See [Partitioning](./index.md#partitioning) for more details. | Perfect rollup | The broker can use the partition information to prune segments early to speed up queries if `partitionDimensions` is explicitly specified during ingestion. Since the broker knows how to hash `partitionDimensions` values to locate a segment, given a query including a filter on all the `partitionDimensions`, the broker can pick up only the segments holding the rows satisfying the filter on `partitionDimensions` for query processing. |\n+| `hashed`  | Moderate | Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See [Partitioning](./index.md#partitioning) for more details. | Perfect rollup | The broker can use the partition information to prune segments early to speed up queries if `partitionDimensions` is explicitly specified during ingestion. Since the broker knows how to hash `partitionDimensions` values to locate a segment, given a query including a filter on all the `partitionDimensions`, the broker can pick up only the segments holding the rows satisfying the filter on `partitionDimensions` for query processing.<br/><br/>Note that `partitionDimensions` and `partitionFunction` must be set to enable segment pruning.|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDcxOA==", "bodyText": "You're right. partitionFunction cannot be null anymore. Rephrased the description.", "url": "https://github.com/apache/druid/pull/10288#discussion_r493080718", "createdAt": "2020-09-22T23:02:51Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -260,7 +260,7 @@ The three `partitionsSpec` types have different characteristics.\n | PartitionsSpec | Ingestion speed | Partitioning method | Supported rollup mode | Segment pruning at query time |\n |----------------|-----------------|---------------------|-----------------------|-------------------------------|\n | `dynamic` | Fastest  | Partitioning based on number of rows in segment. | Best-effort rollup | N/A |\n-| `hashed`  | Moderate | Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See [Partitioning](./index.md#partitioning) for more details. | Perfect rollup | The broker can use the partition information to prune segments early to speed up queries if `partitionDimensions` is explicitly specified during ingestion. Since the broker knows how to hash `partitionDimensions` values to locate a segment, given a query including a filter on all the `partitionDimensions`, the broker can pick up only the segments holding the rows satisfying the filter on `partitionDimensions` for query processing. |\n+| `hashed`  | Moderate | Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See [Partitioning](./index.md#partitioning) for more details. | Perfect rollup | The broker can use the partition information to prune segments early to speed up queries if `partitionDimensions` is explicitly specified during ingestion. Since the broker knows how to hash `partitionDimensions` values to locate a segment, given a query including a filter on all the `partitionDimensions`, the broker can pick up only the segments holding the rows satisfying the filter on `partitionDimensions` for query processing.<br/><br/>Note that `partitionDimensions` and `partitionFunction` must be set to enable segment pruning.|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMDE2Ng=="}, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NTU4NTc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMTowNTowM1rOHWMWWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzowMjo1NFrOHWPQsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMzA0OA==", "bodyText": "I wonder if we'll eventually need a versioning scheme that's considered by the segment pruning (like \"serialization version\"). It doesn't seem needed for now since we only have primitive + String dimension types and the serialization doesn't seem likely to change there.\nIf we had \"complex\" dimensions at some point, I could see that being difficult to support backwards compatibility for if the serialization format of a complex dim changed.", "url": "https://github.com/apache/druid/pull/10288#discussion_r493033048", "createdAt": "2020-09-22T21:05:03Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -298,8 +211,73 @@ private boolean chunkPossibleInDomain(\n     return false;\n   }\n \n-  private static int getBucketIndex(int hash, int numBuckets)\n+  /**\n+   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n+   * are of {@code partitionDimensionsValues}\n+   *\n+   * @param hashPartitionFunction     hash function used to create segments at ingestion time\n+   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n+   *\n+   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n+   */\n+  private boolean isInChunk(HashPartitionFunction hashPartitionFunction, Map<String, String> partitionDimensionsValues)\n   {\n-    return Math.abs(hash % numBuckets);\n+    assert !partitionDimensions.isEmpty();\n+    List<Object> groupKey = Lists.transform(\n+        partitionDimensions,\n+        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n+    );\n+    return hashPartitionFunction.hash(serializeGroupKey(jsonMapper, groupKey), numBuckets) == bucketId;\n+  }\n+\n+  /**\n+   * Serializes a group key into a byte array. The serialization algorithm can affect hash values of partition keys\n+   * since {@link HashPartitionFunction#hash} takes the result of this method as its input. This means, the returned\n+   * byte array should be backwards-compatible in cases where we need to modify this method.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc1Mg==", "bodyText": "Hmm, good point. Do you see some good use cases for using complex dimensions for partitioning? If you want to benefit from secondary partition pruning, you will need to have a filter on partition dimensions. I'm not sure what the filter on complex dimensions would look like.", "url": "https://github.com/apache/druid/pull/10288#discussion_r493080752", "createdAt": "2020-09-22T23:02:54Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -298,8 +211,73 @@ private boolean chunkPossibleInDomain(\n     return false;\n   }\n \n-  private static int getBucketIndex(int hash, int numBuckets)\n+  /**\n+   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n+   * are of {@code partitionDimensionsValues}\n+   *\n+   * @param hashPartitionFunction     hash function used to create segments at ingestion time\n+   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n+   *\n+   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n+   */\n+  private boolean isInChunk(HashPartitionFunction hashPartitionFunction, Map<String, String> partitionDimensionsValues)\n   {\n-    return Math.abs(hash % numBuckets);\n+    assert !partitionDimensions.isEmpty();\n+    List<Object> groupKey = Lists.transform(\n+        partitionDimensions,\n+        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n+    );\n+    return hashPartitionFunction.hash(serializeGroupKey(jsonMapper, groupKey), numBuckets) == bucketId;\n+  }\n+\n+  /**\n+   * Serializes a group key into a byte array. The serialization algorithm can affect hash values of partition keys\n+   * since {@link HashPartitionFunction#hash} takes the result of this method as its input. This means, the returned\n+   * byte array should be backwards-compatible in cases where we need to modify this method.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMzA0OA=="}, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 306}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Mjg5MzA0OnYy", "diffSide": "RIGHT", "path": "docs/querying/query-context.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzoyNlrOHXSXcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMToxMToxNlrOHXsxYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDIxMQ==", "bodyText": "The broker can basically prune segments unnecessary for queries based on a filter on time intervals.\n\nSorry to nitpick, but this reads sort of funny, also I think 'Broker' should be consistently capitalized. How about:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The broker can basically prune segments unnecessary for queries based on a filter on time intervals. If the datasource is further partitioned based on hash or range partitioning, this query context will enable secondary partition pruning so that the broker can eliminate unnecessary segments from the input scan based on a filter on secondary partition dimensions.|\n          \n          \n            \n            |secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The Broker will always prune unnecessary segments from the input scan based on a filter on time intervals, but if the data is further partitioned with hash or range partitioning, this option will enable additional pruning based on a filter on secondary partition dimensions.|", "url": "https://github.com/apache/druid/pull/10288#discussion_r494180211", "createdAt": "2020-09-24T09:43:26Z", "author": {"login": "clintropolis"}, "path": "docs/querying/query-context.md", "diffHunk": "@@ -58,7 +58,7 @@ These parameters apply to all query types.\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |useFilterCNF|`false`| If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects.|\n-|segmentPruning|`true`|Enable segment pruning on the Broker. Segment pruning can be applied to only the segments partitioned by hash or range.|\n+|secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The broker can basically prune segments unnecessary for queries based on a filter on time intervals. If the datasource is further partitioned based on hash or range partitioning, this query context will enable secondary partition pruning so that the broker can eliminate unnecessary segments from the input scan based on a filter on secondary partition dimensions.|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2903072d316d69ad53ae0f6b98674378147037db"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYxMjgzNA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10288#discussion_r494612834", "createdAt": "2020-09-24T21:11:16Z", "author": {"login": "jihoonson"}, "path": "docs/querying/query-context.md", "diffHunk": "@@ -58,7 +58,7 @@ These parameters apply to all query types.\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |useFilterCNF|`false`| If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects.|\n-|segmentPruning|`true`|Enable segment pruning on the Broker. Segment pruning can be applied to only the segments partitioned by hash or range.|\n+|secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The broker can basically prune segments unnecessary for queries based on a filter on time intervals. If the datasource is further partitioned based on hash or range partitioning, this query context will enable secondary partition pruning so that the broker can eliminate unnecessary segments from the input scan based on a filter on secondary partition dimensions.|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDIxMQ=="}, "originalCommit": {"oid": "2903072d316d69ad53ae0f6b98674378147037db"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2285, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}