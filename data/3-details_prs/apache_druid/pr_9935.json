{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIzNjA1NTIz", "number": 9935, "title": "optimize announceHistoricalSegments", "bodyText": "When the offline Hadoop indexing task publishes thousands of segments, the announceHistoricalSegments method takes up to 10 seconds.And this method will occupy the giant lock in TaskLockBox, resulting in kafka indexing task unable to publish segment normally.\nThis patch has the following changes\uff1a\n1.modify announceHistoricalSegments to batch\n2.offline SegmentTransactionalInsertAction uses a separate lock", "createdAt": "2020-05-27T04:51:34Z", "url": "https://github.com/apache/druid/pull/9935", "merged": true, "mergeCommit": {"oid": "3fc8bc0701938b532282b6b50398c0ee6503a517"}, "closed": true, "closedAt": "2020-09-02T20:07:11Z", "author": {"login": "xiangqiao123"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABclRrcTgH2gAyNDIzNjA1NTIzOjExMGI5YTM0NjYzMjI0OGQwMDUzMjk4ZDI2ODNmN2ZjYzI3ZmZlYjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdFBtZkgFqTQ4MTI2MDAwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "110b9a346632248d0053298d2683f7fcc27ffeb6", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/110b9a346632248d0053298d2683f7fcc27ffeb6", "committedDate": "2020-05-27T04:37:07Z", "message": "optimize announceHistoricalSegment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "909081e1ee4ed9243b3c490a3197aa5fafce1e94", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/909081e1ee4ed9243b3c490a3197aa5fafce1e94", "committedDate": "2020-05-27T06:19:54Z", "message": "optimize announceHistoricalSegment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6c4230ad8122b3762b25fbbee8104786a3b4484", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/c6c4230ad8122b3762b25fbbee8104786a3b4484", "committedDate": "2020-05-27T06:21:54Z", "message": "Merge branch 'optimize_announceHistoricalSegment' of github.com:xiangqiao123/druid into optimize_announceHistoricalSegment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7061f281f275b752e4d770cd667f345f2504476", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/e7061f281f275b752e4d770cd667f345f2504476", "committedDate": "2020-05-27T09:14:07Z", "message": "revert offline SegmentTransactionalInsertAction uses a separate lock"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/605bc5b32f25ae2a1b88af09a89417ac2450f87f", "committedDate": "2020-05-28T05:21:54Z", "message": "optimize segmentExistsBatch: Avoid too many elements in the in condition"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMDQ5NTUx", "url": "https://github.com/apache/druid/pull/9935#pullrequestreview-431049551", "createdAt": "2020-06-15T23:02:11Z", "commit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMzowMjoxMVrOGkFj3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMTowNjowOVrOGpyFJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw==", "bodyText": "Out of curiosity, how did you come up with this number? It would be nice to leave a comment if there was some rationale behind it.", "url": "https://github.com/apache/druid/pull/9935#discussion_r440493023", "createdAt": "2020-06-15T23:02:11Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MTQxOQ==", "bodyText": "There is a typo in ANNOUNCE_HISTORICAL_SEGMENG_BATCH (SEGMENG). I would recommend to rename to be more clear such as MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "url": "https://github.com/apache/druid/pull/9935#discussion_r446461419", "createdAt": "2020-06-27T00:34:20Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw==", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463433", "createdAt": "2020-06-27T00:49:45Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzYyNg==", "bodyText": "This will copy the whole set of toInsertSegments which doesn't seem necessary. Can we use toInsertSegment instead in the below for loop?", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463626", "createdAt": "2020-06-27T00:51:38Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTIwMA==", "bodyText": "This will corrupt the logs too. How about modifying as the below?\n      final List<List<DataSegment>> partitionedSegments = Lists.partition(\n          new ArrayList<>(toInsertSegments),\n          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE\n      );\n\n      PreparedBatch preparedBatch = handle.prepareBatch(\n          StringUtils.format(\n              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n              dbTables.getSegmentsTable(),\n              connector.getQuoteString()\n          )\n      );\n      \n      for (List<DataSegment> partition : partitionedSegments) {\n        for (DataSegment segment : partition) {\n          preparedBatch.add()\n                       .bind(\"id\", segment.getId().toString())\n                       .bind(\"dataSource\", segment.getDataSource())\n                       .bind(\"created_date\", DateTimes.nowUtc().toString())\n                       .bind(\"start\", segment.getInterval().getStart().toString())\n                       .bind(\"end\", segment.getInterval().getEnd().toString())\n                       .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n                       .bind(\"version\", segment.getVersion())\n                       .bind(\"used\", usedSegments.contains(segment))\n                       .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n        }\n        final int[] affectedRows = preparedBatch.execute();\n        final boolean succeeded = Arrays.stream(affectedRows).allMatch(eachAffectedRows -> eachAffectedRows == 1);\n        if (succeeded) {\n          log.infoSegments(partition, \"Published segments to DB\");\n        } else {\n          final List<DataSegment> failedToPublish = IntStream.range(0, partition.size())\n                                                             .filter(i -> affectedRows[i] != 1)\n                                                             .mapToObj(partition::get)\n                                                             .collect(Collectors.toList());\n          throw new ISE(\n              \"Failed to publish segments to DB: %s\",\n              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)\n          );\n        }\n      }\n    }", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465200", "createdAt": "2020-06-27T01:04:57Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTMxOA==", "bodyText": "Same here. Better to be log.errorSegments(segments, \"Exception inserting segments\");", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465318", "createdAt": "2020-06-27T01:06:09Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            } else {\n+              throw new ISE(\n+                  \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            }\n+          }\n+        }\n       }\n     }\n     catch (Exception e) {\n-      log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), used);\n+      for (DataSegment segment : segments) {\n+        log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), usedSegments.contains(segment));\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 180}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "835f0d9977764ee9a22799de063de402befc8299", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/835f0d9977764ee9a22799de063de402befc8299", "committedDate": "2020-07-03T14:23:53Z", "message": "add unit test && Modified according to cr"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a702cd11a406bbf5c63c04d102ed258b527e3704", "author": {"user": null}, "url": "https://github.com/apache/druid/commit/a702cd11a406bbf5c63c04d102ed258b527e3704", "committedDate": "2020-07-03T14:40:44Z", "message": "Merge branch 'master' into optimize_announceHistoricalSegment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMjU3Mjg2", "url": "https://github.com/apache/druid/pull/9935#pullrequestreview-481257286", "createdAt": "2020-09-02T20:02:16Z", "commit": {"oid": "a702cd11a406bbf5c63c04d102ed258b527e3704"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMjYwMDAy", "url": "https://github.com/apache/druid/pull/9935#pullrequestreview-481260002", "createdAt": "2020-09-02T20:06:21Z", "commit": {"oid": "a702cd11a406bbf5c63c04d102ed258b527e3704"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2405, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}