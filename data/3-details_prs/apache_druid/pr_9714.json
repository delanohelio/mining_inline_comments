{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA0Mjg4NTU1", "number": 9714, "title": "More Hadoop integration tests", "bodyText": "This PR adds a new set of Hadoop integration tests (not run in Travis CI in this PR).\n\nAdded Hadoop counterpart to the native batch wikipedia test, with variations on the partitioning spec. This reads from HDFS and writes to HDFS. The original ITHadoopIndexTest is preserved as a test within the revised class.\nAdded tests for Hadoop ingestion + cloud stores (Azure, GCS, S3).\n\nNote that using Azure as deep storage with Hadoop 2.8.5 ingestion does not currently work due to conflicts in the azure-storage version that Druid uses (8.6.0) and the one that hadoop-azure uses (2.2.0). Reading files from Azure works.\n\n\n\nSome other changes included in this PR:\n\nEnables the management proxy on the main router service, which makes it easier to troubleshoot the integration tests live using the web console\nRemoves an unneeded -sha256 parameter from the openssl genrsa calls, which was causing problems in my environment with a newer openssl that rejects the unrecognized parameter\nAllows setting extra.datasource.name.suffix for the integration tests, which is needed for these tests as our cloud store extensions currently don't handle the special characters in the datasource names properly.\n\nThis PR has:\n\n been self-reviewed.\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-04-16T11:13:27Z", "url": "https://github.com/apache/druid/pull/9714", "merged": true, "mergeCommit": {"oid": "61295bd00262d10a57f38a4c18add412dc85ceb5"}, "closed": true, "closedAt": "2020-04-30T21:33:02Z", "author": {"login": "jon-wei"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYKnfbgH2gAyNDA0Mjg4NTU1Ojg2NDkzZDE2ZDNjYzQwNGIyYjUyMzc0ZTg1NTQ5NzU0N2FmMzYzYTQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcc0BGzAFqTQwMzg5MjcyNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "86493d16d3cc404b2b52374e855497547af363a4", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/86493d16d3cc404b2b52374e855497547af363a4", "committedDate": "2020-04-16T11:02:27Z", "message": "More Hadoop integration tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "committedDate": "2020-04-16T11:21:56Z", "message": "Add missing s3 instructions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1MDcxODAy", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-395071802", "createdAt": "2020-04-16T23:44:03Z", "commit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo0NDowM1rOGG6_ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDo1ODoxOVrOGG8SYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMTE2Mw==", "bodyText": "Not sure what do you mean by this? Doesn't reading the data from Azure still requires these?", "url": "https://github.com/apache/druid/pull/9714#discussion_r409911163", "createdAt": "2020-04-16T23:44:03Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/azure_to_hdfs", "diffHunk": "@@ -0,0 +1,34 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> with your cloud configs/credentials\n+#\n+druid_storage_type=hdfs\n+druid_storage_storageDirectory=/druid/segments\n+\n+druid_extensions_loadList=[\"druid-azure-extensions\",\"druid-hdfs-storage\"]\n+\n+# Not used since we have HDFS deep storage, but the Druid Azure extension requires these to be defined", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMjcxMw==", "bodyText": "This should be the credentials file on the docker container.\nThe run_cluster script will copy the files in the folder at -Dresource.file.dir.path to /shared/docker/credentials/\nSo if you have the credentials file at /bob/folderx/secret.json then -Dresource.file.dir.path should be set to /bob/folderx/ and GOOGLE_APPLICATION_CREDENTIALS should be set to /shared/docker/credentials/secret.json", "url": "https://github.com/apache/druid/pull/9714#discussion_r409912713", "createdAt": "2020-04-16T23:49:09Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs", "diffHunk": "@@ -0,0 +1,31 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> and <YOUR_GOOGLE_CREDENTIALS_FILE_NAME> with your cloud configs/credentials\n+#\n+druid_storage_type=google\n+druid_google_bucket=<OVERRIDE_THIS>\n+druid_google_prefix=<OVERRIDE_THIS>\n+\n+druid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\n+\n+# Set this to the path of the credentials file on the host where the integration tests are running\n+GOOGLE_APPLICATION_CREDENTIALS=<OVERRIDE_THIS>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMjc3Mg==", "bodyText": "same as above", "url": "https://github.com/apache/druid/pull/9714#discussion_r409912772", "createdAt": "2020-04-16T23:49:19Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_hdfs", "diffHunk": "@@ -0,0 +1,31 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> and <YOUR_GOOGLE_CREDENTIALS_FILE_NAME> with your cloud configs/credentials\n+#\n+druid_storage_type=hdfs\n+druid_storage_storageDirectory=/druid/segments\n+\n+druid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\n+\n+# Set this to the path of the credentials file on the host where the integration tests are running\n+GOOGLE_APPLICATION_CREDENTIALS=<OVERRIDE_THIS>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzM0MQ==", "bodyText": "NIce!!!!", "url": "https://github.com/apache/druid/pull/9714#discussion_r409913341", "createdAt": "2020-04-16T23:51:22Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/router", "diffHunk": "@@ -27,3 +27,5 @@ SERVICE_DRUID_JAVA_OPTS=-server -Xmx128m -XX:+UseG1GC -agentlib:jdwp=transport=d\n druid_auth_basic_common_cacheDirectory=/tmp/authCache/router\n druid_sql_avatica_enable=true\n druid_server_https_crlPath=/tls/revocations.crl\n+druid_router_managementProxy_enabled=true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzkzMA==", "bodyText": "Can we not get gcs-connector-hadoop from pull-deps too?", "url": "https://github.com/apache/druid/pull/9714#discussion_r409913930", "createdAt": "2020-04-16T23:53:11Z", "author": {"login": "maytasm"}, "path": "integration-tests/run_cluster.sh", "diffHunk": "@@ -74,17 +74,18 @@\n   # For druid-kinesis-indexing-service\n   mkdir -p $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n   mv $SHARED_DIR/docker/lib/druid-kinesis-indexing-service-* $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n-  $ For druid-parquet-extensions\n+  # For druid-parquet-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-parquet-extensions\n   mv $SHARED_DIR/docker/lib/druid-parquet-extensions-* $SHARED_DIR/docker/extensions/druid-parquet-extensions\n-  $ For druid-orc-extensions\n+  # For druid-orc-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-orc-extensions\n   mv $SHARED_DIR/docker/lib/druid-orc-extensions-* $SHARED_DIR/docker/extensions/druid-orc-extensions\n \n   # Pull Hadoop dependency if needed\n   if [ -n \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" == true ]\n   then\n-    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5\n+    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5 -h org.apache.hadoop:hadoop-azure:2.8.5\n+    curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar --output $SHARED_DIR/docker/lib/gcs-connector-hadoop2-latest.jar", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxNDE1OQ==", "bodyText": "I believe there is a problem if using hadoop as deep storage and datasource contains special char / whitespace. Was that the reason for this change?", "url": "https://github.com/apache/druid/pull/9714#discussion_r409914159", "createdAt": "2020-04-16T23:53:49Z", "author": {"login": "maytasm"}, "path": "integration-tests/pom.xml", "diffHunk": "@@ -430,7 +431,7 @@\n                                 -Dfile.encoding=UTF-8\n                                 -Ddruid.test.config.dockerIp=${env.DOCKER_IP}\n                                 -Ddruid.test.config.hadoopDir=${env.HADOOP_DIR}\n-                                -Ddruid.test.config.extraDatasourceNameSuffix=\\ \u0420\u043e\u0441\u0441\u0438\u044f\\ \ud55c\uad6d\\ \u4e2d\u56fd!?\n+                                -Ddruid.test.config.extraDatasourceNameSuffix=${extra.datasource.name.suffix}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTUxNQ==", "bodyText": "Maybe worth mentioning about seting the -Dresource.file.dir.path and GOOGLE_APPLICATION_CREDENTIALS since you need to make druid-google-extensions  happy on the druid nodes.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409919515", "createdAt": "2020-04-17T00:12:12Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTczNQ==", "bodyText": "same as above", "url": "https://github.com/apache/druid/pull/9714#discussion_r409919735", "createdAt": "2020-04-17T00:12:55Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n+ *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n+ *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n+ * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n+ * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMDY1Nw==", "bodyText": "step 1 is this step. The location should be /resources/data/batch_index on the hadoop container fs and also at /batch_index on hdfs", "url": "https://github.com/apache/druid/pull/9714#discussion_r409920657", "createdAt": "2020-04-17T00:15:48Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMTIwOA==", "bodyText": "Maybe we should combine TestNGGroup.HADOOP_INDEX with TestNGGroup.HDFS_DEEP_STORAGE", "url": "https://github.com/apache/druid/pull/9714#discussion_r409921208", "createdAt": "2020-04-17T00:17:30Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA==", "bodyText": "currently /batch_index/tsv requires manual setup\nFrom the integration-tests/README.md ...\nCurrently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\nCreate a directory called batchHadoop1 in the hadoop file system\n(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \ninto that directory (as its only file).\n\nWe should automatically setup this dir for the hadoop docker container (similar to how we setup the wikipedia json files). You can create a new dir in integration-tests/src/test/resources/data/batch_index called tsv and copy integration-tests/src/test/resources/hadoop/batch_hadoop.data to integration-tests/src/test/resources/data/batch_index/tsv (the run-cluster script should handle the rest and create /batch_index/tsv with batch_hadoop.data inside)", "url": "https://github.com/apache/druid/pull/9714#discussion_r409922084", "createdAt": "2020-04-17T00:20:34Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNDMyNw==", "bodyText": "I think ITS3InputToHdfsHadoopIndexTest and ITS3InputToS3HadoopIndexTest should be in different groups. The groups can be the different deep storage. The reason for my suggestion is that if you run the whole group then you cannot switch  -Doverride.config.path between different test class. What will happens is that you will run the whole group (ITS3InputToHdfsHadoopIndexTest and ITS3InputToS3HadoopIndexTest) with the same druid config file which basically will be the exact same test (same deep storage). Same for the other cloud storages.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409924327", "createdAt": "2020-04-17T00:28:25Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket, path, and region for your data.\n+ *    This can be done by setting -Ddruid.test.config.cloudBucket, -Ddruid.test.config.cloudPath\n+ *    and -Ddruid.test.config.cloudRegion or setting \"cloud_bucket\",\"cloud_path\", and \"cloud_region\" in the config file.\n+ * 2) Set -Ddruid.s3.accessKey and -Ddruid.s3.secretKey when running the tests to your access/secret keys.\n+ * 3) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your S3 at the location set in step 1.\n+ * 4) Provide -Doverride.config.path=<PATH_TO_FILE> with s3 credentials and hdfs deep storage configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hadoop/s3_to_hdfs for env vars to provide.\n+ * 5) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n+@Test(groups = TestNGGroup.HADOOP_S3)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNjQyMg==", "bodyText": "nit: Maybe we can combine the the different cloud input text task json into one by parameterizing the difference in the jobProperties.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409926422", "createdAt": "2020-04-17T00:35:37Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/resources/hadoop/wikipedia_hadoop_azure_input_index_task.json", "diffHunk": "@@ -0,0 +1,107 @@\n+{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzMTk3MA==", "bodyText": "Maybe also just get rid of the hadoopTestDir in the DockerConfigProvider. I think it's no longer needed. If using hadoop container then everything is automatically setup. If running your own hadoop then they should copy to /batch_index/tsv since the path is hardcoded in the specPathsTransform anyway", "url": "https://github.com/apache/druid/pull/9714#discussion_r409931970", "createdAt": "2020-04-17T00:56:36Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzMjM4NA==", "bodyText": "Can you also update the integration-tests/README.md ITHadoopIndexTest can be run in hadoop docker container hence the existing README.md is out of date.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409932384", "createdAt": "2020-04-17T00:58:19Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc146929a371a0e7e27e5e7349b58af92405986e", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/fc146929a371a0e7e27e5e7349b58af92405986e", "committedDate": "2020-04-28T03:27:35Z", "message": "Merge remote-tracking branch 'upstream/master' into more_hadoop"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/3aafc2c2109f96ab3ec854b0e087968f386a3a58", "committedDate": "2020-04-28T04:21:34Z", "message": "Address PR comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMjQ0MzMy", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-402244332", "createdAt": "2020-04-28T22:02:15Z", "commit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowMjoxNVrOGNoqJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowMzowN1rOGNortw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MDgyMQ==", "bodyText": "nit: typo? both are 1)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416950821", "createdAt": "2020-04-28T22:02:15Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MTIyMw==", "bodyText": "Should we create an issue for this? Seems like might be a problem for new users / going through tutorial?", "url": "https://github.com/apache/druid/pull/9714#discussion_r416951223", "createdAt": "2020-04-28T22:03:07Z", "author": {"login": "maytasm"}, "path": "integration-tests/run_cluster.sh", "diffHunk": "@@ -74,17 +74,18 @@\n   # For druid-kinesis-indexing-service\n   mkdir -p $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n   mv $SHARED_DIR/docker/lib/druid-kinesis-indexing-service-* $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n-  $ For druid-parquet-extensions\n+  # For druid-parquet-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-parquet-extensions\n   mv $SHARED_DIR/docker/lib/druid-parquet-extensions-* $SHARED_DIR/docker/extensions/druid-parquet-extensions\n-  $ For druid-orc-extensions\n+  # For druid-orc-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-orc-extensions\n   mv $SHARED_DIR/docker/lib/druid-orc-extensions-* $SHARED_DIR/docker/extensions/druid-orc-extensions\n \n   # Pull Hadoop dependency if needed\n   if [ -n \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" == true ]\n   then\n-    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5\n+    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5 -h org.apache.hadoop:hadoop-azure:2.8.5\n+    curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar --output $SHARED_DIR/docker/lib/gcs-connector-hadoop2-latest.jar", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzkzMA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMjQ3MjMy", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-402247232", "createdAt": "2020-04-28T22:07:52Z", "commit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowNzo1M1rOGNo0NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjoyMTozNlrOGNpLWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MzM5Nw==", "bodyText": "nit: maybe mention that -Dextra.datasource.name.suffix=''  is due to github issue xxx (not sure if we have an issue for this)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416953397", "createdAt": "2020-04-28T22:07:53Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -29,17 +29,18 @@\n  * To run this test, you must:\n  * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n  *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n- * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ * 2) Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n  *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n  *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n  *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n- * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ * 3) Provide -Dresource.file.dir.path=<PATH_TO_FOLDER> with folder that contains GOOGLE_APPLICATION_CREDENTIALS file\n+ * 4) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n  *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n- * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n+ * 5) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs for env vars to provide.\n- * 4) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ * 6) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1Nzg5NA==", "bodyText": "\"Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster\" should be changed to indicate that ITHadoopIndexTest can be run with both methods (docker or manual)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416957894", "createdAt": "2020-04-28T22:18:14Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n 1) Run your own Druid + Hadoop cluster and specified Hadoop configs in the configuration file (CONFIG_FILE).\n-2) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n \n Currently, hdfs-deep-storage and other <cloud>-deep-storage integration test groups can only be run with \n Druid Docker test clusters by passing -Dstart.hadoop.docker=true to start Hadoop container.\n You will also have to provide -Doverride.config.path=<PATH_TO_FILE> with your Druid's Hadoop configs set. \n See integration-tests/docker/environment-configs/override-examples/hdfs directory for example.\n Note that if the integration test you are running also uses other cloud extension (S3, Azure, GCS), additional\n-credentials/configs may need to be set in the same file as your Druid's Hadoop configs set. \n+credentials/configs may need to be set in the same file as your Druid's Hadoop configs set.\n \n Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\n-Create a directory called batchHadoop1 in the hadoop file system\n-(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \n-into that directory (as its only file).\n-\n-Add this keyword to the configuration file (see above):\n+- Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1OTMyMw==", "bodyText": "-Doverride.config.path actually has no effect if using int-tests-config-file. Sorry maybe the instruction should have been more clear. int-tests-config-file allows you to specify your own Druid cluster. It must be start/stop and configured manually (i.e. you have to go edit the common.runtime.properties etc manually yourself)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416959323", "createdAt": "2020-04-28T22:21:36Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n 1) Run your own Druid + Hadoop cluster and specified Hadoop configs in the configuration file (CONFIG_FILE).\n-2) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n \n Currently, hdfs-deep-storage and other <cloud>-deep-storage integration test groups can only be run with \n Druid Docker test clusters by passing -Dstart.hadoop.docker=true to start Hadoop container.\n You will also have to provide -Doverride.config.path=<PATH_TO_FILE> with your Druid's Hadoop configs set. \n See integration-tests/docker/environment-configs/override-examples/hdfs directory for example.\n Note that if the integration test you are running also uses other cloud extension (S3, Azure, GCS), additional\n-credentials/configs may need to be set in the same file as your Druid's Hadoop configs set. \n+credentials/configs may need to be set in the same file as your Druid's Hadoop configs set.\n \n Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\n-Create a directory called batchHadoop1 in the hadoop file system\n-(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \n-into that directory (as its only file).\n-\n-Add this keyword to the configuration file (see above):\n+- Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+  located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at /batch_index/json/\n+  If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+- Copy batch_hadoop.data located in integration-tests/src/test/resources/data/batch_index/tsv to your HDFS\n+  at /batch_index/tsv/\n+  If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n \n+Run the test using mvn (using the bundled Docker-based Hadoop cluster):\n ```\n-    \"hadoopTestDir\": \"<name_of_dir_containing_batchHadoop1>\"\n+  mvn verify -P integration-tests -Dit.test=ITHadoopIndexTest -Dstart.hadoop.docker=true -Doverride.config.path=docker/environment-configs/override-examples/hdfs -Dextra.datasource.name.suffix=''\n ```\n \n-Run the test using mvn:\n-\n+Run the test using mvn (using config file for existing Hadoop cluster):\n ```\n-  mvn verify -P int-tests-config-file -Dit.test=ITHadoopIndexTest\n+  mvn verify -P int-tests-config-file -Dit.test=ITHadoopIndexTest -Doverride.config.path=docker/environment-configs/override-examples/hdfs -Dextra.datasource.name.suffix=''", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMjYxMjkx", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-402261291", "createdAt": "2020-04-28T22:37:48Z", "commit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjozNzo0OFrOGNpkGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjozNzo0OFrOGNpkGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA==", "bodyText": "I think you also have to exclude this from Travis's other integration test group", "url": "https://github.com/apache/druid/pull/9714#discussion_r416965658", "createdAt": "2020-04-28T22:37:48Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "diffHunk": "@@ -84,6 +82,15 @@\n    */\n   public static final String HDFS_DEEP_STORAGE = \"hdfs-deep-storage\";\n \n+  public static final String HADOOP_S3_TO_S3 = \"hadoop-s3-to-s3-deep-storage\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 13}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4984ec384b1ee9e07e6fb277870e07bbfaac169e", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/4984ec384b1ee9e07e6fb277870e07bbfaac169e", "committedDate": "2020-04-29T02:14:08Z", "message": "Address PR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23f4be169431a7836e84b2ff44fc257f5d40083d", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/23f4be169431a7836e84b2ff44fc257f5d40083d", "committedDate": "2020-04-29T02:33:38Z", "message": "PR comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyNDAwNjM1", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-402400635", "createdAt": "2020-04-29T06:50:14Z", "commit": {"oid": "23f4be169431a7836e84b2ff44fc257f5d40083d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNjo1MDoxNFrOGNxw9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNjo1MDoxNFrOGNxw9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEwMDAyMg==", "bodyText": "missing comma between slow and hadoop (\"slowhadoop\" -> \"slow,hadoop\")", "url": "https://github.com/apache/druid/pull/9714#discussion_r417100022", "createdAt": "2020-04-29T06:50:14Z", "author": {"login": "maytasm"}, "path": ".travis.yml", "diffHunk": "@@ -365,7 +365,7 @@ jobs:\n       name: \"(Compile=openjdk8, Run=openjdk8) other integration test\"\n       jdk: openjdk8\n       services: *integration_test_services\n-      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow' JVM_RUNTIME='-Djvm.runtime=8'\n+      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slowhadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage' JVM_RUNTIME='-Djvm.runtime=8'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23f4be169431a7836e84b2ff44fc257f5d40083d"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyNDAxMzg5", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-402401389", "createdAt": "2020-04-29T06:51:46Z", "commit": {"oid": "23f4be169431a7836e84b2ff44fc257f5d40083d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d12acba6aecb9731c29ed7f695d77748b370e917", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/d12acba6aecb9731c29ed7f695d77748b370e917", "committedDate": "2020-04-29T18:01:29Z", "message": "Fix typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e0178b493bcbae3f07b5d7ed0d52218cb71f647", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/2e0178b493bcbae3f07b5d7ed0d52218cb71f647", "committedDate": "2020-04-30T01:51:49Z", "message": "Merge remote-tracking branch 'upstream/master' into more_hadoop"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzODkyNzI3", "url": "https://github.com/apache/druid/pull/9714#pullrequestreview-403892727", "createdAt": "2020-04-30T21:32:14Z", "commit": {"oid": "2e0178b493bcbae3f07b5d7ed0d52218cb71f647"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2525, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}