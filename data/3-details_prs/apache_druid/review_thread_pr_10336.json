{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc2NzQ4MDU5", "number": 10336, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxMDozNTo0MlrOEe2DPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjoyOTowN1rOEg6v3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwNzc4MzAwOnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxMDozNTo0MlrOHKwb3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMDoyMjoyNVrOHLG2kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA0MTM3NA==", "bodyText": "just curious, can this be private final CircularBuffer<ParseException> itself?", "url": "https://github.com/apache/druid/pull/10336#discussion_r481041374", "createdAt": "2020-09-01T10:35:42Z", "author": {"login": "abhishekagarwal87"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.RE;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+import org.apache.druid.utils.CircularBuffer;\n+\n+import javax.annotation.Nullable;\n+\n+/**\n+ * A handler for {@link ParseException}s thrown during ingestion. Based on the given configuration, this handler can\n+ *\n+ * - log ParseExceptions.\n+ * - keep most recent N ParseExceptions in memory.\n+ * - throw a RuntimeException when it sees more ParseExceptions than {@link #maxAllowedParseExceptions}.\n+ *\n+ * No matter what the handler does, the relevant metric should be updated first.\n+ */\n+public class ParseExceptionHandler\n+{\n+  private static final Logger LOG = new Logger(ParseExceptionHandler.class);\n+\n+  private final RowIngestionMeters rowIngestionMeters;\n+  private final boolean logParseExceptions;\n+  private final int maxAllowedParseExceptions;\n+  @Nullable\n+  private final CircularBuffer<Throwable> savedParseExceptions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwODY1Ng==", "bodyText": "Oh yeah, it can. I just copied existing code without noticing it. Changed it now.", "url": "https://github.com/apache/druid/pull/10336#discussion_r481408656", "createdAt": "2020-09-01T20:22:25Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.RE;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+import org.apache.druid.utils.CircularBuffer;\n+\n+import javax.annotation.Nullable;\n+\n+/**\n+ * A handler for {@link ParseException}s thrown during ingestion. Based on the given configuration, this handler can\n+ *\n+ * - log ParseExceptions.\n+ * - keep most recent N ParseExceptions in memory.\n+ * - throw a RuntimeException when it sees more ParseExceptions than {@link #maxAllowedParseExceptions}.\n+ *\n+ * No matter what the handler does, the relevant metric should be updated first.\n+ */\n+public class ParseExceptionHandler\n+{\n+  private static final Logger LOG = new Logger(ParseExceptionHandler.class);\n+\n+  private final RowIngestionMeters rowIngestionMeters;\n+  private final boolean logParseExceptions;\n+  private final int maxAllowedParseExceptions;\n+  @Nullable\n+  private final CircularBuffer<Throwable> savedParseExceptions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA0MTM3NA=="}, "originalCommit": {"oid": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDY5MDU2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNToxMTo1M1rOHL0HDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNToxMTo1M1rOHL0HDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1MDE1Ng==", "bodyText": "wow, I never thought of this case", "url": "https://github.com/apache/druid/pull/10336#discussion_r482150156", "createdAt": "2020-09-02T15:11:53Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDcyNTgzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNToxOToxMlrOHL0c8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjowNVrOHL-Ffg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1NTc2MQ==", "bodyText": "Instead of printing the whole map, I think it would be better to just print the timestamp string that was unparsable.\nSimilar comment on line 99, this would get rid of the need of rawMapToPrint and guarantees that the invalid timestamp is always logged regardless of where in the event it is.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n          \n          \n            \n                  throw new ParseException(\"Unparseable timestamp found! Timestamp column (%s): %s\", timestampSpec.getTimestampColumn(), theMap.get(timestampSpec.getTimestampColumn()));", "url": "https://github.com/apache/druid/pull/10336#discussion_r482155761", "createdAt": "2020-09-02T15:19:12Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzU5OA==", "bodyText": "It is a good idea to print the timestamp string, but I would like to keep the current behavior as well (this logging is not what I added in this PR). I modified the error message to include timestamp.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313598", "createdAt": "2020-09-02T19:02:05Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1NTc2MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDc0OTEwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNToyNDowOVrOHL0rcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjowOVrOHL-Frw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1OTQ3Mg==", "bodyText": "After your refactoring in this change, the parse functions on line 65 and 70 can be made private and package private (VisibleForTesting) respectively", "url": "https://github.com/apache/druid/pull/10336#discussion_r482159472", "createdAt": "2020-09-02T15:24:09Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {\n+      throw new ParseException(\n+          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          rawMapToPrint(theMap)\n+      );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzY0Nw==", "bodyText": "Done.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313647", "createdAt": "2020-09-02T19:02:09Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {\n+      throw new ParseException(\n+          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          rawMapToPrint(theMap)\n+      );", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1OTQ3Mg=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDc4MDI3OnYy", "diffSide": "RIGHT", "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTozMDo1MFrOHL0-_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjoxNVrOHL-GEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NDQ3Ng==", "bodyText": "nit: Maybe these 3 classes should be moved into their own sub package org.apache.druid.segment.incremental.stats", "url": "https://github.com/apache/druid/pull/10336#discussion_r482164476", "createdAt": "2020-09-02T15:30:50Z", "author": {"login": "suneet-s"}, "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "diffHunk": "@@ -106,6 +103,9 @@\n import org.apache.druid.query.timeseries.TimeseriesQueryEngine;\n import org.apache.druid.query.timeseries.TimeseriesQueryQueryToolChest;\n import org.apache.druid.query.timeseries.TimeseriesQueryRunnerFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.incremental.RowIngestionMetersFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMetersTotals;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzc0Nw==", "bodyText": "That might be nice. I think we probably need new classes for metrics of native batch ingestion. I will reorganize the package in a follow-up PR if is good.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313747", "createdAt": "2020-09-02T19:02:15Z", "author": {"login": "jihoonson"}, "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "diffHunk": "@@ -106,6 +103,9 @@\n import org.apache.druid.query.timeseries.TimeseriesQueryEngine;\n import org.apache.druid.query.timeseries.TimeseriesQueryQueryToolChest;\n import org.apache.druid.query.timeseries.TimeseriesQueryRunnerFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.incremental.RowIngestionMetersFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMetersTotals;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NDQ3Ng=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDc5MDk2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTozMzoxMlrOHL1FiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjoxOFrOHL-GSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NjE1Mg==", "bodyText": "javadocs please", "url": "https://github.com/apache/druid/pull/10336#discussion_r482166152", "createdAt": "2020-09-02T15:33:12Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzgwMQ==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313801", "createdAt": "2020-09-02T19:02:18Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NjE1Mg=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDgyMzU5OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTo0MDoxOVrOHL1Ztg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjoyMlrOHL-Ghg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3MTMxOA==", "bodyText": "Should this also be annotated with @MonotonicNonNull", "url": "https://github.com/apache/druid/pull/10336#discussion_r482171318", "createdAt": "2020-09-02T15:40:19Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -151,7 +151,7 @@ private static String makeTaskId(RealtimeAppenderatorIngestionSpec spec)\n   private volatile Thread runThread = null;\n \n   @JsonIgnore\n-  private CircularBuffer<Throwable> savedParseExceptions;\n+  private ParseExceptionHandler parseExceptionHandler;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzg2Mg==", "bodyText": "Added.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313862", "createdAt": "2020-09-02T19:02:22Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -151,7 +151,7 @@ private static String makeTaskId(RealtimeAppenderatorIngestionSpec spec)\n   private volatile Thread runThread = null;\n \n   @JsonIgnore\n-  private CircularBuffer<Throwable> savedParseExceptions;\n+  private ParseExceptionHandler parseExceptionHandler;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3MTMxOA=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDg2NDg1OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTo0OTozOFrOHL1zUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjo0NVrOHL-H-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3Nzg3NQ==", "bodyText": "This is calculated in 3 places in the code - AbstractBatchIndexTask, InputSourceSampler and SeekableStreamIndexTaskRunner. I think it would be a good idea to consolidate these methods, so that metricNames is always calculated the same way given a dataSchema", "url": "https://github.com/apache/druid/pull/10336#discussion_r482177875", "createdAt": "2020-09-02T15:49:38Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(\n+      File tmpDir,\n+      DataSchema dataSchema,\n+      InputSource inputSource,\n+      @Nullable InputFormat inputFormat,\n+      Predicate<InputRow> rowFilter,\n+      RowIngestionMeters ingestionMeters,\n+      ParseExceptionHandler parseExceptionHandler\n+  ) throws IOException\n+  {\n+    final List<String> metricsNames = Arrays.stream(dataSchema.getAggregators())\n+                                            .map(AggregatorFactory::getName)\n+                                            .collect(Collectors.toList());\n+    final InputSourceReader inputSourceReader = dataSchema.getTransformSpec().decorate(\n+        inputSource.reader(\n+            new InputRowSchema(\n+                dataSchema.getTimestampSpec(),\n+                dataSchema.getDimensionsSpec(),\n+                metricsNames\n+            ),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDIzMw==", "bodyText": "Yeah, it would be useful. Or we can change InputRowSchema to have all type information of metrics as well, so that we can check exceptions in parsing and type casting in only InputEntityReader. It means, after this change, you will not have to worry about parseExceptions in IncrementalIndex anymore. Probably we don't need ParseExceptionHandler at all in that case. I haven't done this refactoring here because it will be quite complicated and this PR is already big enough.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314233", "createdAt": "2020-09-02T19:02:45Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(\n+      File tmpDir,\n+      DataSchema dataSchema,\n+      InputSource inputSource,\n+      @Nullable InputFormat inputFormat,\n+      Predicate<InputRow> rowFilter,\n+      RowIngestionMeters ingestionMeters,\n+      ParseExceptionHandler parseExceptionHandler\n+  ) throws IOException\n+  {\n+    final List<String> metricsNames = Arrays.stream(dataSchema.getAggregators())\n+                                            .map(AggregatorFactory::getName)\n+                                            .collect(Collectors.toList());\n+    final InputSourceReader inputSourceReader = dataSchema.getTransformSpec().decorate(\n+        inputSource.reader(\n+            new InputRowSchema(\n+                dataSchema.getTimestampSpec(),\n+                dataSchema.getDimensionsSpec(),\n+                metricsNames\n+            ),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3Nzg3NQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNDkwMTIxOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTo1NzoxOFrOHL2JPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMjo1NFrOHL-Icw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4MzQ4Ng==", "bodyText": "parseExceptionHandler can be null if the task.run() hasn't been called. This is probably unlikely?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482183486", "createdAt": "2020-09-02T15:57:18Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -550,7 +546,9 @@ public Response getUnparseableEvents(\n   )\n   {\n     IndexTaskUtils.datasourceAuthorizationCheck(req, Action.READ, getDataSource(), authorizerMapper);\n-    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(savedParseExceptions);\n+    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(\n+        parseExceptionHandler.getSavedParseExceptions()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDM1NQ==", "bodyText": "This is fine. Any APIs cannot be called until chatHandler is registered. The chatHandler is registered in run() after parseExceptionHandler is created.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314355", "createdAt": "2020-09-02T19:02:54Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -550,7 +546,9 @@ public Response getUnparseableEvents(\n   )\n   {\n     IndexTaskUtils.datasourceAuthorizationCheck(req, Action.READ, getDataSource(), authorizerMapper);\n-    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(savedParseExceptions);\n+    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(\n+        parseExceptionHandler.getSavedParseExceptions()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4MzQ4Ng=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTAxNjk2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/BatchAppenderators.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjoyNjoyNFrOHL3Sfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOToxNToxNVrOHL-zNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ==", "bodyText": "Is there ever a case where the rowIngestionMeters is different than the one used in parseExceptionHandler? If not, I think we should just pass in the parseExceptionHandler.\nSimilar comment on line 45", "url": "https://github.com/apache/druid/pull/10336#discussion_r482202239", "createdAt": "2020-09-02T16:26:24Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/BatchAppenderators.java", "diffHunk": "@@ -59,7 +65,9 @@ public static Appenderator newAppenderator(\n       TaskToolbox toolbox,\n       DataSchema dataSchema,\n       AppenderatorConfig appenderatorConfig,\n-      DataSegmentPusher segmentPusher\n+      DataSegmentPusher segmentPusher,\n+      RowIngestionMeters rowIngestionMeters,\n+      ParseExceptionHandler parseExceptionHandler", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDY1NQ==", "bodyText": "I see your point, but I'm not sure if it's good to have parseExceptionHandler be a rowIngestionMeters provider. I guess we have a couple of options for alternatives:\n\nMerge RowIngestioMeters and ParseExceptionHandler. RowIngestionMeters will have handle(ParseException) method and throw an exception when it hits the limit. This seems a bit strange to me since metrics registry can throw an exception.\nMake RowIngestionMeters to be only a registry and use individual Meters. For example, ParseExceptionHandler will have only a Meter for counting unparseable events. AppenderatorImpl will have only a Meter for counting processed events. This seems sane, but can make code error-prone since we will have multiple Meters which should be properly chosen whenever we use them.\n\nI'm not sure any of these are better than now. Do you have any better idea?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314655", "createdAt": "2020-09-02T19:03:15Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/BatchAppenderators.java", "diffHunk": "@@ -59,7 +65,9 @@ public static Appenderator newAppenderator(\n       TaskToolbox toolbox,\n       DataSchema dataSchema,\n       AppenderatorConfig appenderatorConfig,\n-      DataSegmentPusher segmentPusher\n+      DataSegmentPusher segmentPusher,\n+      RowIngestionMeters rowIngestionMeters,\n+      ParseExceptionHandler parseExceptionHandler", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMyNTMwMQ==", "bodyText": "fwiw, I think current implementation is better one.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482325301", "createdAt": "2020-09-02T19:15:15Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/BatchAppenderators.java", "diffHunk": "@@ -59,7 +65,9 @@ public static Appenderator newAppenderator(\n       TaskToolbox toolbox,\n       DataSchema dataSchema,\n       AppenderatorConfig appenderatorConfig,\n-      DataSegmentPusher segmentPusher\n+      DataSegmentPusher segmentPusher,\n+      RowIngestionMeters rowIngestionMeters,\n+      ParseExceptionHandler parseExceptionHandler", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTAzMTU0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/PeonAppenderatorsManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjozMDowNFrOHL3b3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMzoyMFrOHL-J9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwNDYzOQ==", "bodyText": "General comment here for all the new functions that accept parseExceptionHandler. parseExceptionHandler tends to be lazily initialized throughout the codebase, but I don't see safeguards against using an un-initialized parseExceptionHandler. Perhaps more use of @Nullable annotation will help surface potential places where it may be used before it's initialized", "url": "https://github.com/apache/druid/pull/10336#discussion_r482204639", "createdAt": "2020-09-02T16:30:04Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/PeonAppenderatorsManager.java", "diffHunk": "@@ -129,7 +137,9 @@ public Appenderator createOfflineAppenderatorForTask(\n           dataSegmentPusher,\n           objectMapper,\n           indexIO,\n-          indexMerger\n+          indexMerger,\n+          rowIngestionMeters,\n+          parseExceptionHandler", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDc0MA==", "bodyText": "They must not be null. I added null checks in AppenderatorImpl.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314740", "createdAt": "2020-09-02T19:03:20Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/PeonAppenderatorsManager.java", "diffHunk": "@@ -129,7 +137,9 @@ public Appenderator createOfflineAppenderatorForTask(\n           dataSegmentPusher,\n           objectMapper,\n           indexIO,\n-          indexMerger\n+          indexMerger,\n+          rowIngestionMeters,\n+          parseExceptionHandler", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwNDYzOQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTA4MDQwOnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjo0MzoxM1rOHL37Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNzowMjozNFrOHOmRXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ==", "bodyText": "This is a change in behavior. parseExceptionMessages will add to the parseExceptionMessages that were found on line 165. Previously it was re-set. Was this intentional?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482212651", "createdAt": "2020-09-02T16:43:13Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDg2NA==", "bodyText": "Nice finding. Clearing parseExceptionMessages now.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314864", "createdAt": "2020-09-02T19:03:28Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQwODI0OQ==", "bodyText": "why not just return a List like before? That seems less risky than assuming the list is mutated correctly. In general, I'm not a big fan of returning values from a function using parameters because of cases like this.\nEither way, this comment shouldn't be considered a blocker, just my 2 cents", "url": "https://github.com/apache/druid/pull/10336#discussion_r482408249", "createdAt": "2020-09-02T20:20:24Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUwMTY3Ng==", "bodyText": "I'm not a big fan of that either, but it seems strange to me to return exception messages as a result of aggregation. I'm actually not sure if we can return early without the second aggregation when the first aggregation throws parseExceptions. I don't want to touch that part yet at least in this PR.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482501676", "createdAt": "2020-09-02T21:41:52Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ1NjAzOQ==", "bodyText": "Currently (before and after this PR), a row that contains partially corrupted/incorrect data is aggregated anyway (partially).\nThis may result in wrong statistics inference by the user due to inconsistencies between the number of rows each aggregator accumulated.\nIMO, the decision of allowing this scenario should be of the user.\nBut since currently, there is no way to roll-back aggregation, I don't see how this can be easily implemented.\nMaybe adding parse as a preliminary step to aggregation, but this is an entirely different subject.\n\"return early without the second aggregation\" would only aggravate this scenario, so I don't think it is the way to go either.", "url": "https://github.com/apache/druid/pull/10336#discussion_r484456039", "createdAt": "2020-09-07T14:11:29Z", "author": {"login": "liran-funaro"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA2OTE1MQ==", "bodyText": "Good point. I'm not sure if that behaviour is intended or not, but seems strange to me. I think all parse operations should be done in InputSourceReader.read() as a preliminary step so that we don't have to worry about ParseExceptions in IncrementalIndex. But yeah, this is a different subject.", "url": "https://github.com/apache/druid/pull/10336#discussion_r485069151", "createdAt": "2020-09-08T17:02:34Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTE0ODM3OnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/incremental/OffheapIncrementalIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzowMDozN1rOHL4mvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMzozMFrOHL-KnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMzgwNg==", "bodyText": "This is a change in behavior. Previously the aggs would fail fast, but now it tries to parse all of them before bubbling up all the parse exceptions. Was this change intentional? I see a similar change in at least the Onheap implementation.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482223806", "createdAt": "2020-09-02T17:00:37Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OffheapIncrementalIndex.java", "diffHunk": "@@ -233,16 +232,13 @@ protected AddToFactsResult addToFacts(\n         }\n         catch (ParseException e) {\n           // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-          if (getReportParseExceptions()) {\n-            throw new ParseException(e, \"Encountered parse error for aggregator[%s]\", getMetricAggs()[i].getName());\n-          } else {\n-            log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n-          }\n+          log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n+          parseExceptionMessages.add(e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDkwOA==", "bodyText": "This is intentional to make it same with OnheapIncrementalIndex.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314908", "createdAt": "2020-09-02T19:03:30Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OffheapIncrementalIndex.java", "diffHunk": "@@ -233,16 +232,13 @@ protected AddToFactsResult addToFacts(\n         }\n         catch (ParseException e) {\n           // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-          if (getReportParseExceptions()) {\n-            throw new ParseException(e, \"Encountered parse error for aggregator[%s]\", getMetricAggs()[i].getName());\n-          } else {\n-            log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n-          }\n+          log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n+          parseExceptionMessages.add(e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMzgwNg=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTE1MDc0OnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzowMToxNVrOHL4oRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxOTowMzozNFrOHL-K0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDE5OA==", "bodyText": "Javadocs please.\nShould this just be a singleton?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482224198", "createdAt": "2020-09-02T17:01:15Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+public class NoopRowIngestionMeters implements RowIngestionMeters", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDk2Mw==", "bodyText": "This is used only in RealtimeIndexTask which is used by Tranquility. It can be a singleton, but doesn't seem matter.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314963", "createdAt": "2020-09-02T19:03:34Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+public class NoopRowIngestionMeters implements RowIngestionMeters", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDE5OA=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNjMxMTQ2OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/StreamChunkParser.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMDozMDozNFrOHMEsIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMTo0MjozN1rOHMJmdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQyMTc5NQ==", "bodyText": "I don't think we should use a Predicate<InputRow> here and in other similar places. Throwing a ParseException is expected behavior, so I think callers should know that they need to handle that exception or throw it themselves.\nIt's not obvious to me how the caller in FilteringCloseableInputRowIterator should know they should handle a parse exception https://github.com/apache/druid/pull/10336/files#diff-b7f61f3d28afdd25bedf2efa607fdf88R67\nHow can I check that the ParseException is handled at the correct level everywhere? I realize this is challenging because ParseException is a RuntimeException.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482421795", "createdAt": "2020-09-02T20:30:34Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/StreamChunkParser.java", "diffHunk": "@@ -54,12 +64,16 @@\n       @Nullable InputFormat inputFormat,\n       InputRowSchema inputRowSchema,\n       TransformSpec transformSpec,\n-      File indexingTmpDir\n+      File indexingTmpDir,\n+      Predicate<InputRow> rowFilter,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cdc6434420eb564a8c510abfd99b60d66b708b9"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUwMjI2MA==", "bodyText": "I don't think we should use a Predicate here and in other similar places. Throwing a ParseException is expected behavior, so I think callers should know that they need to handle that exception or throw it themselves.\nIt's not obvious to me how the caller in FilteringCloseableInputRowIterator should know they should handle a parse exception https://github.com/apache/druid/pull/10336/files#diff-b7f61f3d28afdd25bedf2efa607fdf88R67\n\nHmm, I'm not sure I understand your concern. Can you elaborate more?. rowFilter is a simple predicate to filter out unnecessary rows and does nothing with parseExceptions. Instead, it just passes the ParseException to the caller if it is thrown in test(). The intention here is that the callers of FilteringCloseableInputRowIterator can iterate rows without worrying about validation of rows or parseExceptions.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482502260", "createdAt": "2020-09-02T21:42:37Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/StreamChunkParser.java", "diffHunk": "@@ -54,12 +64,16 @@\n       @Nullable InputFormat inputFormat,\n       InputRowSchema inputRowSchema,\n       TransformSpec transformSpec,\n-      File indexingTmpDir\n+      File indexingTmpDir,\n+      Predicate<InputRow> rowFilter,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQyMTc5NQ=="}, "originalCommit": {"oid": "7cdc6434420eb564a8c510abfd99b60d66b708b9"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMjM0NTkyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDoxNDozN1rOHM-5pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDoxNDozN1rOHM-5pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM3NTUyNw==", "bodyText": "Test coverage bot is complaining about a lack of coverage for this condition. I think we should add tests for these branches. Right now AppenderatorTest only tests the case where addResult.isRowAdded() is true.\nCovering all 4 branches should make the code coverage bot happy too.", "url": "https://github.com/apache/druid/pull/10336#discussion_r483375527", "createdAt": "2020-09-04T04:14:37Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java", "diffHunk": "@@ -267,6 +275,12 @@ public AppenderatorAddResult add(\n       throw new SegmentNotWritableException(\"Attempt to add row to swapped-out sink for segment[%s].\", identifier);\n     }\n \n+    if (addResult.isRowAdded()) {\n+      rowIngestionMeters.incrementProcessed();\n+    } else if (addResult.hasParseException()) {\n+      parseExceptionHandler.handle(addResult.getParseException());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyOTUyNDEzOnYy", "diffSide": "RIGHT", "path": "processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjoyOTowN1rOHN9kTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNzowNDoxOFrOHOmVFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQwMjI1Mw==", "bodyText": "If we never intend to log/report this exception, we can remove the try/catch clause entirely.", "url": "https://github.com/apache/druid/pull/10336#discussion_r484402253", "createdAt": "2020-09-07T12:29:07Z", "author": {"login": "liran-funaro"}, "path": "processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java", "diffHunk": "@@ -227,9 +224,7 @@ protected AddToFactsResult addToFacts(\n           }\n           catch (ParseException e) {\n             // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-            if (getReportParseExceptions()) {\n-              throw e;\n-            }\n+            throw e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA3MDEwMA==", "bodyText": "Good catch. Fixed.", "url": "https://github.com/apache/druid/pull/10336#discussion_r485070100", "createdAt": "2020-09-08T17:04:18Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java", "diffHunk": "@@ -227,9 +224,7 @@ protected AddToFactsResult addToFacts(\n           }\n           catch (ParseException e) {\n             // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-            if (getReportParseExceptions()) {\n-              throw e;\n-            }\n+            throw e;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQwMjI1Mw=="}, "originalCommit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3140, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}