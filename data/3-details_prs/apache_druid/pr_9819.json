{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEyNzM0NTg5", "number": 9819, "title": "refactor SeekableStreamSupervisor usage of RecordSupplier", "bodyText": "Fixes #TBD. (need to open an issue about an issue that occur in kinesis ingestion in streams with a large number of shards).\nDescription\nThis PR refactors and fixes a number of issues with SeekableStreamSupervisor, and the Kinesis and Kafka indexing services built on top of it.\nSeekableStreamSupervisor and RecordSupplier\nThe manner in which SeekableSteamSupervisor uses the RecordSupplier has been heavily refactored to be much more chill. Prior to this patch, the main run thread, http threads, and background threads of the SeekableSteamSupervisor would all fight over the RecordSupplier with conflicting assignments, despite the fact that all usages are for the same set of partitions it has assigned to the tasks it is monitoring. This was extra unchill to the kinesis supervisor, which these assignments and subsequent seeks would result in creating and destroying a threadpool  executor multiple times even in a single supervisor run cycle.\nA new method, assignRecordSupplierToPartitionIds has been added which is used to assign the partitionIds to the recordSupplier whenever partitionIds changes, to ensure that the list of partitions we care about are always known to the recordSupplier. Prior users of recordSupplier.assign now count on the partition being known by the recordSupplier, and everything plays together much more harmoniously.\nAdditionally, recordSupplierLock has been changed to a ReentrantLock, to prevent the chance that the same thread can end up deadlocked by making calls to various internal supervisor methods that interact with the recordSupplier.\nSupervisorStateManager.isSteadyState()\nAn additional method has been added to SupervisorStateManager to allow SeekableStreamSupervisor background threads which collect information and emit metrics to be more considerate of supervisor state transitions, and just chill out while the supervisor reconfigures its internal state (no need to emit lag metrics while we are still creating tasks for example). This is just a convenience method to compare the current state to healthySteadyState.\nupdateLatestOffsetsFromStream -> updatePartitionLagFromStream\nupdateLatestOffsetsFromStream has been reworked into updatePartitionLagFromStream, since kinesis doesn't currently have a use for fetching the latest offsets since it can't compare them to the current offsets to compute record lag, so this change allows kafka to fetch the latest offsets, and kinesis to compute the partition time lag instead, dramatically cutting down on the number of AWS api calls and reducing the likely-hood of being rate limited.\nKinesis time lag metric\nThe mechanism that collects the Kinesis supervisor has been reworked significantly, to no longer require holding a lock on the supplier since the method is stateless instead of using the underlying PartitionResources it has been assigned, meaning it can take advantage fo the thread-safe kinesis client to fetch the lag. Additionally, it now only fetches the minimum number of records, 1, instead of the default of 10k. Finally, it is now aware of useEarliestSequenceNumber configuration, and will short circuit and return 0L as the lag for the partition in the case where useEarliestSequenceNumber is false.\nKinesis indexing task\nThe KinesisRecordSupplier used by the KinesisIndexTask will now use a much less aggro Runtime.getRuntime().availableProcessors() * 2 as the default number of fetch threads instead of the number of partitions, which in the case of hundreds of partitions would result in hundreds of threads and finally, a dramatically increased chance for AWS api limit exceptions.\nOther KinesisRecordSupplier stuff\nStarting and stopping the KinesisRecordSupplier has been simplified to use AtomicBoolean.compareAndSet instead of a handful of volatile boolean primitives, to decrease the chance of race conditions, and make the logic less confusing to follow. Additionally, the supervisor now supplies 0 as the number of fetchThreads, since it doesn't have need to utilize background fetching at all, so we can avoid creating the threadpool entirely. Finally, I added a bunch of javadocs to make future incursions into this code much easier.\n\nThis PR has:\n\n been self-reviewed.\n\n using the concurrency checklist (Remove this item if the PR doesn't have any relation to concurrency.)\n\n\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths.\n added integration tests.\n been tested in a test Druid cluster.\n\n\nKey changed/added classes in this PR\n\nSeekableStreamSupervisor\nKinesisSupervisor\nKinesisRecordSupplier\nKafkaSupervisor", "createdAt": "2020-05-04T04:42:41Z", "url": "https://github.com/apache/druid/pull/9819", "merged": true, "mergeCommit": {"oid": "2e9548d93d5a27e824cc93293e1de55af63d158f"}, "closed": true, "closedAt": "2020-05-16T21:09:40Z", "author": {"login": "clintropolis"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcd36nGgH2gAyNDEyNzM0NTg5OjMxMDBlZmM3OGMyZWRjZWRjMTIzNTg4ZGRjMDJmM2EyYzdhZTlkN2M=", "endCursor": "Y3Vyc29yOnYyOpPPAAABchomewgFqTQxMjk2NjkxNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3100efc78c2edcedc123588ddc02f3a2c7ae9d7c", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/3100efc78c2edcedc123588ddc02f3a2c7ae9d7c", "committedDate": "2020-05-04T04:38:41Z", "message": "refactor SeekableStreamSupervisor usage of RecordSupplier to reduce contention between background threads and main thread, refactor KinesisRecordSupplier, refactor Kinesis lag metric collection and emitting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b88703d420a962fab9a99ea33b15543c8bbb4b45", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/b88703d420a962fab9a99ea33b15543c8bbb4b45", "committedDate": "2020-05-04T08:16:14Z", "message": "fix style and test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b34a585f996057a9fe360cd9eae639d771ddf22f", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/b34a585f996057a9fe360cd9eae639d771ddf22f", "committedDate": "2020-05-05T11:52:10Z", "message": "cleanup, refactor, javadocs, test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2MjcyNTI0", "url": "https://github.com/apache/druid/pull/9819#pullrequestreview-406272524", "createdAt": "2020-05-06T02:35:12Z", "commit": {"oid": "b34a585f996057a9fe360cd9eae639d771ddf22f"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwMjozNToxM1rOGRCM0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwMzoyNzo1MFrOGRC33w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUxNTAyNw==", "bodyText": "I think underscoreJoiner should be all caps", "url": "https://github.com/apache/druid/pull/9819#discussion_r420515027", "createdAt": "2020-05-06T02:35:13Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/indexer/TaskIdUtils.java", "diffHunk": "@@ -31,6 +32,8 @@\n {\n   private static final Pattern INVALIDCHARS = Pattern.compile(\"(?s).*[^\\\\S ].*\");\n \n+  private static final Joiner underscoreJoiner = Joiner.on(\"_\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b34a585f996057a9fe360cd9eae639d771ddf22f"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUyNjA0Nw==", "bodyText": "Suggest renaming this to not have Resources in the name since it doesn't use PartitionResource", "url": "https://github.com/apache/druid/pull/9819#discussion_r420526047", "createdAt": "2020-05-06T03:27:50Z", "author": {"login": "jon-wei"}, "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -679,87 +694,204 @@ public String getPosition(StreamPartition<String> partition)\n     );\n   }\n \n-  @Override\n-  public void close()\n+  /**\n+   * Fetch the partition lag, given a stream and set of current partition offsets. This operates independently from\n+   * the {@link PartitionResource} which have been assigned to this record supplier.\n+   */\n+  public Map<String, Long> getPartitionsTimeLag(String stream, Map<String, String> currentOffsets)\n   {\n-    if (this.closed) {\n-      return;\n-    }\n-\n-    assign(ImmutableSet.of());\n-\n-    scheduledExec.shutdown();\n-\n-    try {\n-      if (!scheduledExec.awaitTermination(EXCEPTION_RETRY_DELAY_MS, TimeUnit.MILLISECONDS)) {\n-        scheduledExec.shutdownNow();\n-      }\n-    }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"InterruptedException while shutting down\");\n-      throw new RuntimeException(e);\n+    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n+    for (Map.Entry<String, String> partitionOffset : currentOffsets.entrySet()) {\n+      StreamPartition<String> partition = new StreamPartition<>(stream, partitionOffset.getKey());\n+      long currentLag = getPartitionResourcesTimeLag(partition, partitionOffset.getValue());\n+      partitionLag.put(partitionOffset.getKey(), currentLag);\n     }\n-\n-    this.closed = true;\n+    return partitionLag;\n   }\n \n-  // this is only used for tests\n+  /**\n+   * This method is only used for tests to verify that {@link PartitionResource} in fact tracks it's current lag\n+   * as it is polled for records. This isn't currently used in production at all, but could be one day if we were\n+   * to prefer to get the lag from the running tasks in the same API call which fetches the current task offsets,\n+   * instead of directly calling the AWS Kinesis API with the offsets returned from those tasks\n+   * (see {@link #getPartitionsTimeLag}, which accepts a map of current partition offsets).\n+   */\n   @VisibleForTesting\n-  Map<String, Long> getPartitionTimeLag()\n+  Map<String, Long> getPartitionResourcesTimeLag()\n   {\n     return partitionResources.entrySet()\n                              .stream()\n                              .collect(\n-                                 Collectors.toMap(k -> k.getKey().getPartitionId(), k -> k.getValue().getPartitionTimeLag())\n+                                 Collectors.toMap(\n+                                     k -> k.getKey().getPartitionId(),\n+                                     k -> k.getValue().getPartitionTimeLag()\n+                                 )\n                              );\n   }\n \n-  public Map<String, Long> getPartitionTimeLag(Map<String, String> currentOffsets)\n-      throws InterruptedException, TimeoutException, ExecutionException\n+  @VisibleForTesting\n+  public int bufferSize()\n   {\n-    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n-\n-    List<Future<AbstractMap.SimpleEntry<String, Long>>> longo = scheduledExec.invokeAll(\n-        partitionResources.entrySet()\n-                          .stream()\n-                          .map(partition -> {\n-                            final String partitionId = partition.getKey().getPartitionId();\n-                            return partition.getValue().getLagCallable(partitionId, currentOffsets.get(partitionId));\n-                          })\n-                          .collect(Collectors.toList())\n-\n-    );\n-    for (Future<AbstractMap.SimpleEntry<String, Long>> future : longo) {\n-      AbstractMap.SimpleEntry<String, Long> result = future.get(fetchSequenceNumberTimeout, TimeUnit.MILLISECONDS);\n-      partitionLag.put(result.getKey(), result.getValue());\n-    }\n+    return records.size();\n+  }\n \n-    return partitionLag;\n+  @VisibleForTesting\n+  public boolean isBackgroundFetchRunning()\n+  {\n+    return partitionsFetchStarted.get();\n   }\n \n-  private void seekInternal(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n+  /**\n+   * Check that a {@link PartitionResource} has been assigned to this record supplier, and if so call\n+   * {@link PartitionResource#seek} to move it to the latest offsets. Note that this method does not restart background\n+   * fetch, which should have been stopped prior to calling this method by a call to\n+   * {@link #filterBufferAndResetBackgroundFetch}.\n+   */\n+  private void partitionSeek(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n   {\n     PartitionResource resource = partitionResources.get(partition);\n     if (resource == null) {\n       throw new ISE(\"Partition [%s] has not been assigned\", partition);\n     }\n+    resource.seek(iteratorEnum, sequenceNumber);\n+  }\n \n-    log.debug(\n-        \"Seeking partition [%s] to [%s]\",\n-        partition.getPartitionId(),\n-        sequenceNumber != null ? sequenceNumber : iteratorEnum.toString()\n-    );\n+  /**\n+   * Given a partition and a {@link ShardIteratorType}, create a shard iterator and fetch\n+   * {@link #FETCH_SEQUENCE_NUMBER_RECORD_COUNT} records and return the first sequence number from the result set.\n+   * This method is thread safe as it does not depend on the internal state of the supplier (it doesn't use the\n+   * {@link PartitionResource} which have been assigned to the supplier), and the Kinesis client is thread safe.\n+   */\n+  @Nullable\n+  private String getSequenceNumber(StreamPartition<String> partition, ShardIteratorType iteratorEnum)\n+  {\n+    return wrapExceptions(() -> {\n+      String shardIterator =\n+          kinesis.getShardIterator(partition.getStream(), partition.getPartitionId(), iteratorEnum.toString())\n+                 .getShardIterator();\n+      long timeoutMillis = System.currentTimeMillis() + fetchSequenceNumberTimeout;\n+      GetRecordsResult recordsResult = null;\n+\n+      while (shardIterator != null && System.currentTimeMillis() < timeoutMillis) {\n+\n+        if (closed) {\n+          log.info(\"KinesisRecordSupplier closed while fetching sequenceNumber\");\n+          return null;\n+        }\n+        try {\n+          // we call getRecords with limit 1000 to make sure that we can find the first (earliest) record in the shard.\n+          // In the case where the shard is constantly removing records that are past their retention period, it is possible\n+          // that we never find the first record in the shard if we use a limit of 1.\n+          recordsResult = kinesis.getRecords(\n+              new GetRecordsRequest().withShardIterator(shardIterator).withLimit(FETCH_SEQUENCE_NUMBER_RECORD_COUNT)\n+          );\n+        }\n+        catch (ProvisionedThroughputExceededException e) {\n+          log.warn(\n+              e,\n+              \"encountered ProvisionedThroughputExceededException while fetching records, this means \"\n+              + \"that the request rate for the stream is too high, or the requested data is too large for \"\n+              + \"the available throughput. Reduce the frequency or size of your requests. Consider increasing \"\n+              + \"the number of shards to increase throughput.\"\n+          );\n+          try {\n+            Thread.sleep(PROVISIONED_THROUGHPUT_EXCEEDED_BACKOFF_MS);\n+            continue;\n+          }\n+          catch (InterruptedException e1) {\n+            log.warn(e1, \"Thread interrupted!\");\n+            Thread.currentThread().interrupt();\n+            break;\n+          }\n+        }\n \n-    resource.shardIterator = wrapExceptions(() -> kinesis.getShardIterator(\n-        partition.getStream(),\n-        partition.getPartitionId(),\n-        iteratorEnum.toString(),\n-        sequenceNumber\n-    ).getShardIterator());\n+        List<Record> records = recordsResult.getRecords();\n+\n+        if (!records.isEmpty()) {\n+          return records.get(0).getSequenceNumber();\n+        }\n+\n+        shardIterator = recordsResult.getNextShardIterator();\n+      }\n+\n+      if (shardIterator == null) {\n+        log.info(\"Partition[%s] returned a null shard iterator, is the shard closed?\", partition.getPartitionId());\n+        return KinesisSequenceNumber.END_OF_SHARD_MARKER;\n+      }\n+\n+\n+      // if we reach here, it usually means either the shard has no more records, or records have not been\n+      // added to this shard\n+      log.warn(\n+          \"timed out while trying to fetch position for shard[%s], millisBehindLatest is [%s], likely no more records in shard\",\n+          partition.getPartitionId(),\n+          recordsResult != null ? recordsResult.getMillisBehindLatest() : \"UNKNOWN\"\n+      );\n+      return null;\n+    });\n   }\n \n-  private void filterBufferAndResetFetchRunnable(Set<StreamPartition<String>> partitions) throws InterruptedException\n+  /**\n+   * Given a {@link StreamPartition} and an offset, create a 'shard iterator' for the offset and fetch a single record\n+   * in order to get the lag: {@link GetRecordsResult#getMillisBehindLatest()}. This method is thread safe as it does\n+   * not depend on the internal state of the supplier (it doesn't use the {@link PartitionResource} which have been\n+   * assigned to the supplier), and the Kinesis client is thread safe.\n+   */\n+  private Long getPartitionResourcesTimeLag(StreamPartition<String> partition, String offset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b34a585f996057a9fe360cd9eae639d771ddf22f"}, "originalPosition": 680}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59e5156a21a641937ad9eb8c5b812d6de101062f", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/59e5156a21a641937ad9eb8c5b812d6de101062f", "committedDate": "2020-05-07T07:25:00Z", "message": "fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1c5e732dca61246bb3bb819515331b083923a5f", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/f1c5e732dca61246bb3bb819515331b083923a5f", "committedDate": "2020-05-07T07:38:21Z", "message": "keep collecting current offsets and lag if unhealthy in background reporting thread"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/2b175775e1a3168a8ce6ca4c816bdb69a9d6870f", "committedDate": "2020-05-12T19:39:33Z", "message": "Merge remote-tracking branch 'upstream/master' into kinesis-fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMjE3MzQ3", "url": "https://github.com/apache/druid/pull/9819#pullrequestreview-412217347", "createdAt": "2020-05-14T22:03:23Z", "commit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMDgxMTAy", "url": "https://github.com/apache/druid/pull/9819#pullrequestreview-412081102", "createdAt": "2020-05-14T18:34:39Z", "commit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxODozNDozOVrOGVpTBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMTozOTo1OFrOGVvNBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM0OTg5Mw==", "bodyText": "Should this method compute the lag instead of latestSequenceFromStream? This will modify the current behavior since there could be some difference between computed lag and actual lag as highestCurrentOffsets will be computed periodically. However, I think it would be fine since 1) the behavior of this method is consistent across Kafka and Kinesis and 2) the lag metric doesn't have to be very real time.", "url": "https://github.com/apache/druid/pull/9819#discussion_r425349893", "createdAt": "2020-05-14T18:34:39Z", "author": {"login": "jihoonson"}, "path": "extensions-core/kafka-indexing-service/src/main/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisor.java", "diffHunk": "@@ -334,16 +334,35 @@ protected boolean useExclusiveStartSequenceNumberForNonFirstSequence()\n   }\n \n   @Override\n-  protected void updateLatestSequenceFromStream(\n-      RecordSupplier<Integer, Long> recordSupplier,\n-      Set<StreamPartition<Integer>> partitions\n-  )\n+  protected void updatePartitionLagFromStream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQyMzc5MA==", "bodyText": "Seems worth canceling a ScheduledFuture for fetchRecords() here since it can be called in assign().", "url": "https://github.com/apache/druid/pull/9819#discussion_r425423790", "createdAt": "2020-05-14T20:53:41Z", "author": {"login": "jihoonson"}, "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -112,100 +145,75 @@ private static boolean isServiceExceptionRecoverable(AmazonServiceException ex)\n     // to indicate that this shard has no more records to read\n     @Nullable\n     private volatile String shardIterator;\n-    private volatile boolean started;\n-    private volatile boolean stopRequested;\n-\n     private volatile long currentLagMillis;\n \n-    PartitionResource(StreamPartition<String> streamPartition)\n+    private final AtomicBoolean fetchStarted = new AtomicBoolean();\n+\n+    private PartitionResource(StreamPartition<String> streamPartition)\n     {\n       this.streamPartition = streamPartition;\n     }\n \n-    void startBackgroundFetch()\n+    private void startBackgroundFetch()\n     {\n-      if (started) {\n+      if (!backgroundFetchEnabled) {\n         return;\n       }\n+      // if seek has been called\n+      if (shardIterator == null) {\n+        log.warn(\n+            \"Skipping background fetch for stream[%s] partition[%s] since seek has not been called for this partition\",\n+            streamPartition.getStream(),\n+            streamPartition.getPartitionId()\n+        );\n+        return;\n+      }\n+      if (fetchStarted.compareAndSet(false, true)) {\n+        log.info(\n+            \"Starting scheduled fetch for stream[%s] partition[%s]\",\n+            streamPartition.getStream(),\n+            streamPartition.getPartitionId()\n+        );\n \n-      log.info(\n-          \"Starting scheduled fetch runnable for stream[%s] partition[%s]\",\n-          streamPartition.getStream(),\n-          streamPartition.getPartitionId()\n-      );\n-\n-      stopRequested = false;\n-      started = true;\n-\n-      rescheduleRunnable(fetchDelayMillis);\n-    }\n-\n-    void stopBackgroundFetch()\n-    {\n-      log.info(\n-          \"Stopping scheduled fetch runnable for stream[%s] partition[%s]\",\n-          streamPartition.getStream(),\n-          streamPartition.getPartitionId()\n-      );\n-      stopRequested = true;\n+        scheduleBackgroundFetch(fetchDelayMillis);\n+      }\n     }\n \n-    long getPartitionTimeLag()\n+    private void stopBackgroundFetch()\n     {\n-      return currentLagMillis;\n+      if (fetchStarted.compareAndSet(true, false)) {\n+        log.info(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQzMzY2Nw==", "bodyText": "Hmm, would it be nice if backgroundFetchEnabled is checked in this method or in the callers of this method to make it clear that this method will do nothing if backgroundFetchEnabled = false?", "url": "https://github.com/apache/druid/pull/9819#discussion_r425433667", "createdAt": "2020-05-14T21:12:33Z", "author": {"login": "jihoonson"}, "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -847,36 +881,46 @@ private void checkIfClosed()\n   }\n \n   /**\n-   * Returns an array with the content between the position and limit of \"buffer\". This may be the buffer's backing\n-   * array itself. Does not modify position or limit of the buffer.\n+   * This method must be called before a seek operation ({@link #seek}, {@link #seekToLatest}, or\n+   * {@link #seekToEarliest}).\n+   *\n+   * When called, it will nuke the {@link #scheduledExec} that is shared by all {@link PartitionResource}, filters\n+   * records from the buffer for partitions which will have a seek operation performed, and stops background fetch for\n+   * each {@link PartitionResource} to prepare for the seek. If background fetch is not currently running, the\n+   * {@link #scheduledExec} will not be re-created.\n    */\n-  private static byte[] toByteArray(final ByteBuffer buffer)\n+  private void filterBufferAndResetBackgroundFetch(Set<StreamPartition<String>> partitions) throws InterruptedException\n   {\n-    if (buffer.hasArray()\n-        && buffer.arrayOffset() == 0\n-        && buffer.position() == 0\n-        && buffer.array().length == buffer.limit()) {\n-      return buffer.array();\n-    } else {\n-      final byte[] retVal = new byte[buffer.remaining()];\n-      buffer.duplicate().get(retVal);\n-      return retVal;\n-    }\n-  }\n+    checkIfClosed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "originalPosition": 865}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0MDEzMA==", "bodyText": "nit: wondering if it's better to use fuzzy exponential backoff using RetryUtils. I guess it could be better by avoiding retries which could be repeated at the same time.", "url": "https://github.com/apache/druid/pull/9819#discussion_r425440130", "createdAt": "2020-05-14T21:25:53Z", "author": {"login": "jihoonson"}, "path": "extensions-core/kinesis-indexing-service/src/main/java/org/apache/druid/indexing/kinesis/KinesisRecordSupplier.java", "diffHunk": "@@ -665,180 +694,185 @@ public String getPosition(StreamPartition<String> partition)\n     );\n   }\n \n-  @Override\n-  public void close()\n+  /**\n+   * Fetch the partition lag, given a stream and set of current partition offsets. This operates independently from\n+   * the {@link PartitionResource} which have been assigned to this record supplier.\n+   */\n+  public Map<String, Long> getPartitionsTimeLag(String stream, Map<String, String> currentOffsets)\n   {\n-    if (this.closed) {\n-      return;\n-    }\n-\n-    assign(ImmutableSet.of());\n-\n-    scheduledExec.shutdown();\n-\n-    try {\n-      if (!scheduledExec.awaitTermination(EXCEPTION_RETRY_DELAY_MS, TimeUnit.MILLISECONDS)) {\n-        scheduledExec.shutdownNow();\n-      }\n-    }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"InterruptedException while shutting down\");\n-      throw new RuntimeException(e);\n+    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n+    for (Map.Entry<String, String> partitionOffset : currentOffsets.entrySet()) {\n+      StreamPartition<String> partition = new StreamPartition<>(stream, partitionOffset.getKey());\n+      long currentLag = getPartitionTimeLag(partition, partitionOffset.getValue());\n+      partitionLag.put(partitionOffset.getKey(), currentLag);\n     }\n-\n-    this.closed = true;\n+    return partitionLag;\n   }\n \n-  // this is only used for tests\n+  /**\n+   * This method is only used for tests to verify that {@link PartitionResource} in fact tracks it's current lag\n+   * as it is polled for records. This isn't currently used in production at all, but could be one day if we were\n+   * to prefer to get the lag from the running tasks in the same API call which fetches the current task offsets,\n+   * instead of directly calling the AWS Kinesis API with the offsets returned from those tasks\n+   * (see {@link #getPartitionsTimeLag}, which accepts a map of current partition offsets).\n+   */\n   @VisibleForTesting\n-  Map<String, Long> getPartitionTimeLag()\n+  Map<String, Long> getPartitionResourcesTimeLag()\n   {\n     return partitionResources.entrySet()\n                              .stream()\n                              .collect(\n-                                 Collectors.toMap(k -> k.getKey().getPartitionId(), k -> k.getValue().getPartitionTimeLag())\n+                                 Collectors.toMap(\n+                                     k -> k.getKey().getPartitionId(),\n+                                     k -> k.getValue().getPartitionTimeLag()\n+                                 )\n                              );\n   }\n \n-  public Map<String, Long> getPartitionTimeLag(Map<String, String> currentOffsets)\n+  @VisibleForTesting\n+  public int bufferSize()\n   {\n-    Map<String, Long> partitionLag = Maps.newHashMapWithExpectedSize(currentOffsets.size());\n-    for (Map.Entry<StreamPartition<String>, PartitionResource> partition : partitionResources.entrySet()) {\n-      final String partitionId = partition.getKey().getPartitionId();\n-      partitionLag.put(partitionId, partition.getValue().getPartitionTimeLag(currentOffsets.get(partitionId)));\n-    }\n-    return partitionLag;\n+    return records.size();\n   }\n \n-  private void seekInternal(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n+  @VisibleForTesting\n+  public boolean isBackgroundFetchRunning()\n   {\n-    PartitionResource resource = partitionResources.get(partition);\n-    if (resource == null) {\n-      throw new ISE(\"Partition [%s] has not been assigned\", partition);\n-    }\n-\n-    log.debug(\n-        \"Seeking partition [%s] to [%s]\",\n-        partition.getPartitionId(),\n-        sequenceNumber != null ? sequenceNumber : iteratorEnum.toString()\n-    );\n-\n-    resource.shardIterator = wrapExceptions(() -> kinesis.getShardIterator(\n-        partition.getStream(),\n-        partition.getPartitionId(),\n-        iteratorEnum.toString(),\n-        sequenceNumber\n-    ).getShardIterator());\n-\n-    checkPartitionsStarted = true;\n+    return partitionsFetchStarted.get();\n   }\n \n-  private void filterBufferAndResetFetchRunnable(Set<StreamPartition<String>> partitions) throws InterruptedException\n+  /**\n+   * Check that a {@link PartitionResource} has been assigned to this record supplier, and if so call\n+   * {@link PartitionResource#seek} to move it to the latest offsets. Note that this method does not restart background\n+   * fetch, which should have been stopped prior to calling this method by a call to\n+   * {@link #filterBufferAndResetBackgroundFetch}.\n+   */\n+  private void partitionSeek(StreamPartition<String> partition, String sequenceNumber, ShardIteratorType iteratorEnum)\n   {\n-    scheduledExec.shutdown();\n-\n-    try {\n-      if (!scheduledExec.awaitTermination(EXCEPTION_RETRY_DELAY_MS, TimeUnit.MILLISECONDS)) {\n-        scheduledExec.shutdownNow();\n-      }\n-    }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"InterruptedException while shutting down\");\n-      throw e;\n+    PartitionResource resource = partitionResources.get(partition);\n+    if (resource == null) {\n+      throw new ISE(\"Partition [%s] has not been assigned\", partition);\n     }\n-\n-    scheduledExec = Executors.newScheduledThreadPool(\n-        fetchThreads,\n-        Execs.makeThreadFactory(\"KinesisRecordSupplier-Worker-%d\")\n-    );\n-\n-    // filter records in buffer and only retain ones whose partition was not seeked\n-    BlockingQueue<OrderedPartitionableRecord<String, String>> newQ = new LinkedBlockingQueue<>(recordBufferSize);\n-\n-    records.stream()\n-           .filter(x -> !partitions.contains(x.getStreamPartition()))\n-           .forEachOrdered(newQ::offer);\n-\n-    records = newQ;\n-\n-    // restart fetching threads\n-    partitionResources.values().forEach(x -> x.started = false);\n-    checkPartitionsStarted = true;\n-  }\n-\n-  @Nullable\n-  private String getSequenceNumberInternal(StreamPartition<String> partition, ShardIteratorType iteratorEnum)\n-  {\n-    return wrapExceptions(() -> getSequenceNumberInternal(\n-        partition,\n-        kinesis.getShardIterator(partition.getStream(), partition.getPartitionId(), iteratorEnum.toString())\n-               .getShardIterator()\n-    ));\n+    resource.seek(iteratorEnum, sequenceNumber);\n   }\n \n+  /**\n+   * Given a partition and a {@link ShardIteratorType}, create a shard iterator and fetch\n+   * {@link #FETCH_SEQUENCE_NUMBER_RECORD_COUNT} records and return the first sequence number from the result set.\n+   * This method is thread safe as it does not depend on the internal state of the supplier (it doesn't use the\n+   * {@link PartitionResource} which have been assigned to the supplier), and the Kinesis client is thread safe.\n+   */\n   @Nullable\n-  private String getSequenceNumberInternal(StreamPartition<String> partition, String shardIterator)\n+  private String getSequenceNumber(StreamPartition<String> partition, ShardIteratorType iteratorEnum)\n   {\n-    long timeoutMillis = System.currentTimeMillis() + fetchSequenceNumberTimeout;\n-    GetRecordsResult recordsResult = null;\n-\n-    while (shardIterator != null && System.currentTimeMillis() < timeoutMillis) {\n-\n-      if (closed) {\n-        log.info(\"KinesisRecordSupplier closed while fetching sequenceNumber\");\n-        return null;\n-      }\n-      try {\n-        // we call getRecords with limit 1000 to make sure that we can find the first (earliest) record in the shard.\n-        // In the case where the shard is constantly removing records that are past their retention period, it is possible\n-        // that we never find the first record in the shard if we use a limit of 1.\n-        recordsResult = kinesis.getRecords(new GetRecordsRequest().withShardIterator(shardIterator).withLimit(1000));\n-      }\n-      catch (ProvisionedThroughputExceededException e) {\n-        log.warn(\n-            e,\n-            \"encountered ProvisionedThroughputExceededException while fetching records, this means \"\n-            + \"that the request rate for the stream is too high, or the requested data is too large for \"\n-            + \"the available throughput. Reduce the frequency or size of your requests. Consider increasing \"\n-            + \"the number of shards to increase throughput.\"\n-        );\n+    return wrapExceptions(() -> {\n+      String shardIterator =\n+          kinesis.getShardIterator(partition.getStream(), partition.getPartitionId(), iteratorEnum.toString())\n+                 .getShardIterator();\n+      long timeoutMillis = System.currentTimeMillis() + fetchSequenceNumberTimeout;\n+      GetRecordsResult recordsResult = null;\n+\n+      while (shardIterator != null && System.currentTimeMillis() < timeoutMillis) {\n+\n+        if (closed) {\n+          log.info(\"KinesisRecordSupplier closed while fetching sequenceNumber\");\n+          return null;\n+        }\n         try {\n-          Thread.sleep(PROVISIONED_THROUGHPUT_EXCEEDED_BACKOFF_MS);\n-          continue;\n+          // we call getRecords with limit 1000 to make sure that we can find the first (earliest) record in the shard.\n+          // In the case where the shard is constantly removing records that are past their retention period, it is possible\n+          // that we never find the first record in the shard if we use a limit of 1.\n+          recordsResult = kinesis.getRecords(\n+              new GetRecordsRequest().withShardIterator(shardIterator).withLimit(FETCH_SEQUENCE_NUMBER_RECORD_COUNT)\n+          );\n         }\n-        catch (InterruptedException e1) {\n-          log.warn(e1, \"Thread interrupted!\");\n-          Thread.currentThread().interrupt();\n-          break;\n+        catch (ProvisionedThroughputExceededException e) {\n+          log.warn(\n+              e,\n+              \"encountered ProvisionedThroughputExceededException while fetching records, this means \"\n+              + \"that the request rate for the stream is too high, or the requested data is too large for \"\n+              + \"the available throughput. Reduce the frequency or size of your requests. Consider increasing \"\n+              + \"the number of shards to increase throughput.\"\n+          );\n+          try {\n+            Thread.sleep(PROVISIONED_THROUGHPUT_EXCEEDED_BACKOFF_MS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "originalPosition": 738}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ0NjY2Mg==", "bodyText": "This seems duplicate. Also compute() will execute the function and replace the value even when the key exists. You can do newlyDiscovered.computeIfAbsent(taskGroupId, ArrayList::new).add(partitionId);.", "url": "https://github.com/apache/druid/pull/9819#discussion_r425446662", "createdAt": "2020-05-14T21:39:58Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/supervisor/SeekableStreamSupervisor.java", "diffHunk": "@@ -1998,16 +2009,37 @@ private boolean updatePartitionDataFromStream()\n       partitionGroup.add(partitionId);\n \n       if (partitionOffsets.putIfAbsent(partitionId, getNotSetMarker()) == null) {\n-        log.info(\n+        log.debug(\n             \"New partition [%s] discovered for stream [%s], added to task group [%d]\",\n             partitionId,\n             ioConfig.getStream(),\n             taskGroupId\n         );\n+\n+        newlyDiscovered.compute(taskGroupId, (groupId, partitions) -> {\n+          if (partitions == null) {\n+            partitions = new ArrayList<>();\n+          }\n+          partitions.add(partitionId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b175775e1a3168a8ce6ca4c816bdb69a9d6870f"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b909e9e4cd0fd93b39d691a047757841bef41112", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/b909e9e4cd0fd93b39d691a047757841bef41112", "committedDate": "2020-05-15T09:17:15Z", "message": "review stuffs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56fcb2cb58535ed08a750b2c562cda452ce1eeb7", "author": {"user": {"login": "clintropolis", "name": "Clint Wylie"}}, "url": "https://github.com/apache/druid/commit/56fcb2cb58535ed08a750b2c562cda452ce1eeb7", "committedDate": "2020-05-15T09:21:48Z", "message": "add comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyOTY2OTE2", "url": "https://github.com/apache/druid/pull/9819#pullrequestreview-412966916", "createdAt": "2020-05-15T21:03:49Z", "commit": {"oid": "56fcb2cb58535ed08a750b2c562cda452ce1eeb7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2297, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}