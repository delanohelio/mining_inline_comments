{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3NzI3Mzk4", "number": 10171, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODoyOToxMlrOEOKHVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMTozNDo1NVrOEQzgDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjgxMjM4OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/docker-compose.base.yml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODoyOToxMlrOGxJqBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwMToyMzoxNFrOGxq0yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE5MTYyMA==", "bodyText": "doesn't need to be changed in this PR, but is this useful to include in the base compose file? I guess all the funny routers are here too, so not a big deal, but we might want to consider moving these definitions into the compose files that actually use them", "url": "https://github.com/apache/druid/pull/10171#discussion_r454191620", "createdAt": "2020-07-14T08:29:12Z", "author": {"login": "clintropolis"}, "path": "integration-tests/docker/docker-compose.base.yml", "diffHunk": "@@ -142,12 +142,30 @@ services:\n       - ./environment-configs/common\n       - ./environment-configs/historical\n \n+  druid-historical-for-query-retry-test:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDczNTA1MA==", "bodyText": "Yeah, I agree. Moved it to docker-compose.query-retry-test.yml.", "url": "https://github.com/apache/druid/pull/10171#discussion_r454735050", "createdAt": "2020-07-15T01:23:14Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/docker-compose.base.yml", "diffHunk": "@@ -142,12 +142,30 @@ services:\n       - ./environment-configs/common\n       - ./environment-configs/historical\n \n+  druid-historical-for-query-retry-test:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE5MTYyMA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjgzNDMyOnYy", "diffSide": "RIGHT", "path": "integration-tests/stop_cluster.sh", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODozNTowNVrOGxJ3UA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwMToyMzoxNlrOGxq01A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE5NTAyNA==", "bodyText": "I don't think this is necessary in this PR, but I think we should consider moving this cluster stop script to using docker compose to tear down the thing that it previously created", "url": "https://github.com/apache/druid/pull/10171#discussion_r454195024", "createdAt": "2020-07-14T08:35:05Z", "author": {"login": "clintropolis"}, "path": "integration-tests/stop_cluster.sh", "diffHunk": "@@ -20,7 +20,7 @@ if [ -n \"$DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TES\n     exit 0\n   fi\n \n-for node in druid-historical druid-coordinator druid-overlord druid-router druid-router-permissive-tls druid-router-no-client-auth-tls druid-router-custom-check-tls druid-broker druid-middlemanager druid-zookeeper-kafka druid-metadata-storage druid-it-hadoop;\n+for node in druid-historical druid-historical-for-query-retry-test druid-coordinator druid-overlord druid-router druid-router-permissive-tls druid-router-no-client-auth-tls druid-router-custom-check-tls druid-broker druid-middlemanager druid-zookeeper-kafka druid-metadata-storage druid-it-hadoop;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDczNTA2MA==", "bodyText": "Yeah that would be nice.", "url": "https://github.com/apache/druid/pull/10171#discussion_r454735060", "createdAt": "2020-07-15T01:23:16Z", "author": {"login": "jihoonson"}, "path": "integration-tests/stop_cluster.sh", "diffHunk": "@@ -20,7 +20,7 @@ if [ -n \"$DRUID_INTEGRATION_TEST_SKIP_RUN_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TES\n     exit 0\n   fi\n \n-for node in druid-historical druid-coordinator druid-overlord druid-router druid-router-permissive-tls druid-router-no-client-auth-tls druid-router-custom-check-tls druid-broker druid-middlemanager druid-zookeeper-kafka druid-metadata-storage druid-it-hadoop;\n+for node in druid-historical druid-historical-for-query-retry-test druid-coordinator druid-overlord druid-router druid-router-permissive-tls druid-router-no-client-auth-tls druid-router-custom-check-tls druid-broker druid-middlemanager druid-zookeeper-kafka druid-metadata-storage druid-it-hadoop;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE5NTAyNA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjgzOTUyOnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODozNjozOVrOGxJ6mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwMToyMzoyNVrOGxq0_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE5NTg2NA==", "bodyText": "what is this change for?", "url": "https://github.com/apache/druid/pull/10171#discussion_r454195864", "createdAt": "2020-07-14T08:36:39Z", "author": {"login": "clintropolis"}, "path": "processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java", "diffHunk": "@@ -138,7 +139,9 @@ public TimeseriesQueryQueryToolChest(TimeseriesQueryMetricsFactory queryMetricsF\n \n       final Sequence<Result<TimeseriesResultValue>> finalSequence;\n \n-      if (query.getGranularity().equals(Granularities.ALL) && !query.isSkipEmptyBuckets()) {\n+      if (query.getGranularity().equals(Granularities.ALL) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDczNTEwMw==", "bodyText": "To be more sql-compliant, the timeseries query returns something if there is no grouping key even when there is nothing to return. However, this behavior is not preferred I think when bySegment is set because, bySegment is used for caching in historicals or debugging where exact results are supposed to be returned. When caching is enabled, it will populate empty result in historicals after this change, but the broker will return the correct result since bySegment won't be set for it. I added some comment for it.", "url": "https://github.com/apache/druid/pull/10171#discussion_r454735103", "createdAt": "2020-07-15T01:23:25Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/query/timeseries/TimeseriesQueryQueryToolChest.java", "diffHunk": "@@ -138,7 +139,9 @@ public TimeseriesQueryQueryToolChest(TimeseriesQueryMetricsFactory queryMetricsF\n \n       final Sequence<Result<TimeseriesResultValue>> finalSequence;\n \n-      if (query.getGranularity().equals(Granularities.ALL) && !query.isSkipEmptyBuckets()) {\n+      if (query.getGranularity().equals(Granularities.ALL) &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE5NTg2NA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjkyMTYzOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/main/java/org/apache/druid/cli/MainForQueryRetryTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODo1OTowMFrOGxKt5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwMzozMDo0N1rOGzEJgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIwODk5OA==", "bodyText": "Is a custom main method easier than using theCliCommandCreator extension point? I thought you just need something like this:\npublic class QueryRetryTestCommandCreator implements CliCommandCreator\n{\n  @Override\n  public void addCommands(Cli.CliBuilder builder)\n  {\n    builder.withGroup(\"server\").withCommands(CliHistoricalForQueryRetryTest.class);\n  }\n}\nand a META-INF/services entry for it, but I can't remember, maybe there is something more to it", "url": "https://github.com/apache/druid/pull/10171#discussion_r454208998", "createdAt": "2020-07-14T08:59:00Z", "author": {"login": "clintropolis"}, "path": "integration-tests/src/main/java/org/apache/druid/cli/MainForQueryRetryTest.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.cli;\n+\n+import com.google.inject.Injector;\n+import io.airlift.airline.Cli;\n+import io.airlift.airline.Help;\n+import io.airlift.airline.ParseException;\n+import io.netty.util.SuppressForbidden;\n+import org.apache.druid.guice.ExtensionsConfig;\n+import org.apache.druid.guice.GuiceInjectors;\n+import org.apache.druid.initialization.Initialization;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.ServiceLoader;\n+\n+/**\n+ * Main class for query retry testing.\n+ *\n+ * @see CliHistoricalForQueryRetryTest\n+ */\n+public class MainForQueryRetryTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDczNjIwOQ==", "bodyText": "Thanks for pointing it out. I didn't know that is a thing. However, looking at that, it seems like a useful way to develop a custom service based on Druid's extension system. This will work for now since the integration tests load all extensions with the same class loader, but won't work once we fix them to use Druid's extension loading system. I would keep the current implementation for now.", "url": "https://github.com/apache/druid/pull/10171#discussion_r454736209", "createdAt": "2020-07-15T01:27:42Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/cli/MainForQueryRetryTest.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.cli;\n+\n+import com.google.inject.Injector;\n+import io.airlift.airline.Cli;\n+import io.airlift.airline.Help;\n+import io.airlift.airline.ParseException;\n+import io.netty.util.SuppressForbidden;\n+import org.apache.druid.guice.ExtensionsConfig;\n+import org.apache.druid.guice.GuiceInjectors;\n+import org.apache.druid.initialization.Initialization;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.ServiceLoader;\n+\n+/**\n+ * Main class for query retry testing.\n+ *\n+ * @see CliHistoricalForQueryRetryTest\n+ */\n+public class MainForQueryRetryTest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIwODk5OA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE5ODUzMA==", "bodyText": "Hmm, first of all, I didn't notice that our integration tests already use Druid's extension system. This is a pretty good improvement! \ud83d\ude42 Looking at this more, the suggested way seems working since it's the main thread which loads and executes this command. I have added an integration-tests extension and made it to be loaded for query-retry testing group.", "url": "https://github.com/apache/druid/pull/10171#discussion_r456198530", "createdAt": "2020-07-17T03:30:47Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/main/java/org/apache/druid/cli/MainForQueryRetryTest.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.cli;\n+\n+import com.google.inject.Injector;\n+import io.airlift.airline.Cli;\n+import io.airlift.airline.Help;\n+import io.airlift.airline.ParseException;\n+import io.netty.util.SuppressForbidden;\n+import org.apache.druid.guice.ExtensionsConfig;\n+import org.apache.druid.guice.GuiceInjectors;\n+import org.apache.druid.initialization.Initialization;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.ServiceLoader;\n+\n+/**\n+ * Main class for query retry testing.\n+ *\n+ * @see CliHistoricalForQueryRetryTest\n+ */\n+public class MainForQueryRetryTest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIwODk5OA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjk1MTM1OnYy", "diffSide": "RIGHT", "path": ".travis.yml", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwOTowNzowMFrOGxLABg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMzo0NzozN1rOG0jEPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIxMzYzOA==", "bodyText": "heh, this list is getting kind of long \ud83d\ude05", "url": "https://github.com/apache/druid/pull/10171#discussion_r454213638", "createdAt": "2020-07-14T09:07:00Z", "author": {"login": "clintropolis"}, "path": ".travis.yml", "diffHunk": "@@ -397,7 +405,7 @@ jobs:\n       name: \"(Compile=openjdk8, Run=openjdk8) other integration test\"\n       jdk: openjdk8\n       services: *integration_test_services\n-      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'\n+      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,query-retry,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDczNjIyNA==", "bodyText": "", "url": "https://github.com/apache/druid/pull/10171#discussion_r454736224", "createdAt": "2020-07-15T01:27:45Z", "author": {"login": "jihoonson"}, "path": ".travis.yml", "diffHunk": "@@ -397,7 +405,7 @@ jobs:\n       name: \"(Compile=openjdk8, Run=openjdk8) other integration test\"\n       jdk: openjdk8\n       services: *integration_test_services\n-      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'\n+      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,query-retry,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIxMzYzOA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcwNDcxMA==", "bodyText": "Maybe it's better to have a includedGroup list instead of excludedGroups. AFAIK, there is much more groups we are excluding compare to groups that we are adding to the \"other integration test\" over the last 6 months. Also, the list of groups you can add is somewhat limited as there is a timeout of 50 mins, hence you can probably only add a few more. On the other hand, the list of excludedGroups is infinite and can go on and on and on.", "url": "https://github.com/apache/druid/pull/10171#discussion_r457704710", "createdAt": "2020-07-20T21:34:03Z", "author": {"login": "maytasm"}, "path": ".travis.yml", "diffHunk": "@@ -397,7 +405,7 @@ jobs:\n       name: \"(Compile=openjdk8, Run=openjdk8) other integration test\"\n       jdk: openjdk8\n       services: *integration_test_services\n-      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'\n+      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,query-retry,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIxMzYzOA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc1MzY2Mg==", "bodyText": "Yeah it makes sense. There were not many tests to exclude when I added \"other integration test\", but now it is pretty long.", "url": "https://github.com/apache/druid/pull/10171#discussion_r457753662", "createdAt": "2020-07-20T23:47:37Z", "author": {"login": "jihoonson"}, "path": ".travis.yml", "diffHunk": "@@ -397,7 +405,7 @@ jobs:\n       name: \"(Compile=openjdk8, Run=openjdk8) other integration test\"\n       jdk: openjdk8\n       services: *integration_test_services\n-      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'\n+      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,input-format,perfect-rollup-parallel-batch-index,kafka-index,query,query-retry,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kinesis-data-format,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow,kafka-data-format,hadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage,aliyun-oss-deep-storage' JVM_RUNTIME='-Djvm.runtime=11'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIxMzYzOA=="}, "originalCommit": {"oid": "77712b6b6bac4ca457b8e9e107bacf4061a3c4d7"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NTY5Njg2OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/environment-configs/middlemanager", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxOToyNjo0NlrOG0cKnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMDo1NTowN1rOG0e8IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY0MDYwNw==", "bodyText": "I guess this technically isn't needed anymore, but maybe is still useful? Should we check if it isn't set and set this as a default value somewhere centralized?", "url": "https://github.com/apache/druid/pull/10171#discussion_r457640607", "createdAt": "2020-07-20T19:26:46Z", "author": {"login": "clintropolis"}, "path": "integration-tests/docker/environment-configs/middlemanager", "diffHunk": "@@ -19,6 +19,7 @@\n \n DRUID_SERVICE=middleManager\n DRUID_LOG_PATH=/shared/logs/middlemanager.log\n+DRUID_MAIN=org.apache.druid.cli.Main", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04695125277ebcb885b5e95efd0b702c8bd65b47"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY4NjA0OA==", "bodyText": "Thanks, I forgot to remove it. Since we can add custom services for integration tests, I'm not sure this variable could become useful for anything yet. I removed it for now. Maybe we can add it back later if needed.", "url": "https://github.com/apache/druid/pull/10171#discussion_r457686048", "createdAt": "2020-07-20T20:55:07Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/environment-configs/middlemanager", "diffHunk": "@@ -19,6 +19,7 @@\n \n DRUID_SERVICE=middleManager\n DRUID_LOG_PATH=/shared/logs/middlemanager.log\n+DRUID_MAIN=org.apache.druid.cli.Main", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY0MDYwNw=="}, "originalCommit": {"oid": "04695125277ebcb885b5e95efd0b702c8bd65b47"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDQ5ODQ1OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/environment-configs/historical-for-query-retry-test", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMToxNDoyOFrOG1KBXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxODo0ODo0MlrOG1vtHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5MTkwMA==", "bodyText": "address=5007 is already used by the default historical.", "url": "https://github.com/apache/druid/pull/10171#discussion_r458391900", "createdAt": "2020-07-21T21:14:28Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/historical-for-query-retry-test", "diffHunk": "@@ -0,0 +1,33 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+DRUID_SERVICE=historical-for-query-retry-test\n+DRUID_LOG_PATH=/shared/logs/historical-for-query-retry-test.log\n+\n+# JAVA OPTS\n+SERVICE_DRUID_JAVA_OPTS=-server -Xmx512m -Xms512m -XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseG1GC -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5007", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTMwOA==", "bodyText": "Good catch. Changed to 5010.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459009308", "createdAt": "2020-07-22T18:48:42Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/environment-configs/historical-for-query-retry-test", "diffHunk": "@@ -0,0 +1,33 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+DRUID_SERVICE=historical-for-query-retry-test\n+DRUID_LOG_PATH=/shared/logs/historical-for-query-retry-test.log\n+\n+# JAVA OPTS\n+SERVICE_DRUID_JAVA_OPTS=-server -Xmx512m -Xms512m -XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseG1GC -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5007", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5MTkwMA=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDUxMjc0OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/docker-compose.query-retry-test.yml", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMToxODo1M1rOG1KJ_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQyMDoyMTo1OVrOG1yzVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5NDEwOQ==", "bodyText": "If the only difference between this one and the base yml is the new druid-historical-for-query-retry-test, you can just have the new druid-historical-for-query-retry-test in a yml and run two yml when you want to start druid (docker-compose -f ${DOCKERDIR}/docker-compose.yml -f ${DOCKERDIR}/docker-compose.query-retry-test.yml  up -d", "url": "https://github.com/apache/druid/pull/10171#discussion_r458394109", "createdAt": "2020-07-21T21:18:53Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/docker-compose.query-retry-test.yml", "diffHunk": "@@ -0,0 +1,128 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+version: \"2.2\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5NDMwNw==", "bodyText": "This is to avoid repeating things like druid-zookeeper-kafka etc.", "url": "https://github.com/apache/druid/pull/10171#discussion_r458394307", "createdAt": "2020-07-21T21:19:17Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/docker-compose.query-retry-test.yml", "diffHunk": "@@ -0,0 +1,128 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+version: \"2.2\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5NDEwOQ=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTMzMA==", "bodyText": "This docker compose file doesn't include middleManager to save memory. I would like to exclude overlord as well, seems like I need a way to avoid the check in SuiteListener that waits for the overlord to be ready. I haven't looked at it closely yet.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459009330", "createdAt": "2020-07-22T18:48:45Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/docker-compose.query-retry-test.yml", "diffHunk": "@@ -0,0 +1,128 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+version: \"2.2\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5NDEwOQ=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA2MDA1Mw==", "bodyText": "I see. My only concern is duplicated code and the maintainability.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459060053", "createdAt": "2020-07-22T20:21:59Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/docker-compose.query-retry-test.yml", "diffHunk": "@@ -0,0 +1,128 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+version: \"2.2\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5NDEwOQ=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDU2NDYwOnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/druid.sh", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMTozNDo1NVrOG1KpJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQyMDo1MDowOVrOG1ztMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwMjA4NA==", "bodyText": "Don't have to be in this PR but maybe we should make this a flag passed in my mvn (like -Dstart.hadoop.docker=true). So something like -Ddruid.preload.test.data=true\nSo that this list doesn't get long in the future.", "url": "https://github.com/apache/druid/pull/10171#discussion_r458402084", "createdAt": "2020-07-21T21:34:55Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/druid.sh", "diffHunk": "@@ -82,14 +83,18 @@ setupData()\n   # The \"query\" and \"security\" test groups require data to be setup before running the tests.\n   # In particular, they requires segments to be download from a pre-existing s3 bucket.\n   # This is done by using the loadSpec put into metadatastore and s3 credientials set below.\n-  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then\n+  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query-retry\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwOTQxMA==", "bodyText": "Hmm, do you think there will be more test groups like these in the future? I'm not sure about it. A downside of using a flag is that you have to add the flag when you run a test manually. Also you have to be aware of that flag when you add a new test. If your concern is about code complexity, then we could provide a developer-friendly configuration where we can easily add or remove test groups which our docker script can handle in a more structured way.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459009410", "createdAt": "2020-07-22T18:48:53Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/druid.sh", "diffHunk": "@@ -82,14 +83,18 @@ setupData()\n   # The \"query\" and \"security\" test groups require data to be setup before running the tests.\n   # In particular, they requires segments to be download from a pre-existing s3 bucket.\n   # This is done by using the loadSpec put into metadatastore and s3 credientials set below.\n-  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then\n+  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query-retry\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwMjA4NA=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTQ0MA==", "bodyText": "I am imaging the flags as easy control to the Druid cluster. Basically, there's a list of all the flags on the README and if you are writing a new test group, you can easily pick and choose how you want the state of your Druid cluster to be. i.e. you need to start x service then set this flag, you need pre-load data set this flag, you need hadoop set this flag, you want indexer instead of MM set this flag, etc.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459059440", "createdAt": "2020-07-22T20:20:50Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/druid.sh", "diffHunk": "@@ -82,14 +83,18 @@ setupData()\n   # The \"query\" and \"security\" test groups require data to be setup before running the tests.\n   # In particular, they requires segments to be download from a pre-existing s3 bucket.\n   # This is done by using the loadSpec put into metadatastore and s3 credientials set below.\n-  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then\n+  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query-retry\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwMjA4NA=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA2MjU5MA==", "bodyText": "What is the main use case of the flag system? As what each test group needs are fixed (e.g., hadoop cluster must be started for hadoop task test), I'm not sure how it's better compared to that the docker script automatically launches necessary things depending on the test group.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459062590", "createdAt": "2020-07-22T20:26:58Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/druid.sh", "diffHunk": "@@ -82,14 +83,18 @@ setupData()\n   # The \"query\" and \"security\" test groups require data to be setup before running the tests.\n   # In particular, they requires segments to be download from a pre-existing s3 bucket.\n   # This is done by using the loadSpec put into metadatastore and s3 credientials set below.\n-  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then\n+  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query-retry\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwMjA4NA=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA2NzcxNw==", "bodyText": "I believe it might be easier for a person who does not know the internal working of our Docker/Compose scripts to write new integration tests. Basically you do not need to know where all the bit and pieces are like to preload the data you need to modify the if check in druid.sh, and to do other stuff maybe we in other places, etc.\nFor example, if I want to write a new integration test group that requires pre load data and hadoop running and maybe indexer running instead of MM, all of that might be in 3 different places in different files. With the flag system, you just need to specify what you want in the travis file for the mvn cmd.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459067717", "createdAt": "2020-07-22T20:36:43Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/druid.sh", "diffHunk": "@@ -82,14 +83,18 @@ setupData()\n   # The \"query\" and \"security\" test groups require data to be setup before running the tests.\n   # In particular, they requires segments to be download from a pre-existing s3 bucket.\n   # This is done by using the loadSpec put into metadatastore and s3 credientials set below.\n-  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then\n+  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query-retry\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwMjA4NA=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA3NDg2Ng==", "bodyText": "If your concern is about code complexity, then we could provide a developer-friendly configuration where we can easily add or remove test groups which our docker script can handle in a more structured way.\n\nYeah, this comment above was to provide developer-friendly configurations so that they can easily add new integration tests. I guess what I think is pretty similar to yours, something like a centralized configuration system where you can define what you need for your integration test so that you don't have to worry about flags or configs at test run time. What I'm not sure is if they should be runtime flags. Maybe they could be if it's easier to implement.", "url": "https://github.com/apache/druid/pull/10171#discussion_r459074866", "createdAt": "2020-07-22T20:50:09Z", "author": {"login": "jihoonson"}, "path": "integration-tests/docker/druid.sh", "diffHunk": "@@ -82,14 +83,18 @@ setupData()\n   # The \"query\" and \"security\" test groups require data to be setup before running the tests.\n   # In particular, they requires segments to be download from a pre-existing s3 bucket.\n   # This is done by using the loadSpec put into metadatastore and s3 credientials set below.\n-  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then\n+  if [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"query-retry\" ] || [ \"$DRUID_INTEGRATION_TEST_GROUP\" = \"security\" ]; then", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwMjA4NA=="}, "originalCommit": {"oid": "ef06f3b18a4dd8bfacc7f132d69853045598c12d"}, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2222, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}