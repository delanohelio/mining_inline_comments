{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkwODQ1NDE5", "number": 10419, "title": "Automatically determine numShards for parallel ingestion hash partitioning", "bodyText": "This PR allows parallel batch ingestion to automatically determine numShards when forceGuaranteedRollup is used with hash partitioning.\nThis is accomplished with a new phase of subtasks (PartialDimensionCardinalityTask). These subtasks build a map of <Interval, HyperLogLogCollector> where the HLL collector records the cardinality of the partitioning dimensions for each segment granularity interval in the input data.\nThe supervisor task aggregates the HLL collectors by interval, and determines the highest cardinality across all the intervals. This max cardinality is divided by maxRowsPerSegment to determine numShards automatically.\nThis PR has:\n\n been self-reviewed.\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths, ensuring the threshold for code coverage is met.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-09-22T10:36:56Z", "url": "https://github.com/apache/druid/pull/10419", "merged": true, "mergeCommit": {"oid": "cb30b1fe2353dc28601c50984def9e83adb89571"}, "closed": true, "closedAt": "2020-09-24T20:47:54Z", "author": {"login": "jon-wei"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLVZy3gH2gAyNDkwODQ1NDE5OjYxZjhjYWE1NTM3YjczMmRhY2Q2ZWNhY2IyMjg0MjE1MjQ1OGUxYTY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdMFZUBAFqTQ5NTg0MTAzNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "61f8caa5537b732dacd6ecacb22842152458e1a6", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/61f8caa5537b732dacd6ecacb22842152458e1a6", "committedDate": "2020-09-22T10:26:35Z", "message": "Automatically determine numShards for parallel ingestion hash partitioning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/85e06516fc84377eedf6b948ba2acd9fe946797e", "committedDate": "2020-09-22T20:22:22Z", "message": "Fix inspection, tests, coverage"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzODk5OTk0", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-493899994", "createdAt": "2020-09-22T22:53:31Z", "commit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMjo1MzozMlrOHWPEeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzo1MTozOVrOHWQMBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA3NzYyNg==", "bodyText": "Does it make sense to keep the @VisibleForTesting since this is now public (so that it can be used in PartialDimensionCardinalityTask)?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493077626", "createdAt": "2020-09-22T22:53:32Z", "author": {"login": "ccaominh"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions\n   }\n \n   @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n+  public static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc0Nw==", "bodyText": "Using \"single_dim\" instead of \"ranged\" in the error message maps better to the naming in the docs", "url": "https://github.com/apache/druid/pull/10419#discussion_r493080747", "createdAt": "2020-09-22T23:02:54Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MzUwMA==", "bodyText": "What do you think about changing the logging level to reduce the logging amount since this is printed for each row?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493083500", "createdAt": "2020-09-22T23:11:32Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4NTAzMQ==", "bodyText": "What do you think about using HllSketch instead of HyperLogLogCollector since HllSketch provides much more accurate estimates if the cardinality does not exceed the sketch's k value: http://datasketches.apache.org/docs/HLL/HllSketchVsDruidHyperLogLogCollector.html. Using HllSketch also will be more accurate and faster when the partial HLLs are merged.\nUsing HllSketch would mean that the implementation for parallel ingestion is different from the one for sequential ingestion though.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493085031", "createdAt": "2020-09-22T23:15:57Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HyperLogLogCollector> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      LOG.info(\"TS: \" + timestamp + \" INTV: \" + interval + \" GSC: \" + granularitySpec.getClass());\n+\n+      HyperLogLogCollector hllCollector = intervalToCardinalities.computeIfAbsent(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ==", "bodyText": "I believe the logic in PartialHashSegmentGenerateTask.isReady() will need to be adjusted; otherwise if PartialDimensionCardinalityTask runs and grabs the locks here it'll get stuck. For example, PartialRangeSegmentGenerateTask.isReady() does not grab the locks since they're acquired earlier by PartialDimensionDistributionTask.\nI think a good place to add a test would be in HashPartitionMultiPhaseParallelIndexingTest. Currently, its test cases all specify a value of 2 for numShards, so we can add cases with null.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493091201", "createdAt": "2020-09-22T23:36:13Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MjUyOQ==", "bodyText": "https://github.com/apache/druid/blob/master/docs/ingestion/native-batch.md#hash-based-partitioning needs to be updated to say that numShards is no longer required and also to mention the new partial dimension cardinality task.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493092529", "createdAt": "2020-09-22T23:40:12Z", "author": {"login": "ccaominh"}, "path": "core/src/main/java/org/apache/druid/indexer/partitions/HashedPartitionsSpec.java", "diffHunk": "@@ -160,7 +160,7 @@ public Integer getNumShards()\n   @Override\n   public String getForceGuaranteedRollupIncompatiblityReason()\n   {\n-    return getNumShards() == null ? NUM_SHARDS + \" must be specified\" : FORCE_GUARANTEED_ROLLUP_COMPATIBLE;\n+    return FORCE_GUARANTEED_ROLLUP_COMPATIBLE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NDc3NA==", "bodyText": "Checking isForceGuaranteedRollup() is probably redundant here", "url": "https://github.com/apache/druid/pull/10419#discussion_r493094774", "createdAt": "2020-09-22T23:47:41Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -499,17 +518,62 @@ private TaskStatus runMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n \n   private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n   {\n+    TaskState state;\n+\n+    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n+      // only range and hash partitioning is supported for multiphase parallel ingestion, see runMultiPhaseParallel()\n+      throw new ISE(\n+          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a ranged or hash partition spec.\",\n+          ingestionSchema.getTuningConfig().getPartitionsSpec()\n+      );\n+    }\n+\n+    final Integer numShardsOverride;\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n+    if (ingestionSchema.getTuningConfig().isForceGuaranteedRollup() && partitionsSpec.getNumShards() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5NTk0MQ==", "bodyText": "I like the extensive set of tests!", "url": "https://github.com/apache/druid/pull/10419#discussion_r493095941", "createdAt": "2020-09-22T23:51:39Z", "author": {"login": "ccaominh"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTaskTest.java", "diffHunk": "@@ -0,0 +1,398 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import org.apache.druid.client.indexing.NoopIndexingServiceClient;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.data.input.impl.InlineInputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskState;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.DynamicPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.PartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskInfoProvider;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.stats.DropwizardRowIngestionMetersFactory;\n+import org.apache.druid.indexing.common.task.IndexTaskClientFactory;\n+import org.apache.druid.java.util.common.Intervals;\n+import org.apache.druid.java.util.common.granularity.Granularities;\n+import org.apache.druid.segment.TestHelper;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.UniformGranularitySpec;\n+import org.apache.druid.testing.junit.LoggerCaptureRule;\n+import org.apache.logging.log4j.core.LogEvent;\n+import org.easymock.Capture;\n+import org.easymock.EasyMock;\n+import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n+import org.joda.time.Interval;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+@RunWith(Enclosed.class)\n+public class PartialDimensionCardinalityTaskTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 69}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ff17937ae0bce578d0e3c12dd36eb489c7bc884", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/1ff17937ae0bce578d0e3c12dd36eb489c7bc884", "committedDate": "2020-09-23T02:12:01Z", "message": "Docs and some PR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0125a564bb64478ef5ef0a5939df835506865dec", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/0125a564bb64478ef5ef0a5939df835506865dec", "committedDate": "2020-09-23T03:23:24Z", "message": "Adjust locking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66365b5eb3d90c30788c3e076e4a2fcaad1acc11", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/66365b5eb3d90c30788c3e076e4a2fcaad1acc11", "committedDate": "2020-09-23T04:08:43Z", "message": "Use HllSketch instead of HyperLogLogCollector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/6d694d2113a9bc55d981f3ec99620b007ef25e44", "committedDate": "2020-09-23T04:22:51Z", "message": "Fix tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk0NDkxNjQw", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-494491640", "createdAt": "2020-09-23T09:54:18Z", "commit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwOTo1NDoxOFrOHWiO5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwOTo1NDoxOFrOHWiO5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzM5MTU4OA==", "bodyText": "how about using 0 instead here?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493391588", "createdAt": "2020-09-23T09:54:18Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 150}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk0NTEzMTkx", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-494513191", "createdAt": "2020-09-23T10:22:36Z", "commit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMDoyMjozNlrOHWj18g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMDoyMjozNlrOHWj18g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzQxNzk3MA==", "bodyText": "out of curiosity, what is the effect if we pass jsonMapper.writeValueAsBytes(groupKey) directly to hllSketch? does it affect performance or accuracy in any way? hllSketch is already doing hashing on the byte array input.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493417970", "createdAt": "2020-09-23T10:22:36Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),\n+          inputRow\n+      );\n+\n+      try {\n+        hllSketch.update(\n+            IndexTask.HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(groupKey)).asBytes()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 226}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MDE0NTI5", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-495014529", "createdAt": "2020-09-23T20:15:58Z", "commit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MDIzODky", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-495023892", "createdAt": "2020-09-23T20:29:51Z", "commit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMDoyOTo1MlrOHW_ytQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMToyMzo1NlrOHXBfqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg3NTg5Mw==", "bodyText": "Please add the new parameter and its description.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493875893", "createdAt": "2020-09-23T20:29:52Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5MTQ2Ng==", "bodyText": "It seems like that it will guarantee that you will not have segments than the computed numShards, but it will not be guaranteed that number of rows per segment doesn't exceed maxRowsPerSegment since the partition dimensions can be skewed. Is this correct? Then, I suggest targetRowsPerSegment since it's not a hard limit.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493891466", "createdAt": "2020-09-23T20:58:59Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.\n+- The `partial_dimension_cardinality` phase is an optional phase that only runs if `numShards` is not specified.\n+The Parallel task splits the input data and assigns them to worker tasks based on the split hint spec.\n+Each worker task (type `partial_dimension_cardinality`) gathers estimates of partitioning dimensions cardinality for\n+each time chunk. The Parallel task will aggregate these estimates from the worker tasks and determine the highest\n+cardinality across all of the time chunks in the input data, dividing this cardinality by `maxRowsPerSegment` to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NDkwNQ==", "bodyText": "Hmm.. Should we fail instead? Since the timeline in the coordinator and the broker will explode if you have this many segments per interval.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493894905", "createdAt": "2020-09-23T21:05:39Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -582,6 +652,50 @@ private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) thro\n     return TaskStatus.fromCode(getId(), mergeState);\n   }\n \n+  @VisibleForTesting\n+  public static int determineNumShardsFromCardinalityReport(\n+      Collection<DimensionCardinalityReport> reports,\n+      int maxRowsPerSegment\n+  )\n+  {\n+    // aggregate all the sub-reports\n+    Map<Interval, Union> finalCollectors = new HashMap<>();\n+    reports.forEach(report -> {\n+      Map<Interval, byte[]> intervalToCardinality = report.getIntervalToCardinalities();\n+      for (Map.Entry<Interval, byte[]> entry : intervalToCardinality.entrySet()) {\n+        Union union = finalCollectors.computeIfAbsent(\n+            entry.getKey(),\n+            (key) -> {\n+              return new Union(DimensionCardinalityReport.HLL_SKETCH_LOG_K);\n+            }\n+        );\n+        HllSketch entryHll = HllSketch.wrap(Memory.wrap(entry.getValue()));\n+        union.update(entryHll);\n+      }\n+    });\n+\n+    // determine the highest cardinality in any interval\n+    long maxCardinality = Long.MIN_VALUE;\n+    for (Union union : finalCollectors.values()) {\n+      maxCardinality = Math.max(maxCardinality, (long) union.getEstimate());\n+    }\n+\n+    LOG.info(\"Estimated max cardinality: \" + maxCardinality);\n+\n+    // determine numShards based on maxRowsPerSegment and the highest per-interval cardinality\n+    long numShards = maxCardinality / maxRowsPerSegment;\n+    if (maxCardinality % maxRowsPerSegment != 0) {\n+      // if there's a remainder add 1 so we stay under maxRowsPerSegment\n+      numShards += 1;\n+    }\n+    try {\n+      return Math.toIntExact(numShards);\n+    }\n+    catch (ArithmeticException ae) {\n+      return Integer.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5NjY0Mw==", "bodyText": "When I wrote this, my intention was showing the name of phases (e.g., https://github.com/apache/druid/pull/10419/files#diff-05bbc55d565a3d9462353d9b4771cb09R34). This phase name is not shown anywhere currently, but will be available in the task live reports and metrics after #10352. How about removing underscores from the here too?", "url": "https://github.com/apache/druid/pull/10419#discussion_r493896643", "createdAt": "2020-09-23T21:09:08Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -294,11 +294,17 @@ How the worker task creates segments is:\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `hashed`|none|yes|\n-|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|yes|\n+|numShards|Directly specify the number of shards to create. If this is specified and `intervals` is specified in the `granularitySpec`, the index task can skip the determine intervals/partitions pass through the data.|null|no|\n |partitionDimensions|The dimensions to partition on. Leave blank to select all dimensions.|null|no|\n \n The Parallel task with hash-based partitioning is similar to [MapReduce](https://en.wikipedia.org/wiki/MapReduce).\n-The task runs in 2 phases, i.e., `partial segment generation` and `partial segment merge`.\n+The task runs in up to 3 phases: `partial_dimension_cardinality`, `partial segment generation` and `partial segment merge`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzg5OTg4Nw==", "bodyText": "I think acquiring locks here should be fine since it's idempotent. The supervisor task and all its subtasks share the same lock based on their groupId. I actually think it's better to call tryTimeChunkLock() in every subtask since it will make the task fail early when its lock is revoked. Otherwise, the task will fail when it publishes segments which happens at the last stage in batch ingestion.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493899887", "createdAt": "2020-09-23T21:15:18Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.hll.HyperLogLogCollector;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTIwMQ=="}, "originalCommit": {"oid": "85e06516fc84377eedf6b948ba2acd9fe946797e"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMTY0OA==", "bodyText": "This timestamp should be bucketed based on the query granularity. See https://github.com/apache/druid/blob/master/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java#L146.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493901648", "createdAt": "2020-09-23T21:19:17Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {\n+        continue;\n+      }\n+\n+      DateTime timestamp = inputRow.getTimestamp();\n+      //noinspection OptionalGetWithoutIsPresent (InputRowIterator returns rows with present intervals)\n+      Interval interval = granularitySpec.bucketInterval(timestamp).get();\n+\n+      HllSketch hllSketch = intervalToCardinalities.computeIfAbsent(\n+          interval,\n+          (intervalKey) -> {\n+            return DimensionCardinalityReport.createHllSketchForReport();\n+          }\n+      );\n+      List<Object> groupKey = HashBasedNumberedShardSpec.getGroupKey(\n+          partitionDimensions,\n+          interval.getStartMillis(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMjI3OQ==", "bodyText": "nit: inputRow is safely non-null since FilteringCloseableInputRowIterator filters out all null rows. See AbstractBatchIndexTask.defaultRowFilter().", "url": "https://github.com/apache/druid/pull/10419#discussion_r493902279", "createdAt": "2020-09-23T21:20:37Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .build()\n+    ) {\n+      Map<Interval, byte[]> cardinalities = determineCardinalities(\n+          iterator,\n+          granularitySpec,\n+          partitionDimensions\n+      );\n+\n+      sendReport(\n+          toolbox,\n+          new DimensionCardinalityReport(getId(), cardinalities)\n+      );\n+    }\n+\n+    return TaskStatus.success(getId());\n+  }\n+\n+  private Map<Interval, byte[]> determineCardinalities(\n+      HandlingInputRowIterator inputRowIterator,\n+      GranularitySpec granularitySpec,\n+      List<String> partitionDimensions\n+  )\n+  {\n+    Map<Interval, HllSketch> intervalToCardinalities = new HashMap<>();\n+    while (inputRowIterator.hasNext()) {\n+      InputRow inputRow = inputRowIterator.next();\n+      if (inputRow == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwMzc4NA==", "bodyText": "nit: DefaultIndexTaskInputRowIteratorBuilder effectively does nothing here since its core functionality has been moved to FilteringCloseableInputRowIterator in #10336. I haven't cleaned up this interface yet.  Now, it's only useful in range partitioning as some more useful inputRowHandlers are appended to the default builder.", "url": "https://github.com/apache/druid/pull/10419#discussion_r493903784", "createdAt": "2020-09-23T21:23:56Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionCardinalityTask.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.indexing.common.task.batch.parallel;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.datasketches.hll.HllSketch;\n+import org.apache.druid.data.input.HandlingInputRowIterator;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputSource;\n+import org.apache.druid.indexer.TaskStatus;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexing.common.TaskToolbox;\n+import org.apache.druid.indexing.common.actions.TaskActionClient;\n+import org.apache.druid.indexing.common.task.AbstractBatchIndexTask;\n+import org.apache.druid.indexing.common.task.ClientBasedTaskInfoProvider;\n+import org.apache.druid.indexing.common.task.IndexTask;\n+import org.apache.druid.indexing.common.task.TaskResource;\n+import org.apache.druid.indexing.common.task.batch.parallel.iterator.DefaultIndexTaskInputRowIteratorBuilder;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.segment.incremental.ParseExceptionHandler;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.indexing.DataSchema;\n+import org.apache.druid.segment.indexing.granularity.GranularitySpec;\n+import org.apache.druid.timeline.partition.HashBasedNumberedShardSpec;\n+import org.joda.time.DateTime;\n+import org.joda.time.Interval;\n+\n+import javax.annotation.Nullable;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class PartialDimensionCardinalityTask extends PerfectRollupWorkerTask\n+{\n+  public static final String TYPE = \"partial_dimension_cardinality\";\n+  private static final Logger LOG = new Logger(PartialDimensionCardinalityTask.class);\n+\n+  private final int numAttempts;\n+  private final ParallelIndexIngestionSpec ingestionSchema;\n+  private final String supervisorTaskId;\n+\n+  private final ObjectMapper jsonMapper;\n+\n+  @JsonCreator\n+  PartialDimensionCardinalityTask(\n+      // id shouldn't be null except when this task is created by ParallelIndexSupervisorTask\n+      @JsonProperty(\"id\") @Nullable String id,\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"resource\") final TaskResource taskResource,\n+      @JsonProperty(\"supervisorTaskId\") final String supervisorTaskId,\n+      @JsonProperty(\"numAttempts\") final int numAttempts, // zero-based counting\n+      @JsonProperty(\"spec\") final ParallelIndexIngestionSpec ingestionSchema,\n+      @JsonProperty(\"context\") final Map<String, Object> context,\n+      @JacksonInject ObjectMapper jsonMapper\n+  )\n+  {\n+    super(\n+        getOrMakeId(id, TYPE, ingestionSchema.getDataSchema().getDataSource()),\n+        groupId,\n+        taskResource,\n+        ingestionSchema.getDataSchema(),\n+        ingestionSchema.getTuningConfig(),\n+        context\n+    );\n+\n+    Preconditions.checkArgument(\n+        ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec,\n+        \"%s partitionsSpec required\",\n+        HashedPartitionsSpec.NAME\n+    );\n+\n+    this.numAttempts = numAttempts;\n+    this.ingestionSchema = ingestionSchema;\n+    this.supervisorTaskId = supervisorTaskId;\n+    this.jsonMapper = jsonMapper;\n+  }\n+\n+  @JsonProperty\n+  private int getNumAttempts()\n+  {\n+    return numAttempts;\n+  }\n+\n+  @JsonProperty(\"spec\")\n+  private ParallelIndexIngestionSpec getIngestionSchema()\n+  {\n+    return ingestionSchema;\n+  }\n+\n+  @JsonProperty\n+  private String getSupervisorTaskId()\n+  {\n+    return supervisorTaskId;\n+  }\n+\n+  @Override\n+  public String getType()\n+  {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public boolean isReady(TaskActionClient taskActionClient) throws Exception\n+  {\n+    return tryTimeChunkLock(\n+        taskActionClient,\n+        getIngestionSchema().getDataSchema().getGranularitySpec().inputIntervals()\n+    );\n+  }\n+\n+  @Override\n+  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n+  {\n+    DataSchema dataSchema = ingestionSchema.getDataSchema();\n+    GranularitySpec granularitySpec = dataSchema.getGranularitySpec();\n+    ParallelIndexTuningConfig tuningConfig = ingestionSchema.getTuningConfig();\n+\n+    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) tuningConfig.getPartitionsSpec();\n+    Preconditions.checkNotNull(partitionsSpec, \"partitionsSpec required in tuningConfig\");\n+\n+    List<String> partitionDimensions = partitionsSpec.getPartitionDimensions();\n+    if (partitionDimensions == null) {\n+      partitionDimensions = HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS;\n+    }\n+\n+    InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n+        ingestionSchema.getDataSchema().getParser()\n+    );\n+    InputFormat inputFormat = inputSource.needsFormat()\n+                              ? ParallelIndexSupervisorTask.getInputFormat(ingestionSchema)\n+                              : null;\n+    final RowIngestionMeters buildSegmentsMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n+    final ParseExceptionHandler parseExceptionHandler = new ParseExceptionHandler(\n+        buildSegmentsMeters,\n+        tuningConfig.isLogParseExceptions(),\n+        tuningConfig.getMaxParseExceptions(),\n+        tuningConfig.getMaxSavedParseExceptions()\n+    );\n+\n+    try (\n+        final CloseableIterator<InputRow> inputRowIterator = AbstractBatchIndexTask.inputSourceReader(\n+            toolbox.getIndexingTmpDir(),\n+            dataSchema,\n+            inputSource,\n+            inputFormat,\n+            AbstractBatchIndexTask.defaultRowFilter(granularitySpec),\n+            buildSegmentsMeters,\n+            parseExceptionHandler\n+        );\n+        HandlingInputRowIterator iterator =\n+            new DefaultIndexTaskInputRowIteratorBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d694d2113a9bc55d981f3ec99620b007ef25e44"}, "originalPosition": 175}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee473864db6caeb7582ae692393afd7025e6f1f7", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/ee473864db6caeb7582ae692393afd7025e6f1f7", "committedDate": "2020-09-24T00:26:33Z", "message": "Address some PR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e792972c9ae40b0380040774898afa582242e15", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/5e792972c9ae40b0380040774898afa582242e15", "committedDate": "2020-09-24T00:50:08Z", "message": "Fix granularity bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b424712ec01fd9b4495daa22ef20c2300d493e2b", "author": {"user": {"login": "jon-wei", "name": "Jonathan Wei"}}, "url": "https://github.com/apache/druid/commit/b424712ec01fd9b4495daa22ef20c2300d493e2b", "committedDate": "2020-09-24T00:52:55Z", "message": "Small doc fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1ODM0MTkw", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-495834190", "createdAt": "2020-09-24T18:15:10Z", "commit": {"oid": "b424712ec01fd9b4495daa22ef20c2300d493e2b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1ODQxMDM1", "url": "https://github.com/apache/druid/pull/10419#pullrequestreview-495841035", "createdAt": "2020-09-24T18:21:30Z", "commit": {"oid": "b424712ec01fd9b4495daa22ef20c2300d493e2b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3242, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}