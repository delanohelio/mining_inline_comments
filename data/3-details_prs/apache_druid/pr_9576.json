{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk1MDQwMjEw", "number": 9576, "title": "Add Integration Test for functionality of kinesis ingestion", "bodyText": "Add Integration Test for functionality of kinesis ingestion\nDescription\nThe new set of integration test for Kinesis follows the same concept as Bring Your Own Cloud (BYOC) that S3, GCS, Azure Integration tests uses (#9501). Basically, anyone running will have to provide their own Kinesis credentials in a conf file and pass the file to mvn using -Doverride.config.path\nAdded following Integration Test for functionality of kinesis ingestion:\n\nFunctional tests when Druid and Kafka are in stable state\n\n\nlegacy parser\ninputFormat\nGreater than 1 taskCount\n\n\nFunctional tests when Druid is in an unstable state\n\n\nlosing nodes\nStop/start supervisor\n\n\nFunctional tests when Kafka is in an unstable state\n\n\nadding partitions\nremoving partitions\n\nTo verify ingestion:\n\nKafka lag should be minimal, the consumer should be able to pull off the queue at a comparable rate to the producer.\nRealtime queries works from the indexing tasks\nQueries works reading from historical segments (after handed off)\nQueries return expected count/value/etc.\n\nAdded integration test infrastructure/helper:\n\nData generator - Generate data to kinesis stream (Kafka can also be refactor to use this later)\nDruidAdminClient - to control the Integration test's Druid Docker cluster\nKinesisAdminClient - Create stream, delete stream, etc.\nSince tests can fail or be killed un-expectedly, added druid-ci-expire-after tag to all stream created by integration test. The value to this tag is a timestamp that can be used by a lambda function to remove unused stream. (help make clean up easier)\nAlso made debugging integration test easier by automatically enabling debug mode and exposing debug port on integration test's druids docker processes\n\nThis PR has:\n\n been self-reviewed.\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-03-28T03:28:11Z", "url": "https://github.com/apache/druid/pull/9576", "merged": true, "mergeCommit": {"oid": "1852bf33ea43cfc155e38ede45a2a2c61b081597"}, "closed": true, "closedAt": "2020-04-03T16:45:22Z", "author": {"login": "maytasm"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcQ6ghMgH2gAyMzk1MDQwMjEwOjVmMGYzNmZiYjM3NjE0ZDEzYmM2M2EwZGJiNWIxOWEyNDBiNjAzZjQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcUDuCVAFqTM4NzQxNzczOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5f0f36fbb37614d13bc63a0dbb5b19a240b603f4", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/5f0f36fbb37614d13bc63a0dbb5b19a240b603f4", "committedDate": "2020-03-24T22:18:53Z", "message": "kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fde2a1adb5e80cf02d7ffe6f88cc77caf2064146", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/fde2a1adb5e80cf02d7ffe6f88cc77caf2064146", "committedDate": "2020-03-24T23:43:40Z", "message": "Merge remote-tracking branch 'upstream/master' into IMPLY-2223"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d6a35390d4de09199f59b508ad1bc3a11fa543f", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/8d6a35390d4de09199f59b508ad1bc3a11fa543f", "committedDate": "2020-03-26T19:06:08Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "747dcf7aa5469ac2be0b6c83ca859eac4a3b0d4d", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/747dcf7aa5469ac2be0b6c83ca859eac4a3b0d4d", "committedDate": "2020-03-27T07:11:42Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0b0e30a49fbdeff3a354027b32e7e30a664f2d1", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/e0b0e30a49fbdeff3a354027b32e7e30a664f2d1", "committedDate": "2020-03-28T03:15:22Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "052bc095ed56c209e1f7447fc0eefb3e20e09a16", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/052bc095ed56c209e1f7447fc0eefb3e20e09a16", "committedDate": "2020-03-28T03:23:46Z", "message": "fix conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77599e6a247af7d56aefaf56ed6dfd6a7477676c", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/77599e6a247af7d56aefaf56ed6dfd6a7477676c", "committedDate": "2020-03-28T03:39:35Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d76fcc44dd3cfd3168af07df405d73732df62b50", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/d76fcc44dd3cfd3168af07df405d73732df62b50", "committedDate": "2020-03-29T08:46:38Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fd928f2ca33274e26b1a351840556927163c4c17", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/fd928f2ca33274e26b1a351840556927163c4c17", "committedDate": "2020-03-30T03:27:41Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08edce8a4a796ae50c205fab182de4709e677ae5", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/08edce8a4a796ae50c205fab182de4709e677ae5", "committedDate": "2020-03-30T04:07:35Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b70712a292c7c2e8f4ff67e550a3fb83fb88fac3", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/b70712a292c7c2e8f4ff67e550a3fb83fb88fac3", "committedDate": "2020-03-30T06:53:03Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a26fa690b8ff851be47a9d83771c852f1938672", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/7a26fa690b8ff851be47a9d83771c852f1938672", "committedDate": "2020-03-30T08:19:18Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65e0ca4101bee4539fef7730de10860c5c324b93", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/65e0ca4101bee4539fef7730de10860c5c324b93", "committedDate": "2020-03-30T10:47:03Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ddfe1abc87b80028174289aeb3b30710156ea62", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2ddfe1abc87b80028174289aeb3b30710156ea62", "committedDate": "2020-03-30T10:54:20Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8505e79a9f2301fc49bdfbea99896c5b2a0fd501", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/8505e79a9f2301fc49bdfbea99896c5b2a0fd501", "committedDate": "2020-03-30T20:44:19Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c89471549778d0e9d415a74d66e5fca5af04c33", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/7c89471549778d0e9d415a74d66e5fca5af04c33", "committedDate": "2020-03-30T20:55:03Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d7a20ea5ba966a7ab50658410eb7b8f41df3340", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/1d7a20ea5ba966a7ab50658410eb7b8f41df3340", "committedDate": "2020-03-31T06:54:56Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "29a43884199ecbe3424868d0f0358143c904a4fc", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/29a43884199ecbe3424868d0f0358143c904a4fc", "committedDate": "2020-03-31T08:43:29Z", "message": "fix kinesis timeout"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f3ae22f87ed3f6304c5e5b5d6cf567f2b030e10", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2f3ae22f87ed3f6304c5e5b5d6cf567f2b030e10", "committedDate": "2020-03-31T09:42:52Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5f4b141f79b8ea2fe11713015f787021d4c89fd", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/c5f4b141f79b8ea2fe11713015f787021d4c89fd", "committedDate": "2020-03-31T10:47:30Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb24f1cf3c8d15071bdfe27628ad027b70c02fe5", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/bb24f1cf3c8d15071bdfe27628ad027b70c02fe5", "committedDate": "2020-03-31T12:15:17Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/d7637d53ff7fdee5ff28b15c49178b176bbf3a39", "committedDate": "2020-03-31T12:20:50Z", "message": "Kinesis IT"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg0OTE5MTM3", "url": "https://github.com/apache/druid/pull/9576#pullrequestreview-384919137", "createdAt": "2020-03-31T16:46:56Z", "commit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNjo0Njo1N1rOF-e4DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxNjo1MDowOFrOF-fAiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTA2MTkwMQ==", "bodyText": "\ud83c\udf89", "url": "https://github.com/apache/druid/pull/9576#discussion_r401061901", "createdAt": "2020-03-31T16:46:57Z", "author": {"login": "suneet-s"}, "path": "integration-tests/README.md", "diffHunk": "@@ -68,6 +68,21 @@ can either be 8 or 11.\n Druid's configuration (using Docker) can be overrided by providing -Doverride.config.path=<PATH_TO_FILE>. \n The file must contain one property per line, the key must start with `druid_` and the format should be snake case. \n \n+## Debugging Druid while running tests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTA2NDA3Mw==", "bodyText": "nit: No need to change if everything else looks good. If I saw the log line as is, it's a little ambiguous - which script? what's the impact of skipping running again?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              echo \"Script was ran already. Skip running again.\"\n          \n          \n            \n              echo \"Using existing tls keys since /tls/server.key exists - skipping generation of all certs. To generate certs, delete this file\"", "url": "https://github.com/apache/druid/pull/9576#discussion_r401064073", "createdAt": "2020-03-31T16:50:08Z", "author": {"login": "suneet-s"}, "path": "integration-tests/docker/tls/generate-server-certs-and-keystores.sh", "diffHunk": "@@ -17,6 +17,12 @@\n \n cd /tls\n \n+FILE_CHECK_IF_RAN=/tls/server.key\n+if [ -f \"$FILE_CHECK_IF_RAN\" ]; then\n+  echo \"Script was ran already. Skip running again.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg1MjMzOTE2", "url": "https://github.com/apache/druid/pull/9576#pullrequestreview-385233916", "createdAt": "2020-04-01T02:54:57Z", "commit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwMjo1NDo1N1rOF-vF9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQwMzo0NDozOVrOF-vz8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMyNzYwNw==", "bodyText": "Looks like something from a conflict", "url": "https://github.com/apache/druid/pull/9576#discussion_r401327607", "createdAt": "2020-04-01T02:54:57Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -107,6 +122,7 @@ Then run the tests using a command similar to:\n   # Run all integration tests that have been verified to work against a quickstart cluster.\n   mvn verify -P int-tests-config-file -Dgroups=quickstart-compatible\n ```\n+>>>>>>> upstream/master", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMzNTc1NQ==", "bodyText": "North Americ -> North America", "url": "https://github.com/apache/druid/pull/9576#discussion_r401335755", "createdAt": "2020-04-01T03:29:05Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/main/java/org/apache/druid/testing/utils/WikipediaStreamEventStreamGenerator.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.testing.utils;\n+\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class WikipediaStreamEventStreamGenerator extends SyntheticStreamGenerator\n+{\n+  private static final DateTimeFormatter DATE_TIME_FORMATTER = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'Z'\");\n+\n+  public WikipediaStreamEventStreamGenerator(int eventsPerSeconds, long cyclePaddingMs)\n+  {\n+    super(eventsPerSeconds, cyclePaddingMs);\n+  }\n+\n+  @Override\n+  Object getEvent(int i, DateTime timestamp)\n+  {\n+    Map<String, Object> event = new HashMap<>();\n+    event.put(\"page\", \"Gypsy Danger\");\n+    event.put(\"language\", \"en\");\n+    event.put(\"user\", \"nuclear\");\n+    event.put(\"unpatrolled\", \"true\");\n+    event.put(\"newPage\", \"true\");\n+    event.put(\"robot\", \"false\");\n+    event.put(\"anonymous\", \"false\");\n+    event.put(\"namespace\", \"article\");\n+    event.put(\"continent\", \"North Americ\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMzNjYyNg==", "bodyText": "How is the expire tag used?", "url": "https://github.com/apache/druid/pull/9576#discussion_r401336626", "createdAt": "2020-04-01T03:32:31Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,443 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.KinesisAdminClient;\n+import org.apache.druid.testing.utils.KinesisEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceTest extends AbstractITBatchIndexTest\n+{\n+  private static final Logger LOG = new Logger(AbstractKafkaIndexerTest.class);\n+  private static final int KINESIS_SHARD_COUNT = 2;\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMzNzgwNw==", "bodyText": "For the resharding test, I think you'll want to have longer timers for the event generation, with only 3s here I think it's maybe possible that AWS doesn't actually begin the resharding until you've already finished this second phase. Maybe 30s is better.\nOr maybe it could check for the stream status becoming UPDATING and start the second phase then.", "url": "https://github.com/apache/druid/pull/9576#discussion_r401337807", "createdAt": "2020-04-01T03:37:59Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,443 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.KinesisAdminClient;\n+import org.apache.druid.testing.utils.KinesisEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceTest extends AbstractITBatchIndexTest\n+{\n+  private static final Logger LOG = new Logger(AbstractKafkaIndexerTest.class);\n+  private static final int KINESIS_SHARD_COUNT = 2;\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  // format for the querying interval\n+  private static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  private static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  private static final int EVENTS_PER_SECOND = 6;\n+  private static final long CYCLE_PADDING_MS = 100;\n+  private static final int TOTAL_NUMBER_OF_SECOND = 10;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private String streamName;\n+  private String fullDatasourceName;\n+  private KinesisAdminClient kinesisAdminClient;\n+  private KinesisEventWriter kinesisEventWriter;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+  private Function<String, String> kinesisIngestionPropsTransform;\n+  private Function<String, String> kinesisQueryPropsTransform;\n+  private String supervisorId;\n+  private int secondsToGenerateRemaining;\n+\n+  @BeforeClass\n+  public void beforeClass() throws Exception\n+  {\n+    kinesisAdminClient = new KinesisAdminClient(config.getStreamEndpoint());\n+    kinesisEventWriter = new KinesisEventWriter(config.getStreamEndpoint(), false);\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  @AfterClass\n+  public void tearDown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+    kinesisEventWriter.shutdown();\n+  }\n+\n+  @BeforeMethod\n+  public void before()\n+  {\n+    streamName = \"kinesis_index_test_\" + UUID.randomUUID();\n+    String datasource = \"kinesis_indexing_service_test_\" + UUID.randomUUID();\n+    Map<String, String> tags = ImmutableMap.of(STREAM_EXPIRE_TAG, Long.toString(DateTimes.nowUtc().plusMinutes(30).getMillis()));\n+    kinesisAdminClient.createStream(streamName, KINESIS_SHARD_COUNT, tags);\n+    ITRetryUtil.retryUntil(\n+        () -> kinesisAdminClient.isStreamActive(streamName),\n+        true,\n+        10000,\n+        30,\n+        \"Wait for stream active\"\n+    );\n+    secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+    fullDatasourceName = datasource + config.getExtraDatasourceNameSuffix();\n+    kinesisIngestionPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_TYPE%%\",\n+            \"kinesis\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_KEY%%\",\n+            \"stream\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_VALUE%%\",\n+            streamName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%USE_EARLIEST_KEY%%\",\n+            \"useEarliestSequenceNumber\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_KEY%%\",\n+            \"endpoint\"\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_VALUE%%\",\n+            jsonMapper.writeValueAsString(config.getStreamEndpoint())\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+    kinesisQueryPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MAXTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MINTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_START%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_END%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1).plusMinutes(2))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_ADDED%%\",\n+            Long.toString(getSumOfEventSequence(EVENTS_PER_SECOND) * TOTAL_NUMBER_OF_SECOND)\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_NUMEVENTS%%\",\n+            Integer.toString(EVENTS_PER_SECOND * TOTAL_NUMBER_OF_SECOND)\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+  }\n+\n+  @AfterMethod\n+  public void teardown()\n+  {\n+    try {\n+      kinesisEventWriter.flush();\n+      indexer.shutdownSupervisor(supervisorId);\n+      unloader(fullDatasourceName);\n+      kinesisAdminClient.deleteStream(streamName);\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start Kinesis data generator\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start Kinesis data generator\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(supervisorId);\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(supervisorId);\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithKinesisReshardSplit() throws Exception\n+  {\n+    // Reshard the supervisor by split from KINESIS_SHARD_COUNT to KINESIS_SHARD_COUNT * 2\n+    testIndexWithKinesisReshardHelper(KINESIS_SHARD_COUNT * 2);\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithKinesisReshardMerge() throws Exception\n+  {\n+    // Reshard the supervisor by split from KINESIS_SHARD_COUNT to KINESIS_SHARD_COUNT / 2\n+    testIndexWithKinesisReshardHelper(KINESIS_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  private void testIndexWithKinesisReshardHelper(int newShardCount) throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Reshard the supervisor by split from KINESIS_SHARD_COUNT to newShardCount\n+      kinesisAdminClient.updateShardCount(streamName, newShardCount);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for kinesis stream to finish resharding", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 385}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMzODk3NQ==", "bodyText": "Suggest having a supervisor healthy check as well before the resharding occurs, so the resharding occurs while the supervisor is running", "url": "https://github.com/apache/druid/pull/9576#discussion_r401338975", "createdAt": "2020-04-01T03:43:01Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,443 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.KinesisAdminClient;\n+import org.apache.druid.testing.utils.KinesisEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceTest extends AbstractITBatchIndexTest\n+{\n+  private static final Logger LOG = new Logger(AbstractKafkaIndexerTest.class);\n+  private static final int KINESIS_SHARD_COUNT = 2;\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  // format for the querying interval\n+  private static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  private static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  private static final int EVENTS_PER_SECOND = 6;\n+  private static final long CYCLE_PADDING_MS = 100;\n+  private static final int TOTAL_NUMBER_OF_SECOND = 10;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private String streamName;\n+  private String fullDatasourceName;\n+  private KinesisAdminClient kinesisAdminClient;\n+  private KinesisEventWriter kinesisEventWriter;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+  private Function<String, String> kinesisIngestionPropsTransform;\n+  private Function<String, String> kinesisQueryPropsTransform;\n+  private String supervisorId;\n+  private int secondsToGenerateRemaining;\n+\n+  @BeforeClass\n+  public void beforeClass() throws Exception\n+  {\n+    kinesisAdminClient = new KinesisAdminClient(config.getStreamEndpoint());\n+    kinesisEventWriter = new KinesisEventWriter(config.getStreamEndpoint(), false);\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  @AfterClass\n+  public void tearDown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+    kinesisEventWriter.shutdown();\n+  }\n+\n+  @BeforeMethod\n+  public void before()\n+  {\n+    streamName = \"kinesis_index_test_\" + UUID.randomUUID();\n+    String datasource = \"kinesis_indexing_service_test_\" + UUID.randomUUID();\n+    Map<String, String> tags = ImmutableMap.of(STREAM_EXPIRE_TAG, Long.toString(DateTimes.nowUtc().plusMinutes(30).getMillis()));\n+    kinesisAdminClient.createStream(streamName, KINESIS_SHARD_COUNT, tags);\n+    ITRetryUtil.retryUntil(\n+        () -> kinesisAdminClient.isStreamActive(streamName),\n+        true,\n+        10000,\n+        30,\n+        \"Wait for stream active\"\n+    );\n+    secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+    fullDatasourceName = datasource + config.getExtraDatasourceNameSuffix();\n+    kinesisIngestionPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_TYPE%%\",\n+            \"kinesis\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_KEY%%\",\n+            \"stream\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_VALUE%%\",\n+            streamName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%USE_EARLIEST_KEY%%\",\n+            \"useEarliestSequenceNumber\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_KEY%%\",\n+            \"endpoint\"\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_VALUE%%\",\n+            jsonMapper.writeValueAsString(config.getStreamEndpoint())\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+    kinesisQueryPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MAXTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MINTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_START%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_END%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1).plusMinutes(2))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_ADDED%%\",\n+            Long.toString(getSumOfEventSequence(EVENTS_PER_SECOND) * TOTAL_NUMBER_OF_SECOND)\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_NUMEVENTS%%\",\n+            Integer.toString(EVENTS_PER_SECOND * TOTAL_NUMBER_OF_SECOND)\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+  }\n+\n+  @AfterMethod\n+  public void teardown()\n+  {\n+    try {\n+      kinesisEventWriter.flush();\n+      indexer.shutdownSupervisor(supervisorId);\n+      unloader(fullDatasourceName);\n+      kinesisAdminClient.deleteStream(streamName);\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start Kinesis data generator\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start Kinesis data generator\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(supervisorId);\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(supervisorId);\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithKinesisReshardSplit() throws Exception\n+  {\n+    // Reshard the supervisor by split from KINESIS_SHARD_COUNT to KINESIS_SHARD_COUNT * 2\n+    testIndexWithKinesisReshardHelper(KINESIS_SHARD_COUNT * 2);\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithKinesisReshardMerge() throws Exception\n+  {\n+    // Reshard the supervisor by split from KINESIS_SHARD_COUNT to KINESIS_SHARD_COUNT / 2\n+    testIndexWithKinesisReshardHelper(KINESIS_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  private void testIndexWithKinesisReshardHelper(int newShardCount) throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Reshard the supervisor by split from KINESIS_SHARD_COUNT to newShardCount\n+      kinesisAdminClient.updateShardCount(streamName, newShardCount);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for kinesis stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> kinesisAdminClient.isStreamActive(streamName),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for Kinesis stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after suspension", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 395}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMzOTA0Nw==", "bodyText": "kafka -> kinesis", "url": "https://github.com/apache/druid/pull/9576#discussion_r401339047", "createdAt": "2020-04-01T03:43:24Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,443 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.KinesisAdminClient;\n+import org.apache.druid.testing.utils.KinesisEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceTest extends AbstractITBatchIndexTest\n+{\n+  private static final Logger LOG = new Logger(AbstractKafkaIndexerTest.class);\n+  private static final int KINESIS_SHARD_COUNT = 2;\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  // format for the querying interval\n+  private static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  private static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  private static final int EVENTS_PER_SECOND = 6;\n+  private static final long CYCLE_PADDING_MS = 100;\n+  private static final int TOTAL_NUMBER_OF_SECOND = 10;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private String streamName;\n+  private String fullDatasourceName;\n+  private KinesisAdminClient kinesisAdminClient;\n+  private KinesisEventWriter kinesisEventWriter;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+  private Function<String, String> kinesisIngestionPropsTransform;\n+  private Function<String, String> kinesisQueryPropsTransform;\n+  private String supervisorId;\n+  private int secondsToGenerateRemaining;\n+\n+  @BeforeClass\n+  public void beforeClass() throws Exception\n+  {\n+    kinesisAdminClient = new KinesisAdminClient(config.getStreamEndpoint());\n+    kinesisEventWriter = new KinesisEventWriter(config.getStreamEndpoint(), false);\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  @AfterClass\n+  public void tearDown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+    kinesisEventWriter.shutdown();\n+  }\n+\n+  @BeforeMethod\n+  public void before()\n+  {\n+    streamName = \"kinesis_index_test_\" + UUID.randomUUID();\n+    String datasource = \"kinesis_indexing_service_test_\" + UUID.randomUUID();\n+    Map<String, String> tags = ImmutableMap.of(STREAM_EXPIRE_TAG, Long.toString(DateTimes.nowUtc().plusMinutes(30).getMillis()));\n+    kinesisAdminClient.createStream(streamName, KINESIS_SHARD_COUNT, tags);\n+    ITRetryUtil.retryUntil(\n+        () -> kinesisAdminClient.isStreamActive(streamName),\n+        true,\n+        10000,\n+        30,\n+        \"Wait for stream active\"\n+    );\n+    secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+    fullDatasourceName = datasource + config.getExtraDatasourceNameSuffix();\n+    kinesisIngestionPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_TYPE%%\",\n+            \"kinesis\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_KEY%%\",\n+            \"stream\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_VALUE%%\",\n+            streamName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%USE_EARLIEST_KEY%%\",\n+            \"useEarliestSequenceNumber\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_KEY%%\",\n+            \"endpoint\"\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_VALUE%%\",\n+            jsonMapper.writeValueAsString(config.getStreamEndpoint())\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+    kinesisQueryPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MAXTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MINTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_START%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_END%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1).plusMinutes(2))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_ADDED%%\",\n+            Long.toString(getSumOfEventSequence(EVENTS_PER_SECOND) * TOTAL_NUMBER_OF_SECOND)\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_NUMEVENTS%%\",\n+            Integer.toString(EVENTS_PER_SECOND * TOTAL_NUMBER_OF_SECOND)\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+  }\n+\n+  @AfterMethod\n+  public void teardown()\n+  {\n+    try {\n+      kinesisEventWriter.flush();\n+      indexer.shutdownSupervisor(supervisorId);\n+      unloader(fullDatasourceName);\n+      kinesisAdminClient.deleteStream(streamName);\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLegacyParserStableState() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_LEGACY_PARSER));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start Kinesis data generator\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithInputFormatStableState() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start Kinesis data generator\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, TOTAL_NUMBER_OF_SECOND, FIRST_EVENT_TIME);\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingCoordinator() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartCoordinatorContainer(), () -> druidClusterAdminClient.waitUntilCoordinatorReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingOverlord() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartIndexerContainer(), () -> druidClusterAdminClient.waitUntilIndexerReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLosingHistorical() throws Exception\n+  {\n+    testIndexWithLosingNodeHelper(() -> druidClusterAdminClient.restartHistoricalContainer(), () -> druidClusterAdminClient.waitUntilHistoricalReady());\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithStartStopSupervisor() throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating half of the data\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 2;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Suspend the supervisor\n+      indexer.suspendSupervisor(supervisorId);\n+      // Start generating remainning half of the data\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Resume the supervisor\n+      indexer.resumeSupervisor(supervisorId);\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithKinesisReshardSplit() throws Exception\n+  {\n+    // Reshard the supervisor by split from KINESIS_SHARD_COUNT to KINESIS_SHARD_COUNT * 2\n+    testIndexWithKinesisReshardHelper(KINESIS_SHARD_COUNT * 2);\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithKinesisReshardMerge() throws Exception\n+  {\n+    // Reshard the supervisor by split from KINESIS_SHARD_COUNT to KINESIS_SHARD_COUNT / 2\n+    testIndexWithKinesisReshardHelper(KINESIS_SHARD_COUNT / 2);\n+  }\n+\n+  private void testIndexWithLosingNodeHelper(Runnable restartRunnable, Runnable waitForReadyRunnable) throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before restarting)\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Restart Druid process\n+      LOG.info(\"Restarting Druid process\");\n+      restartRunnable.run();\n+      LOG.info(\"Restarted Druid process\");\n+      // Start generating one third of the data (while restarting)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for Druid process to be available\n+      LOG.info(\"Waiting for Druid process to be available\");\n+      waitForReadyRunnable.run();\n+      LOG.info(\"Druid process is now available\");\n+      // Start generating remainding data (after restarting)\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor ingested all data\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  private void testIndexWithKinesisReshardHelper(int newShardCount) throws Exception\n+  {\n+    try (\n+        final Closeable ignored1 = unloader(fullDatasourceName)\n+    ) {\n+      final String taskSpec = kinesisIngestionPropsTransform.apply(getResourceAsString(INDEXER_FILE_INPUT_FORMAT));\n+      LOG.info(\"supervisorSpec: [%s]\\n\", taskSpec);\n+      // Start supervisor\n+      supervisorId = indexer.submitSupervisor(taskSpec);\n+      LOG.info(\"Submitted supervisor\");\n+      // Start generating one third of the data (before resharding)\n+      int secondsToGenerateFirstRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateFirstRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateFirstRound, FIRST_EVENT_TIME);\n+      // Reshard the supervisor by split from KINESIS_SHARD_COUNT to newShardCount\n+      kinesisAdminClient.updateShardCount(streamName, newShardCount);\n+      // Start generating one third of the data (while resharding)\n+      int secondsToGenerateSecondRound = TOTAL_NUMBER_OF_SECOND / 3;\n+      secondsToGenerateRemaining = secondsToGenerateRemaining - secondsToGenerateSecondRound;\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateSecondRound, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound));\n+      // Wait for kinesis stream to finish resharding\n+      ITRetryUtil.retryUntil(\n+          () -> kinesisAdminClient.isStreamActive(streamName),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for Kinesis stream to finish resharding\"\n+      );\n+      // Start generating remainding data (after resharding)\n+      wikipediaStreamEventGenerator.start(streamName, kinesisEventWriter, secondsToGenerateRemaining, FIRST_EVENT_TIME.plusSeconds(secondsToGenerateFirstRound + secondsToGenerateSecondRound));\n+      // Verify supervisor is healthy after suspension\n+      ITRetryUtil.retryUntil(\n+          () -> SupervisorStateManager.BasicState.RUNNING.equals(indexer.getSupervisorStatus(supervisorId)),\n+          true,\n+          10000,\n+          30,\n+          \"Waiting for supervisor to be healthy\"\n+      );\n+      // Verify that supervisor can catch up with the stream\n+      verifyIngestedData(supervisorId);\n+    }\n+  }\n+\n+  private void verifyIngestedData(String supervisorId) throws Exception\n+  {\n+    // Wait for supervisor to consume events\n+    LOG.info(\"Waiting for [%s] millis for Kafka indexing tasks to consume events\", WAIT_TIME_MILLIS);\n+    Thread.sleep(WAIT_TIME_MILLIS);\n+    // Query data\n+    final String querySpec = kinesisQueryPropsTransform.apply(getResourceAsString(QUERIES_FILE));\n+    // this query will probably be answered from the indexing tasks but possibly from 2 historical segments / 2 indexing\n+    this.queryHelper.testQueriesFromString(querySpec, 2);\n+    LOG.info(\"Shutting down supervisor\");\n+    indexer.shutdownSupervisor(supervisorId);\n+    // wait for all kafka indexing tasks to finish", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 419}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTMzOTM3Ng==", "bodyText": "In the test names, Kinese -> Kinesis here and elsewhere", "url": "https://github.com/apache/druid/pull/9576#discussion_r401339376", "createdAt": "2020-04-01T03:44:39Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/indexer/ITKinesisIndexingServiceTest.java", "diffHunk": "@@ -0,0 +1,443 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.indexer;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n+import org.apache.druid.indexing.overlord.supervisor.SupervisorStateManager;\n+import org.apache.druid.java.util.common.DateTimes;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.testing.utils.DruidClusterAdminClient;\n+import org.apache.druid.testing.utils.ITRetryUtil;\n+import org.apache.druid.testing.utils.KinesisAdminClient;\n+import org.apache.druid.testing.utils.KinesisEventWriter;\n+import org.apache.druid.testing.utils.WikipediaStreamEventStreamGenerator;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.joda.time.DateTime;\n+import org.joda.time.format.DateTimeFormat;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.testng.annotations.AfterClass;\n+import org.testng.annotations.AfterMethod;\n+import org.testng.annotations.BeforeClass;\n+import org.testng.annotations.BeforeMethod;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+import java.io.Closeable;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+@Test(groups = TestNGGroup.KINESIS_INDEX)\n+@Guice(moduleFactory = DruidTestModuleFactory.class)\n+public class ITKinesisIndexingServiceTest extends AbstractITBatchIndexTest\n+{\n+  private static final Logger LOG = new Logger(AbstractKafkaIndexerTest.class);\n+  private static final int KINESIS_SHARD_COUNT = 2;\n+  private static final String STREAM_EXPIRE_TAG = \"druid-ci-expire-after\";\n+  private static final long WAIT_TIME_MILLIS = 3 * 60 * 1000L;\n+  private static final DateTime FIRST_EVENT_TIME = DateTimes.of(1994, 4, 29, 1, 0);\n+  private static final String INDEXER_FILE_LEGACY_PARSER = \"/indexer/stream_supervisor_spec_legacy_parser.json\";\n+  private static final String INDEXER_FILE_INPUT_FORMAT = \"/indexer/stream_supervisor_spec_input_format.json\";\n+  private static final String QUERIES_FILE = \"/indexer/stream_index_queries.json\";\n+  // format for the querying interval\n+  private static final DateTimeFormatter INTERVAL_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:'00Z'\");\n+  // format for the expected timestamp in a query response\n+  private static final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n+  private static final int EVENTS_PER_SECOND = 6;\n+  private static final long CYCLE_PADDING_MS = 100;\n+  private static final int TOTAL_NUMBER_OF_SECOND = 10;\n+\n+  @Inject\n+  private DruidClusterAdminClient druidClusterAdminClient;\n+\n+  private String streamName;\n+  private String fullDatasourceName;\n+  private KinesisAdminClient kinesisAdminClient;\n+  private KinesisEventWriter kinesisEventWriter;\n+  private WikipediaStreamEventStreamGenerator wikipediaStreamEventGenerator;\n+  private Function<String, String> kinesisIngestionPropsTransform;\n+  private Function<String, String> kinesisQueryPropsTransform;\n+  private String supervisorId;\n+  private int secondsToGenerateRemaining;\n+\n+  @BeforeClass\n+  public void beforeClass() throws Exception\n+  {\n+    kinesisAdminClient = new KinesisAdminClient(config.getStreamEndpoint());\n+    kinesisEventWriter = new KinesisEventWriter(config.getStreamEndpoint(), false);\n+    wikipediaStreamEventGenerator = new WikipediaStreamEventStreamGenerator(EVENTS_PER_SECOND, CYCLE_PADDING_MS);\n+  }\n+\n+  @AfterClass\n+  public void tearDown()\n+  {\n+    wikipediaStreamEventGenerator.shutdown();\n+    kinesisEventWriter.shutdown();\n+  }\n+\n+  @BeforeMethod\n+  public void before()\n+  {\n+    streamName = \"kinesis_index_test_\" + UUID.randomUUID();\n+    String datasource = \"kinesis_indexing_service_test_\" + UUID.randomUUID();\n+    Map<String, String> tags = ImmutableMap.of(STREAM_EXPIRE_TAG, Long.toString(DateTimes.nowUtc().plusMinutes(30).getMillis()));\n+    kinesisAdminClient.createStream(streamName, KINESIS_SHARD_COUNT, tags);\n+    ITRetryUtil.retryUntil(\n+        () -> kinesisAdminClient.isStreamActive(streamName),\n+        true,\n+        10000,\n+        30,\n+        \"Wait for stream active\"\n+    );\n+    secondsToGenerateRemaining = TOTAL_NUMBER_OF_SECOND;\n+    fullDatasourceName = datasource + config.getExtraDatasourceNameSuffix();\n+    kinesisIngestionPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_TYPE%%\",\n+            \"kinesis\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_KEY%%\",\n+            \"stream\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TOPIC_VALUE%%\",\n+            streamName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%USE_EARLIEST_KEY%%\",\n+            \"useEarliestSequenceNumber\"\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_KEY%%\",\n+            \"endpoint\"\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%STREAM_PROPERTIES_VALUE%%\",\n+            jsonMapper.writeValueAsString(config.getStreamEndpoint())\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+    kinesisQueryPropsTransform = spec -> {\n+      try {\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%DATASOURCE%%\",\n+            fullDatasourceName\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MAXTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMEBOUNDARY_RESPONSE_MINTIME%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_START%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_QUERY_END%%\",\n+            INTERVAL_FMT.print(FIRST_EVENT_TIME.plusSeconds(TOTAL_NUMBER_OF_SECOND - 1).plusMinutes(2))\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_RESPONSE_TIMESTAMP%%\",\n+            TIMESTAMP_FMT.print(FIRST_EVENT_TIME)\n+        );\n+        spec = StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_ADDED%%\",\n+            Long.toString(getSumOfEventSequence(EVENTS_PER_SECOND) * TOTAL_NUMBER_OF_SECOND)\n+        );\n+        return StringUtils.replace(\n+            spec,\n+            \"%%TIMESERIES_NUMEVENTS%%\",\n+            Integer.toString(EVENTS_PER_SECOND * TOTAL_NUMBER_OF_SECOND)\n+        );\n+      }\n+      catch (Exception e) {\n+        throw new RuntimeException(e);\n+      }\n+    };\n+  }\n+\n+  @AfterMethod\n+  public void teardown()\n+  {\n+    try {\n+      kinesisEventWriter.flush();\n+      indexer.shutdownSupervisor(supervisorId);\n+      unloader(fullDatasourceName);\n+      kinesisAdminClient.deleteStream(streamName);\n+    }\n+    catch (Exception e) {\n+      // Best effort cleanup\n+    }\n+  }\n+\n+  @Test\n+  public void testKineseIndexDataWithLegacyParserStableState() throws Exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7637d53ff7fdee5ff28b15c49178b176bbf3a39"}, "originalPosition": 225}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0aa938c2a859abcdb45d6406938b309ceeba5d60", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/0aa938c2a859abcdb45d6406938b309ceeba5d60", "committedDate": "2020-04-02T00:52:27Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c094119f17e6533ba703aae78151e537837e0f21", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/c094119f17e6533ba703aae78151e537837e0f21", "committedDate": "2020-04-02T01:47:00Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDE3NzM4", "url": "https://github.com/apache/druid/pull/9576#pullrequestreview-387417738", "createdAt": "2020-04-03T16:44:34Z", "commit": {"oid": "c094119f17e6533ba703aae78151e537837e0f21"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2746, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}