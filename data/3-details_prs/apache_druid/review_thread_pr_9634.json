{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwMDYxMjA3", "number": 9634, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwMTowMjowMlrODvy5hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxOTo0Mjo0NlrODwI2Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxNDQzNTkxOnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/filter/CnfHelper.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwMTowMjowMlrOGCbbAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwMToyNToyM1rOGCbzBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE5OTYxNw==", "bodyText": "Looking at how the file has multiple inclusions from other projects, can we split the Calcite-based methods into their own class? I think it'd be easier to tell what's from what that way", "url": "https://github.com/apache/druid/pull/9634#discussion_r405199617", "createdAt": "2020-04-08T01:02:02Z", "author": {"login": "jon-wei"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/CnfHelper.java", "diffHunk": "@@ -0,0 +1,464 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.filter;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import org.apache.druid.query.filter.BooleanFilter;\n+import org.apache.druid.query.filter.Filter;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+/**\n+ * A helper class to convert a filter to CNF.\n+ *\n+ * The methods in this class are mainly adopted from Apache Hive and Apache Calcite.\n+ */\n+public class CnfHelper\n+{\n+  public static Filter toCnf(Filter current)\n+  {\n+    current = pushDownNot(current);\n+    current = flatten(current);\n+    current = pull(current);\n+    current = convertToCNFInternal(current);\n+    current = flatten(current);\n+    return current;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  @VisibleForTesting\n+  static Filter pushDownNot(Filter current)\n+  {\n+    if (current instanceof NotFilter) {\n+      Filter child = ((NotFilter) current).getBaseFilter();\n+      if (child instanceof NotFilter) {\n+        return pushDownNot(((NotFilter) child).getBaseFilter());\n+      }\n+      if (child instanceof AndFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((AndFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new OrFilter(children);\n+      }\n+      if (child instanceof OrFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((OrFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new AndFilter(children);\n+      }\n+    }\n+\n+    if (current instanceof AndFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((AndFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new AndFilter(children);\n+    }\n+\n+    if (current instanceof OrFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((OrFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new OrFilter(children);\n+    }\n+    return current;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  private static Filter convertToCNFInternal(Filter current)\n+  {\n+    if (current instanceof NotFilter) {\n+      return new NotFilter(convertToCNFInternal(((NotFilter) current).getBaseFilter()));\n+    }\n+    if (current instanceof AndFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((AndFilter) current).getFilters()) {\n+        children.add(convertToCNFInternal(child));\n+      }\n+      return new AndFilter(children);\n+    }\n+    if (current instanceof OrFilter) {\n+      // a list of leaves that weren't under AND expressions\n+      List<Filter> nonAndList = new ArrayList<Filter>();\n+      // a list of AND expressions that we need to distribute\n+      List<Filter> andList = new ArrayList<Filter>();\n+      for (Filter child : ((OrFilter) current).getFilters()) {\n+        if (child instanceof AndFilter) {\n+          andList.add(child);\n+        } else if (child instanceof OrFilter) {\n+          // pull apart the kids of the OR expression\n+          nonAndList.addAll(((OrFilter) child).getFilters());\n+        } else {\n+          nonAndList.add(child);\n+        }\n+      }\n+      if (!andList.isEmpty()) {\n+        Set<Filter> result = new HashSet<>();\n+        generateAllCombinations(result, andList, nonAndList);\n+        return new AndFilter(result);\n+      }\n+    }\n+    return current;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  @VisibleForTesting\n+  static Filter flatten(Filter root)\n+  {\n+    if (root instanceof BooleanFilter) {\n+      List<Filter> children = new ArrayList<>(((BooleanFilter) root).getFilters());\n+      // iterate through the index, so that if we add more children,\n+      // they don't get re-visited\n+      for (int i = 0; i < children.size(); ++i) {\n+        Filter child = flatten(children.get(i));\n+        // do we need to flatten?\n+        if (child.getClass() == root.getClass() && !(child instanceof NotFilter)) {\n+          boolean first = true;\n+          Set<Filter> grandKids = ((BooleanFilter) child).getFilters();\n+          for (Filter grandkid : grandKids) {\n+            // for the first grandkid replace the original parent\n+            if (first) {\n+              first = false;\n+              children.set(i, grandkid);\n+            } else {\n+              children.add(++i, grandkid);\n+            }\n+          }\n+        } else {\n+          children.set(i, child);\n+        }\n+      }\n+      // if we have a singleton AND or OR, just return the child\n+      if (children.size() == 1 && (root instanceof AndFilter || root instanceof OrFilter)) {\n+        return children.get(0);\n+      }\n+\n+      if (root instanceof AndFilter) {\n+        return new AndFilter(children);\n+      } else if (root instanceof OrFilter) {\n+        return new OrFilter(children);\n+      }\n+    }\n+    return root;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  private static void generateAllCombinations(\n+      Set<Filter> result,\n+      List<Filter> andList,\n+      List<Filter> nonAndList\n+  )\n+  {\n+    Set<Filter> children = ((AndFilter) andList.get(0)).getFilters();\n+    if (result.isEmpty()) {\n+      for (Filter child : children) {\n+        Set<Filter> a = new HashSet<>(nonAndList);\n+        a.add(child);\n+        result.add(new OrFilter(a));\n+      }\n+    } else {\n+      List<Filter> work = new ArrayList<>(result);\n+      result.clear();\n+      for (Filter child : children) {\n+        for (Filter or : work) {\n+          Set<Filter> a = new HashSet<>((((OrFilter) or).getFilters()));\n+          a.add(child);\n+          result.add(new OrFilter(a));\n+        }\n+      }\n+    }\n+    if (andList.size() > 1) {\n+      generateAllCombinations(result, andList.subList(1, andList.size()), nonAndList);\n+    }\n+  }\n+\n+  // All functions below were basically adopted from Apache Calcite and modified to use them in Druid.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9821ee1b85006fd38d770b8c2038baa59a41769"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTIwNTc2Nw==", "bodyText": "Sounds good. Split into CalciteCnfHelper and HiveCnfHelper.", "url": "https://github.com/apache/druid/pull/9634#discussion_r405205767", "createdAt": "2020-04-08T01:25:23Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/CnfHelper.java", "diffHunk": "@@ -0,0 +1,464 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.filter;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import org.apache.druid.query.filter.BooleanFilter;\n+import org.apache.druid.query.filter.Filter;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+/**\n+ * A helper class to convert a filter to CNF.\n+ *\n+ * The methods in this class are mainly adopted from Apache Hive and Apache Calcite.\n+ */\n+public class CnfHelper\n+{\n+  public static Filter toCnf(Filter current)\n+  {\n+    current = pushDownNot(current);\n+    current = flatten(current);\n+    current = pull(current);\n+    current = convertToCNFInternal(current);\n+    current = flatten(current);\n+    return current;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  @VisibleForTesting\n+  static Filter pushDownNot(Filter current)\n+  {\n+    if (current instanceof NotFilter) {\n+      Filter child = ((NotFilter) current).getBaseFilter();\n+      if (child instanceof NotFilter) {\n+        return pushDownNot(((NotFilter) child).getBaseFilter());\n+      }\n+      if (child instanceof AndFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((AndFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new OrFilter(children);\n+      }\n+      if (child instanceof OrFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((OrFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new AndFilter(children);\n+      }\n+    }\n+\n+    if (current instanceof AndFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((AndFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new AndFilter(children);\n+    }\n+\n+    if (current instanceof OrFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((OrFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new OrFilter(children);\n+    }\n+    return current;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  private static Filter convertToCNFInternal(Filter current)\n+  {\n+    if (current instanceof NotFilter) {\n+      return new NotFilter(convertToCNFInternal(((NotFilter) current).getBaseFilter()));\n+    }\n+    if (current instanceof AndFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((AndFilter) current).getFilters()) {\n+        children.add(convertToCNFInternal(child));\n+      }\n+      return new AndFilter(children);\n+    }\n+    if (current instanceof OrFilter) {\n+      // a list of leaves that weren't under AND expressions\n+      List<Filter> nonAndList = new ArrayList<Filter>();\n+      // a list of AND expressions that we need to distribute\n+      List<Filter> andList = new ArrayList<Filter>();\n+      for (Filter child : ((OrFilter) current).getFilters()) {\n+        if (child instanceof AndFilter) {\n+          andList.add(child);\n+        } else if (child instanceof OrFilter) {\n+          // pull apart the kids of the OR expression\n+          nonAndList.addAll(((OrFilter) child).getFilters());\n+        } else {\n+          nonAndList.add(child);\n+        }\n+      }\n+      if (!andList.isEmpty()) {\n+        Set<Filter> result = new HashSet<>();\n+        generateAllCombinations(result, andList, nonAndList);\n+        return new AndFilter(result);\n+      }\n+    }\n+    return current;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  @VisibleForTesting\n+  static Filter flatten(Filter root)\n+  {\n+    if (root instanceof BooleanFilter) {\n+      List<Filter> children = new ArrayList<>(((BooleanFilter) root).getFilters());\n+      // iterate through the index, so that if we add more children,\n+      // they don't get re-visited\n+      for (int i = 0; i < children.size(); ++i) {\n+        Filter child = flatten(children.get(i));\n+        // do we need to flatten?\n+        if (child.getClass() == root.getClass() && !(child instanceof NotFilter)) {\n+          boolean first = true;\n+          Set<Filter> grandKids = ((BooleanFilter) child).getFilters();\n+          for (Filter grandkid : grandKids) {\n+            // for the first grandkid replace the original parent\n+            if (first) {\n+              first = false;\n+              children.set(i, grandkid);\n+            } else {\n+              children.add(++i, grandkid);\n+            }\n+          }\n+        } else {\n+          children.set(i, child);\n+        }\n+      }\n+      // if we have a singleton AND or OR, just return the child\n+      if (children.size() == 1 && (root instanceof AndFilter || root instanceof OrFilter)) {\n+        return children.get(0);\n+      }\n+\n+      if (root instanceof AndFilter) {\n+        return new AndFilter(children);\n+      } else if (root instanceof OrFilter) {\n+        return new OrFilter(children);\n+      }\n+    }\n+    return root;\n+  }\n+\n+  // A helper function adapted from Apache Hive, see:\n+  // https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+  private static void generateAllCombinations(\n+      Set<Filter> result,\n+      List<Filter> andList,\n+      List<Filter> nonAndList\n+  )\n+  {\n+    Set<Filter> children = ((AndFilter) andList.get(0)).getFilters();\n+    if (result.isEmpty()) {\n+      for (Filter child : children) {\n+        Set<Filter> a = new HashSet<>(nonAndList);\n+        a.add(child);\n+        result.add(new OrFilter(a));\n+      }\n+    } else {\n+      List<Filter> work = new ArrayList<>(result);\n+      result.clear();\n+      for (Filter child : children) {\n+        for (Filter or : work) {\n+          Set<Filter> a = new HashSet<>((((OrFilter) or).getFilters()));\n+          a.add(child);\n+          result.add(new OrFilter(a));\n+        }\n+      }\n+    }\n+    if (andList.size() > 1) {\n+      generateAllCombinations(result, andList.subList(1, andList.size()), nonAndList);\n+    }\n+  }\n+\n+  // All functions below were basically adopted from Apache Calcite and modified to use them in Druid.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE5OTYxNw=="}, "originalCommit": {"oid": "d9821ee1b85006fd38d770b8c2038baa59a41769"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxODAyOTY4OnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/filter/Filters.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxOTo0MjowNlrOGC-LRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDo1NzowNVrOGDGByQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2OTAyOA==", "bodyText": "It would be nice to add some commentary on each line describing (possibly with example) why it exists. I know there is something in PR description but this is non-trivial.", "url": "https://github.com/apache/druid/pull/9634#discussion_r405769028", "createdAt": "2020-04-08T19:42:06Z", "author": {"login": "himanshug"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/Filters.java", "diffHunk": "@@ -426,175 +424,21 @@ public static Filter convertToCNFFromQueryContext(Query query, @Nullable Filter\n       return null;\n     }\n     boolean useCNF = query.getContextBoolean(CTX_KEY_USE_FILTER_CNF, false);\n-    return useCNF ? toCNF(filter) : filter;\n+    return useCNF ? Filters.toCnf(filter) : filter;\n   }\n \n-  public static Filter toCNF(Filter current)\n+  public static Filter toCnf(Filter current)\n   {\n-    current = pushDownNot(current);\n-    current = flatten(current);\n-    current = convertToCNFInternal(current);\n-    current = flatten(current);\n+    current = HiveCnfHelper.pushDownNot(current);\n+    current = HiveCnfHelper.flatten(current);\n+    // Pull out AND filters first to convert the filter into a conjunctive form.\n+    // This is important to not create a huge CNF.\n+    current = CalciteCnfHelper.pull(current);\n+    current = HiveCnfHelper.convertToCNFInternal(current);\n+    current = HiveCnfHelper.flatten(current);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "258421b60aae35bbd4be0ebf9d6f6c1edb230c32"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTgxNDUzMQ==", "bodyText": "This code was adopted from Apache Hive as well and I don't know the exact reason for why we are doing in this order.. I tried to add some. Hopefully it helps.", "url": "https://github.com/apache/druid/pull/9634#discussion_r405814531", "createdAt": "2020-04-08T21:05:31Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/Filters.java", "diffHunk": "@@ -426,175 +424,21 @@ public static Filter convertToCNFFromQueryContext(Query query, @Nullable Filter\n       return null;\n     }\n     boolean useCNF = query.getContextBoolean(CTX_KEY_USE_FILTER_CNF, false);\n-    return useCNF ? toCNF(filter) : filter;\n+    return useCNF ? Filters.toCnf(filter) : filter;\n   }\n \n-  public static Filter toCNF(Filter current)\n+  public static Filter toCnf(Filter current)\n   {\n-    current = pushDownNot(current);\n-    current = flatten(current);\n-    current = convertToCNFInternal(current);\n-    current = flatten(current);\n+    current = HiveCnfHelper.pushDownNot(current);\n+    current = HiveCnfHelper.flatten(current);\n+    // Pull out AND filters first to convert the filter into a conjunctive form.\n+    // This is important to not create a huge CNF.\n+    current = CalciteCnfHelper.pull(current);\n+    current = HiveCnfHelper.convertToCNFInternal(current);\n+    current = HiveCnfHelper.flatten(current);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2OTAyOA=="}, "originalCommit": {"oid": "258421b60aae35bbd4be0ebf9d6f6c1edb230c32"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg2NjAyOA==", "bodyText": "thanks, those comments do help.\nI think I got/remember it now.\n\"pushDownNot, flatten, convertToCnf, flatten\" sequence is to convert original boolean expression into the CNF form, which doesn't necessarily \"optimize\"(from evaluation perspective) but CNF form is useful for analysis on boolean expressions(often  use to  prove equivalence of two different boolean expressions), in  our case, separating filter evaluation into pre/post cursor use for applying filters.\nyour PRs (this and #9608 ) are towards optimizing the boolean expression. Are your PRs driven by similar work done in Calcite or some know work or based  on your own heuristics ? For example there are optimizations like (not sure if they are taken care of or not)\na OR (NOT  a) = TRUE\na AND (NOT A) = FALSE\n... and more listed in section \"Ten Basic Rules of Boolean Algebra, in https://grace.bluegrass.kctcs.edu/~kdunn0001/files/Simplification/4_Simplification_print.html\nunfortunate thing is that this is a major research topic with no proven algorithm that works in all cases.\nBTW: this comment is not a PR code review but just general things I noted while thinking more about this and wanted to document here.", "url": "https://github.com/apache/druid/pull/9634#discussion_r405866028", "createdAt": "2020-04-08T23:10:19Z", "author": {"login": "himanshug"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/Filters.java", "diffHunk": "@@ -426,175 +424,21 @@ public static Filter convertToCNFFromQueryContext(Query query, @Nullable Filter\n       return null;\n     }\n     boolean useCNF = query.getContextBoolean(CTX_KEY_USE_FILTER_CNF, false);\n-    return useCNF ? toCNF(filter) : filter;\n+    return useCNF ? Filters.toCnf(filter) : filter;\n   }\n \n-  public static Filter toCNF(Filter current)\n+  public static Filter toCnf(Filter current)\n   {\n-    current = pushDownNot(current);\n-    current = flatten(current);\n-    current = convertToCNFInternal(current);\n-    current = flatten(current);\n+    current = HiveCnfHelper.pushDownNot(current);\n+    current = HiveCnfHelper.flatten(current);\n+    // Pull out AND filters first to convert the filter into a conjunctive form.\n+    // This is important to not create a huge CNF.\n+    current = CalciteCnfHelper.pull(current);\n+    current = HiveCnfHelper.convertToCNFInternal(current);\n+    current = HiveCnfHelper.flatten(current);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2OTAyOA=="}, "originalCommit": {"oid": "258421b60aae35bbd4be0ebf9d6f6c1edb230c32"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5NzY3Mw==", "bodyText": "Yeah, my two PRs are for creating more optimized CNFs in terms of size since we have seen that a filter of the same pattern with that at #9634 (comment) blows up memory after CNF conversion. Those PRs are mostly based on my heuristics. I happened to find RexUtil.pullFactors() of Calcite which seems useful to address the size issue.\n\nFor example there are optimizations like (not sure if they are taken care of or not)\na OR (NOT a) = TRUE\na AND (NOT A) = FALSE\n\nGood point. I'm aware of that our CNF conversion doesn't reduce such filters by evaluating them, but I hope Calcite could reduce most of them in the SQL layer. After the join support was added, now there are two main use cases for the filter in CNF. One is cursor filter push down to use bitmaps if possible (the one you mentioned) and another is join filter push down to place filters at proper joins (both happen in historicals). I think it would be safe to assume that the filter is already optimized by Calcite for these use cases since SQL is now the primary way for querying.\n\nunfortunate thing is that this is a major research topic with no proven algorithm that works in all cases.\n\nI totally agree. @jon-wei and I have been talking about it and perhaps the best would be using a well-tested third-party library. But I haven't seen such a library yet.", "url": "https://github.com/apache/druid/pull/9634#discussion_r405897673", "createdAt": "2020-04-09T00:57:05Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/Filters.java", "diffHunk": "@@ -426,175 +424,21 @@ public static Filter convertToCNFFromQueryContext(Query query, @Nullable Filter\n       return null;\n     }\n     boolean useCNF = query.getContextBoolean(CTX_KEY_USE_FILTER_CNF, false);\n-    return useCNF ? toCNF(filter) : filter;\n+    return useCNF ? Filters.toCnf(filter) : filter;\n   }\n \n-  public static Filter toCNF(Filter current)\n+  public static Filter toCnf(Filter current)\n   {\n-    current = pushDownNot(current);\n-    current = flatten(current);\n-    current = convertToCNFInternal(current);\n-    current = flatten(current);\n+    current = HiveCnfHelper.pushDownNot(current);\n+    current = HiveCnfHelper.flatten(current);\n+    // Pull out AND filters first to convert the filter into a conjunctive form.\n+    // This is important to not create a huge CNF.\n+    current = CalciteCnfHelper.pull(current);\n+    current = HiveCnfHelper.convertToCNFInternal(current);\n+    current = HiveCnfHelper.flatten(current);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2OTAyOA=="}, "originalCommit": {"oid": "258421b60aae35bbd4be0ebf9d6f6c1edb230c32"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxODAzMjA3OnYy", "diffSide": "RIGHT", "path": "processing/src/main/java/org/apache/druid/segment/filter/cnf/HiveCnfHelper.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxOTo0Mjo0NlrOGC-MuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQyMTowNTo1NFrOGDA9rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2OTQwMA==", "bodyText": "is this still \"Internal\" ?", "url": "https://github.com/apache/druid/pull/9634#discussion_r405769400", "createdAt": "2020-04-08T19:42:46Z", "author": {"login": "himanshug"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/cnf/HiveCnfHelper.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.filter.cnf;\n+\n+import org.apache.druid.query.filter.BooleanFilter;\n+import org.apache.druid.query.filter.Filter;\n+import org.apache.druid.segment.filter.AndFilter;\n+import org.apache.druid.segment.filter.NotFilter;\n+import org.apache.druid.segment.filter.OrFilter;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * All functions in this class were basically adopted from Apache Hive and modified to use them in Druid.\n+ * See https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+ * for original implementation.\n+ */\n+public class HiveCnfHelper\n+{\n+  public static Filter pushDownNot(Filter current)\n+  {\n+    if (current instanceof NotFilter) {\n+      Filter child = ((NotFilter) current).getBaseFilter();\n+      if (child instanceof NotFilter) {\n+        return pushDownNot(((NotFilter) child).getBaseFilter());\n+      }\n+      if (child instanceof AndFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((AndFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new OrFilter(children);\n+      }\n+      if (child instanceof OrFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((OrFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new AndFilter(children);\n+      }\n+    }\n+\n+    if (current instanceof AndFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((AndFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new AndFilter(children);\n+    }\n+\n+    if (current instanceof OrFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((OrFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new OrFilter(children);\n+    }\n+    return current;\n+  }\n+\n+  public static Filter convertToCNFInternal(Filter current)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "258421b60aae35bbd4be0ebf9d6f6c1edb230c32"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTgxNDcwMw==", "bodyText": "Oops, renamed to convertToCnf().", "url": "https://github.com/apache/druid/pull/9634#discussion_r405814703", "createdAt": "2020-04-08T21:05:54Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/filter/cnf/HiveCnfHelper.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.filter.cnf;\n+\n+import org.apache.druid.query.filter.BooleanFilter;\n+import org.apache.druid.query.filter.Filter;\n+import org.apache.druid.segment.filter.AndFilter;\n+import org.apache.druid.segment.filter.NotFilter;\n+import org.apache.druid.segment.filter.OrFilter;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * All functions in this class were basically adopted from Apache Hive and modified to use them in Druid.\n+ * See https://github.com/apache/hive/blob/branch-2.0/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+ * for original implementation.\n+ */\n+public class HiveCnfHelper\n+{\n+  public static Filter pushDownNot(Filter current)\n+  {\n+    if (current instanceof NotFilter) {\n+      Filter child = ((NotFilter) current).getBaseFilter();\n+      if (child instanceof NotFilter) {\n+        return pushDownNot(((NotFilter) child).getBaseFilter());\n+      }\n+      if (child instanceof AndFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((AndFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new OrFilter(children);\n+      }\n+      if (child instanceof OrFilter) {\n+        Set<Filter> children = new HashSet<>();\n+        for (Filter grandChild : ((OrFilter) child).getFilters()) {\n+          children.add(pushDownNot(new NotFilter(grandChild)));\n+        }\n+        return new AndFilter(children);\n+      }\n+    }\n+\n+    if (current instanceof AndFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((AndFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new AndFilter(children);\n+    }\n+\n+    if (current instanceof OrFilter) {\n+      Set<Filter> children = new HashSet<>();\n+      for (Filter child : ((OrFilter) current).getFilters()) {\n+        children.add(pushDownNot(child));\n+      }\n+      return new OrFilter(children);\n+    }\n+    return current;\n+  }\n+\n+  public static Filter convertToCNFInternal(Filter current)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTc2OTQwMA=="}, "originalCommit": {"oid": "258421b60aae35bbd4be0ebf9d6f6c1edb230c32"}, "originalPosition": 81}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2502, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}