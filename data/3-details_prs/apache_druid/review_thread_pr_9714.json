{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA0Mjg4NTU1", "number": 9714, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo0NDowM1rODytsPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNjo1MDoxNFrOD3fyjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTAzOTk2OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/environment-configs/override-examples/hadoop/azure_to_hdfs", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo0NDowM1rOGG6_ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzo0Mzo1NVrOGNBMhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMTE2Mw==", "bodyText": "Not sure what do you mean by this? Doesn't reading the data from Azure still requires these?", "url": "https://github.com/apache/druid/pull/9714#discussion_r409911163", "createdAt": "2020-04-16T23:44:03Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/azure_to_hdfs", "diffHunk": "@@ -0,0 +1,34 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> with your cloud configs/credentials\n+#\n+druid_storage_type=hdfs\n+druid_storage_storageDirectory=/druid/segments\n+\n+druid_extensions_loadList=[\"druid-azure-extensions\",\"druid-hdfs-storage\"]\n+\n+# Not used since we have HDFS deep storage, but the Druid Azure extension requires these to be defined", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMwNDI2MA==", "bodyText": "Hadoop has its own mechanisms for reading from Azure, so it doesn't use those Druid-side properties for getting the input data", "url": "https://github.com/apache/druid/pull/9714#discussion_r416304260", "createdAt": "2020-04-28T03:43:55Z", "author": {"login": "jon-wei"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/azure_to_hdfs", "diffHunk": "@@ -0,0 +1,34 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> with your cloud configs/credentials\n+#\n+druid_storage_type=hdfs\n+druid_storage_storageDirectory=/druid/segments\n+\n+druid_extensions_loadList=[\"druid-azure-extensions\",\"druid-hdfs-storage\"]\n+\n+# Not used since we have HDFS deep storage, but the Druid Azure extension requires these to be defined", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMTE2Mw=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA1MDA3OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo0OTowOVrOGG7FiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyMzo0OVrOGNB6-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMjcxMw==", "bodyText": "This should be the credentials file on the docker container.\nThe run_cluster script will copy the files in the folder at -Dresource.file.dir.path to /shared/docker/credentials/\nSo if you have the credentials file at /bob/folderx/secret.json then -Dresource.file.dir.path should be set to /bob/folderx/ and GOOGLE_APPLICATION_CREDENTIALS should be set to /shared/docker/credentials/secret.json", "url": "https://github.com/apache/druid/pull/9714#discussion_r409912713", "createdAt": "2020-04-16T23:49:09Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs", "diffHunk": "@@ -0,0 +1,31 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> and <YOUR_GOOGLE_CREDENTIALS_FILE_NAME> with your cloud configs/credentials\n+#\n+druid_storage_type=google\n+druid_google_bucket=<OVERRIDE_THIS>\n+druid_google_prefix=<OVERRIDE_THIS>\n+\n+druid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\n+\n+# Set this to the path of the credentials file on the host where the integration tests are running\n+GOOGLE_APPLICATION_CREDENTIALS=<OVERRIDE_THIS>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjE1NQ==", "bodyText": "I adjusted this to follow what the base gcs example had", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316155", "createdAt": "2020-04-28T04:23:49Z", "author": {"login": "jon-wei"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs", "diffHunk": "@@ -0,0 +1,31 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> and <YOUR_GOOGLE_CREDENTIALS_FILE_NAME> with your cloud configs/credentials\n+#\n+druid_storage_type=google\n+druid_google_bucket=<OVERRIDE_THIS>\n+druid_google_prefix=<OVERRIDE_THIS>\n+\n+druid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\n+\n+# Set this to the path of the credentials file on the host where the integration tests are running\n+GOOGLE_APPLICATION_CREDENTIALS=<OVERRIDE_THIS>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMjcxMw=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA1MDQ2OnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_hdfs", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo0OToxOVrOGG7FxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyMzo1M1rOGNB7DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMjc3Mg==", "bodyText": "same as above", "url": "https://github.com/apache/druid/pull/9714#discussion_r409912772", "createdAt": "2020-04-16T23:49:19Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_hdfs", "diffHunk": "@@ -0,0 +1,31 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> and <YOUR_GOOGLE_CREDENTIALS_FILE_NAME> with your cloud configs/credentials\n+#\n+druid_storage_type=hdfs\n+druid_storage_storageDirectory=/druid/segments\n+\n+druid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\n+\n+# Set this to the path of the credentials file on the host where the integration tests are running\n+GOOGLE_APPLICATION_CREDENTIALS=<OVERRIDE_THIS>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjE3Mw==", "bodyText": "I adjusted this to follow what the base gcs example had", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316173", "createdAt": "2020-04-28T04:23:53Z", "author": {"login": "jon-wei"}, "path": "integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_hdfs", "diffHunk": "@@ -0,0 +1,31 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+#\n+# Example of override config file to provide.\n+# Please replace <OVERRIDE_THIS> and <YOUR_GOOGLE_CREDENTIALS_FILE_NAME> with your cloud configs/credentials\n+#\n+druid_storage_type=hdfs\n+druid_storage_storageDirectory=/druid/segments\n+\n+druid_extensions_loadList=[\"druid-google-extensions\",\"druid-hdfs-storage\"]\n+\n+# Set this to the path of the credentials file on the host where the integration tests are running\n+GOOGLE_APPLICATION_CREDENTIALS=<OVERRIDE_THIS>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMjc3Mg=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA1NDExOnYy", "diffSide": "RIGHT", "path": "integration-tests/docker/environment-configs/router", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo1MToyMlrOGG7H_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo1MToyMlrOGG7H_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzM0MQ==", "bodyText": "NIce!!!!", "url": "https://github.com/apache/druid/pull/9714#discussion_r409913341", "createdAt": "2020-04-16T23:51:22Z", "author": {"login": "maytasm"}, "path": "integration-tests/docker/environment-configs/router", "diffHunk": "@@ -27,3 +27,5 @@ SERVICE_DRUID_JAVA_OPTS=-server -Xmx128m -XX:+UseG1GC -agentlib:jdwp=transport=d\n druid_auth_basic_common_cacheDirectory=/tmp/authCache/router\n druid_sql_avatica_enable=true\n druid_server_https_crlPath=/tls/revocations.crl\n+druid_router_managementProxy_enabled=true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA1ODE3OnYy", "diffSide": "RIGHT", "path": "integration-tests/run_cluster.sh", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo1MzoxMVrOGG7KSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowMzowN1rOGNortw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzkzMA==", "bodyText": "Can we not get gcs-connector-hadoop from pull-deps too?", "url": "https://github.com/apache/druid/pull/9714#discussion_r409913930", "createdAt": "2020-04-16T23:53:11Z", "author": {"login": "maytasm"}, "path": "integration-tests/run_cluster.sh", "diffHunk": "@@ -74,17 +74,18 @@\n   # For druid-kinesis-indexing-service\n   mkdir -p $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n   mv $SHARED_DIR/docker/lib/druid-kinesis-indexing-service-* $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n-  $ For druid-parquet-extensions\n+  # For druid-parquet-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-parquet-extensions\n   mv $SHARED_DIR/docker/lib/druid-parquet-extensions-* $SHARED_DIR/docker/extensions/druid-parquet-extensions\n-  $ For druid-orc-extensions\n+  # For druid-orc-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-orc-extensions\n   mv $SHARED_DIR/docker/lib/druid-orc-extensions-* $SHARED_DIR/docker/extensions/druid-orc-extensions\n \n   # Pull Hadoop dependency if needed\n   if [ -n \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" == true ]\n   then\n-    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5\n+    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5 -h org.apache.hadoop:hadoop-azure:2.8.5\n+    curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar --output $SHARED_DIR/docker/lib/gcs-connector-hadoop2-latest.jar", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjU1OQ==", "bodyText": "Hm, I actually hit what I think was a bug with pull-deps, I kept getting a class not found exception for something OkHttp related, so I was never able to test this with pull-deps", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316559", "createdAt": "2020-04-28T04:25:08Z", "author": {"login": "jon-wei"}, "path": "integration-tests/run_cluster.sh", "diffHunk": "@@ -74,17 +74,18 @@\n   # For druid-kinesis-indexing-service\n   mkdir -p $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n   mv $SHARED_DIR/docker/lib/druid-kinesis-indexing-service-* $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n-  $ For druid-parquet-extensions\n+  # For druid-parquet-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-parquet-extensions\n   mv $SHARED_DIR/docker/lib/druid-parquet-extensions-* $SHARED_DIR/docker/extensions/druid-parquet-extensions\n-  $ For druid-orc-extensions\n+  # For druid-orc-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-orc-extensions\n   mv $SHARED_DIR/docker/lib/druid-orc-extensions-* $SHARED_DIR/docker/extensions/druid-orc-extensions\n \n   # Pull Hadoop dependency if needed\n   if [ -n \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" == true ]\n   then\n-    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5\n+    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5 -h org.apache.hadoop:hadoop-azure:2.8.5\n+    curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar --output $SHARED_DIR/docker/lib/gcs-connector-hadoop2-latest.jar", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzkzMA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MTIyMw==", "bodyText": "Should we create an issue for this? Seems like might be a problem for new users / going through tutorial?", "url": "https://github.com/apache/druid/pull/9714#discussion_r416951223", "createdAt": "2020-04-28T22:03:07Z", "author": {"login": "maytasm"}, "path": "integration-tests/run_cluster.sh", "diffHunk": "@@ -74,17 +74,18 @@\n   # For druid-kinesis-indexing-service\n   mkdir -p $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n   mv $SHARED_DIR/docker/lib/druid-kinesis-indexing-service-* $SHARED_DIR/docker/extensions/druid-kinesis-indexing-service\n-  $ For druid-parquet-extensions\n+  # For druid-parquet-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-parquet-extensions\n   mv $SHARED_DIR/docker/lib/druid-parquet-extensions-* $SHARED_DIR/docker/extensions/druid-parquet-extensions\n-  $ For druid-orc-extensions\n+  # For druid-orc-extensions\n   mkdir -p $SHARED_DIR/docker/extensions/druid-orc-extensions\n   mv $SHARED_DIR/docker/lib/druid-orc-extensions-* $SHARED_DIR/docker/extensions/druid-orc-extensions\n \n   # Pull Hadoop dependency if needed\n   if [ -n \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" ] && [ \"$DRUID_INTEGRATION_TEST_START_HADOOP_DOCKER\" == true ]\n   then\n-    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5\n+    java -cp \"$SHARED_DIR/docker/lib/*\" -Ddruid.extensions.hadoopDependenciesDir=\"$SHARED_DIR/hadoop-dependencies\" org.apache.druid.cli.Main tools pull-deps -h org.apache.hadoop:hadoop-client:2.8.5 -h org.apache.hadoop:hadoop-aws:2.8.5 -h org.apache.hadoop:hadoop-azure:2.8.5\n+    curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar --output $SHARED_DIR/docker/lib/gcs-connector-hadoop2-latest.jar", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxMzkzMA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA1OTc2OnYy", "diffSide": "RIGHT", "path": "integration-tests/pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQyMzo1Mzo0OVrOGG7LLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzo0NjoxNFrOGNBPLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxNDE1OQ==", "bodyText": "I believe there is a problem if using hadoop as deep storage and datasource contains special char / whitespace. Was that the reason for this change?", "url": "https://github.com/apache/druid/pull/9714#discussion_r409914159", "createdAt": "2020-04-16T23:53:49Z", "author": {"login": "maytasm"}, "path": "integration-tests/pom.xml", "diffHunk": "@@ -430,7 +431,7 @@\n                                 -Dfile.encoding=UTF-8\n                                 -Ddruid.test.config.dockerIp=${env.DOCKER_IP}\n                                 -Ddruid.test.config.hadoopDir=${env.HADOOP_DIR}\n-                                -Ddruid.test.config.extraDatasourceNameSuffix=\\ \u0420\u043e\u0441\u0441\u0438\u044f\\ \ud55c\uad6d\\ \u4e2d\u56fd!?\n+                                -Ddruid.test.config.extraDatasourceNameSuffix=${extra.datasource.name.suffix}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMwNDk0MA==", "bodyText": "Yes, that's why this is introduced, the disable the special chars for the hadoop tests", "url": "https://github.com/apache/druid/pull/9714#discussion_r416304940", "createdAt": "2020-04-28T03:46:14Z", "author": {"login": "jon-wei"}, "path": "integration-tests/pom.xml", "diffHunk": "@@ -430,7 +431,7 @@\n                                 -Dfile.encoding=UTF-8\n                                 -Ddruid.test.config.dockerIp=${env.DOCKER_IP}\n                                 -Ddruid.test.config.hadoopDir=${env.HADOOP_DIR}\n-                                -Ddruid.test.config.extraDatasourceNameSuffix=\\ \u0420\u043e\u0441\u0441\u0438\u044f\\ \ud55c\uad6d\\ \u4e2d\u56fd!?\n+                                -Ddruid.test.config.extraDatasourceNameSuffix=${extra.datasource.name.suffix}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxNDE1OQ=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA5NDgwOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDoxMjoxMlrOGG7gGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyNTozNVrOGNB8_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTUxNQ==", "bodyText": "Maybe worth mentioning about seting the -Dresource.file.dir.path and GOOGLE_APPLICATION_CREDENTIALS since you need to make druid-google-extensions  happy on the druid nodes.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409919515", "createdAt": "2020-04-17T00:12:12Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjY3MQ==", "bodyText": "Added a note about -Dresource.file.dir.path", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316671", "createdAt": "2020-04-28T04:25:35Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTUxNQ=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTA5NjMwOnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDoxMjo1NVrOGG7g9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyNTo0NFrOGNB9Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTczNQ==", "bodyText": "same as above", "url": "https://github.com/apache/druid/pull/9714#discussion_r409919735", "createdAt": "2020-04-17T00:12:55Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n+ *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n+ *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n+ * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n+ * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjcyNg==", "bodyText": "Added a note about -Dresource.file.dir.path", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316726", "createdAt": "2020-04-28T04:25:44Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n+ *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n+ *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n+ * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n+ * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTczNQ=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTEwMjQ5OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDoxNTo0OFrOGG7kkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyNjowNFrOGNB9qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMDY1Nw==", "bodyText": "step 1 is this step. The location should be /resources/data/batch_index on the hadoop container fs and also at /batch_index on hdfs", "url": "https://github.com/apache/druid/pull/9714#discussion_r409920657", "createdAt": "2020-04-17T00:15:48Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjg0Mw==", "bodyText": "I updated the instructions to address both the json and tsv directories", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316843", "createdAt": "2020-04-28T04:26:04Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMDY1Nw=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTEwNjM1OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDoxNzozMFrOGG7muA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyNjoyMFrOGNB9_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMTIwOA==", "bodyText": "Maybe we should combine TestNGGroup.HADOOP_INDEX with TestNGGroup.HDFS_DEEP_STORAGE", "url": "https://github.com/apache/druid/pull/9714#discussion_r409921208", "createdAt": "2020-04-17T00:17:30Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjkyNQ==", "bodyText": "I combined the groups into HDFS_DEEP_STORAGE and deleted HADOOP_INDEX", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316925", "createdAt": "2020-04-28T04:26:20Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMTIwOA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTExMjM0OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDoyMDozNFrOGG7qJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyODo1M1rOGNCBCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA==", "bodyText": "currently /batch_index/tsv requires manual setup\nFrom the integration-tests/README.md ...\nCurrently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\nCreate a directory called batchHadoop1 in the hadoop file system\n(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \ninto that directory (as its only file).\n\nWe should automatically setup this dir for the hadoop docker container (similar to how we setup the wikipedia json files). You can create a new dir in integration-tests/src/test/resources/data/batch_index called tsv and copy integration-tests/src/test/resources/hadoop/batch_hadoop.data to integration-tests/src/test/resources/data/batch_index/tsv (the run-cluster script should handle the rest and create /batch_index/tsv with batch_hadoop.data inside)", "url": "https://github.com/apache/druid/pull/9714#discussion_r409922084", "createdAt": "2020-04-17T00:20:34Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzMTk3MA==", "bodyText": "Maybe also just get rid of the hadoopTestDir in the DockerConfigProvider. I think it's no longer needed. If using hadoop container then everything is automatically setup. If running your own hadoop then they should copy to /batch_index/tsv since the path is hardcoded in the specPathsTransform anyway", "url": "https://github.com/apache/druid/pull/9714#discussion_r409931970", "createdAt": "2020-04-17T00:56:36Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzMjM4NA==", "bodyText": "Can you also update the integration-tests/README.md ITHadoopIndexTest can be run in hadoop docker container hence the existing README.md is out of date.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409932384", "createdAt": "2020-04-17T00:58:19Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNzcwNg==", "bodyText": "For the first comment, I had already moved the batch_hadoop.data file to integration-tests/src/test/resources/data/batch_index/tsv as you suggest so we're good there.\nI got rid of hadoopTestDir and updated the README.md file with new instructions", "url": "https://github.com/apache/druid/pull/9714#discussion_r416317706", "createdAt": "2020-04-28T04:28:53Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTEyNzQ3OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDoyODoyNVrOGG7y5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyOTowNlrOGNCBUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNDMyNw==", "bodyText": "I think ITS3InputToHdfsHadoopIndexTest and ITS3InputToS3HadoopIndexTest should be in different groups. The groups can be the different deep storage. The reason for my suggestion is that if you run the whole group then you cannot switch  -Doverride.config.path between different test class. What will happens is that you will run the whole group (ITS3InputToHdfsHadoopIndexTest and ITS3InputToS3HadoopIndexTest) with the same druid config file which basically will be the exact same test (same deep storage). Same for the other cloud storages.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409924327", "createdAt": "2020-04-17T00:28:25Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket, path, and region for your data.\n+ *    This can be done by setting -Ddruid.test.config.cloudBucket, -Ddruid.test.config.cloudPath\n+ *    and -Ddruid.test.config.cloudRegion or setting \"cloud_bucket\",\"cloud_path\", and \"cloud_region\" in the config file.\n+ * 2) Set -Ddruid.s3.accessKey and -Ddruid.s3.secretKey when running the tests to your access/secret keys.\n+ * 3) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your S3 at the location set in step 1.\n+ * 4) Provide -Doverride.config.path=<PATH_TO_FILE> with s3 credentials and hdfs deep storage configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hadoop/s3_to_hdfs for env vars to provide.\n+ * 5) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n+@Test(groups = TestNGGroup.HADOOP_S3)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNzc3Nw==", "bodyText": "Good point, I moved each of these into their own group", "url": "https://github.com/apache/druid/pull/9714#discussion_r416317777", "createdAt": "2020-04-28T04:29:06Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket, path, and region for your data.\n+ *    This can be done by setting -Ddruid.test.config.cloudBucket, -Ddruid.test.config.cloudPath\n+ *    and -Ddruid.test.config.cloudRegion or setting \"cloud_bucket\",\"cloud_path\", and \"cloud_region\" in the config file.\n+ * 2) Set -Ddruid.s3.accessKey and -Ddruid.s3.secretKey when running the tests to your access/secret keys.\n+ * 3) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your S3 at the location set in step 1.\n+ * 4) Provide -Doverride.config.path=<PATH_TO_FILE> with s3 credentials and hdfs deep storage configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hadoop/s3_to_hdfs for env vars to provide.\n+ * 5) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n+@Test(groups = TestNGGroup.HADOOP_S3)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNDMyNw=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NTE0MTk1OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/resources/hadoop/wikipedia_hadoop_azure_input_index_task.json", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QwMDozNTozN1rOGG77Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoyOTo0OVrOGNCCSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNjQyMg==", "bodyText": "nit: Maybe we can combine the the different cloud input text task json into one by parameterizing the difference in the jobProperties.", "url": "https://github.com/apache/druid/pull/9714#discussion_r409926422", "createdAt": "2020-04-17T00:35:37Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/resources/hadoop/wikipedia_hadoop_azure_input_index_task.json", "diffHunk": "@@ -0,0 +1,107 @@\n+{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxODAyNw==", "bodyText": "I thought about doing that, but I was concerned that there might be even more differences in the future and there'd be too many parameters, I think I'll leave this for now", "url": "https://github.com/apache/druid/pull/9714#discussion_r416318027", "createdAt": "2020-04-28T04:29:49Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/resources/hadoop/wikipedia_hadoop_azure_input_index_task.json", "diffHunk": "@@ -0,0 +1,107 @@\n+{", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNjQyMg=="}, "originalCommit": {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDE1ODM4OnYy", "diffSide": "RIGHT", "path": "integration-tests/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowMjoxNVrOGNoqJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowMjoxNVrOGNoqJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MDgyMQ==", "bodyText": "nit: typo? both are 1)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416950821", "createdAt": "2020-04-28T22:02:15Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDE3NTQ5OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjowNzo1M1rOGNo0NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMjozMzo1M1rOGNtyvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MzM5Nw==", "bodyText": "nit: maybe mention that -Dextra.datasource.name.suffix=''  is due to github issue xxx (not sure if we have an issue for this)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416953397", "createdAt": "2020-04-28T22:07:53Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -29,17 +29,18 @@\n  * To run this test, you must:\n  * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n  *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n- * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ * 2) Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n  *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n  *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n  *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n- * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ * 3) Provide -Dresource.file.dir.path=<PATH_TO_FOLDER> with folder that contains GOOGLE_APPLICATION_CREDENTIALS file\n+ * 4) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n  *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n- * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n+ * 5) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs for env vars to provide.\n- * 4) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ * 6) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNDk0Mw==", "bodyText": "Added a note about that to the README", "url": "https://github.com/apache/druid/pull/9714#discussion_r417034943", "createdAt": "2020-04-29T02:33:53Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -29,17 +29,18 @@\n  * To run this test, you must:\n  * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n  *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n- * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ * 2) Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n  *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n  *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n  *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n- * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ * 3) Provide -Dresource.file.dir.path=<PATH_TO_FOLDER> with folder that contains GOOGLE_APPLICATION_CREDENTIALS file\n+ * 4) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n  *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n- * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n+ * 5) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs for env vars to provide.\n- * 4) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ * 6) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MzM5Nw=="}, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDIwNDk1OnYy", "diffSide": "RIGHT", "path": "integration-tests/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjoxODoxNFrOGNpFxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMjozNDoxNVrOGNtzDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1Nzg5NA==", "bodyText": "\"Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster\" should be changed to indicate that ITHadoopIndexTest can be run with both methods (docker or manual)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416957894", "createdAt": "2020-04-28T22:18:14Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n 1) Run your own Druid + Hadoop cluster and specified Hadoop configs in the configuration file (CONFIG_FILE).\n-2) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n \n Currently, hdfs-deep-storage and other <cloud>-deep-storage integration test groups can only be run with \n Druid Docker test clusters by passing -Dstart.hadoop.docker=true to start Hadoop container.\n You will also have to provide -Doverride.config.path=<PATH_TO_FILE> with your Druid's Hadoop configs set. \n See integration-tests/docker/environment-configs/override-examples/hdfs directory for example.\n Note that if the integration test you are running also uses other cloud extension (S3, Azure, GCS), additional\n-credentials/configs may need to be set in the same file as your Druid's Hadoop configs set. \n+credentials/configs may need to be set in the same file as your Druid's Hadoop configs set.\n \n Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\n-Create a directory called batchHadoop1 in the hadoop file system\n-(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \n-into that directory (as its only file).\n-\n-Add this keyword to the configuration file (see above):\n+- Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNTAyMg==", "bodyText": "Updated", "url": "https://github.com/apache/druid/pull/9714#discussion_r417035022", "createdAt": "2020-04-29T02:34:15Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n 1) Run your own Druid + Hadoop cluster and specified Hadoop configs in the configuration file (CONFIG_FILE).\n-2) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n \n Currently, hdfs-deep-storage and other <cloud>-deep-storage integration test groups can only be run with \n Druid Docker test clusters by passing -Dstart.hadoop.docker=true to start Hadoop container.\n You will also have to provide -Doverride.config.path=<PATH_TO_FILE> with your Druid's Hadoop configs set. \n See integration-tests/docker/environment-configs/override-examples/hdfs directory for example.\n Note that if the integration test you are running also uses other cloud extension (S3, Azure, GCS), additional\n-credentials/configs may need to be set in the same file as your Druid's Hadoop configs set. \n+credentials/configs may need to be set in the same file as your Druid's Hadoop configs set.\n \n Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\n-Create a directory called batchHadoop1 in the hadoop file system\n-(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \n-into that directory (as its only file).\n-\n-Add this keyword to the configuration file (see above):\n+- Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1Nzg5NA=="}, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDIxNDMxOnYy", "diffSide": "RIGHT", "path": "integration-tests/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjoyMTozNlrOGNpLWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMjozNDoyNVrOGNtzNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1OTMyMw==", "bodyText": "-Doverride.config.path actually has no effect if using int-tests-config-file. Sorry maybe the instruction should have been more clear. int-tests-config-file allows you to specify your own Druid cluster. It must be start/stop and configured manually (i.e. you have to go edit the common.runtime.properties etc manually yourself)", "url": "https://github.com/apache/druid/pull/9714#discussion_r416959323", "createdAt": "2020-04-28T22:21:36Z", "author": {"login": "maytasm"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n 1) Run your own Druid + Hadoop cluster and specified Hadoop configs in the configuration file (CONFIG_FILE).\n-2) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n \n Currently, hdfs-deep-storage and other <cloud>-deep-storage integration test groups can only be run with \n Druid Docker test clusters by passing -Dstart.hadoop.docker=true to start Hadoop container.\n You will also have to provide -Doverride.config.path=<PATH_TO_FILE> with your Druid's Hadoop configs set. \n See integration-tests/docker/environment-configs/override-examples/hdfs directory for example.\n Note that if the integration test you are running also uses other cloud extension (S3, Azure, GCS), additional\n-credentials/configs may need to be set in the same file as your Druid's Hadoop configs set. \n+credentials/configs may need to be set in the same file as your Druid's Hadoop configs set.\n \n Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\n-Create a directory called batchHadoop1 in the hadoop file system\n-(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \n-into that directory (as its only file).\n-\n-Add this keyword to the configuration file (see above):\n+- Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+  located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at /batch_index/json/\n+  If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+- Copy batch_hadoop.data located in integration-tests/src/test/resources/data/batch_index/tsv to your HDFS\n+  at /batch_index/tsv/\n+  If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n \n+Run the test using mvn (using the bundled Docker-based Hadoop cluster):\n ```\n-    \"hadoopTestDir\": \"<name_of_dir_containing_batchHadoop1>\"\n+  mvn verify -P integration-tests -Dit.test=ITHadoopIndexTest -Dstart.hadoop.docker=true -Doverride.config.path=docker/environment-configs/override-examples/hdfs -Dextra.datasource.name.suffix=''\n ```\n \n-Run the test using mvn:\n-\n+Run the test using mvn (using config file for existing Hadoop cluster):\n ```\n-  mvn verify -P int-tests-config-file -Dit.test=ITHadoopIndexTest\n+  mvn verify -P int-tests-config-file -Dit.test=ITHadoopIndexTest -Doverride.config.path=docker/environment-configs/override-examples/hdfs -Dextra.datasource.name.suffix=''", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNTA2MQ==", "bodyText": "Removed that part for the manual example", "url": "https://github.com/apache/druid/pull/9714#discussion_r417035061", "createdAt": "2020-04-29T02:34:25Z", "author": {"login": "jon-wei"}, "path": "integration-tests/README.md", "diffHunk": "@@ -214,31 +214,32 @@ of the integration test run discussed above.  This is because druid\n test clusters might not, in general, have access to hadoop.\n This also applies to integration test that uses Hadoop HDFS as an inputSource or as a deep storage. \n To run integration test that uses Hadoop, you will have to run a Hadoop cluster. This can be done in two ways:\n+1) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n 1) Run your own Druid + Hadoop cluster and specified Hadoop configs in the configuration file (CONFIG_FILE).\n-2) Run Druid Docker test clusters with Hadoop container by passing -Dstart.hadoop.docker=true to the mvn command. \n \n Currently, hdfs-deep-storage and other <cloud>-deep-storage integration test groups can only be run with \n Druid Docker test clusters by passing -Dstart.hadoop.docker=true to start Hadoop container.\n You will also have to provide -Doverride.config.path=<PATH_TO_FILE> with your Druid's Hadoop configs set. \n See integration-tests/docker/environment-configs/override-examples/hdfs directory for example.\n Note that if the integration test you are running also uses other cloud extension (S3, Azure, GCS), additional\n-credentials/configs may need to be set in the same file as your Druid's Hadoop configs set. \n+credentials/configs may need to be set in the same file as your Druid's Hadoop configs set.\n \n Currently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\n-Create a directory called batchHadoop1 in the hadoop file system\n-(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \n-into that directory (as its only file).\n-\n-Add this keyword to the configuration file (see above):\n+- Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+  located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at /batch_index/json/\n+  If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+- Copy batch_hadoop.data located in integration-tests/src/test/resources/data/batch_index/tsv to your HDFS\n+  at /batch_index/tsv/\n+  If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n \n+Run the test using mvn (using the bundled Docker-based Hadoop cluster):\n ```\n-    \"hadoopTestDir\": \"<name_of_dir_containing_batchHadoop1>\"\n+  mvn verify -P integration-tests -Dit.test=ITHadoopIndexTest -Dstart.hadoop.docker=true -Doverride.config.path=docker/environment-configs/override-examples/hdfs -Dextra.datasource.name.suffix=''\n ```\n \n-Run the test using mvn:\n-\n+Run the test using mvn (using config file for existing Hadoop cluster):\n ```\n-  mvn verify -P int-tests-config-file -Dit.test=ITHadoopIndexTest\n+  mvn verify -P int-tests-config-file -Dit.test=ITHadoopIndexTest -Doverride.config.path=docker/environment-configs/override-examples/hdfs -Dextra.datasource.name.suffix=''", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1OTMyMw=="}, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NDI1NjE5OnYy", "diffSide": "RIGHT", "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQyMjozNzo0OFrOGNpkGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwMjozNDozMlrOGNtzUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA==", "bodyText": "I think you also have to exclude this from Travis's other integration test group", "url": "https://github.com/apache/druid/pull/9714#discussion_r416965658", "createdAt": "2020-04-28T22:37:48Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "diffHunk": "@@ -84,6 +82,15 @@\n    */\n   public static final String HDFS_DEEP_STORAGE = \"hdfs-deep-storage\";\n \n+  public static final String HADOOP_S3_TO_S3 = \"hadoop-s3-to-s3-deep-storage\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2OTQ4Mg==", "bodyText": "Ahhh actually you don't since we exclude the hadoop test package in testng.xml.", "url": "https://github.com/apache/druid/pull/9714#discussion_r416969482", "createdAt": "2020-04-28T22:47:50Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "diffHunk": "@@ -84,6 +82,15 @@\n    */\n   public static final String HDFS_DEEP_STORAGE = \"hdfs-deep-storage\";\n \n+  public static final String HADOOP_S3_TO_S3 = \"hadoop-s3-to-s3-deep-storage\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA=="}, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk3MjQxMg==", "bodyText": "Wait then your -Dgroups will not work. I believe if you run mvn cmd with -Dgroups it will use the suite defined in testng.xml. This suite excludes running hadoop package (org.apache.druid.tests.hadoop). I think one solution is you will have to remove the exclusion of the hadoop package from testng.xml and add all the new groups to the exclusion list in the Travis's other integration test group", "url": "https://github.com/apache/druid/pull/9714#discussion_r416972412", "createdAt": "2020-04-28T22:55:51Z", "author": {"login": "maytasm"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "diffHunk": "@@ -84,6 +82,15 @@\n    */\n   public static final String HDFS_DEEP_STORAGE = \"hdfs-deep-storage\";\n \n+  public static final String HADOOP_S3_TO_S3 = \"hadoop-s3-to-s3-deep-storage\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA=="}, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNTA5MA==", "bodyText": "Updated as suggested", "url": "https://github.com/apache/druid/pull/9714#discussion_r417035090", "createdAt": "2020-04-29T02:34:32Z", "author": {"login": "jon-wei"}, "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "diffHunk": "@@ -84,6 +82,15 @@\n    */\n   public static final String HDFS_DEEP_STORAGE = \"hdfs-deep-storage\";\n \n+  public static final String HADOOP_S3_TO_S3 = \"hadoop-s3-to-s3-deep-storage\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA=="}, "originalCommit": {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5NTE5MTE5OnYy", "diffSide": "RIGHT", "path": ".travis.yml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNjo1MDoxNFrOGNxw9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQwNjo1MDoxNFrOGNxw9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEwMDAyMg==", "bodyText": "missing comma between slow and hadoop (\"slowhadoop\" -> \"slow,hadoop\")", "url": "https://github.com/apache/druid/pull/9714#discussion_r417100022", "createdAt": "2020-04-29T06:50:14Z", "author": {"login": "maytasm"}, "path": ".travis.yml", "diffHunk": "@@ -365,7 +365,7 @@ jobs:\n       name: \"(Compile=openjdk8, Run=openjdk8) other integration test\"\n       jdk: openjdk8\n       services: *integration_test_services\n-      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slow' JVM_RUNTIME='-Djvm.runtime=8'\n+      env: TESTNG_GROUPS='-DexcludedGroups=batch-index,perfect-rollup-parallel-batch-index,kafka-index,query,realtime-index,security,s3-deep-storage,gcs-deep-storage,azure-deep-storage,hdfs-deep-storage,s3-ingestion,kinesis-index,kafka-transactional-index,kafka-index-slow,kafka-transactional-index-slowhadoop-s3-to-s3-deep-storage,hadoop-s3-to-hdfs-deep-storage,hadoop-azure-to-azure-deep-storage,hadoop-azure-to-hdfs-deep-storage,hadoop-gcs-to-gcs-deep-storage,hadoop-gcs-to-hdfs-deep-storage' JVM_RUNTIME='-Djvm.runtime=8'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23f4be169431a7836e84b2ff44fc257f5d40083d"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2554, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}