{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIzNjA1NTIz", "number": 9935, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMzowMjoxMVrOEFuYhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMTowNjowOVrOEJTOPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NDM4Mjc4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMzowMjoxMVrOGkFj3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxMzowMjowOVrOGsvtyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw==", "bodyText": "Out of curiosity, how did you come up with this number? It would be nice to leave a comment if there was some rationale behind it.", "url": "https://github.com/apache/druid/pull/9935#discussion_r440493023", "createdAt": "2020-06-15T23:02:11Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MTQxOQ==", "bodyText": "There is a typo in ANNOUNCE_HISTORICAL_SEGMENG_BATCH (SEGMENG). I would recommend to rename to be more clear such as MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "url": "https://github.com/apache/druid/pull/9935#discussion_r446461419", "createdAt": "2020-06-27T00:34:20Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3MjI5Ng==", "bodyText": "The number 100 is just experience.\nVariable name has been changed to MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE.", "url": "https://github.com/apache/druid/pull/9935#discussion_r449572296", "createdAt": "2020-07-03T13:02:09Z", "author": {"login": "xiangqiao123"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -77,12 +79,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n \n /**\n  */\n public class IndexerSQLMetadataStorageCoordinator implements IndexerMetadataStorageCoordinator\n {\n   private static final Logger log = new Logger(IndexerSQLMetadataStorageCoordinator.class);\n+  private static final int ANNOUNCE_HISTORICAL_SEGMENG_BATCH = 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAyMw=="}, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTg2MjE2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMDo0OTo0NVrOGpx9yQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxMzoxMzozNFrOGswBMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw==", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463433", "createdAt": "2020-06-27T00:49:45Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NjE2NQ==", "bodyText": "There is a log.infoSegments method that takes a collection of segments that should be used if logging a potentially large list of segments.", "url": "https://github.com/apache/druid/pull/9935#discussion_r446466165", "createdAt": "2020-06-27T01:14:35Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw=="}, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NzI2NQ==", "bodyText": "This will corrupt the overlord logs by printing the similar logs over again. How about printing them all at once? You can do by doing like this:\n      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n      log.info(\"Found these segments already exist in DB: %s\", existedSegments);\n      for (DataSegment segment : segments) {\n        if (!existedSegments.contains(segment.getId().toString())) {\n          toInsertSegments.add(segment);\n        }\n      }\n\nModified as required.", "url": "https://github.com/apache/druid/pull/9935#discussion_r449577265", "createdAt": "2020-07-03T13:13:34Z", "author": {"login": "xiangqiao123"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzQzMw=="}, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTg2MzU3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMDo1MTozOFrOGpx-ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMDo1MTozOFrOGpx-ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MzYyNg==", "bodyText": "This will copy the whole set of toInsertSegments which doesn't seem necessary. Can we use toInsertSegment instead in the below for loop?", "url": "https://github.com/apache/druid/pull/9935#discussion_r446463626", "createdAt": "2020-06-27T00:51:38Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTg3NDkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMTowNDo1N1rOGpyEsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMTowNDo1N1rOGpyEsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTIwMA==", "bodyText": "This will corrupt the logs too. How about modifying as the below?\n      final List<List<DataSegment>> partitionedSegments = Lists.partition(\n          new ArrayList<>(toInsertSegments),\n          MAX_NUM_SEGMENTS_TO_ANNOUNCE_AT_ONCE\n      );\n\n      PreparedBatch preparedBatch = handle.prepareBatch(\n          StringUtils.format(\n              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n              dbTables.getSegmentsTable(),\n              connector.getQuoteString()\n          )\n      );\n      \n      for (List<DataSegment> partition : partitionedSegments) {\n        for (DataSegment segment : partition) {\n          preparedBatch.add()\n                       .bind(\"id\", segment.getId().toString())\n                       .bind(\"dataSource\", segment.getDataSource())\n                       .bind(\"created_date\", DateTimes.nowUtc().toString())\n                       .bind(\"start\", segment.getInterval().getStart().toString())\n                       .bind(\"end\", segment.getInterval().getEnd().toString())\n                       .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n                       .bind(\"version\", segment.getVersion())\n                       .bind(\"used\", usedSegments.contains(segment))\n                       .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n        }\n        final int[] affectedRows = preparedBatch.execute();\n        final boolean succeeded = Arrays.stream(affectedRows).allMatch(eachAffectedRows -> eachAffectedRows == 1);\n        if (succeeded) {\n          log.infoSegments(partition, \"Published segments to DB\");\n        } else {\n          final List<DataSegment> failedToPublish = IntStream.range(0, partition.size())\n                                                             .filter(i -> affectedRows[i] != 1)\n                                                             .mapToObj(partition::get)\n                                                             .collect(Collectors.toList());\n          throw new ISE(\n              \"Failed to publish segments to DB: %s\",\n              SegmentUtils.commaSeparatedIdentifiers(failedToPublish)\n          );\n        }\n      }\n    }", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465200", "createdAt": "2020-06-27T01:04:57Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTg3NTgxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMTowNjowOVrOGpyFJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QwMTowNjowOVrOGpyFJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2NTMxOA==", "bodyText": "Same here. Better to be log.errorSegments(segments, \"Exception inserting segments\");", "url": "https://github.com/apache/druid/pull/9935#discussion_r446465318", "createdAt": "2020-06-27T01:06:09Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/metadata/IndexerSQLMetadataStorageCoordinator.java", "diffHunk": "@@ -932,83 +930,100 @@ public int deletePendingSegments(String dataSource)\n    * Attempts to insert a single segment to the database. If the segment already exists, will do nothing; although,\n    * this checking is imperfect and callers must be prepared to retry their entire transaction on exceptions.\n    *\n-   * @return true if the segment was added, false if it already existed\n+   * @return DataSegment set inserted\n    */\n-  private boolean announceHistoricalSegment(\n+  private Set<DataSegment> announceHistoricalSegmentBatch(\n       final Handle handle,\n-      final DataSegment segment,\n-      final boolean used\n+      final Set<DataSegment> segments,\n+      final Set<DataSegment> usedSegments\n   ) throws IOException\n   {\n+    final Set<DataSegment> toInsertSegments = new HashSet<>();\n     try {\n-      if (segmentExists(handle, segment)) {\n-        log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n-        return false;\n+      Set<String> existedSegments = segmentExistsBatch(handle, segments);\n+      for (DataSegment segment : segments) {\n+        if (existedSegments.contains(segment.getId().toString())) {\n+          log.info(\"Found [%s] in DB, not updating DB\", segment.getId());\n+        } else {\n+          toInsertSegments.add(segment);\n+        }\n       }\n \n       // SELECT -> INSERT can fail due to races; callers must be prepared to retry.\n       // Avoiding ON DUPLICATE KEY since it's not portable.\n       // Avoiding try/catch since it may cause inadvertent transaction-splitting.\n-      final int numRowsInserted = handle.createStatement(\n+      final List<DataSegment> segmentList = new ArrayList<>(toInsertSegments);\n+\n+      PreparedBatch preparedBatch = handle.prepareBatch(\n           StringUtils.format(\n-              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, \"\n-              + \"payload) \"\n-              + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n+              \"INSERT INTO %1$s (id, dataSource, created_date, start, %2$send%2$s, partitioned, version, used, payload) \"\n+                  + \"VALUES (:id, :dataSource, :created_date, :start, :end, :partitioned, :version, :used, :payload)\",\n               dbTables.getSegmentsTable(),\n               connector.getQuoteString()\n           )\n-      )\n+      );\n+\n+      for (int i = 0; i < segmentList.size(); i++) {\n+        DataSegment segment = segmentList.get(i);\n+        preparedBatch.add()\n             .bind(\"id\", segment.getId().toString())\n             .bind(\"dataSource\", segment.getDataSource())\n             .bind(\"created_date\", DateTimes.nowUtc().toString())\n             .bind(\"start\", segment.getInterval().getStart().toString())\n             .bind(\"end\", segment.getInterval().getEnd().toString())\n             .bind(\"partitioned\", (segment.getShardSpec() instanceof NoneShardSpec) ? false : true)\n             .bind(\"version\", segment.getVersion())\n-            .bind(\"used\", used)\n-            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n-            .execute();\n-\n-      if (numRowsInserted == 1) {\n-        log.info(\n-            \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else if (numRowsInserted == 0) {\n-        throw new ISE(\n-            \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n-      } else {\n-        throw new ISE(\n-            \"numRowsInserted[%s] is larger than 1 after inserting segment[%s] with used flag[%s], json[%s]\",\n-            numRowsInserted,\n-            segment.getId(),\n-            used,\n-            jsonMapper.writeValueAsString(segment)\n-        );\n+            .bind(\"used\", usedSegments.contains(segment))\n+            .bind(\"payload\", jsonMapper.writeValueAsBytes(segment));\n+\n+        if ((i + 1) % ANNOUNCE_HISTORICAL_SEGMENG_BATCH == 0 || i == segmentList.size() - 1) {\n+          int[] affectedRows = preparedBatch.execute();\n+          for (int j = 0; j < affectedRows.length; j++) {\n+            DataSegment insertSegment = segmentList.get(i / ANNOUNCE_HISTORICAL_SEGMENG_BATCH * ANNOUNCE_HISTORICAL_SEGMENG_BATCH + j);\n+            if (affectedRows[j] == 1) {\n+              log.info(\n+                  \"Published segment [%s] to DB with used flag [%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            } else {\n+              throw new ISE(\n+                  \"Failed to publish segment[%s] to DB with used flag[%s], json[%s]\",\n+                  insertSegment.getId(),\n+                  usedSegments.contains(insertSegment),\n+                  jsonMapper.writeValueAsString(insertSegment)\n+              );\n+            }\n+          }\n+        }\n       }\n     }\n     catch (Exception e) {\n-      log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), used);\n+      for (DataSegment segment : segments) {\n+        log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getId(), usedSegments.contains(segment));\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "605bc5b32f25ae2a1b88af09a89417ac2450f87f"}, "originalPosition": 180}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2476, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}