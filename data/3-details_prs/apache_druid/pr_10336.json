{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc2NzQ4MDU5", "number": 10336, "title": "More structured way to handle parse exceptions", "bodyText": "Description\nThis PR has refactoring around handling ParseExceptions in ingestion, so that we can easily support the same logic for parallel task as well. The parseException can be thrown in 2 places on the ingestion side, i.e., when you parse the input data based on input format (in InputEntityReader) and when you convert the parsed object to a desired data type (in IncrementalIndex). To handle the exceptions in a more structured way, I added ParseExceptionHandler which handles all parseExceptions thrown on the ingestion side.\nFilteringCloseableInputRowIterator was added to handle common row filters. It also has the parseExceptionHandler to handle parseExceptions thrown while parsing input data. The same parseExceptionHandler instance is passed down to AppenderatorImpl to handle parseExceptions thrown while converting parsed objects.\n\nThis PR has:\n\n been self-reviewed.\n\n using the concurrency checklist (Remove this item if the PR doesn't have any relation to concurrency.)\n\n\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths, ensuring the threshold for code coverage is met.\n added integration tests.\n been tested in a test Druid cluster.\n\n\nKey changed/added classes in this PR\n\nParseExceptionHandler\nFilteringCloseableInputRowIterator", "createdAt": "2020-09-01T01:24:21Z", "url": "https://github.com/apache/druid/pull/10336", "merged": true, "mergeCommit": {"oid": "8f14ac814e1fdf11878a5ba9bdba58fb745b4c8f"}, "closed": true, "closedAt": "2020-09-11T23:31:11Z", "author": {"login": "jihoonson"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdEc46JAH2gAyNDc2NzQ4MDU5OjkxMzgxZTNjYTUyNjhjMDMzNGNlYzRhN2NjYjZhYzQ1NDIwMjBlNWE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHXeKYAH2gAyNDc2NzQ4MDU5OjhlNTAzNjIzMjU3MWUzMzc5YmZkYWY5ZjZhYjY3MDVjNDYzY2U4MWM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "91381e3ca5268c0334cec4a7ccb6ac4542020e5a", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/91381e3ca5268c0334cec4a7ccb6ac4542020e5a", "committedDate": "2020-09-01T01:12:26Z", "message": "More structured way to handle parse exceptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "067d742daf0377521359beb2d6cdc37cd115df0a", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/067d742daf0377521359beb2d6cdc37cd115df0a", "committedDate": "2020-09-01T04:46:53Z", "message": "checkstyle; add more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6fde0beb21c3c9955a81e0956ac5b9920e173cd4", "committedDate": "2020-09-01T05:43:08Z", "message": "forbidden api; test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5NjE0MDg0", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-479614084", "createdAt": "2020-09-01T10:35:42Z", "commit": {"oid": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxMDozNTo0MlrOHKwb3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxMDozNTo0MlrOHKwb3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA0MTM3NA==", "bodyText": "just curious, can this be private final CircularBuffer<ParseException> itself?", "url": "https://github.com/apache/druid/pull/10336#discussion_r481041374", "createdAt": "2020-09-01T10:35:42Z", "author": {"login": "abhishekagarwal87"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.RE;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+import org.apache.druid.utils.CircularBuffer;\n+\n+import javax.annotation.Nullable;\n+\n+/**\n+ * A handler for {@link ParseException}s thrown during ingestion. Based on the given configuration, this handler can\n+ *\n+ * - log ParseExceptions.\n+ * - keep most recent N ParseExceptions in memory.\n+ * - throw a RuntimeException when it sees more ParseExceptions than {@link #maxAllowedParseExceptions}.\n+ *\n+ * No matter what the handler does, the relevant metric should be updated first.\n+ */\n+public class ParseExceptionHandler\n+{\n+  private static final Logger LOG = new Logger(ParseExceptionHandler.class);\n+\n+  private final RowIngestionMeters rowIngestionMeters;\n+  private final boolean logParseExceptions;\n+  private final int maxAllowedParseExceptions;\n+  @Nullable\n+  private final CircularBuffer<Throwable> savedParseExceptions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4"}, "originalPosition": 47}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/d7e51be606e39aa696fccaf559460f36327d0a9d", "committedDate": "2020-09-01T20:21:40Z", "message": "address comment; new test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwOTI1ODk5", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-480925899", "createdAt": "2020-09-02T15:11:53Z", "commit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNToxMTo1M1rOHL0HDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNTo1NzoxOFrOHL2JPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1MDE1Ng==", "bodyText": "wow, I never thought of this case", "url": "https://github.com/apache/druid/pull/10336#discussion_r482150156", "createdAt": "2020-09-02T15:11:53Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1NTc2MQ==", "bodyText": "Instead of printing the whole map, I think it would be better to just print the timestamp string that was unparsable.\nSimilar comment on line 99, this would get rid of the need of rawMapToPrint and guarantees that the invalid timestamp is always logged regardless of where in the event it is.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n          \n          \n            \n                  throw new ParseException(\"Unparseable timestamp found! Timestamp column (%s): %s\", timestampSpec.getTimestampColumn(), theMap.get(timestampSpec.getTimestampColumn()));", "url": "https://github.com/apache/druid/pull/10336#discussion_r482155761", "createdAt": "2020-09-02T15:19:12Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1OTQ3Mg==", "bodyText": "After your refactoring in this change, the parse functions on line 65 and 70 can be made private and package private (VisibleForTesting) respectively", "url": "https://github.com/apache/druid/pull/10336#discussion_r482159472", "createdAt": "2020-09-02T15:24:09Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {\n+      throw new ParseException(\n+          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          rawMapToPrint(theMap)\n+      );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NDQ3Ng==", "bodyText": "nit: Maybe these 3 classes should be moved into their own sub package org.apache.druid.segment.incremental.stats", "url": "https://github.com/apache/druid/pull/10336#discussion_r482164476", "createdAt": "2020-09-02T15:30:50Z", "author": {"login": "suneet-s"}, "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "diffHunk": "@@ -106,6 +103,9 @@\n import org.apache.druid.query.timeseries.TimeseriesQueryEngine;\n import org.apache.druid.query.timeseries.TimeseriesQueryQueryToolChest;\n import org.apache.druid.query.timeseries.TimeseriesQueryRunnerFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.incremental.RowIngestionMetersFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMetersTotals;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NjE1Mg==", "bodyText": "javadocs please", "url": "https://github.com/apache/druid/pull/10336#discussion_r482166152", "createdAt": "2020-09-02T15:33:12Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3MTMxOA==", "bodyText": "Should this also be annotated with @MonotonicNonNull", "url": "https://github.com/apache/druid/pull/10336#discussion_r482171318", "createdAt": "2020-09-02T15:40:19Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -151,7 +151,7 @@ private static String makeTaskId(RealtimeAppenderatorIngestionSpec spec)\n   private volatile Thread runThread = null;\n \n   @JsonIgnore\n-  private CircularBuffer<Throwable> savedParseExceptions;\n+  private ParseExceptionHandler parseExceptionHandler;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3Nzg3NQ==", "bodyText": "This is calculated in 3 places in the code - AbstractBatchIndexTask, InputSourceSampler and SeekableStreamIndexTaskRunner. I think it would be a good idea to consolidate these methods, so that metricNames is always calculated the same way given a dataSchema", "url": "https://github.com/apache/druid/pull/10336#discussion_r482177875", "createdAt": "2020-09-02T15:49:38Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(\n+      File tmpDir,\n+      DataSchema dataSchema,\n+      InputSource inputSource,\n+      @Nullable InputFormat inputFormat,\n+      Predicate<InputRow> rowFilter,\n+      RowIngestionMeters ingestionMeters,\n+      ParseExceptionHandler parseExceptionHandler\n+  ) throws IOException\n+  {\n+    final List<String> metricsNames = Arrays.stream(dataSchema.getAggregators())\n+                                            .map(AggregatorFactory::getName)\n+                                            .collect(Collectors.toList());\n+    final InputSourceReader inputSourceReader = dataSchema.getTransformSpec().decorate(\n+        inputSource.reader(\n+            new InputRowSchema(\n+                dataSchema.getTimestampSpec(),\n+                dataSchema.getDimensionsSpec(),\n+                metricsNames\n+            ),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4MzQ4Ng==", "bodyText": "parseExceptionHandler can be null if the task.run() hasn't been called. This is probably unlikely?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482183486", "createdAt": "2020-09-02T15:57:18Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -550,7 +546,9 @@ public Response getUnparseableEvents(\n   )\n   {\n     IndexTaskUtils.datasourceAuthorizationCheck(req, Action.READ, getDataSource(), authorizerMapper);\n-    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(savedParseExceptions);\n+    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(\n+        parseExceptionHandler.getSavedParseExceptions()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwOTkzODI2", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-480993826", "createdAt": "2020-09-02T16:26:24Z", "commit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNjoyNjoyNFrOHL3Sfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzowMToxNVrOHL4oRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ==", "bodyText": "Is there ever a case where the rowIngestionMeters is different than the one used in parseExceptionHandler? If not, I think we should just pass in the parseExceptionHandler.\nSimilar comment on line 45", "url": "https://github.com/apache/druid/pull/10336#discussion_r482202239", "createdAt": "2020-09-02T16:26:24Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/BatchAppenderators.java", "diffHunk": "@@ -59,7 +65,9 @@ public static Appenderator newAppenderator(\n       TaskToolbox toolbox,\n       DataSchema dataSchema,\n       AppenderatorConfig appenderatorConfig,\n-      DataSegmentPusher segmentPusher\n+      DataSegmentPusher segmentPusher,\n+      RowIngestionMeters rowIngestionMeters,\n+      ParseExceptionHandler parseExceptionHandler", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwNDYzOQ==", "bodyText": "General comment here for all the new functions that accept parseExceptionHandler. parseExceptionHandler tends to be lazily initialized throughout the codebase, but I don't see safeguards against using an un-initialized parseExceptionHandler. Perhaps more use of @Nullable annotation will help surface potential places where it may be used before it's initialized", "url": "https://github.com/apache/druid/pull/10336#discussion_r482204639", "createdAt": "2020-09-02T16:30:04Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/PeonAppenderatorsManager.java", "diffHunk": "@@ -129,7 +137,9 @@ public Appenderator createOfflineAppenderatorForTask(\n           dataSegmentPusher,\n           objectMapper,\n           indexIO,\n-          indexMerger\n+          indexMerger,\n+          rowIngestionMeters,\n+          parseExceptionHandler", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ==", "bodyText": "This is a change in behavior. parseExceptionMessages will add to the parseExceptionMessages that were found on line 165. Previously it was re-set. Was this intentional?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482212651", "createdAt": "2020-09-02T16:43:13Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMzgwNg==", "bodyText": "This is a change in behavior. Previously the aggs would fail fast, but now it tries to parse all of them before bubbling up all the parse exceptions. Was this change intentional? I see a similar change in at least the Onheap implementation.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482223806", "createdAt": "2020-09-02T17:00:37Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OffheapIncrementalIndex.java", "diffHunk": "@@ -233,16 +232,13 @@ protected AddToFactsResult addToFacts(\n         }\n         catch (ParseException e) {\n           // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-          if (getReportParseExceptions()) {\n-            throw new ParseException(e, \"Encountered parse error for aggregator[%s]\", getMetricAggs()[i].getName());\n-          } else {\n-            log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n-          }\n+          log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n+          parseExceptionMessages.add(e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDE5OA==", "bodyText": "Javadocs please.\nShould this just be a singleton?", "url": "https://github.com/apache/druid/pull/10336#discussion_r482224198", "createdAt": "2020-09-02T17:01:15Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+public class NoopRowIngestionMeters implements RowIngestionMeters", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/7cdc6434420eb564a8c510abfd99b60d66b708b9", "committedDate": "2020-09-02T19:01:37Z", "message": "address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMjY5NDcz", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-481269473", "createdAt": "2020-09-02T20:20:24Z", "commit": {"oid": "7cdc6434420eb564a8c510abfd99b60d66b708b9"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMDoyMDoyNFrOHMD3OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMDozMDozNFrOHMEsIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQwODI0OQ==", "bodyText": "why not just return a List like before? That seems less risky than assuming the list is mutated correctly. In general, I'm not a big fan of returning values from a function using parameters because of cases like this.\nEither way, this comment shouldn't be considered a blocker, just my 2 cents", "url": "https://github.com/apache/druid/pull/10336#discussion_r482408249", "createdAt": "2020-09-02T20:20:24Z", "author": {"login": "suneet-s"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQyMTc5NQ==", "bodyText": "I don't think we should use a Predicate<InputRow> here and in other similar places. Throwing a ParseException is expected behavior, so I think callers should know that they need to handle that exception or throw it themselves.\nIt's not obvious to me how the caller in FilteringCloseableInputRowIterator should know they should handle a parse exception https://github.com/apache/druid/pull/10336/files#diff-b7f61f3d28afdd25bedf2efa607fdf88R67\nHow can I check that the ParseException is handled at the correct level everywhere? I realize this is challenging because ParseException is a RuntimeException.", "url": "https://github.com/apache/druid/pull/10336#discussion_r482421795", "createdAt": "2020-09-02T20:30:34Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/StreamChunkParser.java", "diffHunk": "@@ -54,12 +64,16 @@\n       @Nullable InputFormat inputFormat,\n       InputRowSchema inputRowSchema,\n       TransformSpec transformSpec,\n-      File indexingTmpDir\n+      File indexingTmpDir,\n+      Predicate<InputRow> rowFilter,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cdc6434420eb564a8c510abfd99b60d66b708b9"}, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6eacb0e49639ca11d655d2f5c4fcf54f2ac48b5f", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6eacb0e49639ca11d655d2f5c4fcf54f2ac48b5f", "committedDate": "2020-09-02T21:41:36Z", "message": "javadoc for parseException; remove redundant parseException in streaming ingestion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/cfc693c1edbddad63947556951f114f4af7c3c85", "committedDate": "2020-09-02T22:49:35Z", "message": "fix tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyMzQ0ODgw", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-482344880", "createdAt": "2020-09-04T04:14:37Z", "commit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDoxNDozN1rOHM-5pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwNDoxNDozN1rOHM-5pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM3NTUyNw==", "bodyText": "Test coverage bot is complaining about a lack of coverage for this condition. I think we should add tests for these branches. Right now AppenderatorTest only tests the case where addResult.isRowAdded() is true.\nCovering all 4 branches should make the code coverage bot happy too.", "url": "https://github.com/apache/druid/pull/10336#discussion_r483375527", "createdAt": "2020-09-04T04:14:37Z", "author": {"login": "suneet-s"}, "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java", "diffHunk": "@@ -267,6 +275,12 @@ public AppenderatorAddResult add(\n       throw new SegmentNotWritableException(\"Attempt to add row to swapped-out sink for segment[%s].\", identifier);\n     }\n \n+    if (addResult.isRowAdded()) {\n+      rowIngestionMeters.incrementProcessed();\n+    } else if (addResult.hasParseException()) {\n+      parseExceptionHandler.handle(addResult.getParseException());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNTE3OTI4", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-483517928", "createdAt": "2020-09-07T12:29:06Z", "commit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjoyOTowN1rOHN9kTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNDoxMToyOVrOHOA2Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQwMjI1Mw==", "bodyText": "If we never intend to log/report this exception, we can remove the try/catch clause entirely.", "url": "https://github.com/apache/druid/pull/10336#discussion_r484402253", "createdAt": "2020-09-07T12:29:07Z", "author": {"login": "liran-funaro"}, "path": "processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java", "diffHunk": "@@ -227,9 +224,7 @@ protected AddToFactsResult addToFacts(\n           }\n           catch (ParseException e) {\n             // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-            if (getReportParseExceptions()) {\n-              throw e;\n-            }\n+            throw e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ1NjAzOQ==", "bodyText": "Currently (before and after this PR), a row that contains partially corrupted/incorrect data is aggregated anyway (partially).\nThis may result in wrong statistics inference by the user due to inconsistencies between the number of rows each aggregator accumulated.\nIMO, the decision of allowing this scenario should be of the user.\nBut since currently, there is no way to roll-back aggregation, I don't see how this can be easily implemented.\nMaybe adding parse as a preliminary step to aggregation, but this is an entirely different subject.\n\"return early without the second aggregation\" would only aggravate this scenario, so I don't think it is the way to go either.", "url": "https://github.com/apache/druid/pull/10336#discussion_r484456039", "createdAt": "2020-09-07T14:11:29Z", "author": {"login": "liran-funaro"}, "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, "originalCommit": {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c7b7aaac0a1fdd5fb6043eebfbddb51ac2fa2d8", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/8c7b7aaac0a1fdd5fb6043eebfbddb51ac2fa2d8", "committedDate": "2020-09-08T17:03:53Z", "message": "unnecessary catch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2da5c4008c048c614c93b97f9ec53e18358ad1f5", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/2da5c4008c048c614c93b97f9ec53e18358ad1f5", "committedDate": "2020-09-09T16:47:14Z", "message": "unused imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "238381ebbe0413b3634865e02ce55df352d74b4e", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/238381ebbe0413b3634865e02ce55df352d74b4e", "committedDate": "2020-09-10T01:22:11Z", "message": "appenderator test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NTE2OTcx", "url": "https://github.com/apache/druid/pull/10336#pullrequestreview-485516971", "createdAt": "2020-09-10T02:08:49Z", "commit": {"oid": "238381ebbe0413b3634865e02ce55df352d74b4e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e5036232571e3379bfdaf9f6ab6705c463ce81c", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/8e5036232571e3379bfdaf9f6ab6705c463ce81c", "committedDate": "2020-09-10T02:35:28Z", "message": "unused import"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3522, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}