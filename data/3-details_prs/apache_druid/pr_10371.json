{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyNDQ4NDU1", "number": 10371, "title": "Auto-compaction snapshot status API", "bodyText": "Add Auto-compaction snapshot status API\nDescription\nRight now, after users set up auto-compaction, the main ways of understanding how far it's progressed, whether it is progressing or stuck, if datasources are properly compacted, etc, are either:\n\nLooking at the segments yourself and deciding if they look good enough.\nReading messages logged by the Coordinator periodically.\n\nNeither of these is a great solution. Instead this PR adds an API that tells you what you need to know regarding your auto compaction status and progress.\nThis API provides the status of the latest auto compaction run (coordinator duty run) including statistics and information from the coordinator. The API provides getting for a single dataSource or for all dataSources.\nThe API is in the format of:\n/druid/coordinator/v1/compaction/status?dataSource={dataSource}\n/druid/coordinator/v1/compaction/status\nMore details:\n\nonly returns datasource which has/had auto compaction enabled.\nAuto compaction scheduling status: \"NOT_ENABLED\" and \"RUNNING\" w.r.t the dataSource having an auto compaction config submitted to /druid/coordinator/v1/config/compaction/ or not.\nDatasource with auto compaction scheduling set to NOT_ENABLED will not have it's \"bytesAwaitingCompaction\", \"bytesCompressed\", \"bytesSkipped\", \"segmentCountAwaitingCompaction\", \"segmentCountCompressed\", \"segmentCountSkipped\", \"intervalCountAwaitingCompaction\", \"\nintervalCountCompressed\", \"intervalCountSkipped\" update in subsequent auto compaction runs.\n\nExample response:\n{\n\"latestSnapshots\":[\n {\n        \"dataSource\":\"wikipedia\",\n        \"scheduleStatus\": \"RUNNING\",\n        \"bytesAwaitingCompaction\": 3000,\n        \"bytesCompressed\": 5000,\n        \"bytesSkipped\": 30,\n        \"segmentCountAwaitingCompaction\": 200,\n        \"segmentCountCompressed\": 500,\n        \"segmentCountSkipped\": 2,\n        \"intervalCountAwaitingCompaction\": 30,\n        \"intervalCountCompressed\": 50\n        \"intervalCountSkipped\": 5,\n    }\n]\n}\n\nDescription of field in response:\n\n\"dataSource\": name of the datasource for this status information\n\"scheduleStatus\": auto compaction scheduling status: \"NOT_ENABLED\" and \"RUNNING\" w.r.t the dataSource having an auto compaction config submitted to /druid/coordinator/v1/config/compaction/ or not\n\"bytesAwaitingCompaction\": total byte of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that is eligible for auto compaction)\n\"bytesCompressed\":  total byte of this datasource that is already compacted with the spec set in the auto compaction.\n\"bytesSkipped\":  total byte of this datasource that is skipped by the auto compaction.\n\"segmentCountAwaitingCompaction\": total number of segments of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that is eligible for auto compaction)\n\"segmentCountCompressed\": total number of segments of this datasource that is already compacted with the spec set in the auto compaction.\n\"segmentCountSkipped\":  total number of segments of this datasource that is skipped by the auto compaction.\n\"intervalCountAwaitingCompaction\":  total number of intervals of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that is eligible for auto compaction)\n\"intervalCountCompressed\": total number of intervals of this datasource that is already compacted with the spec set in the auto compaction.\n\"intervalCountSkipped\":  total number of intervals of this datasource that is skipped by the auto compaction.\n\nDescription of metrics:\nThese are global\n\ncompact/task/count: Number of task issued in this autocompaction run at this time\ncompactTask/maxSlot/count: Number of max task slot in this autocompaction run at this time (this is min of ratio * total slot or max compact slot set)\ncompactTask/availableSlot/count: Number of available task slot in this autocompaction run at this time (this is max slot minus any currently running compaction task)\n\nThese are per datasource (datasource is a dimension)\n\nsegment/waitCompact/bytes:  total byte of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that is eligible for auto compaction)\nsegment/waitCompact/count: total number of segments of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that is eligible for auto compaction)\ninterval/waitCompact/count: total number of intervals of this datasource waiting to be compacted by the auto compaction (only consider intervals/segments that is eligible for auto compaction)\nsegment/compacted/bytes: total byte of this datasource that is already compacted with the spec set in the auto compaction.\nsegment/compacted/count: total number of segments of this datasource that is already compacted with the spec set in the auto compaction.\ninterval/compacted/count: total number of intervals of this datasource that is already compacted with the spec set in the auto compaction.\nsegment/skipCompact/bytes:  total byte of this datasource that is skipped by the auto compaction\nsegment/skipCompact/count: total number of segments of this datasource that is skipped by the auto compaction\ninterval/skipCompact/count: total number of intervals of this datasource that is skipped by the auto compaction\n\nThis PR also fix and improve the emitted metrics for auto-compaction via the CoordinatorStats.\n\nEmits the above Compacted vs Remaining stats\nEmits details for auto compaction task scheduling. These are tasks scheduled, max compaction slot and available compaction slot during each auto compaction duty run\n\nConcurrency:\nThis PR deals with an existing concurrency issue in CompactSegment class. Updating totalSizesOfSegmentsAwaitingCompactionPerDataSource was not thread-safe, but it should be. This applies same to autoCompactionSnapshotPerDataSource (totalSizesOfSegmentsAwaitingCompactionPerDataSource is replace with autoCompactionSnapshotPerDataSource in this PR). The concurrency issue is solved by storing autoCompactionSnapshotPerDataSource in an AtomicReference so that we can atomically update the reference to the hash map whenever we compute new snapshots.\nThis PR has:\n\n been self-reviewed.\n\n using the concurrency checklist (Remove this item if the PR doesn't have any relation to concurrency.)\n\n\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths, ensuring the threshold for code coverage is met.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-09-09T03:03:50Z", "url": "https://github.com/apache/druid/pull/10371", "merged": true, "mergeCommit": {"oid": "e78d7862a8ae60b8504355cbd969a6f49cda6065"}, "closed": true, "closedAt": "2020-09-18T23:37:59Z", "author": {"login": "maytasm"}, "timelineItems": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdHDN4dgH2gAyNDgyNDQ4NDU1OmVlODA2NmIyYWMyNDdiNTBiYmQzMTBiN2VlODc0MDBmOWQzZTJiODU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdKNmgPAFqTQ5MTgyMTAwNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ee8066b2ac247b50bbd310b7ee87400f9d3e2b85", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/ee8066b2ac247b50bbd310b7ee87400f9d3e2b85", "committedDate": "2020-09-09T02:59:35Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a373b26b31b276789f46a2e00f2b2a167e0288a", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2a373b26b31b276789f46a2e00f2b2a167e0288a", "committedDate": "2020-09-09T03:01:33Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95a788f9ff60623a01cfc75a0aedc0c279e01c0d", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/95a788f9ff60623a01cfc75a0aedc0c279e01c0d", "committedDate": "2020-09-09T05:50:44Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab5454b570d95266cde27530c41cacc1d358b72f", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/ab5454b570d95266cde27530c41cacc1d358b72f", "committedDate": "2020-09-09T05:51:55Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "957b1c856bdafdfbdfe2ded518b81abda8e559bc", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/957b1c856bdafdfbdfe2ded518b81abda8e559bc", "committedDate": "2020-09-09T07:25:48Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b66b0a523c9f48ed97fa1531e14ba280254d257", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/7b66b0a523c9f48ed97fa1531e14ba280254d257", "committedDate": "2020-09-09T07:27:39Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/cd789960af5122ee84362147be7e261b6b9ece21", "committedDate": "2020-09-09T09:18:30Z", "message": "Auto-compaction snapshot API"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MjI1MTcy", "url": "https://github.com/apache/druid/pull/10371#pullrequestreview-486225172", "createdAt": "2020-09-10T18:57:24Z", "commit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxODo1NzoyNFrOHQBvag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxOTowNzoxM1rOHQCDww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU2Nzc4Ng==", "bodyText": "We shouldn't change the metric name for backwards compatibility.", "url": "https://github.com/apache/druid/pull/10371#discussion_r486567786", "createdAt": "2020-09-10T18:57:24Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -296,18 +296,87 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n \n     emitter.emit(\n         new ServiceMetricEvent.Builder().build(\n-            \"compact/task/count\",\n+            \"compact/task/scheduled/count\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU2OTUxMg==", "bodyText": "Even though this changes the metric name, it seems fine since this metrics has never been emitted because of the metric name mismatch (segmentsWaitCompact and segmentSizeWaitCompact).", "url": "https://github.com/apache/druid/pull/10371#discussion_r486569512", "createdAt": "2020-09-10T19:00:32Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -296,18 +296,87 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n \n     emitter.emit(\n         new ServiceMetricEvent.Builder().build(\n-            \"compact/task/count\",\n+            \"compact/task/scheduled/count\",\n             stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/task/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/task/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n     stats.forEachDataSourceStat(\n-        \"segmentsWaitCompact\",\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_COMPACTED,\n         (final String dataSource, final long count) -> {\n           emitter.emit(\n               new ServiceMetricEvent.Builder()\n                   .setDimension(DruidMetrics.DATASOURCE, dataSource)\n-                  .build(\"segment/waitCompact/count\", count)\n+                  .build(\"segment/compacted/intervalCount\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjU3Mjk5NQ==", "bodyText": "Why not byteCountProcessed to align with other metrics?", "url": "https://github.com/apache/druid/pull/10371#discussion_r486572995", "createdAt": "2020-09-10T19:07:13Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteAwaitingCompaction;\n+  @JsonProperty\n+  private long byteProcessed;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd789960af5122ee84362147be7e261b6b9ece21"}, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2de2eb7dafd11d17c7568fc09bd15ecbf3dd7afa", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2de2eb7dafd11d17c7568fc09bd15ecbf3dd7afa", "committedDate": "2020-09-14T08:33:36Z", "message": "fix when not all compacted segments are iterated"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "746fe821c991813bc2fc0348574f33ae45cc0fbb", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/746fe821c991813bc2fc0348574f33ae45cc0fbb", "committedDate": "2020-09-14T11:16:51Z", "message": "add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1e946c2ab9df7a5c0cf0543ed4f3841a8720fb8", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/b1e946c2ab9df7a5c0cf0543ed4f3841a8720fb8", "committedDate": "2020-09-14T11:17:51Z", "message": "add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1c2102bfc745830f2941adc76f22178b16a4cf4", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/e1c2102bfc745830f2941adc76f22178b16a4cf4", "committedDate": "2020-09-14T11:26:17Z", "message": "add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60fb025aaaebd009daafe4422f113d7bd2146c6f", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/60fb025aaaebd009daafe4422f113d7bd2146c6f", "committedDate": "2020-09-14T22:10:19Z", "message": "add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9bcc85d42a9f3545d1ad8a827434ab67d8cb20bb", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/9bcc85d42a9f3545d1ad8a827434ab67d8cb20bb", "committedDate": "2020-09-14T22:11:18Z", "message": "add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/4509c98a6c4c64619c2f61eba09c166318c2f14e", "committedDate": "2020-09-14T22:11:49Z", "message": "add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/ff6aae471ae934a288b5cedd6a4dfd03771770db", "committedDate": "2020-09-15T02:27:52Z", "message": "add some tests to make code cov happy"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4MjU0MzEy", "url": "https://github.com/apache/druid/pull/10371#pullrequestreview-488254312", "createdAt": "2020-09-15T01:08:20Z", "commit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwMTowODoyMFrOHRs0ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQwNDo1MToyOFrOHRwlUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ==", "bodyText": "suggest just \"bytes\" instead of \"byteCount\" for the property name", "url": "https://github.com/apache/druid/pull/10371#discussion_r488322171", "createdAt": "2020-09-15T01:08:20Z", "author": {"login": "jon-wei"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzcyMw==", "bodyText": "I don't think I understand this comment, if all iteration is done (by that do you mean compactibleTimelineObjectHolderCursor.hasNext returns false?), then iterateAllSegments would do nothing.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488383723", "createdAt": "2020-09-15T04:51:04Z", "author": {"login": "jon-wei"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -112,27 +114,38 @@\n   }\n \n   @Override\n-  public Object2LongOpenHashMap<String> totalRemainingSegmentsSizeBytes()\n+  public Map<String, CompactionStatistics> totalRemainingStatistics()\n   {\n-    final Object2LongOpenHashMap<String> resultMap = new Object2LongOpenHashMap<>();\n-    resultMap.defaultReturnValue(UNKNOWN_TOTAL_REMAINING_SEGMENTS_SIZE);\n-    for (QueueEntry entry : queue) {\n-      final VersionedIntervalTimeline<String, DataSegment> timeline = dataSources.get(entry.getDataSource());\n-      final Interval interval = new Interval(timeline.first().getInterval().getStart(), entry.interval.getEnd());\n-\n-      final List<TimelineObjectHolder<String, DataSegment>> holders = timeline.lookup(interval);\n-\n-      long size = 0;\n-      for (DataSegment segment : FluentIterable\n-          .from(holders)\n-          .transformAndConcat(TimelineObjectHolder::getObject)\n-          .transform(PartitionChunk::getObject)) {\n-        size += segment.getSize();\n-      }\n+    return remainingSegments;\n+  }\n+\n+  @Override\n+  public Map<String, CompactionStatistics> totalProcessedStatistics()\n+  {\n+    return processedSegments;\n+  }\n \n-      resultMap.put(entry.getDataSource(), size);\n+  @Override\n+  public void flushAllSegments()\n+  {\n+    if (queue.isEmpty()) {\n+      return;\n+    }\n+    QueueEntry entry;\n+    while ((entry = queue.poll()) != null) {\n+      final List<DataSegment> resultSegments = entry.segments;\n+      final String dataSourceName = resultSegments.get(0).getDataSource();\n+      // This entry was in the queue, meaning that it was not processed. Hence, also aggregates it's\n+      // statistic to the remaining segments counts.\n+      collectSegmentStatistics(remainingSegments, dataSourceName, new SegmentsToCompact(entry.segments));\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor = timelineIterators.get(\n+          dataSourceName\n+      );\n+      // WARNING: This iterates the compactibleTimelineObjectHolderCursor.\n+      // Since this method is intended to only be use after all necessary iteration is done on this iterator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM4MzgyNQ==", "bodyText": "Are there any concerns with performance overhead from this?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488383825", "createdAt": "2020-09-15T04:51:28Z", "author": {"login": "jon-wei"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ff6aae471ae934a288b5cedd6a4dfd03771770db"}, "originalPosition": 214}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/e83dbec9d9840ed40de1a072fffec013502962f8", "committedDate": "2020-09-15T05:22:47Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4OTU1MzI5", "url": "https://github.com/apache/druid/pull/10371#pullrequestreview-488955329", "createdAt": "2020-09-15T18:31:48Z", "commit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxODozMTo0OFrOHSOynQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMToyNToyMVrOHSVGDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODg3ODc0OQ==", "bodyText": "IIRC, it was byte not bytes. bytes sounds better to me too.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488878749", "createdAt": "2020-09-15T18:31:48Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODMyMjE3MQ=="}, "originalCommit": {"oid": "4509c98a6c4c64619c2f61eba09c166318c2f14e"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0MzM5OA==", "bodyText": "This looks error-prone since all fields in this class are mutable. Suppose that this class was used as a key in a hash set. If you updated a field in this class after adding it to the set, its hash key would be different which will cause an unintended result. Even though it seems that hashCode() and equals() are used only in unit tests, I would suggest to make this class immutable.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488943398", "createdAt": "2020-09-15T20:09:13Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/AutoCompactionSnapshot.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.server.coordinator;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nullable;\n+import javax.validation.constraints.NotNull;\n+import java.util.Objects;\n+\n+public class AutoCompactionSnapshot\n+{\n+  public enum AutoCompactionScheduleStatus\n+  {\n+    NOT_ENABLED,\n+    RUNNING\n+  }\n+\n+  @JsonProperty\n+  private String dataSource;\n+  @JsonProperty\n+  private AutoCompactionScheduleStatus scheduleStatus;\n+  @JsonProperty\n+  private String latestScheduledTaskId;\n+  @JsonProperty\n+  private long byteCountAwaitingCompaction;\n+  @JsonProperty\n+  private long byteCountProcessed;\n+  @JsonProperty\n+  private long segmentCountAwaitingCompaction;\n+  @JsonProperty\n+  private long segmentCountProcessed;\n+  @JsonProperty\n+  private long intervalCountAwaitingCompaction;\n+  @JsonProperty\n+  private long intervalCountProcessed;\n+\n+  @JsonCreator\n+  public AutoCompactionSnapshot(\n+      @JsonProperty @NotNull String dataSource,\n+      @JsonProperty @NotNull AutoCompactionScheduleStatus scheduleStatus\n+  )\n+  {\n+    this.dataSource = dataSource;\n+    this.scheduleStatus = scheduleStatus;\n+  }\n+\n+  @NotNull\n+  public String getDataSource()\n+  {\n+    return dataSource;\n+  }\n+\n+  @NotNull\n+  public AutoCompactionScheduleStatus getScheduleStatus()\n+  {\n+    return scheduleStatus;\n+  }\n+\n+  @Nullable\n+  public String getLatestScheduledTaskId()\n+  {\n+    return latestScheduledTaskId;\n+  }\n+\n+  public long getByteCountAwaitingCompaction()\n+  {\n+    return byteCountAwaitingCompaction;\n+  }\n+\n+  public long getByteCountProcessed()\n+  {\n+    return byteCountProcessed;\n+  }\n+\n+  public long getSegmentCountAwaitingCompaction()\n+  {\n+    return segmentCountAwaitingCompaction;\n+  }\n+\n+  public long getSegmentCountProcessed()\n+  {\n+    return segmentCountProcessed;\n+  }\n+\n+  public long getIntervalCountAwaitingCompaction()\n+  {\n+    return intervalCountAwaitingCompaction;\n+  }\n+\n+  public long getIntervalCountProcessed()\n+  {\n+    return intervalCountProcessed;\n+  }\n+\n+  public void setScheduleStatus(AutoCompactionScheduleStatus scheduleStatus)\n+  {\n+    this.scheduleStatus = scheduleStatus;\n+  }\n+\n+  public void setLatestScheduledTaskId(String latestScheduledTaskId)\n+  {\n+    this.latestScheduledTaskId = latestScheduledTaskId;\n+  }\n+\n+  public void setByteCountAwaitingCompaction(long byteCountAwaitingCompaction)\n+  {\n+    this.byteCountAwaitingCompaction = byteCountAwaitingCompaction;\n+  }\n+\n+  public void setByteCountProcessed(long byteCountProcessed)\n+  {\n+    this.byteCountProcessed = byteCountProcessed;\n+  }\n+\n+  public void setSegmentCountAwaitingCompaction(long segmentCountAwaitingCompaction)\n+  {\n+    this.segmentCountAwaitingCompaction = segmentCountAwaitingCompaction;\n+  }\n+\n+  public void setSegmentCountProcessed(long segmentCountProcessed)\n+  {\n+    this.segmentCountProcessed = segmentCountProcessed;\n+  }\n+\n+  public void setIntervalCountAwaitingCompaction(long intervalCountAwaitingCompaction)\n+  {\n+    this.intervalCountAwaitingCompaction = intervalCountAwaitingCompaction;\n+  }\n+\n+  public void setIntervalCountProcessed(long intervalCountProcessed)\n+  {\n+    this.intervalCountProcessed = intervalCountProcessed;\n+  }\n+\n+\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    AutoCompactionSnapshot that = (AutoCompactionSnapshot) o;\n+    return byteCountAwaitingCompaction == that.byteCountAwaitingCompaction &&\n+           byteCountProcessed == that.byteCountProcessed &&\n+           segmentCountAwaitingCompaction == that.segmentCountAwaitingCompaction &&\n+           segmentCountProcessed == that.segmentCountProcessed &&\n+           intervalCountAwaitingCompaction == that.intervalCountAwaitingCompaction &&\n+           intervalCountProcessed == that.intervalCountProcessed &&\n+           dataSource.equals(that.dataSource) &&\n+           Objects.equals(scheduleStatus, that.scheduleStatus) &&\n+           Objects.equals(latestScheduledTaskId, that.latestScheduledTaskId);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(\n+        dataSource,\n+        scheduleStatus,\n+        latestScheduledTaskId,\n+        byteCountAwaitingCompaction,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0NDM1NQ==", "bodyText": "typo: takea -> take a", "url": "https://github.com/apache/druid/pull/10371#discussion_r488944355", "createdAt": "2020-09-15T20:11:00Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -632,8 +643,9 @@ private void stopBeingLeader()\n   {\n     List<CoordinatorDuty> duties = new ArrayList<>();\n     duties.add(new LogUsedSegments());\n-    duties.addAll(makeCompactSegmentsDuty());\n     duties.addAll(indexingServiceDuties);\n+    // CompactSegmentsDuty should be the last duty as it can takea long time", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0NzMyNA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10371#discussion_r488947324", "createdAt": "2020-09-15T20:16:47Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/DruidCoordinator.java", "diffHunk": "@@ -569,7 +580,7 @@ private void becomeLeader()\n       }\n \n       for (final Pair<? extends DutiesRunnable, Duration> dutiesRunnable : dutiesRunnables) {\n-        ScheduledExecutors.scheduleWithFixedDelay(\n+        ScheduledExecutors.scheduleAtFixedRate(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk1MTk0Mw==", "bodyText": "It was my bad that updating totalSizesOfSegmentsAwaitingCompactionPerDataSource was not thread-safe, but it should be. It applies same to autoCompactionSnapshotPerDataSource. I think an easy way is storing autoCompactionSnapshotPerDataSource in an AtomicReference so that we can atomically update the reference to the hash map whenever we compute new snapshots.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488951943", "createdAt": "2020-09-15T20:25:26Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +70,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private HashMap<String, AutoCompactionSnapshot> autoCompactionSnapshotPerDataSource = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk2NzU3Mg==", "bodyText": "Duplicate call to autoCompactionSnapshotPerDataSource.get().", "url": "https://github.com/apache/druid/pull/10371#discussion_r488967572", "createdAt": "2020-09-15T20:55:35Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -238,25 +272,102 @@ private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIter\n   {\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Make sure that the iterator iterate through all the remaining segments so that we can get accurate and correct\n+    // statistics (remaining, skipped, processed, etc.). The reason we have to do this explicitly here is because\n+    // earlier (when we are iterating to submit compaction tasks) we may have ran out of task slot and were not able\n+    // to iterate to the first segment that needs compaction for some datasource.\n+    iterator.flushAllSegments();\n+    // Statistics of all segments that still need compaction after this run\n+    Map<String, CompactionStatistics> allRemainingStatistics = iterator.totalRemainingStatistics();\n+    // Statistics of all segments either compacted or skipped after this run\n+    Map<String, CompactionStatistics> allProcessedStatistics = iterator.totalProcessedStatistics();\n+\n+    for (Map.Entry<String, AutoCompactionSnapshot> autoCompactionSnapshotEntry : autoCompactionSnapshotPerDataSource.entrySet()) {\n+      final String dataSource = autoCompactionSnapshotEntry.getKey();\n+      CompactionStatistics remainingStatistics = allRemainingStatistics.get(dataSource);\n+      CompactionStatistics processedStatistics = allProcessedStatistics.get(dataSource);\n+\n+      long byteAwaitingCompaction = 0;\n+      long segmentCountAwaitingCompaction = 0;\n+      long intervalCountAwaitingCompaction = 0;\n+      if (remainingStatistics != null) {\n+        // If null means that all segments are either compacted or skipped.\n+        // Hence, we can leave these set to default value of 0. If not null, we set it to the collected statistic.\n+        byteAwaitingCompaction = remainingStatistics.getByteSum();\n+        segmentCountAwaitingCompaction = remainingStatistics.getSegmentNumberCountSum();\n+        intervalCountAwaitingCompaction = remainingStatistics.getSegmentIntervalCountSum();\n+      }\n+\n+      long byteProcessed = 0;\n+      long segmentCountProcessed = 0;\n+      long intervalCountProcessed = 0;\n+      if (processedStatistics != null) {\n+        byteProcessed = processedStatistics.getByteSum();\n+        segmentCountProcessed = processedStatistics.getSegmentNumberCountSum();\n+        intervalCountProcessed = processedStatistics.getSegmentIntervalCountSum();\n+      }\n+\n+      autoCompactionSnapshotEntry.getValue().setByteCountAwaitingCompaction(byteAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setByteCountProcessed(byteProcessed);\n+      autoCompactionSnapshotEntry.getValue().setSegmentCountAwaitingCompaction(segmentCountAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setSegmentCountProcessed(segmentCountProcessed);\n+      autoCompactionSnapshotEntry.getValue().setIntervalCountAwaitingCompaction(intervalCountAwaitingCompaction);\n+      autoCompactionSnapshotEntry.getValue().setIntervalCountProcessed(intervalCountProcessed);\n+\n+      stats.addToDataSourceStat(\n+          TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          byteAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          segmentCountAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+          dataSource,\n+          intervalCountAwaitingCompaction\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          byteProcessed\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          segmentCountProcessed\n+      );\n+      stats.addToDataSourceStat(\n+          TOTAL_INTERVAL_OF_SEGMENTS_COMPACTED,\n+          dataSource,\n+          intervalCountProcessed\n+      );\n+    }\n+\n     return stats;\n   }\n \n-  @SuppressWarnings(\"deprecation\") // Intentionally using boxing get() to return null if dataSource is unknown\n   @Nullable\n   public Long getTotalSizeOfSegmentsAwaitingCompaction(String dataSource)\n   {\n-    return totalSizesOfSegmentsAwaitingCompactionPerDataSource.get(dataSource);\n+    AutoCompactionSnapshot autoCompactionSnapshot = autoCompactionSnapshotPerDataSource.get(dataSource);\n+    if (autoCompactionSnapshot == null) {\n+      return null;\n+    }\n+    return autoCompactionSnapshotPerDataSource.get(dataSource).getByteCountAwaitingCompaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk2ODczMA==", "bodyText": "When you fix this concurrency issue, please add enough description about what concurrency issue exists here and how it is handled. It would be also useful to check out our concurrency checklist.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488968730", "createdAt": "2020-09-15T20:57:55Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +70,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private HashMap<String, AutoCompactionSnapshot> autoCompactionSnapshotPerDataSource = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk1MTk0Mw=="}, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NDg3NQ==", "bodyText": "Thinking about new methods in this interface, I'm not sure if they are good since now every implementation of this interface should track of remaining and processed segments even though the tracking logic will be likely duplicate (even though we have only one implementation yet \ud83d\ude42). How about adding next(Map<String, CompactionStatistics> stats) so that CompactSegments can pass in an appropriate map? Then, it can just iterate over all remaining entries in the iterator without introducing any methods such as flushAllSegments() which seems to have a complicated contract. CompactionSegmentIterator will not extend Iterator anymore in this case.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488974875", "createdAt": "2020-09-15T21:10:00Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactionSegmentIterator.java", "diffHunk": "@@ -19,22 +19,45 @@\n \n package org.apache.druid.server.coordinator.duty;\n \n-import it.unimi.dsi.fastutil.objects.Object2LongOpenHashMap;\n+import org.apache.druid.server.coordinator.CompactionStatistics;\n import org.apache.druid.timeline.DataSegment;\n \n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n /**\n  * Segments in the lists which are the elements of this iterator are sorted according to the natural segment order\n  * (see {@link DataSegment#compareTo}).\n  */\n public interface CompactionSegmentIterator extends Iterator<List<DataSegment>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjI3Nw==", "bodyText": "I think it should be segmentBytes.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488976277", "createdAt": "2020-09-15T21:12:53Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjM4OQ==", "bodyText": "Same here. I think it should be segmentBytes.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488976389", "createdAt": "2020-09-15T21:13:09Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NjcxNw==", "bodyText": "Should we emit metrics for skipped intervals segments as well?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488976717", "createdAt": "2020-09-15T21:13:55Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,82 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING_COMPACTION,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentByte\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_COMPACTED,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/compacted/segmentCount\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3NzkxMg==", "bodyText": "Should we distinguish segments processed and segments skipped? Information about segments skipped will be useful that you can be aware of how many segments (or intervals) that auto compaction has skipped.", "url": "https://github.com/apache/druid/pull/10371#discussion_r488977912", "createdAt": "2020-09-15T21:16:15Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -336,25 +353,72 @@ private boolean needsCompaction(ClientCompactionTaskQueryTuningConfig tuningConf\n    * @return segments to compact\n    */\n   private SegmentsToCompact findSegmentsToCompact(\n+      final String dataSourceName,\n       final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n       final DataSourceCompactionConfig config\n   )\n   {\n-    final long inputSegmentSize = config.getInputSegmentSizeBytes();\n+    while (compactibleTimelineObjectHolderCursor.hasNext()) {\n+      final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, true)) {\n+        return candidates;\n+      } else {\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+    log.info(\"All segments look good! Nothing to compact\");\n+    return new SegmentsToCompact();\n+  }\n \n+  /**\n+   * Progressively iterates all remaining time intervals (latest first) in the\n+   * timeline {@param compactibleTimelineObjectHolderCursor}. Note that the timeline lookup duration is one day.\n+   * The logic for checking if the segments can be compacted or not is then perform on each iteration.\n+   * This is repeated until no remaining time intervals in {@param compactibleTimelineObjectHolderCursor}.\n+   */\n+  private void iterateAllSegments(\n+      final String dataSourceName,\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n+      final DataSourceCompactionConfig config\n+  )\n+  {\n     while (compactibleTimelineObjectHolderCursor.hasNext()) {\n       final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, false)) {\n+        // Collect statistic for segments that need compaction\n+        collectSegmentStatistics(remainingSegments, dataSourceName, candidates);\n+      } else {\n+        // Collect statistic for segments that does not need compaction\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk3ODI1OA==", "bodyText": "doSegmentsNeedCompaction()?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488978258", "createdAt": "2020-09-15T21:17:02Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java", "diffHunk": "@@ -336,25 +353,72 @@ private boolean needsCompaction(ClientCompactionTaskQueryTuningConfig tuningConf\n    * @return segments to compact\n    */\n   private SegmentsToCompact findSegmentsToCompact(\n+      final String dataSourceName,\n       final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n       final DataSourceCompactionConfig config\n   )\n   {\n-    final long inputSegmentSize = config.getInputSegmentSizeBytes();\n+    while (compactibleTimelineObjectHolderCursor.hasNext()) {\n+      final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, true)) {\n+        return candidates;\n+      } else {\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+    log.info(\"All segments look good! Nothing to compact\");\n+    return new SegmentsToCompact();\n+  }\n \n+  /**\n+   * Progressively iterates all remaining time intervals (latest first) in the\n+   * timeline {@param compactibleTimelineObjectHolderCursor}. Note that the timeline lookup duration is one day.\n+   * The logic for checking if the segments can be compacted or not is then perform on each iteration.\n+   * This is repeated until no remaining time intervals in {@param compactibleTimelineObjectHolderCursor}.\n+   */\n+  private void iterateAllSegments(\n+      final String dataSourceName,\n+      final CompactibleTimelineObjectHolderCursor compactibleTimelineObjectHolderCursor,\n+      final DataSourceCompactionConfig config\n+  )\n+  {\n     while (compactibleTimelineObjectHolderCursor.hasNext()) {\n       final SegmentsToCompact candidates = new SegmentsToCompact(compactibleTimelineObjectHolderCursor.next());\n+      if (isSegmentsNeedCompact(candidates, config, false)) {\n+        // Collect statistic for segments that need compaction\n+        collectSegmentStatistics(remainingSegments, dataSourceName, candidates);\n+      } else {\n+        // Collect statistic for segments that does not need compaction\n+        collectSegmentStatistics(processedSegments, dataSourceName, candidates);\n+      }\n+    }\n+  }\n \n-      if (!candidates.isEmpty()) {\n-        final boolean isCompactibleSize = candidates.getTotalSize() <= inputSegmentSize;\n-        final boolean needsCompaction = needsCompaction(\n-            ClientCompactionTaskQueryTuningConfig.from(config.getTuningConfig(), config.getMaxRowsPerSegment()),\n-            candidates\n-        );\n+  /**\n+   * This method encapsulates the logic for checking if a given {@param candidates} needs compaction or not.\n+   * If {@param logCannotCompactReason} is true then the reason for {@param candidates} not needing compaction is\n+   * logged (for the case that {@param candidates} does not needs compaction).\n+   *\n+   * @return true if the {@param candidates} needs compaction, false if the {@param candidates} does not needs compaction\n+   */\n+  private boolean isSegmentsNeedCompact(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4MjAzMQ==", "bodyText": "Hmm this test looks similar to testRun(). Can they be merged by moving the snapshot verification to assertCompactSegments() (or merging assertCompactSegmentStatistics and assertCompactSegments)?", "url": "https://github.com/apache/druid/pull/10371#discussion_r488982031", "createdAt": "2020-09-15T21:25:21Z", "author": {"login": "jihoonson"}, "path": "server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java", "diffHunk": "@@ -270,12 +275,246 @@ public String get()\n     assertLastSegmentNotCompacted(compactSegments);\n   }\n \n+  @Test\n+  public void testMakeStats()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e83dbec9d9840ed40de1a072fffec013502962f8"}, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e8603a4138ef42a931b5369157ac66c354dfc6e", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/8e8603a4138ef42a931b5369157ac66c354dfc6e", "committedDate": "2020-09-16T08:50:46Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e371765b5cc1bef99d9554c1cd4c593ea598479", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/7e371765b5cc1bef99d9554c1cd4c593ea598479", "committedDate": "2020-09-16T08:51:48Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7894b9c09de5483d0a98bdbb0269ba46ea77938", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/c7894b9c09de5483d0a98bdbb0269ba46ea77938", "committedDate": "2020-09-17T09:29:44Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/461cc3a59f2726e9190d72e934f3cd965e20025a", "committedDate": "2020-09-17T21:09:47Z", "message": "make code coverage happy"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxMTM1NDEy", "url": "https://github.com/apache/druid/pull/10371#pullrequestreview-491135412", "createdAt": "2020-09-18T03:58:57Z", "commit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMzo1ODo1N1rOHT9Pqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwNDoxMjoyMVrOHT9b9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY4ODQyNw==", "bodyText": "nit: you can use while(iterator.hasNext())", "url": "https://github.com/apache/druid/pull/10371#discussion_r490688427", "createdAt": "2020-09-18T03:58:57Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -234,29 +294,174 @@ private CoordinatorStats doRun(\n     return newContext;\n   }\n \n-  private CoordinatorStats makeStats(int numCompactionTasks, CompactionSegmentIterator iterator)\n+  /**\n+   * This method can be use to atomically update the snapshots in {@code autoCompactionSnapshotPerDataSource} when\n+   * no compaction task is schedule in this run. Currently, this method does not update compaction statistics\n+   * (bytes, interval count, segment count, etc) since we skip iterating through the segments and cannot get an update\n+   * on those statistics. Thus, this method only updates the schedule status and task list (compaction statistics\n+   * remains the same as the previous snapshot).\n+   */\n+  private void updateAutoCompactionSnapshotWhenNoCompactTaskScheduled(\n+      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders\n+  )\n+  {\n+    Map<String, AutoCompactionSnapshot> previousSnapshots = autoCompactionSnapshotPerDataSource.get();\n+    for (Map.Entry<String, AutoCompactionSnapshot.Builder> autoCompactionSnapshotBuilderEntry : currentRunAutoCompactionSnapshotBuilders.entrySet()) {\n+      final String dataSource = autoCompactionSnapshotBuilderEntry.getKey();\n+      AutoCompactionSnapshot previousSnapshot = previousSnapshots.get(dataSource);\n+      if (previousSnapshot != null) {\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesAwaitingCompaction(previousSnapshot.getBytesAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesCompacted(previousSnapshot.getBytesCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementBytesSkipped(previousSnapshot.getBytesSkipped());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountAwaitingCompaction(previousSnapshot.getSegmentCountAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountCompacted(previousSnapshot.getSegmentCountCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementSegmentCountSkipped(previousSnapshot.getSegmentCountSkipped());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountAwaitingCompaction(previousSnapshot.getIntervalCountAwaitingCompaction());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountCompacted(previousSnapshot.getIntervalCountCompacted());\n+        autoCompactionSnapshotBuilderEntry.getValue().incrementIntervalCountSkipped(previousSnapshot.getIntervalCountSkipped());\n+      }\n+    }\n+\n+    Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = Maps.transformValues(\n+        currentRunAutoCompactionSnapshotBuilders,\n+        AutoCompactionSnapshot.Builder::build\n+    );\n+    // Atomic update of autoCompactionSnapshotPerDataSource with the latest from this coordinator run\n+    autoCompactionSnapshotPerDataSource.set(currentAutoCompactionSnapshotPerDataSource);\n+  }\n+\n+  private CoordinatorStats makeStats(\n+      Map<String, AutoCompactionSnapshot.Builder> currentRunAutoCompactionSnapshotBuilders,\n+      int numCompactionTasks,\n+      CompactionSegmentIterator iterator\n+  )\n   {\n+    final Map<String, AutoCompactionSnapshot> currentAutoCompactionSnapshotPerDataSource = new HashMap<>();\n     final CoordinatorStats stats = new CoordinatorStats();\n     stats.addToGlobalStat(COMPACTION_TASK_COUNT, numCompactionTasks);\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource = iterator.totalRemainingSegmentsSizeBytes();\n-    totalSizesOfSegmentsAwaitingCompactionPerDataSource.object2LongEntrySet().fastForEach(\n-        entry -> {\n-          final String dataSource = entry.getKey();\n-          final long totalSizeOfSegmentsAwaitingCompaction = entry.getLongValue();\n-          stats.addToDataSourceStat(\n-              TOTAL_SIZE_OF_SEGMENTS_AWAITING_COMPACTION,\n-              dataSource,\n-              totalSizeOfSegmentsAwaitingCompaction\n-          );\n-        }\n-    );\n+\n+    // Iterate through all the remaining segments in the iterator.\n+    // As these segments could be compacted but were not compacted due to lack of task slot, we will aggregates\n+    // the statistic to the AwaitingCompaction statistics\n+    for (; iterator.hasNext();) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDEzNg==", "bodyText": "Could you please add a Javadoc about the concurrent access pattern on autoCompactionSnapshotPerDataSource? I guess it can say \"This variable is updated by the Coordinator thread executing duties and read by HTTP threads processing Coordinator API calls.\"", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690136", "createdAt": "2020-09-18T04:06:12Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +77,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private AtomicReference<Map<String, AutoCompactionSnapshot>> autoCompactionSnapshotPerDataSource = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDE5MA==", "bodyText": "nit: this variable can be final.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690190", "createdAt": "2020-09-18T04:06:23Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/CompactSegments.java", "diffHunk": "@@ -61,7 +77,7 @@\n   private final CompactionSegmentSearchPolicy policy;\n   private final IndexingServiceClient indexingServiceClient;\n \n-  private Object2LongOpenHashMap<String> totalSizesOfSegmentsAwaitingCompactionPerDataSource;\n+  private AtomicReference<Map<String, AutoCompactionSnapshot>> autoCompactionSnapshotPerDataSource = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDEzNg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDkxMg==", "bodyText": "Hmm, should it be interval/waitCompact/count?", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690912", "createdAt": "2020-09-18T04:09:42Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDk4Ng==", "bodyText": "Maybe compactTask/maxSlot/count?", "url": "https://github.com/apache/druid/pull/10371#discussion_r490690986", "createdAt": "2020-09-18T04:09:58Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTA3NQ==", "bodyText": "Similarly, maybe compactTask/availableSlot/count?", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691075", "createdAt": "2020-09-18T04:10:18Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTI3MQ==", "bodyText": "I think segment/waitCompact/bytes would be enough. The second segment in segmentBytes seems duplicate.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691271", "createdAt": "2020-09-18T04:10:59Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTQ1OA==", "bodyText": "Same comment for other metrics for segment size and count.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691458", "createdAt": "2020-09-18T04:11:54Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTI3MQ=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MTU3Mg==", "bodyText": "Same question for other metrics for interval.", "url": "https://github.com/apache/druid/pull/10371#discussion_r490691572", "createdAt": "2020-09-18T04:12:21Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordinator/duty/EmitClusterStatsAndMetrics.java", "diffHunk": "@@ -301,13 +301,115 @@ public DruidCoordinatorRuntimeParams run(DruidCoordinatorRuntimeParams params)\n         )\n     );\n \n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/maxSlot/count\",\n+            stats.getGlobalStat(CompactSegments.MAX_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    emitter.emit(\n+        new ServiceMetricEvent.Builder().build(\n+            \"compact/availableSlot/count\",\n+            stats.getGlobalStat(CompactSegments.AVAILABLE_COMPACTION_TASK_SLOT)\n+        )\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_SIZE_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentBytes\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_COUNT_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/segmentCount\", count)\n+          );\n+        }\n+    );\n+\n+    stats.forEachDataSourceStat(\n+        CompactSegments.TOTAL_INTERVAL_OF_SEGMENTS_AWAITING,\n+        (final String dataSource, final long count) -> {\n+          emitter.emit(\n+              new ServiceMetricEvent.Builder()\n+                  .setDimension(DruidMetrics.DATASOURCE, dataSource)\n+                  .build(\"segment/waitCompact/intervalCount\", count)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY5MDkxMg=="}, "originalCommit": {"oid": "461cc3a59f2726e9190d72e934f3cd965e20025a"}, "originalPosition": 46}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c2003ce5449c54803314321970c66efc9802dfda", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/c2003ce5449c54803314321970c66efc9802dfda", "committedDate": "2020-09-18T05:37:38Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1c205a1e8cb27d53d72d3bcfcbc3392c5b91b55", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/b1c205a1e8cb27d53d72d3bcfcbc3392c5b91b55", "committedDate": "2020-09-18T05:38:37Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxNjcwNjE4", "url": "https://github.com/apache/druid/pull/10371#pullrequestreview-491670618", "createdAt": "2020-09-18T17:54:27Z", "commit": {"oid": "b1c205a1e8cb27d53d72d3bcfcbc3392c5b91b55"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ac24910915f2619c8dfde4f9b53a55438ea6eea", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2ac24910915f2619c8dfde4f9b53a55438ea6eea", "committedDate": "2020-09-18T20:33:51Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2266cbdcbd0b9620669a2fe84bcf35fbc4667eae", "author": {"user": {"login": "maytasm", "name": "Maytas Monsereenusorn"}}, "url": "https://github.com/apache/druid/commit/2266cbdcbd0b9620669a2fe84bcf35fbc4667eae", "committedDate": "2020-09-18T20:36:16Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxODIxMDA2", "url": "https://github.com/apache/druid/pull/10371#pullrequestreview-491821006", "createdAt": "2020-09-18T22:47:18Z", "commit": {"oid": "2266cbdcbd0b9620669a2fe84bcf35fbc4667eae"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3599, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}