{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4MjU2Nzc5", "number": 10288, "title": "Store hash partition function in dataSegment and allow segment pruning only when hash partition function is provided", "bodyText": "Description\nThe segment pruning was recently introduced in #9810. This is a nice feature to have, but could have some potential compatibility issues.\n\nWe need a flag to disable segment pruning just in case where some unknown bugs exist in pruning hash-partitioned segments.\nThe hash function used to create segments at ingestion time can change over time. I'm not sure whether or not we have ever changed it. If we have ever done, the segment pruning can lead to incorrect query results for those segments partitioned with a different hash function.\n\nTo address these issues, this PR adds 2 flags, a new query context, segmentPruning, and a new partitionFunction field in partitionsSpec. The segmentPruning context is to allow disabling segment pruning at query time. It is enabled by default to keep the current behavior for range-partitioned segments.\nThe new partitionFunction field is stored as a new enum, HashPartitionFunction. If users specify partitionFunction in the ingestion spec, it will be stored for each segment in the metadata store after ingestion so that we can retrieve the proper hash function at query time. This doesn't mean accessing metadata store will be required in the query path. Every segment announced will have the proper partitionFunction which was used to create it at ingestion time.\nThe current way for computing hash of partition dimensions is 1) retrieving values of partition dimensions, 2) serializing them into a byte array, 3) and computing hash of the byte array. The new HashPartitionFunction enum only defines how to compute hash of a byte array. This means, it doesn't do anything with serializing partition dimension values into a byte array. I'm thinking to add \"partition expressions\" so that we can use expressions for hash partitioning in the future. The new enum can work with partition expressions since it's decoupled from how to retrieve values of partition keys and how to serialize them.\nFor hash-partitioned segments, the broker can prune segments only when partitionFunction and a non-empty partitionDimensions are stored in shardSpec and segmentPruning is set in query context. The partitionFunction is set to null by default unless it is specified by users explicitly.\nI additionally removed isInChunk() interface from ShardSpec since it's used only in SingleDimensionShardSpec.\n\nThis PR has:\n\n been self-reviewed.\n\n using the concurrency checklist (Remove this item if the PR doesn't have any relation to concurrency.)\n\n\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added or updated version, license, or notice information in licenses.yaml\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths, ensuring the threshold for code coverage is met.\n added integration tests.\n been tested in a test Druid cluster.", "createdAt": "2020-08-15T04:06:13Z", "url": "https://github.com/apache/druid/pull/10288", "merged": true, "mergeCommit": {"oid": "0cc9eb4903e7bddb4c1484984bf87c8fab7648df"}, "closed": true, "closedAt": "2020-09-24T23:32:57Z", "author": {"login": "jihoonson"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc_A5SWgH2gAyNDY4MjU2Nzc5OmU0ODg4OTRhOTQxMzljNjBlZTFhODQxNmZmOTU0ZDI0YzQ0Zjc3ODY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdMH0MIAH2gAyNDY4MjU2Nzc5OjI3YTE1MjEwMTczZmU4OGRmYzM0NDA5ZTA1ZTllNGI2M2RkZjVhMDI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "e488894a94139c60ee1a8416ff954d24c44f7786", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/e488894a94139c60ee1a8416ff954d24c44f7786", "committedDate": "2020-08-15T03:45:53Z", "message": "Store hash partition function in dataSegment and allow segment pruning only when hash partition function is provided"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac812f853b3ac75cd8ee22212e657d8559ac8231", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/ac812f853b3ac75cd8ee22212e657d8559ac8231", "committedDate": "2020-08-15T03:58:46Z", "message": "query context"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6975c18b47e6e87baed5b9228042790b9fbd1002", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6975c18b47e6e87baed5b9228042790b9fbd1002", "committedDate": "2020-08-15T16:25:15Z", "message": "fix tests; add more test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22e4ede88a1531c261d9e9f3df3fac9b880b5d4e", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/22e4ede88a1531c261d9e9f3df3fac9b880b5d4e", "committedDate": "2020-09-15T00:43:51Z", "message": "Merge branch 'master' of github.com:apache/druid into hash-partition-function"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ad79133125547c3ddca8e4b787ca2384c18876a", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/1ad79133125547c3ddca8e4b787ca2384c18876a", "committedDate": "2020-09-16T01:51:48Z", "message": "javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31bf9a2ce9d772d6512b1567d8544ef54bd4451f", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/31bf9a2ce9d772d6512b1567d8544ef54bd4451f", "committedDate": "2020-09-17T01:31:47Z", "message": "docs and more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e138d501ac6ee85a3fd5d6cd069a832e90c71c72", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/e138d501ac6ee85a3fd5d6cd069a832e90c71c72", "committedDate": "2020-09-17T02:01:34Z", "message": "remove default and hadoop tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "003bf3bbc459caee95f373f5084cfdb22fa872da", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/003bf3bbc459caee95f373f5084cfdb22fa872da", "committedDate": "2020-09-17T02:09:35Z", "message": "consistent name and fix javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5b78ee302b8f0cb57bca03d3a0c4b763355a9e3", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/a5b78ee302b8f0cb57bca03d3a0c4b763355a9e3", "committedDate": "2020-09-17T05:55:37Z", "message": "spelling and field name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/2d437b67b05a038813a120c245dcd610bb32cf85", "committedDate": "2020-09-17T06:03:29Z", "message": "Merge branch 'master' of github.com:apache/druid into hash-partition-function"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkyMjM1OTMz", "url": "https://github.com/apache/druid/pull/10288#pullrequestreview-492235933", "createdAt": "2020-09-21T00:08:08Z", "commit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQwMDowODowOFrOHU-Jew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxMTo1Mzo0NlrOHVMVVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MTgwMw==", "bodyText": "it might be worth mentioning why this class should only be used for ingestion so it isn't left as an exercise for the reader", "url": "https://github.com/apache/druid/pull/10288#discussion_r491751803", "createdAt": "2020-09-21T00:08:08Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.Rows;\n+\n+import javax.annotation.Nullable;\n+import java.util.List;\n+\n+/**\n+ * This class is used for hash partitioning during ingestion. The {@link ShardSpecLookup} returned from\n+ * {@link #createHashLookup} is used to determine what hash bucket the given input row will belong to.\n+ *\n+ * Note: this class must be used only for ingestion. For segment pruning at query time,\n+ * {@link HashBasedNumberedShardSpec#partitionFunction} should be used instead.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjEwNg==", "bodyText": "Since the broker is always pruning on interval, should the docs on this clarify a bit more that this pruning happens within a time chunk, or should this be rebranded as something like secondarySegmentPruning since it maps to the secondary partitioning, or something?\nBasically, I'm wondering if it sort of unclear what happens if this isn't set, if I didn't already know how druid was functioning internally.", "url": "https://github.com/apache/druid/pull/10288#discussion_r491752106", "createdAt": "2020-09-21T00:10:50Z", "author": {"login": "clintropolis"}, "path": "docs/querying/query-context.md", "diffHunk": "@@ -58,6 +58,7 @@ These parameters apply to all query types.\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |useFilterCNF|`false`| If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects.|\n+|segmentPruning|`true`|Enable segment pruning on the Broker. Segment pruning can be applied to only the segments partitioned by hash or range.|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjM0MQ==", "bodyText": "should we also stress here that anything added here must be backwards compatible, forever?", "url": "https://github.com/apache/druid/pull/10288#discussion_r491752341", "createdAt": "2020-09-21T00:13:00Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonValue;\n+import com.google.common.hash.Hashing;\n+import org.apache.druid.java.util.common.StringUtils;\n+\n+/**\n+ * An enum of supported hash partition functions. This enum should be updated when we want to use a new function\n+ * for hash partitioning. This function is a part of {@link HashBasedNumberedShardSpec} which is stored\n+ * in the metadata store.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4NDIxMg==", "bodyText": "is there any reason not to set the default in the shard specs used for making new segments?", "url": "https://github.com/apache/druid/pull/10288#discussion_r491984212", "createdAt": "2020-09-21T11:53:46Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java", "diffHunk": "@@ -58,6 +61,7 @@ public BuildingHashBasedNumberedShardSpec(\n     this.partitionDimensions = partitionDimensions == null\n                                ? HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS\n                                : partitionDimensions;\n+    this.partitionFunction = partitionFunction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c39f1cebc04e0ad334968d171e5d220d4a8087b", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/9c39f1cebc04e0ad334968d171e5d220d4a8087b", "committedDate": "2020-09-22T01:18:17Z", "message": "default function for partitionsSpec"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "committedDate": "2020-09-22T02:01:52Z", "message": "other comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzODM2OTIy", "url": "https://github.com/apache/druid/pull/10288#pullrequestreview-493836922", "createdAt": "2020-09-22T20:51:31Z", "commit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo1MTozMVrOHWL6yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDo1OToyMVrOHWMLFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyNTk5NQ==", "bodyText": "suggest rewording the second line to something like \"The current version of Druid will always specify a partitionFunction on newly created segments\"", "url": "https://github.com/apache/druid/pull/10288#discussion_r493025995", "createdAt": "2020-09-22T20:51:31Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -98,141 +107,44 @@ public int getNumBuckets()\n     return partitionDimensions;\n   }\n \n-  @Override\n-  public List<String> getDomainDimensions()\n+  @JsonProperty\n+  public @Nullable HashPartitionFunction getPartitionFunction()\n   {\n-    return partitionDimensions;\n+    return partitionFunction;\n   }\n \n   @Override\n-  public boolean isInChunk(long timestamp, InputRow inputRow)\n-  {\n-    return getBucketIndex(hash(timestamp, inputRow), numBuckets) == bucketId % numBuckets;\n-  }\n-\n-  /**\n-   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n-   * are of {@code partitionDimensionsValues}\n-   *\n-   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n-   *\n-   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n-   */\n-  private boolean isInChunk(Map<String, String> partitionDimensionsValues)\n-  {\n-    assert !partitionDimensions.isEmpty();\n-    List<Object> groupKey = Lists.transform(\n-        partitionDimensions,\n-        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n-    );\n-    try {\n-      return getBucketIndex(hash(jsonMapper, groupKey), numBuckets) == bucketId % numBuckets;\n-    }\n-    catch (JsonProcessingException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  /**\n-   * This method calculates the hash based on whether {@param partitionDimensions} is null or not.\n-   * If yes, then both {@param timestamp} and dimension columns in {@param inputRow} are used {@link Rows#toGroupKey}\n-   * Or else, columns in {@param partitionDimensions} are used\n-   *\n-   * @param timestamp should be bucketed with query granularity\n-   * @param inputRow row from input data\n-   *\n-   * @return hash value\n-   */\n-  protected int hash(long timestamp, InputRow inputRow)\n-  {\n-    return hash(jsonMapper, partitionDimensions, timestamp, inputRow);\n-  }\n-\n-  public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions, long timestamp, InputRow inputRow)\n-  {\n-    final List<Object> groupKey = getGroupKey(partitionDimensions, timestamp, inputRow);\n-    try {\n-      return hash(jsonMapper, groupKey);\n-    }\n-    catch (JsonProcessingException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n-  {\n-    if (partitionDimensions.isEmpty()) {\n-      return Rows.toGroupKey(timestamp, inputRow);\n-    } else {\n-      return Lists.transform(partitionDimensions, inputRow::getDimension);\n-    }\n-  }\n-\n-  @VisibleForTesting\n-  public static int hash(ObjectMapper jsonMapper, List<Object> objects) throws JsonProcessingException\n+  public List<String> getDomainDimensions()\n   {\n-    return HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(objects)).asInt();\n+    return partitionDimensions;\n   }\n \n   @Override\n   public ShardSpecLookup getLookup(final List<? extends ShardSpec> shardSpecs)\n   {\n-    return createHashLookup(jsonMapper, partitionDimensions, shardSpecs, numBuckets);\n-  }\n-\n-  static ShardSpecLookup createHashLookup(\n-      ObjectMapper jsonMapper,\n-      List<String> partitionDimensions,\n-      List<? extends ShardSpec> shardSpecs,\n-      int numBuckets\n-  )\n-  {\n-    return (long timestamp, InputRow row) -> {\n-      int index = getBucketIndex(hash(jsonMapper, partitionDimensions, timestamp, row), numBuckets);\n-      return shardSpecs.get(index);\n-    };\n+    // partitionFunction can be null when you read a shardSpec of a segment created in an old version of Druid.\n+    // It can never be null for segments to create during ingestion.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyOTAwNg==", "bodyText": "is the -1 a typo here?", "url": "https://github.com/apache/druid/pull/10288#discussion_r493029006", "createdAt": "2020-09-22T20:57:09Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/index.md", "diffHunk": "@@ -90,7 +90,7 @@ This table compares the three available options:\n | **Input locations** | Any [`inputSource`](./native-batch.md#input-sources). | Any Hadoop FileSystem or Druid datasource. | Any [`inputSource`](./native-batch.md#input-sources). |\n | **File formats** | Any [`inputFormat`](./data-formats.md#input-format). | Any Hadoop InputFormat. | Any [`inputFormat`](./data-formats.md#input-format). |\n | **[Rollup modes](#rollup)** | Perfect if `forceGuaranteedRollup` = true in the [`tuningConfig`](native-batch.md#tuningconfig).  | Always perfect. | Perfect if `forceGuaranteedRollup` = true in the [`tuningConfig`](native-batch.md#tuningconfig). |\n-| **Partitioning options** | Dynamic, hash-based, and range-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. | Hash-based or range-based partitioning via [`partitionsSpec`](hadoop.md#partitionsspec). | Dynamic and hash-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. |\n+| **Partitioning options** | Dynamic, hash-based, and range-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec) for details. | Hash-based or range-based partitioning via [`partitionsSpec`](hadoop.md#partitionsspec). | Dynamic and hash-based partitioning methods are available. See [Partitions Spec](./native-batch.md#partitionsspec-1) for details. |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMDE2Ng==", "bodyText": "partitionFunction must be set to enable segment pruning.\n\nIs it necessary to set partitionFunction? It's shown as optional in the tables below", "url": "https://github.com/apache/druid/pull/10288#discussion_r493030166", "createdAt": "2020-09-22T20:59:21Z", "author": {"login": "jon-wei"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -260,7 +260,7 @@ The three `partitionsSpec` types have different characteristics.\n | PartitionsSpec | Ingestion speed | Partitioning method | Supported rollup mode | Segment pruning at query time |\n |----------------|-----------------|---------------------|-----------------------|-------------------------------|\n | `dynamic` | Fastest  | Partitioning based on number of rows in segment. | Best-effort rollup | N/A |\n-| `hashed`  | Moderate | Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See [Partitioning](./index.md#partitioning) for more details. | Perfect rollup | The broker can use the partition information to prune segments early to speed up queries if `partitionDimensions` is explicitly specified during ingestion. Since the broker knows how to hash `partitionDimensions` values to locate a segment, given a query including a filter on all the `partitionDimensions`, the broker can pick up only the segments holding the rows satisfying the filter on `partitionDimensions` for query processing. |\n+| `hashed`  | Moderate | Partitioning based on the hash value of partition dimensions. This partitioning may reduce your datasource size and query latency by improving data locality. See [Partitioning](./index.md#partitioning) for more details. | Perfect rollup | The broker can use the partition information to prune segments early to speed up queries if `partitionDimensions` is explicitly specified during ingestion. Since the broker knows how to hash `partitionDimensions` values to locate a segment, given a query including a filter on all the `partitionDimensions`, the broker can pick up only the segments holding the rows satisfying the filter on `partitionDimensions` for query processing.<br/><br/>Note that `partitionDimensions` and `partitionFunction` must be set to enable segment pruning.|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzODQ2MDQx", "url": "https://github.com/apache/druid/pull/10288#pullrequestreview-493846041", "createdAt": "2020-09-22T21:05:03Z", "commit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMTowNTowM1rOHWMWWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMTowNTowM1rOHWMWWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMzA0OA==", "bodyText": "I wonder if we'll eventually need a versioning scheme that's considered by the segment pruning (like \"serialization version\"). It doesn't seem needed for now since we only have primitive + String dimension types and the serialization doesn't seem likely to change there.\nIf we had \"complex\" dimensions at some point, I could see that being difficult to support backwards compatibility for if the serialization format of a complex dim changed.", "url": "https://github.com/apache/druid/pull/10288#discussion_r493033048", "createdAt": "2020-09-22T21:05:03Z", "author": {"login": "jon-wei"}, "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -298,8 +211,73 @@ private boolean chunkPossibleInDomain(\n     return false;\n   }\n \n-  private static int getBucketIndex(int hash, int numBuckets)\n+  /**\n+   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n+   * are of {@code partitionDimensionsValues}\n+   *\n+   * @param hashPartitionFunction     hash function used to create segments at ingestion time\n+   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n+   *\n+   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n+   */\n+  private boolean isInChunk(HashPartitionFunction hashPartitionFunction, Map<String, String> partitionDimensionsValues)\n   {\n-    return Math.abs(hash % numBuckets);\n+    assert !partitionDimensions.isEmpty();\n+    List<Object> groupKey = Lists.transform(\n+        partitionDimensions,\n+        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n+    );\n+    return hashPartitionFunction.hash(serializeGroupKey(jsonMapper, groupKey), numBuckets) == bucketId;\n+  }\n+\n+  /**\n+   * Serializes a group key into a byte array. The serialization algorithm can affect hash values of partition keys\n+   * since {@link HashPartitionFunction#hash} takes the result of this method as its input. This means, the returned\n+   * byte array should be backwards-compatible in cases where we need to modify this method.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96"}, "originalPosition": 306}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d54304a9acd2d9bcc80bb25bab9bd571d2129e2b", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/d54304a9acd2d9bcc80bb25bab9bd571d2129e2b", "committedDate": "2020-09-22T22:54:13Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2903072d316d69ad53ae0f6b98674378147037db", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/2903072d316d69ad53ae0f6b98674378147037db", "committedDate": "2020-09-22T23:02:18Z", "message": "fix tests and spelling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec266e6a7a5dc614b416cc096dd33e23e1194dbf", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/ec266e6a7a5dc614b416cc096dd33e23e1194dbf", "committedDate": "2020-09-23T20:15:47Z", "message": "test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTY1Njcy", "url": "https://github.com/apache/druid/pull/10288#pullrequestreview-495165672", "createdAt": "2020-09-24T01:48:03Z", "commit": {"oid": "ec266e6a7a5dc614b416cc096dd33e23e1194dbf"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1Mzk3MTQw", "url": "https://github.com/apache/druid/pull/10288#pullrequestreview-495397140", "createdAt": "2020-09-24T09:43:26Z", "commit": {"oid": "2903072d316d69ad53ae0f6b98674378147037db"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzoyNlrOHXSXcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0MzoyNlrOHXSXcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDIxMQ==", "bodyText": "The broker can basically prune segments unnecessary for queries based on a filter on time intervals.\n\nSorry to nitpick, but this reads sort of funny, also I think 'Broker' should be consistently capitalized. How about:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The broker can basically prune segments unnecessary for queries based on a filter on time intervals. If the datasource is further partitioned based on hash or range partitioning, this query context will enable secondary partition pruning so that the broker can eliminate unnecessary segments from the input scan based on a filter on secondary partition dimensions.|\n          \n          \n            \n            |secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The Broker will always prune unnecessary segments from the input scan based on a filter on time intervals, but if the data is further partitioned with hash or range partitioning, this option will enable additional pruning based on a filter on secondary partition dimensions.|", "url": "https://github.com/apache/druid/pull/10288#discussion_r494180211", "createdAt": "2020-09-24T09:43:26Z", "author": {"login": "clintropolis"}, "path": "docs/querying/query-context.md", "diffHunk": "@@ -58,7 +58,7 @@ These parameters apply to all query types.\n |parallelMergeInitialYieldRows|`druid.processing.merge.task.initialYieldNumRows`|Number of rows to yield per ForkJoinPool merge task for parallel result merging on the Broker, before forking off a new task to continue merging sequences. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |parallelMergeSmallBatchRows|`druid.processing.merge.task.smallBatchNumRows`|Size of result batches to operate on in ForkJoinPool merge tasks for parallel result merging on the Broker. See [Broker configuration](../configuration/index.html#broker) for more details.|\n |useFilterCNF|`false`| If true, Druid will attempt to convert the query filter to Conjunctive Normal Form (CNF). During query processing, columns can be pre-filtered by intersecting the bitmap indexes of all values that match the eligible filters, often greatly reducing the raw number of rows which need to be scanned. But this effect only happens for the top level filter, or individual clauses of a top level 'and' filter. As such, filters in CNF potentially have a higher chance to utilize a large amount of bitmap indexes on string columns during pre-filtering. However, this setting should be used with great caution, as it can sometimes have a negative effect on performance, and in some cases, the act of computing CNF of a filter can be expensive. We recommend hand tuning your filters to produce an optimal form if possible, or at least verifying through experimentation that using this parameter actually improves your query performance with no ill-effects.|\n-|segmentPruning|`true`|Enable segment pruning on the Broker. Segment pruning can be applied to only the segments partitioned by hash or range.|\n+|secondaryPartitionPruning|`true`|Enable secondary partition pruning on the Broker. The broker can basically prune segments unnecessary for queries based on a filter on time intervals. If the datasource is further partitioned based on hash or range partitioning, this query context will enable secondary partition pruning so that the broker can eliminate unnecessary segments from the input scan based on a filter on secondary partition dimensions.|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2903072d316d69ad53ae0f6b98674378147037db"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fa121fac1cb686e33191611875cb2f7071989b9", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/6fa121fac1cb686e33191611875cb2f7071989b9", "committedDate": "2020-09-24T18:10:34Z", "message": "doc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27a15210173fe88dfc34409e05e9e4b63ddf5a02", "author": {"user": {"login": "jihoonson", "name": "Jihoon Son"}}, "url": "https://github.com/apache/druid/commit/27a15210173fe88dfc34409e05e9e4b63ddf5a02", "committedDate": "2020-09-24T21:10:40Z", "message": "Merge branch 'master' of github.com:apache/druid into hash-partition-function"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1989, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}