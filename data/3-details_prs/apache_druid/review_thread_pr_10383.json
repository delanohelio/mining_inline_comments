{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg0Nzc4ODEx", "number": 10383, "reviewThreads": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNTozNTo0NFrOEiv3KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNzo1NTozN1rOE3WA5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODcxMjA4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNTozNTo0NFrOHQyzPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMlQwNTozNTo0NFrOHQyzPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM3MTU4Mw==", "bodyText": "I haven't looked through the PR yet, but found that the parser created from JsonFactory is Closeable which should be closed when the below CloseableIterator is closed. I will review again when you update the PR.", "url": "https://github.com/apache/druid/pull/10383#discussion_r487371583", "createdAt": "2020-09-12T05:35:44Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "diffHunk": "@@ -49,41 +55,99 @@\n       boolean keepNullColumns\n   )\n   {\n-    super(inputRowSchema, source);\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = source;\n     this.flattener = ObjectFlatteners.create(flattenSpec, new JSONFlattenerMaker(keepNullColumns));\n     this.mapper = mapper;\n   }\n \n-  @Override\n-  public List<InputRow> parseInputRows(String line) throws IOException, ParseException\n-  {\n-    final JsonNode document = mapper.readValue(line, JsonNode.class);\n-    final Map<String, Object> flattened = flattener.flatten(document);\n-    return Collections.singletonList(MapInputRowParser.parse(getInputRowSchema(), flattened));\n-  }\n \n   @Override\n-  public Map<String, Object> toMap(String intermediateRow) throws IOException\n+  public CloseableIterator<InputRow> read() throws IOException\n   {\n-    //noinspection unchecked\n-    return mapper.readValue(intermediateRow, Map.class);\n+    final MappingIterator<JsonNode> delegate = mapper.readValues(\n+        new JsonFactory().createParser(this.source.open()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjIwODI0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/ExceptionThrowingIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwMDowMzoyMlrOHU2GQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMzo0MzoxMFrOHXz75Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYxOTkwNA==", "bodyText": "Hmm, SpotBugs thinks this is not good since it cannot throw NoSuchElementException which is wrong since next() never can be called more than once. But, let's make it happy by simply adding if (!haxNext()) throws NoSuchElementException` here.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491619904", "createdAt": "2020-09-20T00:03:22Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/ExceptionThrowingIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+\n+public class ExceptionThrowingIterator<T> implements CloseableIterator<T>\n+{\n+  private final RuntimeException exception;\n+\n+  private boolean thrown = false;\n+\n+  public ExceptionThrowingIterator(Throwable exception)\n+  {\n+    this.exception = exception instanceof RuntimeException\n+                     ? (RuntimeException) exception\n+                     : new RuntimeException(exception);\n+  }\n+\n+  @Override\n+  public boolean hasNext()\n+  {\n+    return !thrown;\n+  }\n+\n+  @Override\n+  public T next()\n+  {\n+    thrown = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMDIxMw==", "bodyText": "I don't know why SpotBugs didn't report the problem before this class is extracted. But if we adopt the solution that makes JsonReader inherit IntermediateRowParsingReader as you suggest, this modification should be rollback and  I'll check it again if the report of this bug is still there", "url": "https://github.com/apache/druid/pull/10383#discussion_r494730213", "createdAt": "2020-09-25T03:43:10Z", "author": {"login": "FrankChen021"}, "path": "core/src/main/java/org/apache/druid/data/input/ExceptionThrowingIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+\n+public class ExceptionThrowingIterator<T> implements CloseableIterator<T>\n+{\n+  private final RuntimeException exception;\n+\n+  private boolean thrown = false;\n+\n+  public ExceptionThrowingIterator(Throwable exception)\n+  {\n+    this.exception = exception instanceof RuntimeException\n+                     ? (RuntimeException) exception\n+                     : new RuntimeException(exception);\n+  }\n+\n+  @Override\n+  public boolean hasNext()\n+  {\n+    return !thrown;\n+  }\n+\n+  @Override\n+  public T next()\n+  {\n+    thrown = true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYxOTkwNA=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjIxMDY5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/ExceptionThrowingIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwMDowNDoyMFrOHU2IGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTozMDoxMVrOHXx5Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYyMDM3OQ==", "bodyText": "As this class is extracted now, please add some Javaodc describing what this class does and where it is used.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491620379", "createdAt": "2020-09-20T00:04:20Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/ExceptionThrowingIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+\n+public class ExceptionThrowingIterator<T> implements CloseableIterator<T>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5Njc2Ng==", "bodyText": "If JsonReader extends IntermediateRowParsingReader, this class will not have to be extracted.", "url": "https://github.com/apache/druid/pull/10383#discussion_r494696766", "createdAt": "2020-09-25T01:30:11Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/ExceptionThrowingIterator.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.data.input;\n+\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+\n+public class ExceptionThrowingIterator<T> implements CloseableIterator<T>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYyMDM3OQ=="}, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjIxNDE2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwMDowNTozM1rOHU2KfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwMDowNTozM1rOHU2KfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYyMDk4OA==", "bodyText": "typo: splitted -> split.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491620988", "createdAt": "2020-09-20T00:05:33Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "diffHunk": "@@ -41,6 +42,20 @@\n   private final ObjectMapper objectMapper;\n   private final boolean keepNullColumns;\n \n+  /**\n+   * <pre>\n+   * This parameter is introduced to support json string of an object in multiple lines in streaming ingestion records\n+   *\n+   * It indicates whether the input text can be splitted into lines in first, which will then be parsed into JSON objects one by one independently.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjIyNDE3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwMDowOToyNVrOHU2Rxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNTozNzozMFrOHU4QlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYyMjg1NQ==", "bodyText": "The pre tag is a tag for pre-formatted text such as source codes. I think you don't have to use it in this case.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491622855", "createdAt": "2020-09-20T00:09:25Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "diffHunk": "@@ -41,6 +42,20 @@\n   private final ObjectMapper objectMapper;\n   private final boolean keepNullColumns;\n \n+  /**\n+   * <pre>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY1NTMxNw==", "bodyText": "Hmm, I think the Javadoc should be more clear on what this flag means. How about rephrase it as below?\nThis parameter indicates whether or not the given InputEntity should be split by lines before parsing it. If it is set to true, the InputEntity must be split by lines first. If it is set to false, unlike what you could imagine, it means that the InputEntity doesn't have to be split by lines first, but it can still contain multiple lines. A created InputEntityReader from this format will determine by itself if line splitting is necessary.\n\nThis parameter should always be true for batch ingestion and false for streaming ingestion. For more information, see: https://github.com/apache/druid/pull/10383.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491655317", "createdAt": "2020-09-20T05:37:30Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "diffHunk": "@@ -41,6 +42,20 @@\n   private final ObjectMapper objectMapper;\n   private final boolean keepNullColumns;\n \n+  /**\n+   * <pre>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYyMjg1NQ=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjQyMjE5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNDoxNzo0MFrOHU38Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNDoxNzo0MFrOHU38Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY1MDEzMA==", "bodyText": "Same comment here. You don't need a pre tag.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491650130", "createdAt": "2020-09-20T04:17:40Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "diffHunk": "@@ -33,13 +40,98 @@\n \n import java.io.IOException;\n import java.util.Collections;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n+import java.util.NoSuchElementException;\n \n-public class JsonReader extends TextReader\n+/**\n+ * <pre>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NjQzMDkxOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonReaderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNDozMjozNFrOHU4AIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNDozMjozNFrOHU4AIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY1MTEwNg==", "bodyText": "Please verify that e is the exception what we expect.", "url": "https://github.com/apache/druid/pull/10383#discussion_r491651106", "createdAt": "2020-09-20T04:32:34Z", "author": {"login": "jihoonson"}, "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonReaderTest.java", "diffHunk": "@@ -126,138 +149,158 @@ public void testParseRowWithConditional() throws IOException\n     try (CloseableIterator<InputRow> iterator = reader.read()) {\n       int numActualIterations = 0;\n       while (iterator.hasNext()) {\n+\n         final InputRow row = iterator.next();\n-        Assert.assertEquals(\"test\", Iterables.getOnlyElement(row.getDimension(\"bar\")));\n-        Assert.assertEquals(Collections.emptyList(), row.getDimension(\"foo\"));\n-        Assert.assertTrue(row.getDimension(\"baz\").isEmpty());\n-        numActualIterations++;\n-      }\n-      Assert.assertEquals(numExpectedIterations, numActualIterations);\n-    }\n-  }\n \n-  @Test\n-  public void testParseRowKeepNullColumns() throws IOException\n-  {\n-    final JsonInputFormat format = new JsonInputFormat(\n-        new JSONPathSpec(\n-            true,\n-            ImmutableList.of(\n-                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"path_omg\", \"$.o.mg\")\n-            )\n-        ),\n-        null,\n-        true\n-    );\n+        Assert.assertEquals(DateTimes.of(\"2019-01-01\"), row.getTimestamp());\n+        Assert.assertEquals(\"x\", Iterables.getOnlyElement(row.getDimension(\"foo\")));\n+        Assert.assertEquals(\"4\", Iterables.getOnlyElement(row.getDimension(\"baz\")));\n+        Assert.assertEquals(\"4\", Iterables.getOnlyElement(row.getDimension(\"root_baz\")));\n+        Assert.assertEquals(\"1\", Iterables.getOnlyElement(row.getDimension(\"path_omg\")));\n+        Assert.assertEquals(\"1\", Iterables.getOnlyElement(row.getDimension(\"jq_omg\")));\n \n-    final ByteEntity source = new ByteEntity(\n-        StringUtils.toUtf8(\"{\\\"timestamp\\\":\\\"2019-01-01\\\",\\\"bar\\\":null,\\\"foo\\\":\\\"x\\\",\\\"o\\\":{\\\"mg\\\":null}}\")\n-    );\n+        Assert.assertTrue(row.getDimension(\"root_baz2\").isEmpty());\n+        Assert.assertTrue(row.getDimension(\"path_omg2\").isEmpty());\n+        Assert.assertTrue(row.getDimension(\"jq_omg2\").isEmpty());\n \n-    final InputEntityReader reader = format.createReader(\n-        new InputRowSchema(\n-            new TimestampSpec(\"timestamp\", \"iso\", null),\n-            new DimensionsSpec(DimensionsSpec.getDefaultSchemas(Collections.emptyList())),\n-            Collections.emptyList()\n-        ),\n-        source,\n-        null\n-    );\n-    final int numExpectedIterations = 1;\n-    try (CloseableIterator<InputRow> iterator = reader.read()) {\n-      int numActualIterations = 0;\n-      while (iterator.hasNext()) {\n-        final InputRow row = iterator.next();\n-        Assert.assertEquals(Arrays.asList(\"path_omg\", \"timestamp\", \"bar\", \"foo\"), row.getDimensions());\n-        Assert.assertTrue(row.getDimension(\"bar\").isEmpty());\n-        Assert.assertEquals(\"x\", Iterables.getOnlyElement(row.getDimension(\"foo\")));\n-        Assert.assertTrue(row.getDimension(\"path_omg\").isEmpty());\n         numActualIterations++;\n       }\n+\n       Assert.assertEquals(numExpectedIterations, numActualIterations);\n     }\n   }\n \n   @Test\n-  public void testKeepNullColumnsWithNoNullValues() throws IOException\n+  public void testInvalidJSONText() throws IOException\n   {\n     final JsonInputFormat format = new JsonInputFormat(\n         new JSONPathSpec(\n             true,\n             ImmutableList.of(\n-                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"path_omg\", \"$.o.mg\")\n+                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"root_baz\", \"baz\"),\n+                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"root_baz2\", \"baz2\"),\n+                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"path_omg\", \"$.o.mg\"),\n+                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"path_omg2\", \"$.o.mg2\"),\n+                new JSONPathFieldSpec(JSONPathFieldType.JQ, \"jq_omg\", \".o.mg\"),\n+                new JSONPathFieldSpec(JSONPathFieldType.JQ, \"jq_omg2\", \".o.mg2\")\n             )\n         ),\n         null,\n-        true\n+        null\n     );\n \n+    //make sure JsonReader is used\n+    format.setLineSplittable(false);\n+\n     final ByteEntity source = new ByteEntity(\n-        StringUtils.toUtf8(\"{\\\"timestamp\\\":\\\"2019-01-01\\\",\\\"bar\\\":1,\\\"foo\\\":\\\"x\\\",\\\"o\\\":{\\\"mg\\\":\\\"a\\\"}}\")\n+        StringUtils.toUtf8(\"{\\\"timestamp\\\":\\\"2019-01-01\\\",\\\"bar\\\":null,\\\"foo\\\":\\\"x\\\",\\\"baz\\\":4,\\\"o\\\":{\\\"mg\\\":1}}\"\n+                           + \"{\\\"timestamp\\\":\\\"2019-01-01\\\",\\\"bar\\\":null,\\\"foo\\\":\\\"x\\\",\\\"baz\\\":4xxx,\\\"o\\\":{\\\"mg\\\":2}}\" //baz property is illegal\n+                           + \"{\\\"timestamp\\\":\\\"2019-01-01\\\",\\\"bar\\\":null,\\\"foo\\\":\\\"x\\\",\\\"baz\\\":4,\\\"o\\\":{\\\"mg\\\":3}}\")\n     );\n \n     final InputEntityReader reader = format.createReader(\n         new InputRowSchema(\n             new TimestampSpec(\"timestamp\", \"iso\", null),\n-            new DimensionsSpec(DimensionsSpec.getDefaultSchemas(Collections.emptyList())),\n+            new DimensionsSpec(DimensionsSpec.getDefaultSchemas(ImmutableList.of(\"bar\", \"foo\"))),\n             Collections.emptyList()\n         ),\n         source,\n         null\n     );\n+\n+    // the 2nd line is ill-formed, it stops to iterate to the 3rd line.\n+    // So in total, only 1 lines has been parsed\n     final int numExpectedIterations = 1;\n+\n     try (CloseableIterator<InputRow> iterator = reader.read()) {\n       int numActualIterations = 0;\n       while (iterator.hasNext()) {\n-        final InputRow row = iterator.next();\n-        Assert.assertEquals(Arrays.asList(\"path_omg\", \"timestamp\", \"bar\", \"foo\"), row.getDimensions());\n-        Assert.assertEquals(\"1\", Iterables.getOnlyElement(row.getDimension(\"bar\")));\n-        Assert.assertEquals(\"x\", Iterables.getOnlyElement(row.getDimension(\"foo\")));\n-        Assert.assertEquals(\"a\", Iterables.getOnlyElement(row.getDimension(\"path_omg\")));\n-        numActualIterations++;\n+\n+        try {\n+          final InputRow row = iterator.next();\n+\n+          final String msgId = String.valueOf(++numActualIterations);\n+          Assert.assertEquals(DateTimes.of(\"2019-01-01\"), row.getTimestamp());\n+          Assert.assertEquals(\"x\", Iterables.getOnlyElement(row.getDimension(\"foo\")));\n+          Assert.assertEquals(\"4\", Iterables.getOnlyElement(row.getDimension(\"baz\")));\n+          Assert.assertEquals(\"4\", Iterables.getOnlyElement(row.getDimension(\"root_baz\")));\n+          Assert.assertEquals(msgId, Iterables.getOnlyElement(row.getDimension(\"path_omg\")));\n+          Assert.assertEquals(msgId, Iterables.getOnlyElement(row.getDimension(\"jq_omg\")));\n+\n+          Assert.assertTrue(row.getDimension(\"root_baz2\").isEmpty());\n+          Assert.assertTrue(row.getDimension(\"path_omg2\").isEmpty());\n+          Assert.assertTrue(row.getDimension(\"jq_omg2\").isEmpty());\n+        }\n+        catch (Exception e) {\n+          //ignore the exception when parsing the 2nd", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NjE3MjQ2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMToyODozMlrOHXx3tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMzozNjoyOVrOHXz12w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5NjM3Mg==", "bodyText": "Thinking about this new reader, I think the new requirement for parsing multiple JSON objects to multiple rows doesn't fit in the current interface of the sampler. The sampler currently assumes that there is only one JSON object in an input chunk which could have either an array or a nested object. That means, the current interface allows multiple out rows as inputRows is a list of InputRows in InputRowListPlusRawValues, but doesn't allow multiple values in an input chunk as rawValues is a map in InputRowListPlusRawValues. To support the requirement, I think rawValues should be a list of map, so that all raw values in an input chunk can be returned to the sampler.\nIf we do this, JsonReader can simply extend IntermediateRowParsingReader, where the intermediate row will be the inputText created from source.open(). Here is an example.\npublic class JsonReader2 extends IntermediateRowParsingReader<String>\n{\n  private final ObjectFlattener<JsonNode> flattener;\n  private final ObjectMapper mapper;\n  private final InputEntity source;\n  private final InputRowSchema inputRowSchema;\n\n  JsonReader2(\n      InputRowSchema inputRowSchema,\n      InputEntity source,\n      JSONPathSpec flattenSpec,\n      ObjectMapper mapper,\n      boolean keepNullColumns\n  )\n  {\n    this.inputRowSchema = inputRowSchema;\n    this.source = source;\n    this.flattener = ObjectFlatteners.create(flattenSpec, new JSONFlattenerMaker(keepNullColumns));\n    this.mapper = mapper;\n  }\n\n  @Override\n  protected CloseableIterator<String> intermediateRowIterator() throws IOException\n  {\n    return CloseableIterators.withEmptyBaggage(\n        Iterators.singletonIterator(IOUtils.toString(source.open(), StringUtils.UTF8_STRING))\n    );\n  }\n\n  @Override\n  protected List<InputRow> parseInputRows(String intermediateRow) throws IOException, ParseException\n  {\n    try (JsonParser parser = new JsonFactory().createParser(intermediateRow)) {\n      final Iterator<JsonNode> delegate = mapper.readValues(parser, JsonNode.class);\n      return FluentIterable.from(() -> delegate)\n                           .transform(jsonNode -> MapInputRowParser.parse(inputRowSchema, flattener.flatten(jsonNode)))\n                           .toList();\n    }\n  }\n\n  @Override\n  protected List<Map<String, Object>> toMap(String intermediateRow) throws IOException\n  {\n    try (JsonParser parser = new JsonFactory().createParser(intermediateRow)) {\n      final Iterator<Map> delegate = mapper.readValues(parser, Map.class);\n      return FluentIterable.from(() -> delegate).transform(map -> (Map<String, Object>) map).toList();\n    }\n  }\n}", "url": "https://github.com/apache/druid/pull/10383#discussion_r494696372", "createdAt": "2020-09-25T01:28:32Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "diffHunk": "@@ -33,13 +40,98 @@\n \n import java.io.IOException;\n import java.util.Collections;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n+import java.util.NoSuchElementException;\n \n-public class JsonReader extends TextReader\n+/**\n+ * <pre>\n+ * In constract to {@link JsonLineReader} which processes input text line by line independently,\n+ * this class tries to parse the input text as a whole to an array of objects.\n+ *\n+ * The input text can be:\n+ * 1. a JSON string of an object in a line or multiple lines(such as pretty-printed JSON text)\n+ * 2. multiple JSON object strings concated by white space character(s)\n+ *\n+ * For case 2, what should be noticed is that if an exception is thrown when parsing one JSON string,\n+ * the rest JSON text will all be ignored\n+ *\n+ * For more information, see: https://github.com/apache/druid/pull/10383\n+ * </pre>\n+ */\n+public class JsonReader implements InputEntityReader", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDcyODY2Nw==", "bodyText": "The sampler currently assumes that there is only one JSON object in an input chunk which could have either an array or a nested object.\n\nThat's the root cause why ExceptionThrowingIterator is extracted and JsonReader inherits from InputEntityReader directly.\nYour suggestion provides a new and simple way to deal with it. I'll test the code later.", "url": "https://github.com/apache/druid/pull/10383#discussion_r494728667", "createdAt": "2020-09-25T03:36:29Z", "author": {"login": "FrankChen021"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "diffHunk": "@@ -33,13 +40,98 @@\n \n import java.io.IOException;\n import java.util.Collections;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n+import java.util.NoSuchElementException;\n \n-public class JsonReader extends TextReader\n+/**\n+ * <pre>\n+ * In constract to {@link JsonLineReader} which processes input text line by line independently,\n+ * this class tries to parse the input text as a whole to an array of objects.\n+ *\n+ * The input text can be:\n+ * 1. a JSON string of an object in a line or multiple lines(such as pretty-printed JSON text)\n+ * 2. multiple JSON object strings concated by white space character(s)\n+ *\n+ * For case 2, what should be noticed is that if an exception is thrown when parsing one JSON string,\n+ * the rest JSON text will all be ignored\n+ *\n+ * For more information, see: https://github.com/apache/druid/pull/10383\n+ * </pre>\n+ */\n+public class JsonReader implements InputEntityReader", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5NjM3Mg=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NjE5MzE1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTo0MDo0MlrOHXyDrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTo0MDo0MlrOHXyDrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5OTQzNw==", "bodyText": "This doesn't look pretty, but simple enough to handle this exceptional case. Maybe we can introduce another layer on top of inputFormat to make it more pretty, but I don't think it will be worth at least at this point.", "url": "https://github.com/apache/druid/pull/10383#discussion_r494699437", "createdAt": "2020-09-25T01:40:42Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonInputFormat.java", "diffHunk": "@@ -73,7 +88,14 @@ public boolean isSplittable()\n   @Override\n   public InputEntityReader createReader(InputRowSchema inputRowSchema, InputEntity source, File temporaryDirectory)\n   {\n-    return new JsonReader(inputRowSchema, source, getFlattenSpec(), objectMapper, keepNullColumns);\n+    return this.lineSplittable ?\n+           new JsonLineReader(inputRowSchema, source, getFlattenSpec(), objectMapper, keepNullColumns) :\n+           new JsonReader(inputRowSchema, source, getFlattenSpec(), objectMapper, keepNullColumns);\n+  }\n+\n+  public void setLineSplittable(boolean lineSplittable)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTI1ODA0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODoxNTo1MlrOHcoZaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxODoxNTo1MlrOHcoZaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc4NDA0Mw==", "bodyText": "Accidental change?", "url": "https://github.com/apache/druid/pull/10383#discussion_r499784043", "createdAt": "2020-10-05T18:15:52Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -123,7 +138,7 @@ public void close() throws IOException\n \n   /**\n    * Parses the given intermediate row into a list of {@link InputRow}s.\n-   * This should return a non-empty list.\n+   * This should return a non-empty list", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTc0MTY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo0NDozM1rOHctH3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo0NDozM1rOHctH3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MTQ3MQ==", "bodyText": "I think we should check if the sizes of rawColumnsList and rows are same (as in the Javadoc contract) instead of computing a min of them.", "url": "https://github.com/apache/druid/pull/10383#discussion_r499861471", "createdAt": "2020-10-05T20:44:33Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));\n       }\n       catch (IOException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, new ParseException(e, \"Unable to parse row [%s] into inputRow\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0),\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into inputRow\", row)));\n+      }\n+\n+      List<InputRowListPlusRawValues> list = new ArrayList<InputRowListPlusRawValues>();\n+      for (int i = 0; i < Math.min(rows.size(), rawColumnsList.size()); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTc0NzM1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo0NjoxM1rOHctLPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQxMDo0NzowMVrOHffEUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MjMzNQ==", "bodyText": "Should we return the entire rawColumnsList instead of the first element? I thought InputRowListPlusRawValues will be able to have a list of rawColumns.", "url": "https://github.com/apache/druid/pull/10383#discussion_r499862335", "createdAt": "2020-10-05T20:46:13Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MzA4Mw==", "bodyText": "Also, should we fail if rawColumnsList.isEmpty()?", "url": "https://github.com/apache/druid/pull/10383#discussion_r499863083", "createdAt": "2020-10-05T20:47:37Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MjMzNQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDk3MDI1Ng==", "bodyText": "To return the entire rawColumnsList here involves changes on InputRowListPlusRawValues which are heavily used by test cases.\nI checked the code again and find that there's no need to return raw column object when exception occurs. Because InputSourceSample re-throws exception once InputRowListPlusRawValues.getParseException returns exception.\nSo the new commit passes a null to InputRowListPlusRawValues to eliminate this strange code here.", "url": "https://github.com/apache/druid/pull/10383#discussion_r500970256", "createdAt": "2020-10-07T12:27:21Z", "author": {"login": "FrankChen021"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MjMzNQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA3ODAxNA==", "bodyText": "I checked the code again and find that there's no need to return raw column object when exception occurs. Because InputSourceSample re-throws exception once InputRowListPlusRawValues.getParseException returns exception.\n\nActually, there is a clause catching ParseException in the sampler (this is a bad pattern by the way. We should refactor it at some point). In this case, the rawColumns is returned to show the raw data in the UI. I think the entire rawColumnsList should be returned to keep the current behaviour.", "url": "https://github.com/apache/druid/pull/10383#discussion_r502078014", "createdAt": "2020-10-08T23:31:02Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MjMzNQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA3OTc3OQ==", "bodyText": "There are some tests failing in InputSourceSamplerTest. It seems like because of this change.", "url": "https://github.com/apache/druid/pull/10383#discussion_r502079779", "createdAt": "2020-10-08T23:37:25Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MjMzNQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NjkxNA==", "bodyText": "It's been fixed. Now it's ready for final review.", "url": "https://github.com/apache/druid/pull/10383#discussion_r502776914", "createdAt": "2020-10-10T10:47:01Z", "author": {"login": "FrankChen021"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +99,35 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return Collections.singletonList(InputRowListPlusRawValues.of(rawColumnsList.isEmpty() ? null : rawColumnsList.get(0), e));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2MjMzNQ=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTc2NzAyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo1MjowNFrOHctXNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo1MjowNFrOHctXNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2NTM5Ng==", "bodyText": "Where is JsonParseException thrown wrapped in RuntimeException? Can you add a comment about it?", "url": "https://github.com/apache/druid/pull/10383#discussion_r499865396", "createdAt": "2020-10-05T20:52:04Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/impl/JsonReader.java", "diffHunk": "@@ -49,41 +73,57 @@\n       boolean keepNullColumns\n   )\n   {\n-    super(inputRowSchema, source);\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = source;\n     this.flattener = ObjectFlatteners.create(flattenSpec, new JSONFlattenerMaker(keepNullColumns));\n     this.mapper = mapper;\n   }\n \n   @Override\n-  public List<InputRow> parseInputRows(String line) throws IOException, ParseException\n+  protected CloseableIterator<String> intermediateRowIterator() throws IOException\n   {\n-    final JsonNode document = mapper.readValue(line, JsonNode.class);\n-    final Map<String, Object> flattened = flattener.flatten(document);\n-    return Collections.singletonList(MapInputRowParser.parse(getInputRowSchema(), flattened));\n+    return CloseableIterators.withEmptyBaggage(\n+        Iterators.singletonIterator(IOUtils.toString(source.open(), StringUtils.UTF8_STRING))\n+    );\n   }\n \n   @Override\n-  public Map<String, Object> toMap(String intermediateRow) throws IOException\n+  protected List<InputRow> parseInputRows(String intermediateRow) throws IOException, ParseException\n   {\n-    //noinspection unchecked\n-    return mapper.readValue(intermediateRow, Map.class);\n-  }\n+    try (JsonParser parser = new JsonFactory().createParser(intermediateRow)) {\n+      final Iterator<JsonNode> delegate = mapper.readValues(parser, JsonNode.class);\n+      return FluentIterable.from(() -> delegate)\n+                           .transform(jsonNode -> MapInputRowParser.parse(inputRowSchema, flattener.flatten(jsonNode)))\n+                           .toList();\n+    }\n+    catch (RuntimeException e) {\n+      //convert Jackson's JsonParseException into druid's exception for further processing\n+      if (e.getCause() instanceof JsonParseException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTc3MTA4OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonInputFormatTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMDo1MzozMVrOHctZ3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQwMToyMjo1N1rOHe3GeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2NjA3OA==", "bodyText": "lineSplittable shouldn't be ignored, but included in equals() and hashCode().", "url": "https://github.com/apache/druid/pull/10383#discussion_r499866078", "createdAt": "2020-10-05T20:53:31Z", "author": {"login": "jihoonson"}, "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonInputFormatTest.java", "diffHunk": "@@ -69,7 +69,7 @@ public void testEquals()\n               new ObjectMapper(),\n               new ObjectMapper()\n               )\n-              .withIgnoredFields(\"objectMapper\")\n+              .withIgnoredFields(\"objectMapper\", \"lineSplittable\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDk3MTYwMg==", "bodyText": "The new property lineSplittable is not declared as final and EqualsVerifier here will report a warning that equals depends on mutable field, so the new commit calls suppress method on EqualsVerifier to suppress this warning.", "url": "https://github.com/apache/druid/pull/10383#discussion_r500971602", "createdAt": "2020-10-07T12:29:36Z", "author": {"login": "FrankChen021"}, "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonInputFormatTest.java", "diffHunk": "@@ -69,7 +69,7 @@ public void testEquals()\n               new ObjectMapper(),\n               new ObjectMapper()\n               )\n-              .withIgnoredFields(\"objectMapper\")\n+              .withIgnoredFields(\"objectMapper\", \"lineSplittable\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2NjA3OA=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA3OTM2OA==", "bodyText": "Hmm, good point. Yeah, it's not a good convention to include a mutable field in equals() or hashCode() because they can be used as a key of a HashMap or HashSet. I would like to suggest to make lineSplittable immutable, but add a new method withLineSplittable(boolean lineSplittable) instead of setLineSplittable() which creates a new JsonInputFormat instance with the given value.", "url": "https://github.com/apache/druid/pull/10383#discussion_r502079368", "createdAt": "2020-10-08T23:35:53Z", "author": {"login": "jihoonson"}, "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonInputFormatTest.java", "diffHunk": "@@ -69,7 +69,7 @@ public void testEquals()\n               new ObjectMapper(),\n               new ObjectMapper()\n               )\n-              .withIgnoredFields(\"objectMapper\")\n+              .withIgnoredFields(\"objectMapper\", \"lineSplittable\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2NjA3OA=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjEyMjEwNQ==", "bodyText": "Good idea!", "url": "https://github.com/apache/druid/pull/10383#discussion_r502122105", "createdAt": "2020-10-09T01:22:57Z", "author": {"login": "FrankChen021"}, "path": "core/src/test/java/org/apache/druid/data/input/impl/JsonInputFormatTest.java", "diffHunk": "@@ -69,7 +69,7 @@ public void testEquals()\n               new ObjectMapper(),\n               new ObjectMapper()\n               )\n-              .withIgnoredFields(\"objectMapper\")\n+              .withIgnoredFields(\"objectMapper\", \"lineSplittable\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg2NjA3OA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDE4NTc2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMzowMToxM1rOHgP1-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMjozMToyOVrOHg7xZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU3NjA1Ng==", "bodyText": "This doesn't still seem correct to me.. The purpose of returning rawColumn and exception in InputRowListPlusRawValues is showing them in the sampler altogether, so that users will debug their data and input format easily. To do that, we should show exactly what the data was when a ParseException is thrown. Now, thinking about the new behavior parsing a list of JSONs into InputRows, row will contain the list of JSONs here. If a ParseException was thrown while parsing one of them, we should return one InputRowListPlusRawValues which contains row (the whole list of JSONs) and the exception because we don't know exactly in which JSON in the list the ParseException was thrown from. For this, I still think we should change rawValues in InputRowListPlusRawValues to be List<Map<String, Object>>. I understand that you don't want to touch it as it is widely used in unit tests, but I'm not sure if we can fix this without touching it.", "url": "https://github.com/apache/druid/pull/10383#discussion_r503576056", "createdAt": "2020-10-12T23:01:13Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +101,45 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n+      }\n+\n+      if (CollectionUtils.isNullOrEmpty(rawColumnsList)) {\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(\"No map object parsed for row [%s]\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, e)).collect(Collectors.toList());\n       }\n       catch (IOException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, new ParseException(e, \"Unable to parse row [%s] into inputRow\", row));\n+        ParseException exception = new ParseException(e, \"Unable to parse row [%s] into inputRow\", row);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, exception)).collect(Collectors.toList());\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc3MjIxNw==", "bodyText": "I agree with you.\nrawValues in InputRowListPlusRawValues should also be a list to correspond the list of inputRows. Since the rawValues is also passed to SamplerResponseRow when exception occurs, to change rawValues from Map to List<Map> also involves changes of the web console. I don't know if the change of front-end should also be included in this PR.", "url": "https://github.com/apache/druid/pull/10383#discussion_r503772217", "createdAt": "2020-10-13T08:41:26Z", "author": {"login": "FrankChen021"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +101,45 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n+      }\n+\n+      if (CollectionUtils.isNullOrEmpty(rawColumnsList)) {\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(\"No map object parsed for row [%s]\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, e)).collect(Collectors.toList());\n       }\n       catch (IOException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, new ParseException(e, \"Unable to parse row [%s] into inputRow\", row));\n+        ParseException exception = new ParseException(e, \"Unable to parse row [%s] into inputRow\", row);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, exception)).collect(Collectors.toList());\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU3NjA1Ng=="}, "originalCommit": null, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5NTc4MQ==", "bodyText": "Good point. I'm not sure if we have a unit test for parsing sampler response. If we have, the CI will fail and we need to fix it in this PR together. Otherwise, I'm OK with doing the fix in a follow-up. @vogievetsky do you know whether or not we have such a unit test?", "url": "https://github.com/apache/druid/pull/10383#discussion_r504295781", "createdAt": "2020-10-13T22:31:29Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +101,45 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n+      }\n+\n+      if (CollectionUtils.isNullOrEmpty(rawColumnsList)) {\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(\"No map object parsed for row [%s]\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, e)).collect(Collectors.toList());\n       }\n       catch (IOException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, new ParseException(e, \"Unable to parse row [%s] into inputRow\", row));\n+        ParseException exception = new ParseException(e, \"Unable to parse row [%s] into inputRow\", row);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, exception)).collect(Collectors.toList());\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU3NjA1Ng=="}, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDE5MzEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMzowNToxMVrOHgP6QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMzowNToxMVrOHgP6QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU3NzE1Mw==", "bodyText": "I think we should set both rows and rawColumnsList in InputRowListPlusRawValues in this case, so that users will learn how they are different.", "url": "https://github.com/apache/druid/pull/10383#discussion_r503577153", "createdAt": "2020-10-12T23:05:11Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/IntermediateRowParsingReader.java", "diffHunk": "@@ -96,23 +101,45 @@ public void close() throws IOException\n   public CloseableIterator<InputRowListPlusRawValues> sample() throws IOException\n   {\n     return intermediateRowIterator().map(row -> {\n-      final Map<String, Object> rawColumns;\n+\n+      final List<Map<String, Object>> rawColumnsList;\n       try {\n-        rawColumns = toMap(row);\n+        rawColumnsList = toMap(row);\n       }\n       catch (Exception e) {\n-        return InputRowListPlusRawValues.of(null, new ParseException(e, \"Unable to parse row [%s] into JSON\", row));\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(e, \"Unable to parse row [%s] into JSON\", row)));\n+      }\n+\n+      if (CollectionUtils.isNullOrEmpty(rawColumnsList)) {\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,\n+                                                                      new ParseException(\"No map object parsed for row [%s]\", row)));\n       }\n+\n+      List<InputRow> rows;\n       try {\n-        return InputRowListPlusRawValues.of(parseInputRows(row), rawColumns);\n+        rows = parseInputRows(row);\n       }\n       catch (ParseException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, e);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, e)).collect(Collectors.toList());\n       }\n       catch (IOException e) {\n-        return InputRowListPlusRawValues.of(rawColumns, new ParseException(e, \"Unable to parse row [%s] into inputRow\", row));\n+        ParseException exception = new ParseException(e, \"Unable to parse row [%s] into inputRow\", row);\n+        return rawColumnsList.stream().map(rawColumn -> InputRowListPlusRawValues.of(rawColumn, exception)).collect(Collectors.toList());\n+      }\n+\n+      if (rows.size() != rawColumnsList.size()) {\n+        return Collections.singletonList(InputRowListPlusRawValues.of(null,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzY1NTIwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/InputRowListPlusRawValues.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMTozMToxMFrOHm0mqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMTowMTowNVrOHpQv5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2OTgwMg==", "bodyText": "nit: it would be better to use Iterables.getOnlyElement(rawValues) to make sure that you are getting the only element.", "url": "https://github.com/apache/druid/pull/10383#discussion_r510469802", "createdAt": "2020-10-22T21:31:10Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/InputRowListPlusRawValues.java", "diffHunk": "@@ -82,8 +121,16 @@ private InputRowListPlusRawValues(\n     return inputRows;\n   }\n \n+  /**\n+   * This method is left here only for test cases\n+   */\n   @Nullable\n   public Map<String, Object> getRawValues()\n+  {\n+    return CollectionUtils.isNullOrEmpty(rawValues) ? null : rawValues.get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjUzMzk0NA==", "bodyText": "But Iterables.getOnlyElement does not check whether the given input is null or not. If the given input is null, there will be a NPE.", "url": "https://github.com/apache/druid/pull/10383#discussion_r512533944", "createdAt": "2020-10-27T09:27:17Z", "author": {"login": "FrankChen021"}, "path": "core/src/main/java/org/apache/druid/data/input/InputRowListPlusRawValues.java", "diffHunk": "@@ -82,8 +121,16 @@ private InputRowListPlusRawValues(\n     return inputRows;\n   }\n \n+  /**\n+   * This method is left here only for test cases\n+   */\n   @Nullable\n   public Map<String, Object> getRawValues()\n+  {\n+    return CollectionUtils.isNullOrEmpty(rawValues) ? null : rawValues.get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2OTgwMg=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzAyODA2OQ==", "bodyText": "I meant, for rawValues.get(0) to make sure there is only one element in there if this method is called.", "url": "https://github.com/apache/druid/pull/10383#discussion_r513028069", "createdAt": "2020-10-27T21:01:05Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/InputRowListPlusRawValues.java", "diffHunk": "@@ -82,8 +121,16 @@ private InputRowListPlusRawValues(\n     return inputRows;\n   }\n \n+  /**\n+   * This method is left here only for test cases\n+   */\n   @Nullable\n   public Map<String, Object> getRawValues()\n+  {\n+    return CollectionUtils.isNullOrEmpty(rawValues) ? null : rawValues.get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2OTgwMg=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwOTUxODQ0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/InputRowListPlusRawValues.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMDo1NDoyM1rOHoh-PQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMDo1NDoyM1rOHoh-PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2MTY5Mw==", "bodyText": "ParseException is Nullable.", "url": "https://github.com/apache/druid/pull/10383#discussion_r512261693", "createdAt": "2020-10-26T20:54:23Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/InputRowListPlusRawValues.java", "diffHunk": "@@ -53,21 +54,59 @@ public static InputRowListPlusRawValues of(@Nullable InputRow inputRow, Map<Stri\n \n   public static InputRowListPlusRawValues of(@Nullable List<InputRow> inputRows, Map<String, Object> rawColumns)\n   {\n-    return new InputRowListPlusRawValues(inputRows, Preconditions.checkNotNull(rawColumns, \"rawColumns\"), null);\n+    return new InputRowListPlusRawValues(inputRows,\n+                                         Collections.singletonList(Preconditions.checkNotNull(rawColumns, \"rawColumns\")),\n+                                         null);\n   }\n \n   public static InputRowListPlusRawValues of(@Nullable Map<String, Object> rawColumns, ParseException parseException)\n   {\n     return new InputRowListPlusRawValues(\n         null,\n-        rawColumns,\n+        rawColumns == null ? null : Collections.singletonList(rawColumns),\n         Preconditions.checkNotNull(parseException, \"parseException\")\n     );\n   }\n \n+  public static InputRowListPlusRawValues ofList(@Nullable List<Map<String, Object>> rawColumnsList, ParseException parseException)\n+  {\n+    return ofList(rawColumnsList, null, parseException);\n+  }\n+\n+  /**\n+   * Create an instance of {@link InputRowListPlusRawValues}\n+   *\n+   * Make sure the size of given rawColumnsList and inputRows are the same if both of them are not null\n+   */\n+  public static InputRowListPlusRawValues ofList(@Nullable List<Map<String, Object>> rawColumnsList,\n+                                                 @Nullable List<InputRow> inputRows)\n+  {\n+    return ofList(rawColumnsList, inputRows, null);\n+  }\n+\n+  /**\n+   * Create an instance of {@link InputRowListPlusRawValues}\n+   *\n+   * Make sure the size of given rawColumnsList and inputRows are the same if both of them are not null\n+   */\n+  public static InputRowListPlusRawValues ofList(@Nullable List<Map<String, Object>> rawColumnsList,\n+                                                 @Nullable List<InputRow> inputRows,\n+                                                 ParseException parseException)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwOTU0MjA1OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMTowMToxMFrOHoiMow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMTo1NToyNlrOHrkGKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2NTM3OQ==", "bodyText": "Can this introduce duplicate rows in responseRows? Suppose you have 3 rawColumns in rawColumnsList including an unparseable row at the second position. The first row will be added to both index and thus a new SamplerResponseRow of the first rawColumns will be added to responseRows. But for the second row, index.add() will throw a ParseException which will execute these lines. In this case, 2 duplicate rawColumns of the first row will be added in responseRows.\nLooking at what JsonReader does, it seems throwing away the whole intermediateRow when there is any unparseable row. The sampler behavior should match to the actual ingestion.", "url": "https://github.com/apache/druid/pull/10383#discussion_r512265379", "createdAt": "2020-10-26T21:01:10Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -131,17 +130,27 @@ public SamplerResponse sample(\n             continue;\n           }\n \n-          for (InputRow row : inputRowListPlusRawValues.getInputRows()) {\n-            index.add(new SamplerInputRow(row, counter), true);\n+          for (int i = 0; i < rawColumnsList.size(); i++) {\n+            Map<String, Object> rawColumns = rawColumnsList.get(i);\n+            InputRow row = inputRowListPlusRawValues.getInputRows().get(i);\n+\n+            //keep the index of the row to be added to responseRows for further use\n+            final int rowIndex = responseRows.size();\n+            index.add(new SamplerInputRow(row, rowIndex), true);\n+\n             // store the raw value; will be merged with the data from the IncrementalIndex later\n-            responseRows[counter] = new SamplerResponseRow(rawColumns, null, null, null);\n-            counter++;\n+            responseRows.add(new SamplerResponseRow(rawColumns, null, null, null));\n             numRowsIndexed++;\n           }\n         }\n         catch (ParseException e) {\n-          responseRows[counter] = new SamplerResponseRow(rawColumns, null, true, e.getMessage());\n-          counter++;\n+          if (rawColumnsList != null) {\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, e.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU0NDMwMA==", "bodyText": "I notice that index.add does not throw ParseException but returns the exception in its returning result. The previous version of InputSourceSampler checks the result to rethrow this exception, while the change introduced by #10336 deletes these exception related code, which means the code here won't throw ParseException.\nOf course, there's possibility that the code in for-loop throws ParseException in future. I've made little changes to avoid this potential situation to happen. Let's check it once the CI passes.\nBTW, my local branch has failed to build since last time it was rebased on master, I have to reply on the CI to check if there're any test case failures.", "url": "https://github.com/apache/druid/pull/10383#discussion_r512544300", "createdAt": "2020-10-27T09:43:29Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -131,17 +130,27 @@ public SamplerResponse sample(\n             continue;\n           }\n \n-          for (InputRow row : inputRowListPlusRawValues.getInputRows()) {\n-            index.add(new SamplerInputRow(row, counter), true);\n+          for (int i = 0; i < rawColumnsList.size(); i++) {\n+            Map<String, Object> rawColumns = rawColumnsList.get(i);\n+            InputRow row = inputRowListPlusRawValues.getInputRows().get(i);\n+\n+            //keep the index of the row to be added to responseRows for further use\n+            final int rowIndex = responseRows.size();\n+            index.add(new SamplerInputRow(row, rowIndex), true);\n+\n             // store the raw value; will be merged with the data from the IncrementalIndex later\n-            responseRows[counter] = new SamplerResponseRow(rawColumns, null, null, null);\n-            counter++;\n+            responseRows.add(new SamplerResponseRow(rawColumns, null, null, null));\n             numRowsIndexed++;\n           }\n         }\n         catch (ParseException e) {\n-          responseRows[counter] = new SamplerResponseRow(rawColumns, null, true, e.getMessage());\n-          counter++;\n+          if (rawColumnsList != null) {\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, e.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2NTM3OQ=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzAyODAzNw==", "bodyText": "I notice that index.add does not throw ParseException but returns the exception in its returning result. The previous version of InputSourceSampler checks the result to rethrow this exception, while the change introduced by #10336 deletes these exception related code, which means the code here won't throw ParseException.\nOf course, there's possibility that the code in for-loop throws ParseException in future. I've made little changes to avoid this potential situation to happen. Let's check it once the CI passes.\n\nOops, good point. I forgot what I have done in #10336.. I think index.add() should never throw parseException directly, but return them in IncrementalIndexAddResult. We can add this contract on the Javadoc of IncrementalIndex.add(). I suggest removing the catch clause here to avoid future confusion (there is no point in catching exceptions which cannot be thrown here).", "url": "https://github.com/apache/druid/pull/10383#discussion_r513028037", "createdAt": "2020-10-27T21:01:02Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -131,17 +130,27 @@ public SamplerResponse sample(\n             continue;\n           }\n \n-          for (InputRow row : inputRowListPlusRawValues.getInputRows()) {\n-            index.add(new SamplerInputRow(row, counter), true);\n+          for (int i = 0; i < rawColumnsList.size(); i++) {\n+            Map<String, Object> rawColumns = rawColumnsList.get(i);\n+            InputRow row = inputRowListPlusRawValues.getInputRows().get(i);\n+\n+            //keep the index of the row to be added to responseRows for further use\n+            final int rowIndex = responseRows.size();\n+            index.add(new SamplerInputRow(row, rowIndex), true);\n+\n             // store the raw value; will be merged with the data from the IncrementalIndex later\n-            responseRows[counter] = new SamplerResponseRow(rawColumns, null, null, null);\n-            counter++;\n+            responseRows.add(new SamplerResponseRow(rawColumns, null, null, null));\n             numRowsIndexed++;\n           }\n         }\n         catch (ParseException e) {\n-          responseRows[counter] = new SamplerResponseRow(rawColumns, null, true, e.getMessage());\n-          counter++;\n+          if (rawColumnsList != null) {\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, e.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2NTM3OQ=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIzNDYxNQ==", "bodyText": "Got it. BTW, should we handle the exception returned in IncrementalIndexAddResult by adding the corresponding row to responseRows ?", "url": "https://github.com/apache/druid/pull/10383#discussion_r513234615", "createdAt": "2020-10-28T07:43:25Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -131,17 +130,27 @@ public SamplerResponse sample(\n             continue;\n           }\n \n-          for (InputRow row : inputRowListPlusRawValues.getInputRows()) {\n-            index.add(new SamplerInputRow(row, counter), true);\n+          for (int i = 0; i < rawColumnsList.size(); i++) {\n+            Map<String, Object> rawColumns = rawColumnsList.get(i);\n+            InputRow row = inputRowListPlusRawValues.getInputRows().get(i);\n+\n+            //keep the index of the row to be added to responseRows for further use\n+            final int rowIndex = responseRows.size();\n+            index.add(new SamplerInputRow(row, rowIndex), true);\n+\n             // store the raw value; will be merged with the data from the IncrementalIndex later\n-            responseRows[counter] = new SamplerResponseRow(rawColumns, null, null, null);\n-            counter++;\n+            responseRows.add(new SamplerResponseRow(rawColumns, null, null, null));\n             numRowsIndexed++;\n           }\n         }\n         catch (ParseException e) {\n-          responseRows[counter] = new SamplerResponseRow(rawColumns, null, true, e.getMessage());\n-          counter++;\n+          if (rawColumnsList != null) {\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, e.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2NTM3OQ=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY0NjQwOQ==", "bodyText": "Yes, I think so because the sampler result should match to what will actually be ingested to Druid.", "url": "https://github.com/apache/druid/pull/10383#discussion_r513646409", "createdAt": "2020-10-28T17:49:42Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -131,17 +130,27 @@ public SamplerResponse sample(\n             continue;\n           }\n \n-          for (InputRow row : inputRowListPlusRawValues.getInputRows()) {\n-            index.add(new SamplerInputRow(row, counter), true);\n+          for (int i = 0; i < rawColumnsList.size(); i++) {\n+            Map<String, Object> rawColumns = rawColumnsList.get(i);\n+            InputRow row = inputRowListPlusRawValues.getInputRows().get(i);\n+\n+            //keep the index of the row to be added to responseRows for further use\n+            final int rowIndex = responseRows.size();\n+            index.add(new SamplerInputRow(row, rowIndex), true);\n+\n             // store the raw value; will be merged with the data from the IncrementalIndex later\n-            responseRows[counter] = new SamplerResponseRow(rawColumns, null, null, null);\n-            counter++;\n+            responseRows.add(new SamplerResponseRow(rawColumns, null, null, null));\n             numRowsIndexed++;\n           }\n         }\n         catch (ParseException e) {\n-          responseRows[counter] = new SamplerResponseRow(rawColumns, null, true, e.getMessage());\n-          counter++;\n+          if (rawColumnsList != null) {\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, e.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2NTM3OQ=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ0MjIxOA==", "bodyText": "A new case testIndexParseException in InputSouceSamplerTest has been added to check whether the exception returned by index.add has been processed properly. And also some little changes are applied in IncrementalIndex.getCombinedParseException and SampleInputRow to make exception message more accurate.", "url": "https://github.com/apache/druid/pull/10383#discussion_r515442218", "createdAt": "2020-10-31T01:55:26Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -131,17 +130,27 @@ public SamplerResponse sample(\n             continue;\n           }\n \n-          for (InputRow row : inputRowListPlusRawValues.getInputRows()) {\n-            index.add(new SamplerInputRow(row, counter), true);\n+          for (int i = 0; i < rawColumnsList.size(); i++) {\n+            Map<String, Object> rawColumns = rawColumnsList.get(i);\n+            InputRow row = inputRowListPlusRawValues.getInputRows().get(i);\n+\n+            //keep the index of the row to be added to responseRows for further use\n+            final int rowIndex = responseRows.size();\n+            index.add(new SamplerInputRow(row, rowIndex), true);\n+\n             // store the raw value; will be merged with the data from the IncrementalIndex later\n-            responseRows[counter] = new SamplerResponseRow(rawColumns, null, null, null);\n-            counter++;\n+            responseRows.add(new SamplerResponseRow(rawColumns, null, null, null));\n             numRowsIndexed++;\n           }\n         }\n         catch (ParseException e) {\n-          responseRows[counter] = new SamplerResponseRow(rawColumns, null, true, e.getMessage());\n-          counter++;\n+          if (rawColumnsList != null) {\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, e.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjI2NTM3OQ=="}, "originalCommit": {"oid": "a9eb12a548e961ab700a43b064735d52f4495c40"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1OTg5OTcxOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxODowNjoxNFrOHv7H5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwNDoxNjoyNFrOHwLHag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAxMzc5OQ==", "bodyText": "nit: with this change, when a parseException is thrown while parsing a list of rows in one message, those rows in the message will be added as separate rows in responseRows but with the same parseException. This can be confusing since the parseException error message seem irrelevant to the associated rawColumns in the same responseRow. IMO, the better fix in this case would be storing the whole rawColumnsList in one SamplerResponseRow. Then, the parseException can indicate that it was thrown while parsing one of the rows in rawColumnsList. However, this requires a change on the web console side as well. I'm OK with fixing this in a follow-up PR.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520013799", "createdAt": "2020-11-09T18:06:14Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -111,38 +113,49 @@ public SamplerResponse sample(\n     try (final CloseableIterator<InputRowListPlusRawValues> iterator = reader.sample();\n          final IncrementalIndex<Aggregator> index = buildIncrementalIndex(nonNullSamplerConfig, nonNullDataSchema);\n          final Closer closer1 = closer) {\n-      SamplerResponseRow[] responseRows = new SamplerResponseRow[nonNullSamplerConfig.getNumRows()];\n-      int counter = 0, numRowsIndexed = 0;\n-\n-      while (counter < responseRows.length && iterator.hasNext()) {\n-        Map<String, Object> rawColumns = null;\n-        try {\n-          final InputRowListPlusRawValues inputRowListPlusRawValues = iterator.next();\n-\n-          if (inputRowListPlusRawValues.getRawValues() != null) {\n-            rawColumns = inputRowListPlusRawValues.getRawValues();\n-          }\n-\n-          if (inputRowListPlusRawValues.getParseException() != null) {\n-            throw inputRowListPlusRawValues.getParseException();\n+      List<SamplerResponseRow> responseRows = new ArrayList<>(nonNullSamplerConfig.getNumRows());\n+      int numRowsIndexed = 0;\n+\n+      while (responseRows.size() < nonNullSamplerConfig.getNumRows() && iterator.hasNext()) {\n+        final InputRowListPlusRawValues inputRowListPlusRawValues = iterator.next();\n+\n+        final List<Map<String, Object>> rawColumnsList = inputRowListPlusRawValues.getRawValuesList();\n+\n+        final ParseException parseException = inputRowListPlusRawValues.getParseException();\n+        if (parseException != null) {\n+          if (rawColumnsList != null) {\n+            // add all rows to response\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, parseException.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI3NTgxOA==", "bodyText": "Yes, returning the whole rawColumnsList involves some changes at the web-console side which is currently out of my ability. Thanks for understanding.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520275818", "createdAt": "2020-11-10T04:16:24Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/sampler/InputSourceSampler.java", "diffHunk": "@@ -111,38 +113,49 @@ public SamplerResponse sample(\n     try (final CloseableIterator<InputRowListPlusRawValues> iterator = reader.sample();\n          final IncrementalIndex<Aggregator> index = buildIncrementalIndex(nonNullSamplerConfig, nonNullDataSchema);\n          final Closer closer1 = closer) {\n-      SamplerResponseRow[] responseRows = new SamplerResponseRow[nonNullSamplerConfig.getNumRows()];\n-      int counter = 0, numRowsIndexed = 0;\n-\n-      while (counter < responseRows.length && iterator.hasNext()) {\n-        Map<String, Object> rawColumns = null;\n-        try {\n-          final InputRowListPlusRawValues inputRowListPlusRawValues = iterator.next();\n-\n-          if (inputRowListPlusRawValues.getRawValues() != null) {\n-            rawColumns = inputRowListPlusRawValues.getRawValues();\n-          }\n-\n-          if (inputRowListPlusRawValues.getParseException() != null) {\n-            throw inputRowListPlusRawValues.getParseException();\n+      List<SamplerResponseRow> responseRows = new ArrayList<>(nonNullSamplerConfig.getNumRows());\n+      int numRowsIndexed = 0;\n+\n+      while (responseRows.size() < nonNullSamplerConfig.getNumRows() && iterator.hasNext()) {\n+        final InputRowListPlusRawValues inputRowListPlusRawValues = iterator.next();\n+\n+        final List<Map<String, Object>> rawColumnsList = inputRowListPlusRawValues.getRawValuesList();\n+\n+        final ParseException parseException = inputRowListPlusRawValues.getParseException();\n+        if (parseException != null) {\n+          if (rawColumnsList != null) {\n+            // add all rows to response\n+            responseRows.addAll(rawColumnsList.stream()\n+                                              .map(rawColumns -> new SamplerResponseRow(rawColumns, null, true, parseException.getMessage()))\n+                                              .collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAxMzc5OQ=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1OTkzMTM1OnYy", "diffSide": "RIGHT", "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxODoxNDo1MlrOHv7bew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwNDoxNDo1OVrOHwLGEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAxODgxMQ==", "bodyText": "Did you intend StringUtils.toUtf8(illformed)? The variable illformed is not in use.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520018811", "createdAt": "2020-11-09T18:14:52Z", "author": {"login": "jihoonson"}, "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "diffHunk": "@@ -2726,4 +2726,83 @@ public void close()\n         null\n     );\n   }\n+\n+  @Test(timeout = 60_000L)\n+  public void testMultipleLinesJSONText() throws Exception\n+  {\n+    reportParseExceptions = false;\n+    maxParseExceptions = 1000;\n+    maxSavedParseExceptions = 2;\n+\n+    // Insert data\n+    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n+      kafkaProducer.initTransactions();\n+      kafkaProducer.beginTransaction();\n+\n+      //multiple objects in one Kafka record will yield 2 rows in druid\n+      String wellformed = toJsonString(true, \"2049\", \"d2\", \"y\", \"10\", \"22.0\", \"2.0\") +\n+                     toJsonString(true, \"2049\", \"d3\", \"y\", \"10\", \"23.0\", \"3.0\");\n+\n+      //multiple objects in one Kafka record but some objects are in ill-formed format\n+      //the whole ProducerRecord will be discarded\n+      String illformed = \"{\\\"timestamp\\\":2049, \\\"dim1\\\": \\\"d4\\\", \\\"dim2\\\":\\\"x\\\", \\\"dimLong\\\": 10, \\\"dimFloat\\\":\\\"24.0\\\", \\\"met1\\\":\\\"2.0\\\" }\" +\n+                     \"{\\\"timestamp\\\":2049, \\\"dim1\\\": \\\"d5\\\", \\\"dim2\\\":\\\"y\\\", \\\"dimLong\\\": 10, \\\"dimFloat\\\":\\\"24.0\\\", \\\"met1\\\":invalidFormat }\" +\n+                     \"{\\\"timestamp\\\":2049, \\\"dim1\\\": \\\"d6\\\", \\\"dim2\\\":\\\"z\\\", \\\"dimLong\\\": 10, \\\"dimFloat\\\":\\\"24.0\\\", \\\"met1\\\":\\\"3.0\\\" }\";\n+\n+      ProducerRecord[] producerRecords = new ProducerRecord[]{\n+          // pretty formatted\n+          new ProducerRecord<>(topic, 0, null, jb(true, \"2049\", \"d1\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+          //well-formed\n+          new ProducerRecord<>(topic, 0, null, StringUtils.toUtf8(wellformed)),\n+          //ill-formed\n+          new ProducerRecord<>(topic, 0, null, StringUtils.toUtf8(\"illformed\")),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI3NTQ3Mw==", "bodyText": "Yes, illformed here should be a variable instead of a text string.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520275473", "createdAt": "2020-11-10T04:14:59Z", "author": {"login": "FrankChen021"}, "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "diffHunk": "@@ -2726,4 +2726,83 @@ public void close()\n         null\n     );\n   }\n+\n+  @Test(timeout = 60_000L)\n+  public void testMultipleLinesJSONText() throws Exception\n+  {\n+    reportParseExceptions = false;\n+    maxParseExceptions = 1000;\n+    maxSavedParseExceptions = 2;\n+\n+    // Insert data\n+    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n+      kafkaProducer.initTransactions();\n+      kafkaProducer.beginTransaction();\n+\n+      //multiple objects in one Kafka record will yield 2 rows in druid\n+      String wellformed = toJsonString(true, \"2049\", \"d2\", \"y\", \"10\", \"22.0\", \"2.0\") +\n+                     toJsonString(true, \"2049\", \"d3\", \"y\", \"10\", \"23.0\", \"3.0\");\n+\n+      //multiple objects in one Kafka record but some objects are in ill-formed format\n+      //the whole ProducerRecord will be discarded\n+      String illformed = \"{\\\"timestamp\\\":2049, \\\"dim1\\\": \\\"d4\\\", \\\"dim2\\\":\\\"x\\\", \\\"dimLong\\\": 10, \\\"dimFloat\\\":\\\"24.0\\\", \\\"met1\\\":\\\"2.0\\\" }\" +\n+                     \"{\\\"timestamp\\\":2049, \\\"dim1\\\": \\\"d5\\\", \\\"dim2\\\":\\\"y\\\", \\\"dimLong\\\": 10, \\\"dimFloat\\\":\\\"24.0\\\", \\\"met1\\\":invalidFormat }\" +\n+                     \"{\\\"timestamp\\\":2049, \\\"dim1\\\": \\\"d6\\\", \\\"dim2\\\":\\\"z\\\", \\\"dimLong\\\": 10, \\\"dimFloat\\\":\\\"24.0\\\", \\\"met1\\\":\\\"3.0\\\" }\";\n+\n+      ProducerRecord[] producerRecords = new ProducerRecord[]{\n+          // pretty formatted\n+          new ProducerRecord<>(topic, 0, null, jb(true, \"2049\", \"d1\", \"y\", \"10\", \"20.0\", \"1.0\")),\n+          //well-formed\n+          new ProducerRecord<>(topic, 0, null, StringUtils.toUtf8(wellformed)),\n+          //ill-formed\n+          new ProducerRecord<>(topic, 0, null, StringUtils.toUtf8(\"illformed\")),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAxODgxMQ=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1OTk3MDk1OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxODoyNToxM1rOHv7zNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMDowMDo1NVrOHwU3kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNDg4Nw==", "bodyText": "Thanks for adding this test! Similar to https://github.com/apache/druid/pull/10383/files#diff-ef25ac1cc1f275b47b939b65e1d0c8b8b8512aeada52d06b8541b8f381df03eeR2731, could you please add a unit test for sampling a block of multiple JSON strings? The unit test can be run only when parserType is STR_JSON. We usually just return in the unit test when the parameter is not what we want to test with. It would be nice if the test verifies the followings:\n\nThe sampler response when there is no parseException with a list of multiple JSON strings.\nThe sampler response when there is a parseException thrown while parsing a list of multiple JSON strings. Maybe you can improve this unit test to do it as well.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520024887", "createdAt": "2020-11-09T18:25:13Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1060,6 +1062,107 @@ public void testWithFilter() throws IOException\n     );\n   }\n \n+  @Test\n+  public void testIndexParseException() throws IOException", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI3NTUwMw==", "bodyText": "A block of multiple json strings is only supported in stream ingestion as we discussed above, that's also why the test cases are added in kafka-indexing-service module.It's implemented by setting the new property on JsonInputFormat in RecordSupplierInputSource.\nFor test cases for InputSourceSampler here, the input source is InlineInputSource which always uses InputEntityIteratingReader even when input format is JsonInputFormat, there's no chance to test that. So I think we don't need add test cases here.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520275503", "createdAt": "2020-11-10T04:15:06Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1060,6 +1062,107 @@ public void testWithFilter() throws IOException\n     );\n   }\n \n+  @Test\n+  public void testIndexParseException() throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNDg4Nw=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDI4NjAwNA==", "bodyText": "I think we need such tests because KafkaIndexTaskTest only covers the actual indexing side, but not the sampling side. Without those tests, we could break the sampler without being noticed.\n\nFor test cases for InputSourceSampler here, the input source is InlineInputSource which always uses InputEntityIteratingReader even when input format is JsonInputFormat, there's no chance to test that. So I think we don't need add test cases here.\n\nMaybe this is not a good place for the new tests, but you don't have to use the InlineInputSource for them. Rather, you should use RecordSupplierInputSource as you mentioned. Since RecordSupplierInputSource accepts the RecordSupplier interface as a parameter which is the data supplier, I think you could implement a mock for testing (I don't think we should test Firehose as it's deprecated). Or do you see some reason you cannot? If it's hard, I'm also OK with adding them in a follow-up.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520286004", "createdAt": "2020-11-10T04:55:52Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1060,6 +1062,107 @@ public void testWithFilter() throws IOException\n     );\n   }\n \n+  @Test\n+  public void testIndexParseException() throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNDg4Nw=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDM0NzgwNQ==", "bodyText": "I think we need such tests because KafkaIndexTaskTest only covers the actual indexing side, but not the sampling side. Without those tests, we could break the sampler without being noticed.\n\nI forgot that test cases in KafkaIndexTaskTest only cover the indexing process. It's OK for me to add such tests.\nBTW, integration tests failed, I don't think it's related to changes in this PR, and the failure can also be seen in my other PR, is there any other problem ?", "url": "https://github.com/apache/druid/pull/10383#discussion_r520347805", "createdAt": "2020-11-10T07:37:35Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1060,6 +1062,107 @@ public void testWithFilter() throws IOException\n     );\n   }\n \n+  @Test\n+  public void testIndexParseException() throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNDg4Nw=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDM1NDE1Ng==", "bodyText": "Thanks for understanding. Do you want to add those tests in this PR? For the integration test failure, I don\u2019t think it\u2019s relevant to the changes in this PR. I just retriggered it.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520354156", "createdAt": "2020-11-10T07:51:07Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1060,6 +1062,107 @@ public void testWithFilter() throws IOException\n     );\n   }\n \n+  @Test\n+  public void testIndexParseException() throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNDg4Nw=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDQzNTYwMg==", "bodyText": "I've added a test testMultipleJsonStringInOneBlock,  please check it.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520435602", "createdAt": "2020-11-10T10:00:55Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1060,6 +1062,107 @@ public void testWithFilter() throws IOException\n     );\n   }\n \n+  @Test\n+  public void testIndexParseException() throws IOException", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDAyNDg4Nw=="}, "originalCommit": {"oid": "3a9076cfe7b8c767b02888f03b98133f9a480f26"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDY3ODEzOnYy", "diffSide": "RIGHT", "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNzo1NTozN1rOHwomsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwNTo1Nzo1OVrOHxroTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc1ODk2MQ==", "bodyText": "[ERROR] indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java:1221 -- Can be replaced with 'Collectors.joining'\n\nSeems like the Intellij Inspection CI doesn't like this line.", "url": "https://github.com/apache/druid/pull/10383#discussion_r520758961", "createdAt": "2020-11-10T17:55:37Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1163,6 +1172,121 @@ public void testIndexParseException() throws IOException\n     );\n   }\n \n+  /**\n+   *\n+   * This case tests sampling for multiple json lines in one text block\n+   * Currently only RecordSupplierInputSource supports this kind of input, see https://github.com/apache/druid/pull/10383 for more information\n+   *\n+   * This test combines illegal json block and legal json block together to verify:\n+   * 1. all lines in the illegal json block should not be parsed\n+   * 2. the illegal json block should not affect the processing of the 2nd record\n+   * 3. all lines in legal json block should be parsed successfully\n+   *\n+   */\n+  @Test\n+  public void testMultipleJsonStringInOneBlock() throws IOException\n+  {\n+    if (!ParserType.STR_JSON.equals(parserType) || !useInputFormatApi) {\n+      return;\n+    }\n+\n+    final TimestampSpec timestampSpec = new TimestampSpec(\"t\", null, null);\n+    final DimensionsSpec dimensionsSpec = new DimensionsSpec(\n+        ImmutableList.of(StringDimensionSchema.create(\"dim1PlusBar\"))\n+    );\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(new ExpressionTransform(\"dim1PlusBar\", \"concat(dim1 + 'bar')\", TestExprMacroTable.INSTANCE))\n+    );\n+    final AggregatorFactory[] aggregatorFactories = {new LongSumAggregatorFactory(\"met1\", \"met1\")};\n+    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n+        Granularities.DAY,\n+        Granularities.HOUR,\n+        true,\n+        null\n+    );\n+    final DataSchema dataSchema = createDataSchema(\n+        timestampSpec,\n+        dimensionsSpec,\n+        aggregatorFactories,\n+        granularitySpec,\n+        transformSpec\n+    );\n+\n+    List<String> jsonBlockList = ImmutableList.of(\n+        // include the line which can't be parsed into JSON object to form a illegal json block\n+        String.join(\"\", STR_JSON_ROWS),\n+\n+        // exclude the last line to form a legal json block\n+        String.join(\"\", STR_JSON_ROWS.stream().limit(STR_JSON_ROWS.size() - 1).collect(Collectors.toList()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "19fae71afb3d6f750a468c5bca2e8cdddb55a71f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTAwMTgzNw==", "bodyText": "Every time I push a commit to a branch that is being merge, I run the test cases in the module which contains the changes in that commit. If it's OK, I'll push the commit. But I find that there's a high probability that CI fails. Sometime it's related to inspection check, sometimes it's caused by failures of test cases in other modules, sometime it's about dependency check, sometime it has something with license check.\nI wonder what steps do you follow to check before push a commit ? Do you run all the cases in all modules ? Or is there a simple way to run the checks mentioned above ?", "url": "https://github.com/apache/druid/pull/10383#discussion_r521001837", "createdAt": "2020-11-11T01:38:55Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1163,6 +1172,121 @@ public void testIndexParseException() throws IOException\n     );\n   }\n \n+  /**\n+   *\n+   * This case tests sampling for multiple json lines in one text block\n+   * Currently only RecordSupplierInputSource supports this kind of input, see https://github.com/apache/druid/pull/10383 for more information\n+   *\n+   * This test combines illegal json block and legal json block together to verify:\n+   * 1. all lines in the illegal json block should not be parsed\n+   * 2. the illegal json block should not affect the processing of the 2nd record\n+   * 3. all lines in legal json block should be parsed successfully\n+   *\n+   */\n+  @Test\n+  public void testMultipleJsonStringInOneBlock() throws IOException\n+  {\n+    if (!ParserType.STR_JSON.equals(parserType) || !useInputFormatApi) {\n+      return;\n+    }\n+\n+    final TimestampSpec timestampSpec = new TimestampSpec(\"t\", null, null);\n+    final DimensionsSpec dimensionsSpec = new DimensionsSpec(\n+        ImmutableList.of(StringDimensionSchema.create(\"dim1PlusBar\"))\n+    );\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(new ExpressionTransform(\"dim1PlusBar\", \"concat(dim1 + 'bar')\", TestExprMacroTable.INSTANCE))\n+    );\n+    final AggregatorFactory[] aggregatorFactories = {new LongSumAggregatorFactory(\"met1\", \"met1\")};\n+    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n+        Granularities.DAY,\n+        Granularities.HOUR,\n+        true,\n+        null\n+    );\n+    final DataSchema dataSchema = createDataSchema(\n+        timestampSpec,\n+        dimensionsSpec,\n+        aggregatorFactories,\n+        granularitySpec,\n+        transformSpec\n+    );\n+\n+    List<String> jsonBlockList = ImmutableList.of(\n+        // include the line which can't be parsed into JSON object to form a illegal json block\n+        String.join(\"\", STR_JSON_ROWS),\n+\n+        // exclude the last line to form a legal json block\n+        String.join(\"\", STR_JSON_ROWS.stream().limit(STR_JSON_ROWS.size() - 1).collect(Collectors.toList()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc1ODk2MQ=="}, "originalCommit": {"oid": "19fae71afb3d6f750a468c5bca2e8cdddb55a71f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTA3OTc0MQ==", "bodyText": "could you re-trigger the CI ? The failure does not related to changes in the PR.\nFailed with bad exit code during 'Extraction'\n\n[ERROR] Failed to execute goal on project druid-sql: Could not resolve dependencies for project org.apache.druid:druid-sql:jar:0.21.0-SNAPSHOT: Failed to collect dependencies at org.apache.calcite:calcite-core:jar:1.21.0 -> org.codehaus.janino:janino:jar:3.0.11: Failed to read artifact descriptor for org.codehaus.janino:janino:jar:3.0.11: Could not transfer artifact org.codehaus.janino:janino-parent:pom:3.0.11 from/to central (https://repo.maven.apache.org/maven2): Failed to transfer file https://repo.maven.apache.org/maven2/org/codehaus/janino/janino-parent/3.0.11/janino-parent-3.0.11.pom with status code 503 -> [Help 1]", "url": "https://github.com/apache/druid/pull/10383#discussion_r521079741", "createdAt": "2020-11-11T03:23:48Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1163,6 +1172,121 @@ public void testIndexParseException() throws IOException\n     );\n   }\n \n+  /**\n+   *\n+   * This case tests sampling for multiple json lines in one text block\n+   * Currently only RecordSupplierInputSource supports this kind of input, see https://github.com/apache/druid/pull/10383 for more information\n+   *\n+   * This test combines illegal json block and legal json block together to verify:\n+   * 1. all lines in the illegal json block should not be parsed\n+   * 2. the illegal json block should not affect the processing of the 2nd record\n+   * 3. all lines in legal json block should be parsed successfully\n+   *\n+   */\n+  @Test\n+  public void testMultipleJsonStringInOneBlock() throws IOException\n+  {\n+    if (!ParserType.STR_JSON.equals(parserType) || !useInputFormatApi) {\n+      return;\n+    }\n+\n+    final TimestampSpec timestampSpec = new TimestampSpec(\"t\", null, null);\n+    final DimensionsSpec dimensionsSpec = new DimensionsSpec(\n+        ImmutableList.of(StringDimensionSchema.create(\"dim1PlusBar\"))\n+    );\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(new ExpressionTransform(\"dim1PlusBar\", \"concat(dim1 + 'bar')\", TestExprMacroTable.INSTANCE))\n+    );\n+    final AggregatorFactory[] aggregatorFactories = {new LongSumAggregatorFactory(\"met1\", \"met1\")};\n+    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n+        Granularities.DAY,\n+        Granularities.HOUR,\n+        true,\n+        null\n+    );\n+    final DataSchema dataSchema = createDataSchema(\n+        timestampSpec,\n+        dimensionsSpec,\n+        aggregatorFactories,\n+        granularitySpec,\n+        transformSpec\n+    );\n+\n+    List<String> jsonBlockList = ImmutableList.of(\n+        // include the line which can't be parsed into JSON object to form a illegal json block\n+        String.join(\"\", STR_JSON_ROWS),\n+\n+        // exclude the last line to form a legal json block\n+        String.join(\"\", STR_JSON_ROWS.stream().limit(STR_JSON_ROWS.size() - 1).collect(Collectors.toList()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc1ODk2MQ=="}, "originalCommit": {"oid": "19fae71afb3d6f750a468c5bca2e8cdddb55a71f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTA4MzYzOQ==", "bodyText": "Re-triggered LGTM", "url": "https://github.com/apache/druid/pull/10383#discussion_r521083639", "createdAt": "2020-11-11T03:29:51Z", "author": {"login": "suneet-s"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1163,6 +1172,121 @@ public void testIndexParseException() throws IOException\n     );\n   }\n \n+  /**\n+   *\n+   * This case tests sampling for multiple json lines in one text block\n+   * Currently only RecordSupplierInputSource supports this kind of input, see https://github.com/apache/druid/pull/10383 for more information\n+   *\n+   * This test combines illegal json block and legal json block together to verify:\n+   * 1. all lines in the illegal json block should not be parsed\n+   * 2. the illegal json block should not affect the processing of the 2nd record\n+   * 3. all lines in legal json block should be parsed successfully\n+   *\n+   */\n+  @Test\n+  public void testMultipleJsonStringInOneBlock() throws IOException\n+  {\n+    if (!ParserType.STR_JSON.equals(parserType) || !useInputFormatApi) {\n+      return;\n+    }\n+\n+    final TimestampSpec timestampSpec = new TimestampSpec(\"t\", null, null);\n+    final DimensionsSpec dimensionsSpec = new DimensionsSpec(\n+        ImmutableList.of(StringDimensionSchema.create(\"dim1PlusBar\"))\n+    );\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(new ExpressionTransform(\"dim1PlusBar\", \"concat(dim1 + 'bar')\", TestExprMacroTable.INSTANCE))\n+    );\n+    final AggregatorFactory[] aggregatorFactories = {new LongSumAggregatorFactory(\"met1\", \"met1\")};\n+    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n+        Granularities.DAY,\n+        Granularities.HOUR,\n+        true,\n+        null\n+    );\n+    final DataSchema dataSchema = createDataSchema(\n+        timestampSpec,\n+        dimensionsSpec,\n+        aggregatorFactories,\n+        granularitySpec,\n+        transformSpec\n+    );\n+\n+    List<String> jsonBlockList = ImmutableList.of(\n+        // include the line which can't be parsed into JSON object to form a illegal json block\n+        String.join(\"\", STR_JSON_ROWS),\n+\n+        // exclude the last line to form a legal json block\n+        String.join(\"\", STR_JSON_ROWS.stream().limit(STR_JSON_ROWS.size() - 1).collect(Collectors.toList()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc1ODk2MQ=="}, "originalCommit": {"oid": "19fae71afb3d6f750a468c5bca2e8cdddb55a71f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI1OTY2MA==", "bodyText": "Hi @suneet-s , do you have any comments ?", "url": "https://github.com/apache/druid/pull/10383#discussion_r521259660", "createdAt": "2020-11-11T10:25:04Z", "author": {"login": "FrankChen021"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1163,6 +1172,121 @@ public void testIndexParseException() throws IOException\n     );\n   }\n \n+  /**\n+   *\n+   * This case tests sampling for multiple json lines in one text block\n+   * Currently only RecordSupplierInputSource supports this kind of input, see https://github.com/apache/druid/pull/10383 for more information\n+   *\n+   * This test combines illegal json block and legal json block together to verify:\n+   * 1. all lines in the illegal json block should not be parsed\n+   * 2. the illegal json block should not affect the processing of the 2nd record\n+   * 3. all lines in legal json block should be parsed successfully\n+   *\n+   */\n+  @Test\n+  public void testMultipleJsonStringInOneBlock() throws IOException\n+  {\n+    if (!ParserType.STR_JSON.equals(parserType) || !useInputFormatApi) {\n+      return;\n+    }\n+\n+    final TimestampSpec timestampSpec = new TimestampSpec(\"t\", null, null);\n+    final DimensionsSpec dimensionsSpec = new DimensionsSpec(\n+        ImmutableList.of(StringDimensionSchema.create(\"dim1PlusBar\"))\n+    );\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(new ExpressionTransform(\"dim1PlusBar\", \"concat(dim1 + 'bar')\", TestExprMacroTable.INSTANCE))\n+    );\n+    final AggregatorFactory[] aggregatorFactories = {new LongSumAggregatorFactory(\"met1\", \"met1\")};\n+    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n+        Granularities.DAY,\n+        Granularities.HOUR,\n+        true,\n+        null\n+    );\n+    final DataSchema dataSchema = createDataSchema(\n+        timestampSpec,\n+        dimensionsSpec,\n+        aggregatorFactories,\n+        granularitySpec,\n+        transformSpec\n+    );\n+\n+    List<String> jsonBlockList = ImmutableList.of(\n+        // include the line which can't be parsed into JSON object to form a illegal json block\n+        String.join(\"\", STR_JSON_ROWS),\n+\n+        // exclude the last line to form a legal json block\n+        String.join(\"\", STR_JSON_ROWS.stream().limit(STR_JSON_ROWS.size() - 1).collect(Collectors.toList()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc1ODk2MQ=="}, "originalCommit": {"oid": "19fae71afb3d6f750a468c5bca2e8cdddb55a71f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTg1NzEwMA==", "bodyText": "Every time I push a commit to a branch that is being merge, I run the test cases in the module which contains the changes in that commit. If it's OK, I'll push the commit. But I find that there's a high probability that CI fails. Sometime it's related to inspection check, sometimes it's caused by failures of test cases in other modules, sometime it's about dependency check, sometime it has something with license check.\nI wonder what steps do you follow to check before push a commit ? Do you run all the cases in all modules ? Or is there a simple way to run the checks mentioned above ?\n\n@FrankChen021 That's what I usually do as well. To be honest, unexpected CI failures make me annoyed too \ud83d\ude05 You can run those checks on your own by running the same command as what Travis runs. You may want to set up some pre-commit/post-commit hooks. The best would be some automatic correction for trivial issues, but I'm not sure if there is such a tool available which is matured and reliable enough.", "url": "https://github.com/apache/druid/pull/10383#discussion_r521857100", "createdAt": "2020-11-12T05:57:59Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/test/java/org/apache/druid/indexing/overlord/sampler/InputSourceSamplerTest.java", "diffHunk": "@@ -1163,6 +1172,121 @@ public void testIndexParseException() throws IOException\n     );\n   }\n \n+  /**\n+   *\n+   * This case tests sampling for multiple json lines in one text block\n+   * Currently only RecordSupplierInputSource supports this kind of input, see https://github.com/apache/druid/pull/10383 for more information\n+   *\n+   * This test combines illegal json block and legal json block together to verify:\n+   * 1. all lines in the illegal json block should not be parsed\n+   * 2. the illegal json block should not affect the processing of the 2nd record\n+   * 3. all lines in legal json block should be parsed successfully\n+   *\n+   */\n+  @Test\n+  public void testMultipleJsonStringInOneBlock() throws IOException\n+  {\n+    if (!ParserType.STR_JSON.equals(parserType) || !useInputFormatApi) {\n+      return;\n+    }\n+\n+    final TimestampSpec timestampSpec = new TimestampSpec(\"t\", null, null);\n+    final DimensionsSpec dimensionsSpec = new DimensionsSpec(\n+        ImmutableList.of(StringDimensionSchema.create(\"dim1PlusBar\"))\n+    );\n+    final TransformSpec transformSpec = new TransformSpec(\n+        null,\n+        ImmutableList.of(new ExpressionTransform(\"dim1PlusBar\", \"concat(dim1 + 'bar')\", TestExprMacroTable.INSTANCE))\n+    );\n+    final AggregatorFactory[] aggregatorFactories = {new LongSumAggregatorFactory(\"met1\", \"met1\")};\n+    final GranularitySpec granularitySpec = new UniformGranularitySpec(\n+        Granularities.DAY,\n+        Granularities.HOUR,\n+        true,\n+        null\n+    );\n+    final DataSchema dataSchema = createDataSchema(\n+        timestampSpec,\n+        dimensionsSpec,\n+        aggregatorFactories,\n+        granularitySpec,\n+        transformSpec\n+    );\n+\n+    List<String> jsonBlockList = ImmutableList.of(\n+        // include the line which can't be parsed into JSON object to form a illegal json block\n+        String.join(\"\", STR_JSON_ROWS),\n+\n+        // exclude the last line to form a legal json block\n+        String.join(\"\", STR_JSON_ROWS.stream().limit(STR_JSON_ROWS.size() - 1).collect(Collectors.toList()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc1ODk2MQ=="}, "originalCommit": {"oid": "19fae71afb3d6f750a468c5bca2e8cdddb55a71f"}, "originalPosition": 87}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3201, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}