{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyMjE1Mzc4", "number": 10366, "title": "Add caching support to join queries", "bodyText": "This PR adds caching capabilities to queries with join. The support is limited for now and only supported if the right-hand data source in the join is GlobalTableDataSource.  But it can be extended to more data sources in follow-up PRs.\nDescription\nThe cache key is computed independently of Joinable objects. ServerManager, CachingClusteredClient#SpecificQueryRunnable and SinkQuerySegmentWalker use Joinables.computeDataSourceCacheKey which then loops through all PreJoinableClause objects and\n\nbuilds the cache key for data source participating in clause (1)\n\ncalls JoinableFactory.computeJoinCacheKey. Returns absent if caching is not supported. BroadcastTableJoinableFactory returns a key computed using the segment of the broadcast table\n\n\nadds the JoinConditionAnalysis to the above cache key (2)\n\n(1) - Different data sources can have their own logic on how this caching key is built. BroadcastTableJoinableFactory has a single segment and uses this segment's id. RowBasedIndexedTable can use the cache key if it's passed externally. E.g. Broker could construct the cache key for inline data sources and pass it to Historicals. Historicals can use this key to construct the cache key. InlineJoinableFactory need not even construct the RowBasedIndexedTable as it can fetch the cache key directly from InlineDataSource if present. That implementation is for some other day though.\n(2) - Apart from the data source on the right, any change in join condition will invalidate the cache as well. JoinType and the original expression are used to construct the part of the cache key that changes based on the join condition.\n\nThis PR has:\n\n been self-reviewed.\n added documentation for new or modified features or behaviors.\n added Javadocs for most classes and all non-trivial methods. Linked related entities via Javadoc links.\n added comments explaining the \"why\" and the intent of the code wherever would not be obvious for an unfamiliar reader.\n added unit tests or modified existing tests to cover new code paths, ensuring the threshold for code coverage is met.\n added integration tests.\n been tested in a test Druid cluster.\n\n\n\nKey changed/added classes in this PR\n\nServerManager\nSinkQuerySegmentWalker\nCachingClusteredClient\nCachingQueryRunner\nJoinables - has now instance methods for easy mocking/testing\nBroadCastSegmentIndexedTable - This is the only indexed table supported for join cacheability as of now.", "createdAt": "2020-09-08T18:14:36Z", "url": "https://github.com/apache/druid/pull/10366", "merged": true, "mergeCommit": {"oid": "4d2a92f46a121fc8b3f5a02f1480156ccc62bb4a"}, "closed": true, "closedAt": "2020-10-10T00:42:30Z", "author": {"login": "abhishekagarwal87"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdG6CY2gH2gAyNDgyMjE1Mzc4OjQzZTk0OGZhNTQ5Y2FkN2JmODZiMTRmNzUyOWMwNzZjMmE3OTRlMmE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQ9lalAFqTUwNjAxMTEyMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/43e948fa549cad7bf86b14f7529c076c2a794e2a", "committedDate": "2020-09-08T16:17:53Z", "message": "Proposed changes for making joins cacheable"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MjY2OTQx", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-487266941", "createdAt": "2020-09-13T01:26:49Z", "commit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QwMToyNjo1MFrOHQ4pGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QwMjoyMToxNFrOHQ44DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzI5MQ==", "bodyText": "Why does this matter? Shouldn't it just be on the joinable factory to decide if it can cache or not and we should supply the PreJoinableClause or the JoinConditionAnalysis from PreJoinableClause.getCondition to it?", "url": "https://github.com/apache/druid/pull/10366#discussion_r487467291", "createdAt": "2020-09-13T01:26:50Z", "author": {"login": "clintropolis"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +126,47 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that is to be used in segment level and result level caches. The\n+   * data source can either be base (clauses is empty) or RHS of a join (clauses is non-empty). In both of the cases,\n+   * a non-null cache is returned. However, the cache key is null if there is a join and some of the right data sources\n+   * participating in the join do not support caching yet\n+   *\n+   * @param dataSourceAnalysis\n+   * @param joinableFactory\n+   * @return\n+   */\n+  public static Optional<byte[]> computeDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis,\n+      final JoinableFactory joinableFactory\n+  )\n+  {\n+    final CacheKeyBuilder keyBuilder;\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      keyBuilder = new CacheKeyBuilder(REGULAR_OPERATION);\n+    } else {\n+      keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+      for (PreJoinableClause clause : clauses) {\n+        if (!clause.getCondition().canHashJoin()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzU4NA==", "bodyText": "Since multiple joinable factories can support the same class of datasource, I guess this means that this method needs to mimic the same selection logic as build to make sure that a joinable factory only computes a cache key for a table that it would build a joinable for in the query? I think we should probably supply the PreJoinableClause or the JoinConditionAnalysis  to this method so that it can have the JoinConditionAnalysis that build has access to. Then these factories can make better decisions about if they can compute a key for the query or not.", "url": "https://github.com/apache/druid/pull/10366#discussion_r487467584", "createdAt": "2020-09-13T01:30:57Z", "author": {"login": "clintropolis"}, "path": "processing/src/main/java/org/apache/druid/segment/join/MapJoinableFactory.java", "diffHunk": "@@ -80,4 +80,21 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     }\n     return maybeJoinable;\n   }\n+\n+  @Override\n+  public Optional<byte[]> computeJoinCacheKey(DataSource dataSource)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw==", "bodyText": "Could this just be a method on SegmentId?", "url": "https://github.com/apache/druid/pull/10366#discussion_r487467627", "createdAt": "2020-09-13T01:31:44Z", "author": {"login": "clintropolis"}, "path": "processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java", "diffHunk": "@@ -241,6 +244,22 @@ public ColumnSelectorFactory makeColumnSelectorFactory(ReadableOffset offset, bo\n     );\n   }\n \n+  @Override\n+  public Optional<byte[]> computeCacheKey()\n+  {\n+    SegmentId segmentId = segment.getId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2OTcyOQ==", "bodyText": "I guess this would bust cache on upgrade since all non-join cache keys would have this new byte prefix? Could we just not add a prefix if the query isn't a join query?", "url": "https://github.com/apache/druid/pull/10366#discussion_r487469729", "createdAt": "2020-09-13T02:00:40Z", "author": {"login": "clintropolis"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +126,47 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that is to be used in segment level and result level caches. The\n+   * data source can either be base (clauses is empty) or RHS of a join (clauses is non-empty). In both of the cases,\n+   * a non-null cache is returned. However, the cache key is null if there is a join and some of the right data sources\n+   * participating in the join do not support caching yet\n+   *\n+   * @param dataSourceAnalysis\n+   * @param joinableFactory\n+   * @return\n+   */\n+  public static Optional<byte[]> computeDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis,\n+      final JoinableFactory joinableFactory\n+  )\n+  {\n+    final CacheKeyBuilder keyBuilder;\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      keyBuilder = new CacheKeyBuilder(REGULAR_OPERATION);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDU2Nw==", "bodyText": "nit: this seems a little strange that the cache key is turned into the namespace and another key is used as the bytes, rather than making some sort of composite, but maybe it is ok?", "url": "https://github.com/apache/druid/pull/10366#discussion_r487470567", "createdAt": "2020-09-13T02:13:21Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java", "diffHunk": "@@ -86,7 +92,15 @@ public ResultLevelCachingQueryRunner(\n     if (useResultCache || populateResultCache) {\n \n       final String cacheKeyStr = StringUtils.fromUtf8(strategy.computeResultLevelCacheKey(query));\n-      final byte[] cachedResultSet = fetchResultsFromResultLevelCache(cacheKeyStr);\n+      DataSourceAnalysis analysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n+      byte[] dataSourceCacheKey = Joinables.computeDataSourceCacheKey(analysis, joinableFactory).orElse(null);\n+      if (null == dataSourceCacheKey) {\n+        return baseRunner.run(\n+            queryPlus,\n+            responseContext\n+        );\n+      }\n+      final byte[] cachedResultSet = fetchResultsFromResultLevelCache(cacheKeyStr, dataSourceCacheKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDc2MA==", "bodyText": "seems like this and build could share a method that gets the ReferenceCountingIndexedTable if possible, and build can make the joinable and this method compute the key on the results", "url": "https://github.com/apache/druid/pull/10366#discussion_r487470760", "createdAt": "2020-09-13T02:16:19Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/segment/join/BroadcastTableJoinableFactory.java", "diffHunk": "@@ -76,4 +76,27 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     }\n     return Optional.empty();\n   }\n+\n+  @Override\n+  public Optional<byte[]> computeJoinCacheKey(DataSource dataSource)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MTExNg==", "bodyText": "CachingQueryRunner already has the machinery to decide whether or not to use the caches, should we just pass the optional in and use it as another input to the existing useCache/populateCache equation so we don't need these if statements in ServerManager and SinkQuerySegmentWalker?", "url": "https://github.com/apache/druid/pull/10366#discussion_r487471116", "createdAt": "2020-09-13T02:21:14Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -293,21 +302,28 @@ public ServerManager(\n         queryMetrics -> queryMetrics.segment(segmentIdString)\n     );\n \n-    CachingQueryRunner<T> cachingQueryRunner = new CachingQueryRunner<>(\n-        segmentIdString,\n-        segmentDescriptor,\n-        objectMapper,\n-        cache,\n-        toolChest,\n-        metricsEmittingQueryRunnerInner,\n-        cachePopulator,\n-        cacheConfig\n-    );\n+    QueryRunner<T> queryRunner;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 71}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff9f92d66175271a14fa827b8f5fc81e0be1b137", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/ff9f92d66175271a14fa827b8f5fc81e0be1b137", "committedDate": "2020-09-14T07:21:04Z", "message": "Merge branch 'master' of github.com:apache/druid into join_cacheable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fbe8d836a3885672779de32c7a3beffc5486100", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/0fbe8d836a3885672779de32c7a3beffc5486100", "committedDate": "2020-09-15T15:16:51Z", "message": "Add unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f97e22421be9cc9d52b510cca0bb0d1a6bcc085", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/9f97e22421be9cc9d52b510cca0bb0d1a6bcc085", "committedDate": "2020-09-16T09:21:19Z", "message": "Fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e065d252cf7c18d26d103fe5058e5a9537d08a0", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/1e065d252cf7c18d26d103fe5058e5a9537d08a0", "committedDate": "2020-09-16T10:02:29Z", "message": "simplify logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78290510b36620f4898164805309d2ae5f70a1b0", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/78290510b36620f4898164805309d2ae5f70a1b0", "committedDate": "2020-09-16T11:09:54Z", "message": "Pull empty byte array logic out of CachingQueryRunner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73dcc8e26e98b048c1a9e61fb65a9dd106a24a22", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/73dcc8e26e98b048c1a9e61fb65a9dd106a24a22", "committedDate": "2020-09-16T11:12:04Z", "message": "remove useless null check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50d585f762daebc2fdbcbbe7bc89eb103e4552dd", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/50d585f762daebc2fdbcbbe7bc89eb103e4552dd", "committedDate": "2020-09-16T13:29:33Z", "message": "Merge branch 'master' of github.com:apache/druid into join_cacheable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ff3b58e9cf5080ae1bf7bca127335f0e66abf30", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/4ff3b58e9cf5080ae1bf7bca127335f0e66abf30", "committedDate": "2020-09-16T13:29:57Z", "message": "Minor refactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03a00f218eb85e33bc343a2744742d7394baadf3", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/03a00f218eb85e33bc343a2744742d7394baadf3", "committedDate": "2020-09-16T18:48:36Z", "message": "Fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12b6104a909c074f272df70797da08247e8b7db6", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/12b6104a909c074f272df70797da08247e8b7db6", "committedDate": "2020-09-17T06:33:51Z", "message": "Merge branch 'master' of github.com:apache/druid into join_cacheable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a05961c00ac017741887fa3b77e9921dcb02a07", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/5a05961c00ac017741887fa3b77e9921dcb02a07", "committedDate": "2020-09-18T10:50:56Z", "message": "Fix segment caching on Broker"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/50ebf74f1080e3a877c4ec19ca95be070825f922", "committedDate": "2020-09-18T12:37:43Z", "message": "Move join cache key computation in Broker\n\nMove join cache key computation in Broker from ResultLevelCachingQueryRunner to CachingClusteredClient"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxODEyNDE5", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-491812419", "createdAt": "2020-09-18T22:17:27Z", "commit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "state": "COMMENTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQyMjoxNzoyN1rOHUdjeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQyMzoyOTo0MFrOHU1G1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng==", "bodyText": "Using a plural name is a convention for utility classes like Collections. Maybe we should rename it if necessary since this class will not be a simple utility class anymore. But, does this class need to be a wrapper class? It doesn't seem like so. Also, some static util methods and non-static util methods are mixed here which seems confusing. Can we keep this class as a non-instantiable utility class?", "url": "https://github.com/apache/druid/pull/10366#discussion_r491217786", "createdAt": "2020-09-18T22:17:27Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -35,57 +39,47 @@\n import javax.annotation.Nullable;\n import java.util.Comparator;\n import java.util.List;\n+import java.util.Optional;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.function.Function;\n \n /**\n- * Utility methods for working with {@link Joinable} related classes.\n+ * A wrapper class over {@link JoinableFactory} for working with {@link Joinable} related classes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyMTcyOA==", "bodyText": "Please remove param and return tags if they don't need descriptions.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491221728", "createdAt": "2020-09-18T22:32:45Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyNzIyNw==", "bodyText": "Yeah, it doesn't seem necessary for now.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491227227", "createdAt": "2020-09-18T22:55:34Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return\n+   */\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n+  )\n+  {\n+\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      throw new IAE(\"No join clauses to build the cache key\");\n+    }\n+\n+    final CacheKeyBuilder keyBuilder;\n+    keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+    for (PreJoinableClause clause : clauses) {\n+      if (!clause.getCondition().canHashJoin()) {\n+        log.debug(\"skipping caching for join since [%s] does not support hash-join\", clause.getCondition());\n+        return Optional.empty();\n+      }\n+      Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource());\n+      if (!bytes.isPresent()) {\n+        // Encountered a data source which didn't support cache yet\n+        log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n+        return Optional.empty();\n+      }\n+      keyBuilder.appendByteArray(bytes.get());\n+      keyBuilder.appendString(clause.getPrefix());    //TODO - prefix shouldn't be required IMO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI3MzQzMw==", "bodyText": "I think this logic should be here instead of in SegmentId since a segmentId represents a whole segment while we can partially read a broadcast segment based on an interval filter in the future.\n@abhishekagarwal87 regarding using CacheUtil, if you just want to add a new method similar to computeSegmentCacheKey(), I think you don't have to since the logic is different anyway. I would rather suggest using CacheKeyBuilder.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491273433", "createdAt": "2020-09-19T05:36:34Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java", "diffHunk": "@@ -241,6 +244,22 @@ public ColumnSelectorFactory makeColumnSelectorFactory(ReadableOffset offset, bo\n     );\n   }\n \n+  @Override\n+  public Optional<byte[]> computeCacheKey()\n+  {\n+    SegmentId segmentId = segment.getId();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA==", "bodyText": "How about supplying a Supplier<Optional<byte[]>> to the CachingQueryRunner? Then, computing cacheKeyPrefix can be done only when useCache or populateCache is true which seems more legit.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491480204", "createdAt": "2020-09-19T18:22:31Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -197,13 +197,16 @@ public ServerManager(\n     }\n \n     // segmentMapFn maps each base Segment into a joined Segment if necessary.\n-    final Function<SegmentReference, SegmentReference> segmentMapFn = Joinables.createSegmentMapFn(\n+    final Function<SegmentReference, SegmentReference> segmentMapFn = joinables.createSegmentMapFn(\n         analysis.getPreJoinableClauses(),\n-        joinableFactory,\n         cpuTimeAccumulator,\n         analysis.getBaseQuery().orElse(query)\n     );\n \n+    final Optional<byte[]> cacheKeyPrefix = analysis.isJoin()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDQ5NA==", "bodyText": "I found a couple of more places with empty param or return tags. Please either add a description or remove them.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491480494", "createdAt": "2020-09-19T18:25:38Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyMTcyOA=="}, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MjMxNQ==", "bodyText": "Per contract of JoinableFactory, build() can return an Optional.empty() even when it is directlyJoinable. I think computeJoinCacheKey() should match with build(); it should return a computed cache key only when build() returns a Joinable.\nAs you mentioned at #10366 (comment), perhaps canHashJoin() could be checked on the caller side. I don't see why not for now except that you should also change the build() method and its callers to do the same. However, I'm not sure if that's better. That means, I'm not sure if this interface or the interface you may change makes sense for other join algorithms we will add in the future. I would suggest to keep this interface until we come up with a good one.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491482315", "createdAt": "2020-09-19T18:48:20Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/JoinableFactory.java", "diffHunk": "@@ -43,8 +43,18 @@\n    *\n    * @param dataSource the datasource to join on\n    * @param condition  the condition to join on\n-   *\n    * @return a Joinable if this datasource + condition combo is joinable; empty if not\n    */\n   Optional<Joinable> build(DataSource dataSource, JoinConditionAnalysis condition);\n+\n+  /**\n+   * Compute the cache key for a data source participating in join operation. This is done separately from {{@link #build(DataSource, JoinConditionAnalysis)}}\n+   * which can be an expensive operation and can potentially be avoided if cached results can be used.\n+   *\n+   * @param dataSource the datasource to join on\n+   */\n+  default Optional<byte[]> computeJoinCacheKey(DataSource dataSource)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4NTQ2Ng==", "bodyText": "Would you please add some comment on this variable? Since the cacheKey is not set in anywhere, people would not understand when it can be null or even how it could be set until they read this PR.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491585466", "createdAt": "2020-09-19T22:52:00Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/table/RowBasedIndexedTable.java", "diffHunk": "@@ -52,6 +54,8 @@\n   private final List<Function<RowType, Object>> columnFunctions;\n   private final Set<String> keyColumns;\n   private final String version;\n+  @Nullable\n+  private final byte[] cacheKey;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4ODQxOQ==", "bodyText": "Please include useful information in the error message such as datasource.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491588419", "createdAt": "2020-09-19T22:58:10Z", "author": {"login": "jihoonson"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return\n+   */\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n+  )\n+  {\n+\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      throw new IAE(\"No join clauses to build the cache key\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4ODY4NA==", "bodyText": "Please use expectedException and verifies the error message as well.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491588684", "createdAt": "2020-09-19T22:58:44Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5MzU0Ng==", "bodyText": "Should this be Assert.assertArrayEquals(cacheKey_1.get(), cacheKey_2.get());?", "url": "https://github.com/apache/druid/pull/10366#discussion_r491593546", "createdAt": "2020-09-19T23:08:46Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)\n+  public void test_computeJoinDataSourceCacheKey_noClauses()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    joinables.computeJoinDataSourceCacheKey(analysis);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_noHashJoin()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x != \\\"h.x\\\"\", \"h.\");\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertFalse(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_cachingUnsupported()\n+  {\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    DataSource dataSource = new LookupDataSource(\"lookup\");\n+    PreJoinableClause clause_2 = makePreJoinableClause(dataSource, \"x == \\\"h.x\\\"\", \"h.\", JoinType.LEFT);\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertFalse(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_usableClauses()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x == \\\"h.x\\\"\", \"h.\");\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertTrue(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithExpression()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"y == \\\"j.y\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithJoinType()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\", JoinType.LEFT);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\", JoinType.INNER);\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithPrefix()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"h.x\\\"\", \"h.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithJoinable()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_sameKeyForSameJoin()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5Mzk5OA==", "bodyText": "We don't use underscore in variable names. Please rename all variables containing underscores.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491593998", "createdAt": "2020-09-19T23:09:41Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)\n+  public void test_computeJoinDataSourceCacheKey_noClauses()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    joinables.computeJoinDataSourceCacheKey(analysis);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_noHashJoin()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NTA4NQ==", "bodyText": "Similar comment for other tests you added. Should they use Assert.assertFalse(Arrays.equals(cacheKey_1.get(), cacheKey_2.get()));?", "url": "https://github.com/apache/druid/pull/10366#discussion_r491595085", "createdAt": "2020-09-19T23:11:59Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)\n+  public void test_computeJoinDataSourceCacheKey_noClauses()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    joinables.computeJoinDataSourceCacheKey(analysis);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_noHashJoin()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x != \\\"h.x\\\"\", \"h.\");\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertFalse(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_cachingUnsupported()\n+  {\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    DataSource dataSource = new LookupDataSource(\"lookup\");\n+    PreJoinableClause clause_2 = makePreJoinableClause(dataSource, \"x == \\\"h.x\\\"\", \"h.\", JoinType.LEFT);\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertFalse(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_usableClauses()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x == \\\"h.x\\\"\", \"h.\");\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertTrue(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithExpression()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"y == \\\"j.y\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithJoinType()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\", JoinType.LEFT);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\", JoinType.INNER);\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithPrefix()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"h.x\\\"\", \"h.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithJoinable()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_sameKeyForSameJoin()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5MzU0Ng=="}, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NjIyMw==", "bodyText": "Please use expectedException and verify the error message as well.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491596223", "createdAt": "2020-09-19T23:14:20Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java", "diffHunk": "@@ -83,6 +84,36 @@ public void testBuildDataSourceIsRegisteredShouldReturnJoinableFromFactory()\n     EasyMock.replay(noopJoinableFactory);\n     Optional<Joinable> joinable = target.build(noopDataSource, condition);\n     Assert.assertEquals(mockJoinable, joinable.get());\n+\n+  }\n+\n+  @Test\n+  public void testComputeJoinCacheKey()\n+  {\n+    Optional<byte[]> expected = Optional.of(new byte[]{1, 2, 3});\n+    EasyMock.expect(noopJoinableFactory.computeJoinCacheKey(noopDataSource)).andReturn(expected);\n+    EasyMock.replay(noopJoinableFactory);\n+    Optional<byte[]> actual = target.computeJoinCacheKey(noopDataSource);\n+    Assert.assertEquals(expected, actual);\n+  }\n+\n+  @Test(expected = ISE.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NzY0NA==", "bodyText": "Should use Assert.assertArrayEquals().", "url": "https://github.com/apache/druid/pull/10366#discussion_r491597644", "createdAt": "2020-09-19T23:17:19Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java", "diffHunk": "@@ -83,6 +84,36 @@ public void testBuildDataSourceIsRegisteredShouldReturnJoinableFromFactory()\n     EasyMock.replay(noopJoinableFactory);\n     Optional<Joinable> joinable = target.build(noopDataSource, condition);\n     Assert.assertEquals(mockJoinable, joinable.get());\n+\n+  }\n+\n+  @Test\n+  public void testComputeJoinCacheKey()\n+  {\n+    Optional<byte[]> expected = Optional.of(new byte[]{1, 2, 3});\n+    EasyMock.expect(noopJoinableFactory.computeJoinCacheKey(noopDataSource)).andReturn(expected);\n+    EasyMock.replay(noopJoinableFactory);\n+    Optional<byte[]> actual = target.computeJoinCacheKey(noopDataSource);\n+    Assert.assertEquals(expected, actual);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMDEyOA==", "bodyText": "Please use Assert.assertArrayEquals(). Also the expected result should be the first argument.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491600128", "createdAt": "2020-09-19T23:22:26Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/table/RowBasedIndexedTableTest.java", "diffHunk": "@@ -179,4 +179,13 @@ public void testVersion()\n     Assert.assertEquals(JoinTestHelper.INDEXED_TABLE_VERSION, countriesTable.version());\n     Assert.assertEquals(JoinTestHelper.INDEXED_TABLE_VERSION, regionsTable.version());\n   }\n+\n+  @Test\n+  public void testIsCacheable() throws IOException\n+  {\n+    Assert.assertFalse(countriesTable.isCacheable());\n+    RowBasedIndexedTable<Map<String, Object>> countriesTableWithCacheKey = JoinTestHelper.createCountriesIndexedTableWithCacheKey();\n+    Assert.assertTrue(countriesTableWithCacheKey.isCacheable());\n+    Assert.assertEquals(countriesTableWithCacheKey.computeCacheKey(), JoinTestHelper.INDEXED_TABLE_CACHE_KEY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMTkxNA==", "bodyText": "Type argument is not required.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491601914", "createdAt": "2020-09-19T23:26:01Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java", "diffHunk": "@@ -224,8 +228,9 @@ public SinkQuerySegmentWalker(\n                       // 1) Only use caching if data is immutable\n                       // 2) Hydrants are not the same between replicas, make sure cache is local\n                       if (hydrantDefinitelySwapped && cache.isLocal()) {\n-                        runner = new CachingQueryRunner<>(\n+                        runner = new CachingQueryRunner<T>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMzA0NQ==", "bodyText": "Why is this change needed?", "url": "https://github.com/apache/druid/pull/10366#discussion_r491603045", "createdAt": "2020-09-19T23:28:24Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -293,8 +300,9 @@ public ServerManager(\n         queryMetrics -> queryMetrics.segment(segmentIdString)\n     );\n \n-    CachingQueryRunner<T> cachingQueryRunner = new CachingQueryRunner<>(\n+    QueryRunner<T> queryRunner = new CachingQueryRunner<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMzY3MA==", "bodyText": "Please use Intervals.utc() instead.", "url": "https://github.com/apache/druid/pull/10366#discussion_r491603670", "createdAt": "2020-09-19T23:29:40Z", "author": {"login": "jihoonson"}, "path": "server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java", "diffHunk": "@@ -340,4 +356,15 @@ private DataSegment createSegment(IncrementalIndex data, String interval, String\n         ImmutableMap.of(\"type\", \"local\", \"path\", segmentDir.getAbsolutePath())\n     );\n   }\n+\n+  private void assertSegmentIdEquals(SegmentId id, byte[] bytes)\n+  {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);\n+    long start = byteBuffer.getLong();\n+    long end = byteBuffer.getLong();\n+    String version = StringUtils.fromUtf8(byteBuffer, StringUtils.estimatedBinaryLengthAsUTF8(id.getVersion()));\n+    String dataSource = StringUtils.fromUtf8(byteBuffer, StringUtils.estimatedBinaryLengthAsUTF8(id.getDataSource()));\n+    int partition = byteBuffer.getInt();\n+    Assert.assertEquals(id, SegmentId.of(dataSource, new Interval(start, end, DateTimeZone.UTC), version, partition));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 72}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12fd5e8d823b9377589fcf1fbaf252d50727b14f", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/12fd5e8d823b9377589fcf1fbaf252d50727b14f", "committedDate": "2020-09-21T09:10:37Z", "message": "Fix compilation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "committedDate": "2020-09-21T13:06:59Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9f1ae8a946ba4c5fc22e3fd780ca8d170d96c72", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/e9f1ae8a946ba4c5fc22e3fd780ca8d170d96c72", "committedDate": "2020-09-22T14:40:53Z", "message": "Add more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9d758467af197e4beee954fba86676c6828941b", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/c9d758467af197e4beee954fba86676c6828941b", "committedDate": "2020-09-22T16:55:29Z", "message": "Fix inspection errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzOTE2MTkz", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-493916193", "createdAt": "2020-09-22T23:38:10Z", "commit": {"oid": "c9d758467af197e4beee954fba86676c6828941b"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzozODoxMVrOHWP8EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMzo0MTo0OFrOHWQAyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTg1Ng==", "bodyText": "Please use StringUtils.format() instead here and in other places.", "url": "https://github.com/apache/druid/pull/10366#discussion_r493091856", "createdAt": "2020-09-22T23:38:11Z", "author": {"login": "jihoonson"}, "path": "processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java", "diffHunk": "@@ -113,6 +118,12 @@ public void testBuildExceptionWhenTwoJoinableFactoryForSameDataSource()\n     EasyMock.expect(noopJoinableFactory.build(noopDataSource, condition)).andReturn(Optional.of(mockJoinable));\n     EasyMock.expect(anotherNoopJoinableFactory.build(noopDataSource, condition)).andReturn(Optional.of(mockJoinable));\n     EasyMock.replay(noopJoinableFactory, anotherNoopJoinableFactory);\n+    expectedException.expect(ISE.class);\n+    expectedException.expectMessage(String.format(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9d758467af197e4beee954fba86676c6828941b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MzA2Ng==", "bodyText": "This is nice \ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10366#discussion_r493093066", "createdAt": "2020-09-22T23:41:48Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -796,4 +749,102 @@ private void addSequencesFromServer(\n           .flatMerge(seq -> seq, query.getResultOrdering());\n     }\n   }\n+\n+  /**\n+   * An inner class that is used solely for computing cache keys. Its a separate class to allow extensive unit testing\n+   * of cache key generation.\n+   */\n+  @VisibleForTesting\n+  static class CacheKeyManager<T>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9d758467af197e4beee954fba86676c6828941b"}, "originalPosition": 126}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/f06269ad504cf0efbfa9939c9c746928ef75b542", "committedDate": "2020-09-23T08:28:12Z", "message": "Pushed condition analysis to JoinableFactory"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk0OTMxNzE3", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-494931717", "createdAt": "2020-09-23T18:26:09Z", "commit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MDMzOTU2", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-496033956", "createdAt": "2020-09-25T00:27:36Z", "commit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMDoyNzozNlrOHXw8rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMToxMzowOFrOHXxpww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4MTI2MA==", "bodyText": "Consider naming this getCacheKey() and extending Cacheable. That interface is mainly used by CacheKeyBuilder, which it doesn't look like we actually need here, but it's still nice to use that interface for things that can generate their own cache keys.", "url": "https://github.com/apache/druid/pull/10366#discussion_r494681260", "createdAt": "2020-09-25T00:27:36Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/segment/join/table/IndexedTable.java", "diffHunk": "@@ -87,6 +88,26 @@ default ColumnSelectorFactory makeColumnSelectorFactory(ReadableOffset offset, b\n     return null;\n   }\n \n+  /**\n+   * Computes a {@code byte[]} key for the table that can be used for computing cache keys for join operations.\n+   * see {@link org.apache.druid.segment.join.JoinableFactory#computeJoinCacheKey}\n+   *\n+   * @return the byte array for cache key\n+   * @throws {@link IAE} if caching is not supported\n+   */\n+  default byte[] computeCacheKey()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4MTQ0NA==", "bodyText": "I think it'd be better to make the change now, since \"Joinables\" is meant to be a non-constructible holder of utility functions, and so making it constructible is unnecessarily changing its character. As to cluttering the PR, IMO adding a new class is likely to yield a less cluttered PR, because it won't involve changes to the pre-existing utility class.", "url": "https://github.com/apache/druid/pull/10366#discussion_r494681444", "createdAt": "2020-09-25T00:28:22Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -35,57 +39,47 @@\n import javax.annotation.Nullable;\n import java.util.Comparator;\n import java.util.List;\n+import java.util.Optional;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.function.Function;\n \n /**\n- * Utility methods for working with {@link Joinable} related classes.\n+ * A wrapper class over {@link JoinableFactory} for working with {@link Joinable} related classes.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng=="}, "originalCommit": {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4Mjk5Mg==", "bodyText": "I think\u00a0you just copied this from somewhere else, but IMO it would be better as a precondition check (i.e. throw ISE or NPE if it fails) instead of an assertion.\nGenerally we'll use assertions to note things that are meant to be impossible, and precondition checks for things that are possible but are incorrect usage. Since it is possible to create this CacheKeyManager class with a null strategy and then call computeQueryCacheKeyWithJoin, it'd be better for this to be a precondition check.", "url": "https://github.com/apache/druid/pull/10366#discussion_r494682992", "createdAt": "2020-09-25T00:34:24Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -770,4 +749,102 @@ private void addSequencesFromServer(\n           .flatMerge(seq -> seq, query.getResultOrdering());\n     }\n   }\n+\n+  /**\n+   * An inner class that is used solely for computing cache keys. Its a separate class to allow extensive unit testing\n+   * of cache key generation.\n+   */\n+  @VisibleForTesting\n+  static class CacheKeyManager<T>\n+  {\n+    private final Query<T> query;\n+    private final CacheStrategy<T, Object, Query<T>> strategy;\n+    private final DataSourceAnalysis dataSourceAnalysis;\n+    private final Joinables joinables;\n+    private final boolean isSegmentLevelCachingEnable;\n+\n+    CacheKeyManager(\n+        final Query<T> query,\n+        final CacheStrategy<T, Object, Query<T>> strategy,\n+        final boolean useCache,\n+        final boolean populateCache,\n+        final DataSourceAnalysis dataSourceAnalysis,\n+        final Joinables joinables\n+    )\n+    {\n+\n+      this.query = query;\n+      this.strategy = strategy;\n+      this.dataSourceAnalysis = dataSourceAnalysis;\n+      this.joinables = joinables;\n+      this.isSegmentLevelCachingEnable = ((populateCache || useCache)\n+                                          && !QueryContexts.isBySegment(query));   // explicit bySegment queries are never cached\n+\n+    }\n+\n+    @Nullable\n+    byte[] computeSegmentLevelQueryCacheKey()\n+    {\n+      if (isSegmentLevelCachingEnable) {\n+        return computeQueryCacheKeyWithJoin();\n+      }\n+      return null;\n+    }\n+\n+    /**\n+     * It computes the ETAG which is used by {@link org.apache.druid.query.ResultLevelCachingQueryRunner} for\n+     * result level caches. queryCacheKey can be null if segment level cache is not being used. However, ETAG\n+     * is still computed since result level cache may still be on.\n+     */\n+    @Nullable\n+    String computeResultLevelCachingEtag(\n+        final Set<SegmentServerSelector> segments,\n+        @Nullable byte[] queryCacheKey\n+    )\n+    {\n+      Hasher hasher = Hashing.sha1().newHasher();\n+      boolean hasOnlyHistoricalSegments = true;\n+      for (SegmentServerSelector p : segments) {\n+        if (!p.getServer().pick().getServer().isSegmentReplicationTarget()) {\n+          hasOnlyHistoricalSegments = false;\n+          break;\n+        }\n+        hasher.putString(p.getServer().getSegment().getId().toString(), StandardCharsets.UTF_8);\n+        // it is important to add the \"query interval\" as part ETag calculation\n+        // to have result level cache work correctly for queries with different\n+        // intervals covering the same set of segments\n+        hasher.putString(p.rhs.getInterval().toString(), StandardCharsets.UTF_8);\n+      }\n+\n+      if (!hasOnlyHistoricalSegments) {\n+        return null;\n+      }\n+\n+      // query cache key can be null if segment level caching is disabled\n+      final byte[] queryCacheKeyFinal = (queryCacheKey == null) ? computeQueryCacheKeyWithJoin() : queryCacheKey;\n+      if (queryCacheKeyFinal == null) {\n+        return null;\n+      }\n+      hasher.putBytes(queryCacheKeyFinal);\n+      String currEtag = StringUtils.encodeBase64String(hasher.hash().asBytes());\n+      return currEtag;\n+    }\n+\n+    /**\n+     * Adds the cache key prefix for join data sources. Return null if its a join but caching is not supported\n+     */\n+    @Nullable\n+    private byte[] computeQueryCacheKeyWithJoin()\n+    {\n+      assert strategy != null;  // implies strategy != null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA==", "bodyText": "I don't think the concept of computing cache keys in CachingClusteredClient works properly here, because of the following scenario:\n\nThere is a broadcast table that we'll be joining against.\nThe Broker (which runs CCC) updates its broadcast table to a newer version.\nThe Broker gets a query and uses the newer version in the cache key.\nIt fans out the query to a data server that hasn't got the newest broadcast table yet.\nThe data server returns results for the older table, and the Broker caches them.\nNow, the Broker's cache is wrong (it refers to older data with a newer key).\n\nThe solution that comes to mind is that the data servers should keep both the old and new version around for a bit when they swap them out, and the Broker should send the specific version that it wants them to use, so it can be sure it's getting the right one. But this is out of the scope of this PR. For now I suggest not implementing caching for join datasources on the Broker. (Broker caching is off by default, anyway, so it's not the end of the world.)", "url": "https://github.com/apache/druid/pull/10366#discussion_r494684538", "createdAt": "2020-09-25T00:39:07Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -293,6 +301,14 @@ private ClusterQueryResult(Sequence<T> sequence, int numQueryServers)\n       this.intervals = dataSourceAnalysis.getBaseQuerySegmentSpec()\n                                          .map(QuerySegmentSpec::getIntervals)\n                                          .orElseGet(() -> query.getIntervals());\n+      this.cacheKeyManager = new CacheKeyManager<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDMxMw==", "bodyText": "Is it not possible for strategy to be null?", "url": "https://github.com/apache/druid/pull/10366#discussion_r494690313", "createdAt": "2020-09-25T01:02:18Z", "author": {"login": "gianm"}, "path": "server/src/main/java/org/apache/druid/client/CachingQueryRunner.java", "diffHunk": "@@ -77,20 +83,15 @@ public CachingQueryRunner(\n   {\n     Query<T> query = queryPlus.getQuery();\n     final CacheStrategy strategy = toolChest.getCacheStrategy(query);\n-    final boolean populateCache = CacheUtil.isPopulateSegmentCache(\n-        query,\n-        strategy,\n-        cacheConfig,\n-        CacheUtil.ServerType.DATA\n-    );\n-    final boolean useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.DATA);\n+    final boolean populateCache = canPopulateCache(query, strategy);\n+    final boolean useCache = canUseCache(query, strategy);\n \n     final Cache.NamedKey key;\n-    if (strategy != null && (useCache || populateCache)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MTg4Mg==", "bodyText": "I think there actually should be a prefix byte here, because the prefix bytes' purpose is to prevent cache key collisions for two implementations that are different but compute cache keys the same way. So each implementation should have its own prefix byte.\nI don't have a strong opinion on where the code should live, but it would be good to share it somehow, if that's not too much trouble.", "url": "https://github.com/apache/druid/pull/10366#discussion_r494691882", "createdAt": "2020-09-25T01:09:06Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java", "diffHunk": "@@ -241,6 +244,22 @@ public ColumnSelectorFactory makeColumnSelectorFactory(ReadableOffset offset, bo\n     );\n   }\n \n+  @Override\n+  public Optional<byte[]> computeCacheKey()\n+  {\n+    SegmentId segmentId = segment.getId();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, "originalCommit": {"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MjgwMw==", "bodyText": "The clause's prefix is important too, because it controls the names of the columns.", "url": "https://github.com/apache/druid/pull/10366#discussion_r494692803", "createdAt": "2020-09-25T01:13:08Z", "author": {"login": "gianm"}, "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +107,68 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis for the join datasource\n+   * @return the optional cache key to be used as part of query cache key\n+   */\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n+  )\n+  {\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      throw new IAE(\"No join clauses to build the cache key for data source [%s]\", dataSourceAnalysis.getDataSource());\n+    }\n+\n+    final CacheKeyBuilder keyBuilder;\n+    keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+    for (PreJoinableClause clause : clauses) {\n+      Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource(), clause.getCondition());\n+      if (!bytes.isPresent()) {\n+        // Encountered a data source which didn't support cache yet\n+        log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n+        return Optional.empty();\n+      }\n+      keyBuilder.appendByteArray(bytes.get());\n+      keyBuilder.appendString(clause.getCondition().getOriginalExpression());\n+      keyBuilder.appendString(clause.getJoinType().name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542"}, "originalPosition": 118}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61ed8df518cecb725bbf9155618ffcac693f476f", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/61ed8df518cecb725bbf9155618ffcac693f476f", "committedDate": "2020-09-28T07:05:20Z", "message": "review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e153fa70cb0610870f76eb9a524cbf20794e0ff5", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/e153fa70cb0610870f76eb9a524cbf20794e0ff5", "committedDate": "2020-09-29T14:21:18Z", "message": "Disable join caching for broker and add prefix key to BroadcastSegmentIndexedTable"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyNzQ0OTk4", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-502744998", "createdAt": "2020-10-06T09:20:36Z", "commit": {"oid": "e153fa70cb0610870f76eb9a524cbf20794e0ff5"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwOToyMDozN1rOHc9gVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwOToyMDozN1rOHc9gVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDEyOTg3Nw==", "bodyText": "JoinableFactoryWrapper still doesn't seem quite right. These walkers all take a JoinableFactory just to make a JoinableFactoryWrapper in the constructor. JoinableFactoryWrapper doesn't seem to have any state of its own, why not just make it in the joinable module and inject it directly? Or maybe it didn't really need to change from static methods?\nRegardless, it doesn't need to change in this PR, we can refactor this in the future, since it feels like maybe there is also another refactor lurking that I haven't quite figured out in wrapping CacheConfig, CachePopulator, Cache (and maybe CacheKeyManager for brokers) into some tidy package to handle caching stuffs for walkers with implementations that do the right thing for where they are running.", "url": "https://github.com/apache/druid/pull/10366#discussion_r500129877", "createdAt": "2020-10-06T09:20:37Z", "author": {"login": "clintropolis"}, "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -117,7 +117,7 @@ public ServerManager(\n \n     this.cacheConfig = cacheConfig;\n     this.segmentManager = segmentManager;\n-    this.joinableFactory = joinableFactory;\n+    this.joinableFactoryWrapper = new JoinableFactoryWrapper(joinableFactory);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e153fa70cb0610870f76eb9a524cbf20794e0ff5"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzMjU0NzEx", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-503254711", "createdAt": "2020-10-06T18:38:20Z", "commit": {"oid": "e153fa70cb0610870f76eb9a524cbf20794e0ff5"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODozODoyMFrOHdVFTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxODozODoyMFrOHdVFTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUxNjE3Mg==", "bodyText": "Please remove these lines. We can add them back later when we enable it.", "url": "https://github.com/apache/druid/pull/10366#discussion_r500516172", "createdAt": "2020-10-06T18:38:20Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -770,4 +749,103 @@ private void addSequencesFromServer(\n           .flatMerge(seq -> seq, query.getResultOrdering());\n     }\n   }\n+\n+  /**\n+   * An inner class that is used solely for computing cache keys. Its a separate class to allow extensive unit testing\n+   * of cache key generation.\n+   */\n+  @VisibleForTesting\n+  static class CacheKeyManager<T>\n+  {\n+    private final Query<T> query;\n+    private final CacheStrategy<T, Object, Query<T>> strategy;\n+    private final DataSourceAnalysis dataSourceAnalysis;\n+    private final JoinableFactoryWrapper joinableFactoryWrapper;\n+    private final boolean isSegmentLevelCachingEnable;\n+\n+    CacheKeyManager(\n+        final Query<T> query,\n+        final CacheStrategy<T, Object, Query<T>> strategy,\n+        final boolean useCache,\n+        final boolean populateCache,\n+        final DataSourceAnalysis dataSourceAnalysis,\n+        final JoinableFactoryWrapper joinableFactoryWrapper\n+    )\n+    {\n+\n+      this.query = query;\n+      this.strategy = strategy;\n+      this.dataSourceAnalysis = dataSourceAnalysis;\n+      this.joinableFactoryWrapper = joinableFactoryWrapper;\n+      this.isSegmentLevelCachingEnable = ((populateCache || useCache)\n+                                          && !QueryContexts.isBySegment(query));   // explicit bySegment queries are never cached\n+\n+    }\n+\n+    @Nullable\n+    byte[] computeSegmentLevelQueryCacheKey()\n+    {\n+      if (isSegmentLevelCachingEnable) {\n+        return computeQueryCacheKeyWithJoin();\n+      }\n+      return null;\n+    }\n+\n+    /**\n+     * It computes the ETAG which is used by {@link org.apache.druid.query.ResultLevelCachingQueryRunner} for\n+     * result level caches. queryCacheKey can be null if segment level cache is not being used. However, ETAG\n+     * is still computed since result level cache may still be on.\n+     */\n+    @Nullable\n+    String computeResultLevelCachingEtag(\n+        final Set<SegmentServerSelector> segments,\n+        @Nullable byte[] queryCacheKey\n+    )\n+    {\n+      Hasher hasher = Hashing.sha1().newHasher();\n+      boolean hasOnlyHistoricalSegments = true;\n+      for (SegmentServerSelector p : segments) {\n+        if (!p.getServer().pick().getServer().isSegmentReplicationTarget()) {\n+          hasOnlyHistoricalSegments = false;\n+          break;\n+        }\n+        hasher.putString(p.getServer().getSegment().getId().toString(), StandardCharsets.UTF_8);\n+        // it is important to add the \"query interval\" as part ETag calculation\n+        // to have result level cache work correctly for queries with different\n+        // intervals covering the same set of segments\n+        hasher.putString(p.rhs.getInterval().toString(), StandardCharsets.UTF_8);\n+      }\n+\n+      if (!hasOnlyHistoricalSegments) {\n+        return null;\n+      }\n+\n+      // query cache key can be null if segment level caching is disabled\n+      final byte[] queryCacheKeyFinal = (queryCacheKey == null) ? computeQueryCacheKeyWithJoin() : queryCacheKey;\n+      if (queryCacheKeyFinal == null) {\n+        return null;\n+      }\n+      hasher.putBytes(queryCacheKeyFinal);\n+      String currEtag = StringUtils.encodeBase64String(hasher.hash().asBytes());\n+      return currEtag;\n+    }\n+\n+    /**\n+     * Adds the cache key prefix for join data sources. Return null if its a join but caching is not supported\n+     */\n+    @Nullable\n+    private byte[] computeQueryCacheKeyWithJoin()\n+    {\n+      Preconditions.checkNotNull(strategy, \"strategy cannot be null\");\n+      if (dataSourceAnalysis.isJoin()) {\n+        return null; // Broker join caching disabled - https://github.com/apache/druid/issues/10444\n+       /* byte[] joinDataSourceCacheKey = joinableFactoryWrapper.computeJoinDataSourceCacheKey(dataSourceAnalysis).orElse(null);\n+        if (null == joinDataSourceCacheKey) {\n+          return null;    // A join operation that does not support caching\n+        }\n+        return Bytes.concat(joinDataSourceCacheKey, strategy.computeCacheKey(query));*/", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e153fa70cb0610870f76eb9a524cbf20794e0ff5"}, "originalPosition": 231}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c10be24e9571deee57fbde958b80862634fb7e9e", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/c10be24e9571deee57fbde958b80862634fb7e9e", "committedDate": "2020-10-07T13:28:49Z", "message": "Remove commented lines"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91c3532abef45b4d7a4759848fc7074847d36f00", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/91c3532abef45b4d7a4759848fc7074847d36f00", "committedDate": "2020-10-08T10:49:54Z", "message": "Fix populateCache"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA1MjQ5NjYz", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-505249663", "createdAt": "2020-10-08T23:45:52Z", "commit": {"oid": "91c3532abef45b4d7a4759848fc7074847d36f00"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMzo0NTo1MlrOHe0qdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMzo0NTo1OVrOHe0qlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA4MjE2Nw==", "bodyText": "I think this check should be not here, but inside CacheUtil.isUseSegmentCache(). The reason we pass the ServerType in isUseSegmentCache() is that we want to make different decisions based on the ServerType. We are disabling the cache for join on brokers, so it should be in there.", "url": "https://github.com/apache/druid/pull/10366#discussion_r502082167", "createdAt": "2020-10-08T23:45:52Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -289,14 +289,17 @@ private ClusterQueryResult(Sequence<T> sequence, int numQueryServers)\n       this.query = queryPlus.getQuery();\n       this.toolChest = warehouse.getToolChest(query);\n       this.strategy = toolChest.getCacheStrategy(query);\n+      this.dataSourceAnalysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n \n-      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n-      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n+      // Broker join caching is disabled - https://github.com/apache/druid/issues/10444\n+      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n+                      && !dataSourceAnalysis.isJoin();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91c3532abef45b4d7a4759848fc7074847d36f00"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA4MjE5OA==", "bodyText": "Same comment.", "url": "https://github.com/apache/druid/pull/10366#discussion_r502082198", "createdAt": "2020-10-08T23:45:59Z", "author": {"login": "jihoonson"}, "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -289,14 +289,17 @@ private ClusterQueryResult(Sequence<T> sequence, int numQueryServers)\n       this.query = queryPlus.getQuery();\n       this.toolChest = warehouse.getToolChest(query);\n       this.strategy = toolChest.getCacheStrategy(query);\n+      this.dataSourceAnalysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n \n-      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n-      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n+      // Broker join caching is disabled - https://github.com/apache/druid/issues/10444\n+      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n+                      && !dataSourceAnalysis.isJoin();\n+      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "91c3532abef45b4d7a4759848fc7074847d36f00"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1a62f8b333da76aa814e3c7b89da95135697645", "author": {"user": {"login": "abhishekagarwal87", "name": "Abhishek Agarwal"}}, "url": "https://github.com/apache/druid/commit/f1a62f8b333da76aa814e3c7b89da95135697645", "committedDate": "2020-10-09T15:32:31Z", "message": "Disable caching for selective datasources\n\nRefactored the code so that we can decide at the data source level, whether to enable cache for broker or data nodes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDExMTIx", "url": "https://github.com/apache/druid/pull/10366#pullrequestreview-506011121", "createdAt": "2020-10-09T22:05:06Z", "commit": {"oid": "f1a62f8b333da76aa814e3c7b89da95135697645"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3587, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}