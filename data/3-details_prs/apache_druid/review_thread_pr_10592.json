{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIyODY1MzQx", "number": 10592, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwNzo1NTowMlrOE6cE0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwODowMToyMVrOE6cNtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NzEyODQ4OnYy", "diffSide": "RIGHT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwNzo1NTowM1rOH1g9ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOTozNToxNVrOH1-34g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTg3NjYzOA==", "bodyText": "this log statement needs a change now", "url": "https://github.com/apache/druid/pull/10592#discussion_r525876638", "createdAt": "2020-11-18T07:55:03Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -541,9 +561,12 @@ private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throw\n       );\n     }\n \n-    final Integer numShardsOverride;\n+    final Map<Interval, Integer> intervalToNumShards;\n     HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n-    if (partitionsSpec.getNumShards() == null) {\n+    final boolean needsInputSampling =\n+        partitionsSpec.getNumShards() == null\n+        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n+    if (needsInputSampling) {\n       // 0. need to determine numShards by scanning the data\n       LOG.info(\"numShards is unspecified, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bef2ff2a03384f77d312ff8a13cf7f98da6f3b0"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM2NjY5MA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/druid/pull/10592#discussion_r526366690", "createdAt": "2020-11-18T19:35:15Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -541,9 +561,12 @@ private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throw\n       );\n     }\n \n-    final Integer numShardsOverride;\n+    final Map<Interval, Integer> intervalToNumShards;\n     HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n-    if (partitionsSpec.getNumShards() == null) {\n+    final boolean needsInputSampling =\n+        partitionsSpec.getNumShards() == null\n+        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n+    if (needsInputSampling) {\n       // 0. need to determine numShards by scanning the data\n       LOG.info(\"numShards is unspecified, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTg3NjYzOA=="}, "originalCommit": {"oid": "9bef2ff2a03384f77d312ff8a13cf7f98da6f3b0"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NzE1MTI3OnYy", "diffSide": "LEFT", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwODowMToyMVrOH1hKqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxOTozNToxNlrOH1-36A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTg3OTk3Ng==", "bodyText": "is there an equivalent info being logged somewhere now?", "url": "https://github.com/apache/druid/pull/10592#discussion_r525879976", "createdAt": "2020-11-18T08:01:21Z", "author": {"login": "abhishekagarwal87"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -552,38 +575,50 @@ private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throw\n               this::createPartialDimensionCardinalityRunner\n           );\n \n-      if (cardinalityRunner == null) {\n-        throw new ISE(\"Could not create cardinality runner for hash partitioning.\");\n-      }\n-\n       state = runNextPhase(cardinalityRunner);\n       if (state.isFailure()) {\n         return TaskStatus.failure(getId());\n       }\n \n-      int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n-                                       ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n-                                       : partitionsSpec.getMaxRowsPerSegment();\n-      LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n+      if (cardinalityRunner.getReports().isEmpty()) {\n+        String msg = \"No valid rows for hash partitioning.\"\n+                     + \" All rows may have invalid timestamps or have been filtered out.\";\n+        LOG.warn(msg);\n+        return TaskStatus.success(getId(), msg);\n+      }\n+\n+      if (partitionsSpec.getNumShards() == null) {\n+        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n+                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n+                                         : partitionsSpec.getMaxRowsPerSegment();\n+        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n \n-      if (cardinalityRunner.getReports() == null) {\n-        throw new ISE(\"Could not determine cardinalities for hash partitioning.\");\n+        intervalToNumShards = determineNumShardsFromCardinalityReport(\n+            cardinalityRunner.getReports().values(),\n+            effectiveMaxRowsPerSegment\n+        );\n+      } else {\n+        intervalToNumShards = CollectionUtils.mapValues(\n+            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n+            k -> partitionsSpec.getNumShards()\n+        );\n       }\n-      numShardsOverride = determineNumShardsFromCardinalityReport(\n-          cardinalityRunner.getReports().values(),\n-          effectiveMaxRowsPerSegment\n-      );\n \n-      LOG.info(\"Automatically determined numShards: \" + numShardsOverride);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bef2ff2a03384f77d312ff8a13cf7f98da6f3b0"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM2NjY5Ng==", "bodyText": "No, I didn't add one for intervalToNumShards because it could have lots of intervals.", "url": "https://github.com/apache/druid/pull/10592#discussion_r526366696", "createdAt": "2020-11-18T19:35:16Z", "author": {"login": "jihoonson"}, "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java", "diffHunk": "@@ -552,38 +575,50 @@ private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throw\n               this::createPartialDimensionCardinalityRunner\n           );\n \n-      if (cardinalityRunner == null) {\n-        throw new ISE(\"Could not create cardinality runner for hash partitioning.\");\n-      }\n-\n       state = runNextPhase(cardinalityRunner);\n       if (state.isFailure()) {\n         return TaskStatus.failure(getId());\n       }\n \n-      int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n-                                       ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n-                                       : partitionsSpec.getMaxRowsPerSegment();\n-      LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n+      if (cardinalityRunner.getReports().isEmpty()) {\n+        String msg = \"No valid rows for hash partitioning.\"\n+                     + \" All rows may have invalid timestamps or have been filtered out.\";\n+        LOG.warn(msg);\n+        return TaskStatus.success(getId(), msg);\n+      }\n+\n+      if (partitionsSpec.getNumShards() == null) {\n+        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n+                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n+                                         : partitionsSpec.getMaxRowsPerSegment();\n+        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n \n-      if (cardinalityRunner.getReports() == null) {\n-        throw new ISE(\"Could not determine cardinalities for hash partitioning.\");\n+        intervalToNumShards = determineNumShardsFromCardinalityReport(\n+            cardinalityRunner.getReports().values(),\n+            effectiveMaxRowsPerSegment\n+        );\n+      } else {\n+        intervalToNumShards = CollectionUtils.mapValues(\n+            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n+            k -> partitionsSpec.getNumShards()\n+        );\n       }\n-      numShardsOverride = determineNumShardsFromCardinalityReport(\n-          cardinalityRunner.getReports().values(),\n-          effectiveMaxRowsPerSegment\n-      );\n \n-      LOG.info(\"Automatically determined numShards: \" + numShardsOverride);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTg3OTk3Ng=="}, "originalCommit": {"oid": "9bef2ff2a03384f77d312ff8a13cf7f98da6f3b0"}, "originalPosition": 154}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2860, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}