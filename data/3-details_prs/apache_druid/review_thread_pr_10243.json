{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYzNTY3Mjk1", "number": 10243, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxNjozOTo1MlrOEYicIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwOToxMTozNlrOEabMTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MTY1NTM2OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxNjozOTo1MlrOHA7VJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQxODowMTo0NFrOHA9ywg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDczNDExOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them.|5000|no|\n          \n          \n            \n            |maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them.|1000|no|", "url": "https://github.com/apache/druid/pull/10243#discussion_r470734118", "createdAt": "2020-08-14T16:39:52Z", "author": {"login": "abhishekagarwal87"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). [Human-readable format](../configuration/human-readable-byte.md) is supported.|500MiB|no|\n+|maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them.|5000|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93a6f45cf0b64d8e0ab6eca097a4e9bccaab65e9"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc3NDQ2Ng==", "bodyText": "Oops, nice catch!", "url": "https://github.com/apache/druid/pull/10243#discussion_r470774466", "createdAt": "2020-08-14T18:01:44Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). [Human-readable format](../configuration/human-readable-byte.md) is supported.|500MiB|no|\n+|maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them.|5000|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDczNDExOA=="}, "originalCommit": {"oid": "93a6f45cf0b64d8e0ab6eca097a4e9bccaab65e9"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjY0MzY3OnYy", "diffSide": "LEFT", "path": "integration-tests/src/test/resources/indexer/wikipedia_parallel_ingest_segment_index_task.json", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjoyMjoyM1rOHBEy7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMTozNDo0OVrOHCAQTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4OTE5OA==", "bodyText": "Should we keep one of these tests to use maxSplitSize so that we know that the old specs continue to work", "url": "https://github.com/apache/druid/pull/10243#discussion_r470889198", "createdAt": "2020-08-14T22:22:23Z", "author": {"login": "suneet-s"}, "path": "integration-tests/src/test/resources/indexer/wikipedia_parallel_ingest_segment_index_task.json", "diffHunk": "@@ -61,7 +61,7 @@\n       \"forceGuaranteedRollup\": \"%%FORCE_GUARANTEED_ROLLUP%%\",\n       \"splitHintSpec\": {\n         \"type\": \"maxSize\",\n-        \"maxSplitSize\": 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2MzM3NA==", "bodyText": "\ud83d\udc4d Changed wikipedia_parallel_index_task.json back to use maxSplitSize.", "url": "https://github.com/apache/druid/pull/10243#discussion_r471863374", "createdAt": "2020-08-18T01:34:49Z", "author": {"login": "jihoonson"}, "path": "integration-tests/src/test/resources/indexer/wikipedia_parallel_ingest_segment_index_task.json", "diffHunk": "@@ -61,7 +61,7 @@\n       \"forceGuaranteedRollup\": \"%%FORCE_GUARANTEED_ROLLUP%%\",\n       \"splitHintSpec\": {\n         \"type\": \"maxSize\",\n-        \"maxSplitSize\": 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg4OTE5OA=="}, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjY1MzE2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjoyODoxNlrOHBE4jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjoyODoxNlrOHBE4jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5MDYzNg==", "bodyText": "This is a great comment! Thank you!", "url": "https://github.com/apache/druid/pull/10243#discussion_r470890636", "createdAt": "2020-08-14T22:28:16Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +44,53 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is consertively chosen as 1000.\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjY3MDA4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjozNzo0NlrOHBFCIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMTozNDo1MlrOHCAQYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5MzA5MQ==", "bodyText": "Should we add a preconditionCheck on maxNumFiles? If the user has entered maxNumFiles, it should be a positive number >= 1\nI think what you've implemented works even with negative numbers, but maybe it's better to tell the user they're doing something strange.", "url": "https://github.com/apache/druid/pull/10243#discussion_r470893091", "createdAt": "2020-08-14T22:37:46Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +44,53 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is consertively chosen as 1000.\n+   */\n+  @VisibleForTesting\n+  static final int DEFAULT_MAX_NUM_FILES = 1000;\n+\n+  private final HumanReadableBytes maxSplitSize;\n+  private final int maxNumFiles;\n \n   @JsonCreator\n-  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  public MaxSizeSplitHintSpec(\n+      @JsonProperty(\"maxSplitSize\") @Nullable HumanReadableBytes maxSplitSize,\n+      @JsonProperty(\"maxNumFiles\") @Nullable Integer maxNumFiles\n+  )\n   {\n     this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+    this.maxNumFiles = maxNumFiles == null ? DEFAULT_MAX_NUM_FILES : maxNumFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2MzM5Mw==", "bodyText": "Good call. Added a check for both maxSplitsize and maxNumFiles.", "url": "https://github.com/apache/druid/pull/10243#discussion_r471863393", "createdAt": "2020-08-18T01:34:52Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +44,53 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is consertively chosen as 1000.\n+   */\n+  @VisibleForTesting\n+  static final int DEFAULT_MAX_NUM_FILES = 1000;\n+\n+  private final HumanReadableBytes maxSplitSize;\n+  private final int maxNumFiles;\n \n   @JsonCreator\n-  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  public MaxSizeSplitHintSpec(\n+      @JsonProperty(\"maxSplitSize\") @Nullable HumanReadableBytes maxSplitSize,\n+      @JsonProperty(\"maxNumFiles\") @Nullable Integer maxNumFiles\n+  )\n   {\n     this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+    this.maxNumFiles = maxNumFiles == null ? DEFAULT_MAX_NUM_FILES : maxNumFiles;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5MzA5MQ=="}, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjY3ODc1OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjo0Mzo0OFrOHBFHSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMTozNDo1NFrOHCAQcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5NDQxMQ==", "bodyText": "Is there a pattern to document the relationship between these 2 configs.\nReading these docs I'm not sure if I need to consider one when setting the other or which one takes precedence.", "url": "https://github.com/apache/druid/pull/10243#discussion_r470894411", "createdAt": "2020-08-14T22:43:48Z", "author": {"login": "suneet-s"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). [Human-readable format](../configuration/human-readable-byte.md) is supported.|500MiB|no|\n+|maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them.|1000|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2MzQwOA==", "bodyText": "Thanks, they don't take precedence over each other, but split boundary will be determined when either of them hits. Added some more clarification.", "url": "https://github.com/apache/druid/pull/10243#discussion_r471863408", "createdAt": "2020-08-18T01:34:54Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). [Human-readable format](../configuration/human-readable-byte.md) is supported.|500MiB|no|\n+|maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them.|1000|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5NDQxMQ=="}, "originalCommit": {"oid": "65657f6eec028817ce32f90dcea15e382e3ed3ef"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTk2NTQyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNjozMjo0NFrOHCFNAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNjo0NTo1NlrOHDQTAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk0NDQ0OA==", "bodyText": "This change seems reasonable to me, but since we're changing the default behavior from 512 MiB to 1GiB, can we add a system property to override this default value. In case users are doing an upgrade to 0.20 and want the old behavior, the system property would give them a way. to have the previous default.", "url": "https://github.com/apache/druid/pull/10243#discussion_r471944448", "createdAt": "2020-08-18T06:32:44Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -44,7 +45,7 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjU0NTQ1Mg==", "bodyText": "Hmm, good question. I would say it could be more useful to have a way to apply different default configurations per datasource since the maxSplitSize should be adjusted based on the shape of input data and partitioning scheme of output data. But for this, I think it could be better to add a supervisor which periodically performs batch ingestion based on the user-provided configurations.\nParticularly regarding keeping the previous default, I'm not sure when it would be good to do. maxSplitSize is mostly for controlling the parallelism of the phase which reads data from inputSource in parallel indexing, but it also affects the number of segments created after the input-read phase. So, there is a trade-off between them. However, I would say increasing maxSplitSize 512 MB to 1 GB wouldn't change things dramatically. If you have a cluster where all subtasks split by the previous default can run at the same time but not with the new default, you might want to use the previous default because, in theory, it will give you 2 times better read performance. However, in practice, you would likely have more than one task to run at the same time, which the cluster resource should be shared across.\nWhat do you think?", "url": "https://github.com/apache/druid/pull/10243#discussion_r472545452", "createdAt": "2020-08-18T23:19:06Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -44,7 +45,7 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk0NDQ0OA=="}, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjY4NzY2OQ==", "bodyText": "Thinking about this more, I can't think of a good reason to have a cluster wide fallback option, other than my paranoia \ud83d\ude05\nYour analysis makes sense to me, and while adding the ability to automatically adjust this based on the shape and partitioning scheme of the data sounds really cool, it's definitely beyond the scope of this change", "url": "https://github.com/apache/druid/pull/10243#discussion_r472687669", "createdAt": "2020-08-19T04:46:09Z", "author": {"login": "suneet-s"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -44,7 +45,7 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk0NDQ0OA=="}, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE3NDc4Nw==", "bodyText": "Your analysis makes sense to me, and while adding the ability to automatically adjust this based on the shape and partitioning scheme of the data sounds really cool, it's definitely beyond the scope of this change\n\nOh, I originally meant a supervisor working based on fixed user configurations, but yeah auto adjusting would be nice.", "url": "https://github.com/apache/druid/pull/10243#discussion_r473174787", "createdAt": "2020-08-19T16:45:56Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -44,7 +45,7 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"512MiB\");\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk0NDQ0OA=="}, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODUxMDQ3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMDo1Mjo1OFrOHDYqlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwNzoyNjozMlrOHDv0OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzMxMTg5NQ==", "bodyText": "is this a typo: 'consertively' -> 'conservatively'?", "url": "https://github.com/apache/druid/pull/10243#discussion_r473311895", "createdAt": "2020-08-19T20:52:58Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +45,55 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is consertively chosen as 1000.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzY5MTE5Mg==", "bodyText": "Oops, thanks.", "url": "https://github.com/apache/druid/pull/10243#discussion_r473691192", "createdAt": "2020-08-20T07:26:32Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +45,55 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is consertively chosen as 1000.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzMxMTg5NQ=="}, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODUxODkxOnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMDo1NToyOFrOHDYv2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwNzoyNjozNVrOHDv0WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzMxMzI0Mw==", "bodyText": "typo: 'Noe' -> 'Note'", "url": "https://github.com/apache/druid/pull/10243#discussion_r473313243", "createdAt": "2020-08-19T20:55:28Z", "author": {"login": "clintropolis"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). Noe that one subtask will not process more files than `maxNumFiles` even if their total size is smaller than `maxSplitSize`. [Human-readable format](../configuration/human-readable-byte.md) is supported.|1GiB|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzY5MTIyNQ==", "bodyText": "Thanks, fixed.", "url": "https://github.com/apache/druid/pull/10243#discussion_r473691225", "createdAt": "2020-08-20T07:26:35Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). Noe that one subtask will not process more files than `maxNumFiles` even if their total size is smaller than `maxSplitSize`. [Human-readable format](../configuration/human-readable-byte.md) is supported.|1GiB|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzMxMzI0Mw=="}, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODUzMzU1OnYy", "diffSide": "RIGHT", "path": "docs/ingestion/native-batch.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMTowMDowMlrOHDY40g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwNzoyNjozOFrOHDv0fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzMxNTUzOA==", "bodyText": "Does this limit apply to the entire parallel task, just the subtasks, or both? It isn't super clear from the docs here, though from my interpretation of the code it looks like this applies to subtasks?", "url": "https://github.com/apache/druid/pull/10243#discussion_r473315538", "createdAt": "2020-08-19T21:00:02Z", "author": {"login": "clintropolis"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). Noe that one subtask will not process more files than `maxNumFiles` even if their total size is smaller than `maxSplitSize`. [Human-readable format](../configuration/human-readable-byte.md) is supported.|1GiB|no|\n+|maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them. Note that one subtask will not process more data than `maxSplitSize` even if the total number of files is smaller than `maxNumFiles`.|1000|no|", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzY5MTI2MQ==", "bodyText": "Hmm, it is applied to only subtasks. I changed single task to single subtask. Is it better?", "url": "https://github.com/apache/druid/pull/10243#discussion_r473691261", "createdAt": "2020-08-20T07:26:38Z", "author": {"login": "jihoonson"}, "path": "docs/ingestion/native-batch.md", "diffHunk": "@@ -232,7 +232,8 @@ The size-based split hint spec is respected by all splittable input sources exce\n |property|description|default|required?|\n |--------|-----------|-------|---------|\n |type|This should always be `maxSize`.|none|yes|\n-|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet).|500MB|no|\n+|maxSplitSize|Maximum number of bytes of input files to process in a single task. If a single file is larger than this number, it will be processed by itself in a single task (Files are never split across tasks yet). Noe that one subtask will not process more files than `maxNumFiles` even if their total size is smaller than `maxSplitSize`. [Human-readable format](../configuration/human-readable-byte.md) is supported.|1GiB|no|\n+|maxNumFiles|Maximum number of input files to process in a single task. This limit is to avoid task failures when the ingestion spec is too long. There are two known limits on the max size of serialized ingestion spec, i.e., the max ZNode size in ZooKeeper (`jute.maxbuffer`) and the max packet size in MySQL (`max_allowed_packet`). These can make ingestion tasks fail if the serialized ingestion spec size hits one of them. Note that one subtask will not process more data than `maxSplitSize` even if the total number of files is smaller than `maxNumFiles`.|1000|no|", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzMxNTUzOA=="}, "originalCommit": {"oid": "c0ab886fe77729b4b5e3626e03779edea40f79b8"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2MTQzOTQ5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwOToxMTozNlrOHD2R3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQxNzo1ODo0MlrOHENIqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc5NzA4NQ==", "bodyText": "Should SegmentsSplitHintSpec be updated to accept HumanReadableBytes for maxInputSegmentBytesPerTask as well?", "url": "https://github.com/apache/druid/pull/10243#discussion_r473797085", "createdAt": "2020-08-20T09:11:36Z", "author": {"login": "clintropolis"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +45,55 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is conservatively chosen as 1000.\n+   */\n+  @VisibleForTesting\n+  static final int DEFAULT_MAX_NUM_FILES = 1000;\n+\n+  private final HumanReadableBytes maxSplitSize;\n+  private final int maxNumFiles;\n \n   @JsonCreator\n-  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  public MaxSizeSplitHintSpec(\n+      @JsonProperty(\"maxSplitSize\") @Nullable HumanReadableBytes maxSplitSize,\n+      @JsonProperty(\"maxNumFiles\") @Nullable Integer maxNumFiles\n+  )\n   {\n     this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+    this.maxNumFiles = maxNumFiles == null ? DEFAULT_MAX_NUM_FILES : maxNumFiles;\n+    Preconditions.checkArgument(this.maxSplitSize.getBytes() > 0, \"maxSplitSize should be larger than 0\");\n+    Preconditions.checkArgument(this.maxNumFiles > 0, \"maxNumFiles should be larger than 0\");\n+  }\n+\n+  @VisibleForTesting\n+  public MaxSizeSplitHintSpec(long maxSplitSize, @Nullable Integer maxNumFiles)\n+  {\n+    this(new HumanReadableBytes(maxSplitSize), maxNumFiles);\n   }\n \n   @JsonProperty\n-  public long getMaxSplitSize()\n+  public HumanReadableBytes getMaxSplitSize()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c363df0e175fdce431e9d1f7581453aff851a8c9"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE3MTU2MA==", "bodyText": "Ah good idea. It actually needs the same change because it needs to be identical to maxSize splitHintSpec at least for now. We could implement a better split based on stats in the future.", "url": "https://github.com/apache/druid/pull/10243#discussion_r474171560", "createdAt": "2020-08-20T17:58:42Z", "author": {"login": "jihoonson"}, "path": "core/src/main/java/org/apache/druid/data/input/MaxSizeSplitHintSpec.java", "diffHunk": "@@ -43,22 +45,55 @@\n   public static final String TYPE = \"maxSize\";\n \n   @VisibleForTesting\n-  static final long DEFAULT_MAX_SPLIT_SIZE = 512 * 1024 * 1024;\n+  static final HumanReadableBytes DEFAULT_MAX_SPLIT_SIZE = new HumanReadableBytes(\"1GiB\");\n \n-  private final long maxSplitSize;\n+  /**\n+   * There are two known issues when a split contains a large list of files.\n+   *\n+   * - 'jute.maxbuffer' in ZooKeeper. This system property controls the max size of ZNode. As its default is 500KB,\n+   *   task allocation can fail if the serialized ingestion spec is larger than this limit.\n+   * - 'max_allowed_packet' in MySQL. This is the max size of a communication packet sent to a MySQL server.\n+   *   The default is either 64MB or 4MB depending on MySQL version. Updating metadata store can fail if the serialized\n+   *   ingestion spec is larger than this limit.\n+   *\n+   * The default is conservatively chosen as 1000.\n+   */\n+  @VisibleForTesting\n+  static final int DEFAULT_MAX_NUM_FILES = 1000;\n+\n+  private final HumanReadableBytes maxSplitSize;\n+  private final int maxNumFiles;\n \n   @JsonCreator\n-  public MaxSizeSplitHintSpec(@JsonProperty(\"maxSplitSize\") @Nullable Long maxSplitSize)\n+  public MaxSizeSplitHintSpec(\n+      @JsonProperty(\"maxSplitSize\") @Nullable HumanReadableBytes maxSplitSize,\n+      @JsonProperty(\"maxNumFiles\") @Nullable Integer maxNumFiles\n+  )\n   {\n     this.maxSplitSize = maxSplitSize == null ? DEFAULT_MAX_SPLIT_SIZE : maxSplitSize;\n+    this.maxNumFiles = maxNumFiles == null ? DEFAULT_MAX_NUM_FILES : maxNumFiles;\n+    Preconditions.checkArgument(this.maxSplitSize.getBytes() > 0, \"maxSplitSize should be larger than 0\");\n+    Preconditions.checkArgument(this.maxNumFiles > 0, \"maxNumFiles should be larger than 0\");\n+  }\n+\n+  @VisibleForTesting\n+  public MaxSizeSplitHintSpec(long maxSplitSize, @Nullable Integer maxNumFiles)\n+  {\n+    this(new HumanReadableBytes(maxSplitSize), maxNumFiles);\n   }\n \n   @JsonProperty\n-  public long getMaxSplitSize()\n+  public HumanReadableBytes getMaxSplitSize()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc5NzA4NQ=="}, "originalCommit": {"oid": "c363df0e175fdce431e9d1f7581453aff851a8c9"}, "originalPosition": 56}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2257, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}